# Polynomials

```{r child="../starter.Rmd"}
```


A big part of the high-school algebra curriculum is about polynomials. In some ways, this is appropriate since polynomials played an outsized part in the historical development of mathematical theory. Indeed, the so-called "Fundamental theorem of algebra" is about polynomials.[^diff-taylor-1]

[^diff-taylor-1]: The fundamental theorem says that an order-n polynomial has n roots (including multiplicities).

For modelers, polynomials are a mixed bag. They are very widely used in modeling. Sometimes this is entirely appropriate, for instance the low-order polynomials that are the subject of @sec-local-approximations. The problems come when high-order polynomials are selected for modeling purposes. Building a reliable model with high-order polynomials requires a deep knowledge of mathematics, and introduces serious potential pitfalls. Modern professional modelers learn the alternatives to high-order polynomials, but newcomers often draw on their experience in high-school and give unwarranted credence to polynomials. This chapter attempts to guide you to the ways you are likely to see polynomials in your future work and to help you avoid them when better alternatives are available.

## Basics of polynomials with one input {#sec-polynomial-basics}

A polynomial is a ***linear combination*** of a particular class of functions: power-law functions with non-negative, integer exponents: 1, 2, 3, .... The individual functions are called ***monomials***, a word that echoes the construction of chemical polymers out of monomers; for instance, the material *polyester* is constructed by chaining together a basic chemical unit called an [*ester*](https://en.wikipedia.org/wiki/Ester)*.*

In one variable, say $x$, the monomials are
$x^1, x^2, x^3$, and so on. (There's also $x^0$, but that's better thought of as the constant function.) An ***n-th order*** polynomial has monomials up to exponent $n$. For example, the form of a third-order polynomial is
$$a_0 + a_1 x^1 + a_2 x^2 + a_3 x^3$$

The ***domain*** of polynomials, like the power-law functions they are assembled from, is the ***real numbers***, that is, the entire number line $-\infty < x < \infty$. But for the purposes of understanding the shape of high-order polynomials, it's helpful to divide the domain into three parts: a ***wriggly domain*** at the center and two ***tail domains*** to the right and left of the center.

```{r echo=FALSE}
#| label: fig-wriggly-polynomial
#| fig-cap: "A $n$th-order polynomial can have up to $n-1$ critical points that it wriggles among. A 7-th order polynomial is shown here in which there are six local maxima or minima alternatingly."
set.seed(101)
n <- 8
Pts <- tibble(x = runif(n, -3, 3), y = rnorm(n))
mod <- lm(y ~ poly(x, n-1), data = Pts)
fmod <- makeFun(mod)
slice_plot(fmod(x) ~ x, domain(x=c(-2.8, 1.65))) %>%
  gf_vline(xintercept = ~ -2.65, color="tan") %>%
  gf_vline(xintercept = ~ 1.6, color="tan") %>%
  gf_text(25 ~ -.5, label="Wriggly domain", color="tan", size=10) %>%
  gf_lims(x=c(-3.5,2.5)) %>%
  gf_text(-20 ~ -3.1, label="Negative tail domain", color="tan", angle=90, size=8) %>%
  gf_text(-20 ~ 2.2, label="Positive tail domain", color="tan", angle=-90, size=8) %>%
  gf_labs(x = "input")
```

@fig-wriggly-polynomial shows a 7th order polynomial---that is, the highest-order term is $x^7$. In one of the tail domains the function value heads off to $\infty$, in the other to $-\infty$. This is a necessary feature of all odd-order polynomials: 1, 3, 5, 7, ...

In contrast, for even-order polynomials (2, 4, 6, ...) the function value in the two tail domains go in the same direction, either both to $\infty$ (Hands up!) or both to $-\infty$.  

In the wriggly domain in @fig-wriggly-polynomial, there are six argmins or argmaxes. 

**An $n$th-order polynomial can have up to $n-1$ extrema.**

Note that the local polynomial approximations in @sec-local-approximations) are at most 2nd order and so there is at most 1 wriggle: a unique argmax. If the approximation does not include the quadratic terms ($x^2$ or $y^2$) then there is no argmax for the function. 

## Multiple inputs?

High-order polynomials are rarely used with multiple inputs. One reason is the proliferation of coefficients. For instance, here is the third-order polynomial in two inputs, $x$, and $y$. 
$$\underbrace{b_0 + b_x x + b_y y}_\text{first-order terms} + \underbrace{b_{xy} x y + b_{xx} x^2 + b_{yy} y^2}_\text{second-order terms} + \underbrace{b_{xxy} x^2 y + b_{xyy} x y^2 + b_{xxx} x^3 + b_{yyy} y^3}_\text{third-order terms}$$ 

This has 10 coefficients. With so many coefficients it's hard to ascribe meaning to any of them individually. And, insofar as some feature of the function does carry meaning in terms of the modeling situation, that meaning is spread out and hard to quantify.

## High-order approximations {#sec-high-order-approx}


The potential attraction of high-order polynomials is that, with their wriggly interior, they can take on a large number of appearances. This chameleon-like behavior has historically made them the tool of choice for understanding the behavior of approximations. That theory has motivated the use of polynomials for modeling patterns in data, but, paradoxically, has shown that high-order polynomials should **not** be the tool of choice for modeling data.^[The mathematical background needed for those better tools won't be available to us until Block 5, when we explore linear algebra.]

Polynomial functions lend themselves well to calculations, since the output from a polynomial function can be calculated using just the basic arithmetic functions: addition, subtraction, multiplication, and division. To illustrate, consider this polynomial: $$g(x) \equiv x - \frac{1}{6} x^3$$
Since the highest-order term is $x^3$ this is a third-order polynomial. (As you'll see, we picked these particular coefficients, 0, 1, 0, -1/6, for a reason.) With such simple coefficients the polynomial is easy to handle by mental arithmetic. For instance, for $g(x=1)$ is $5/6$. Similarly, $g(x=1/2) = 23/48$ and $g(x=2) = 2/3$. A person of today's generation would use an electronic calculator for more complicated inputs, but the mathematicians of Newton's time were accomplished human calculators.  It would have been well within their capabilities to calculate, using paper and pencil, $g(\pi/4) = 0.7046527$.^[Unfortunately for these human calculators, pencils weren't invented until 1795. Prior to the introduction of this advanced, graphite-based computing technology, mathematicians had to use quill and ink.] 

Our example polynomial, $g(x) \equiv x - \frac{1}{6}x^3$, graphed in color in @fig-small-sine, doesn't look exactly like the sinusoid. If we increased the extent of the graphics domain, the disagreement would be even more striking, since the sinusoid's output is always in $-1 \leq \sin(x) \leq 1$, while the polynomial's tails are heading off to $\infty$ and $-\infty$.  But, for a small interval around $x=0$, exactly aligns with the sinusoid.


```{r echo=FALSE}
#| label: fig-small-sine
#| fig-cap: "The polynomial $g(x) \\equiv x -x^3 / 6$ is remarkably similar to $\\sin(x)$ near $x=0$."
slice_plot(sin(x) ~ x, domain(x=c(-2.85, 2.85)), size=3, alpha=0.25) %>%
  slice_plot(x - x^3/6 ~ x, color="magenta")
```

It's clear from the graph that the approximation is excellent near $x=0$ and gets worse as $x$ gets larger. The approximation is poor for $x \approx \pm 2$. We know enough about polynomials to say that the approximation will not get better for larger $x$; the sine function has a range of $-1$ to $1$, while the left and right tails of the polynomial are heading off to $\infty$ and $-\infty$ respectively.


One way to measure the quality of the approximation is the ***error*** ${\cal E}(x)$ which gives, as a function of $x$, the difference between the actual sinusoid and the approximation: $${\cal E}(x) \equiv |\strut\sin(x) - g(x)|$$ The absolute value used in defining the error reflects our interest in how ***far*** the approximation is from the actual function and not so much in whether the approximation is below or above the actual function. @fig-sin-error shows ${\cal E}(x)$ as a function of $x$. Since the error is the same on both sides of $x=0$, only the positive $x$ domain is shown.


```{r echo=FALSE}
#| label: fig-sin-error
#| fig-cap: "The error ${\\cal E}(x)$ of $x - x^3/6$ as an approximation to $\\sin(x)$. Top panel: linear scale. Bottom panel: on a log-log scale."
P <- slice_plot(abs(sin(x) - (x - x^3/6)) ~ x, 
                domain(x=c(0.001, 3)), npts=500) %>% 
  gf_labs(y = "Error(x)")

breaksx <- c(.001,  .005, 0.01,0.05, 0.1,0.3,  0.5, 1, 3)
vticks <- 10^(seq(-16, 0, by=2))
vlabs <- c(paste0("10^",seq(-16,-4,by=2)),  0.01, 1)

P2 <- P %>%
  gf_refine(scale_y_log10(breaks=vticks, labels=vlabs), scale_x_log10(breaks=breaksx, labels=as.character(breaksx)))

gridExtra::grid.arrange(P, P2, nrow=2)

```



@fig-sin-error shows that for $x < 0.3$, the error in the polynomial approximation to $\sin(x)$ is in the 5th decimal place. For instance, $\sin(0.3) = 0.2955202$ while $g(0.3) = 0.2955000$. 



That the graph of ${\cal E}(x)$ is a straight-line on log-log scales diagnoses ${\cal E}(x)$ as a power law. That is: ${\cal E}(x) = A x^p$. As always for power-law functions, we can estimate the exponent $p$ from the slope of the graph. It's easy to see that the slope is positive, so $p$ must also be positive.

The inevitable consequence of ${\cal E}(x)$ being a power-law function with positive $p$ is that $\lim_{x\rightarrow 0} {\cal E}(x) = 0$. That is, the polynomial approximation $x - \frac{1}{6}x^3$ is *exact* as $x \rightarrow 0$.

Throughout this book, we've been using straight-line approximations to functions around an input $x_0$. 
$$g(x) = f(x_0) + \partial_x f(x_0) [x-x_0]$$
One way to look at $g(x)$ is as a straight-line function. Another way is as a first-order polynomial. This raises the question of what a second-order polynomial approximation should be. Rather than the polynomial matching just the slope of $f(x)$ at $x_0$, we can arrange things so that the second-order polynomial will also match the curvature of the $f()$. Since the curvature involves only the first and second derivatives of a function, the polynomial constructed to match both the first and the second derivative  will necessarily match the slope and curvature of $f()$. This can be accomplished by setting the polynomial coefficients appropriately.

Start with a general, second-order polynomial centered around $x_0$:
$$g(x) \equiv a_0 + a_1 [x-x_0] + a_2 [x - x_0]^2$$
The first- and second-derivatives, evaluated at $x=x_0$ are:
$$\partial_x g(x)\left.{\Large\strut}\right|_{x=x_0} = a_1 + 2 a_2 [x  - x_0] \left.{\Large\strut}\right|_{x=x_0} = a_1$$
$$\partial_{xx} g(x)\left.{\Large\strut}\right|_{x=x_0} =  2 a_2$$
Notice the 2 in the above expression. When we want to write the coefficient $a_2$ in terms of the second derivative of $g()$, we'll end up with 

$$a_2 = \frac{1}{2} \partial_{xx} g(x)\left.{\Large\strut}\right|_{x=x_0}$$

To make $g(x)$ approximate $f(x)$ at $x=x_0$, we need merely set
$$a_1 = \partial_x f(x)\left.{\Large\strut}\right|_{x=x_0}$$ and
$$a_2 = \frac{1}{2} \partial_{xx} f(x) \left.{\Large\strut}\right|_{x=x_0}$$
This logic can also be applied to higher-order polynomials. For instance, to match the third derivative of $f(x)$ at $x_0$, set
$$a_3 = \frac{1}{6} \partial_{xxx} f(x)  \left.{\Large\strut}\right|_{x=x_0}$$
Remarkably, each coefficient in the approximating polynomial involves only the corresponding order of derivative. $a_1$ involves only $\partial_x f(x)   \left.{\Large\strut}\right|_{x=x_0}$; the $a_2$ coefficient involves only $\partial_{xx} f(x)     \left.{\Large\strut}\right|_{x=x_0}$; the $a_3$ coefficient involves only $\partial_{xx} f(x)     \left.{\Large\strut}\right|_{x=x_0}$, and so on.

Now we can explain where the polynomial that started this section, $x - \frac{1}{6} x^3$ came from and why those coefficients make the polynmomial approximate the sinusoid near $x=0$.

Order | $\sin(x)$ derivative | $x - \frac{1}{6}x^3$ derivative
------|----------------------|--------------------------------   
0   | $\sin(x) \left.{\Large\strut}\right|_{x=0} = 0$ | $\left( 1 - \frac{1}{6}x^3\right)\left.{\Large\strut}\right|_{x=0} = 0$ 
  1   | $\cos(x) \left.{\Large\strut}\right|_{x=0} = 1$ | $\left(1 - \frac{3}{6} x^2\right) \left.{\Large\strut}\right|_{x=0}= 1$
  2   | $-\sin(x) \left.{\Large\strut}\right|_{x=0} = 0$ | $\left(- \frac{6}{6} x\right) \left.{\Large\strut}\right|_{x=0} = 0$
  3   |  $-\cos(x) \left.{\Large\strut}\right|_{x=0} = -1$ | $- 1\left.{\Large\strut}\right|_{x=0} = -1$ 
  4   |  $\sin(x) \left.{\Large\strut}\right|_{x=0} = 0$ | $0\left.{\Large\strut}\right|_{x=0} = 0$ 

The first four derivatives of $x - \frac{1}{6} x^3$ exactly match, at $x=0$, the first four derivatives of $\sin(x)$.

The polynomial constructed by matching successive derivatives of a function $f(x)$ at some input $x_0$ is called a ***Taylor polynomial***. 

::: {.workedexample   data-latex=""}
Let's construct a 3rd-order Taylor polynomial approximation to $f(x) = e^x$ around $x=0$.

We know it will be a 3rd order polynomial:
$$g_{\exp}(x) \equiv a_0 + a_1 x + a_2 x^2 + a_3 x^3$$
The exponential function is particularly nice for examples because the function value and all it's derivatives are identical: $e^x$. So 
$$f(x= 0) = 1$$  
$$ \partial_x f(x=0) = 1$$
$$\partial_{xx} f(x=0) = 1$$
$$\partial_{xxx} f(x=0) = 1$$ and so on.


The function value and derivatives of $g_{\exp}(x)$ at $x=0$ are:
$$g_{\exp}(x=0) = a_0$$
$$\partial_{x}g_{\exp}(x=0) = a_1$$
$$\partial_{xx}g_{\exp}(x=0) = 2 a_2$$

$$\partial_{xxx}g_{\exp}(x=0) = 2\cdot3\cdot a_3 = 6\, a_3$$
Matching these to the exponential evaluated at $x=0$, we get
$$a_0 = 1$$
$$a_1 = 1$$
$$a_2 = \frac{1}{2}$$
$$a_3 = \frac{1}{2 \cdot 3} = \frac{1}{6}$$

Result: the 3rd-order Taylor polynomial approximation to the exponential at $x=0$ is $$g_{\exp}(x) = 1 + x + \frac{1}{2} x^2 +  \frac{1}{2\cdot 3} x^3 +\frac{1}{2\cdot 3\cdot 4} x^4$$

@fig-taylor-exp-3 shows the exponential function $e^x$ and its 3th-order Taylor polynomial approximation near $x=0$:

```{r echo=FALSE, class.source="code-hide"}
#| label: fig-taylor-exp-3
#| fig-cap: "The 3th-order Taylor polynomial approximation to $e^x$ arount $x=0$"
slice_plot(exp(x) ~ x, domain(x=c(-2,2)), size=3, alpha = 0.25) %>%
  slice_plot(1 + x + x^2/2 + x^3/6  ~ x, color="magenta")
```

The polynomial is exact at $x=0$. The ***error*** ${\cal E}(x)$ grows with increasing distance from $x=0$:

```{r echo=FALSE, class.source="code-hide", fig.show="hold"}
#| label: fig-taylor-exp-5
#| fig-cap: "The error from a 3rd-order Taylor polynomial approximation to $e^x$ around $x=0$ is a power-law function with exponent $4$." 
#| out-width: "50%"
Pts <- tibble(x = seq(-2, 2, length=100),
              y = exp(x),
              y_approx = 1 + x + x^2/2 + x^3/6,
              y_error = abs(y - y_approx),
              x_abs = abs(x))
gf_path(y_error ~ x, data = Pts, color=~sign(x)) %>%
  gf_labs(y="Error(x)") %>%
  gf_refine(scale_color_continuous(guide = 'none'))
gf_path(log10(y_error) ~ log10(x_abs), data = Pts, color=~sign(x)) %>%
  gf_labs(y="Log10(Error(x))") %>%
  gf_refine(scale_color_continuous(guide = 'none'))
```

The plot of $\log_{10} {\cal E}(x)$ versus $\log_{10} | x |$ in @fig-taylor-exp-5 shows that the error grows from zero at $x=0$ as a power-law function. Measuring the exponent of the power-law from the slope of the graph on log-log axes give ${\cal E}(x) = a |x-x_0|^5$. This is typical of Taylor polynomials: for a polynomial of degree $n$, the error will grow as a power-law with exponent $n+1$. This means that the higher is $n$, the faster $\lim_{x\rightarrow x_0}{\cal E}(x) \rightarrow 0$. On the other hand, since ${\cal E}_x$ is a power law function, as $x$ gets further from $x_0$ the error grows as $\left(x-x_0\right)^{n+1}$.
:::

::: {.intheworld data-latex=""} 
[Brooke Taylor](https://en.wikipedia.org/wiki/Brook_Taylor) (1685-1731), a near contemporary of Newton, published his work on approximating polynomials in 1715. Wikipedia reports: "[T]he importance of [this] remained unrecognized until 1772, when Joseph-Louis Lagrange realized its usefulness and termed it 'the main [theoretical] foundation of differential calculus'."[Source](https://en.wikipedia.org/wiki/Brook_Taylor#/media/File:Taylor_Brook_Goupy_NPG.jpg)

```{r echo=FALSE, out.width = "40%", fig.align="center"} 
#| label: fig-brook-taylor
#| fig-cap: "Brook Taylor" 
knitr::include_graphics(normalizePath("www/Brook_Taylor.jpg"))
```

Due to the importance of Taylor polynomials in the development of calculus, and their prominence in many calculus textbooks, many students assume their use extends to constructing models from data. They also assume that third- and higher-order monomials are a good basis for modeling data. Both these assumptions are wrong. Least squares is the proper foundation for working with data.

Taylor's work preceded by about a century the development of techniques for working with data. One of the pioneers in these new techniques was Carl Friedrich Gauss (1777-1855), after whom the gaussian function is named. Gauss's techniques are the foundation of an incredibly important statistical method that is ubiquitous today: ***least squares***. Least squares provides an entirely different way to find the coefficients on approximating polynomials (and an infinite variety of other function forms). The R/mosaic `fitModel()` function for polishing parameter estimates is based on least squares. In Block 5, we'll explore least squares and the mathematics underlying the calculations of least-squares estimates of parameters.
:::

## Indeterminate forms

Let's return to an issue that has bedeviled calculus students since Newton's time. The example we'll use is the function $$\text{sinc}(x)  \equiv \frac{\sin(x)}{x}$$

The sinc() function (pronounced "sink") is still important today, in part because of its role in converting discrete-time measurements (as in an mp3 recording of sound) into continuous signals.

What is the value of $\text{sinc}(0)$? One answer, favored by arithmetic teachers is that $\text{sinc}(0)$ is meaningless, because it involves division by zero.

On the other hand, $\sin(0) = 0$ as well, so the sinc function evaluated at zero involves 0/0. This quotient is called an ***indeterminant form***. The logic is this: Suppose we assume that $0/0 = b$ for some number $b$. then $0 = 0 \times b = 0$. So any value of $b$ would do; the value of $0/0$ is "indeterminant."

Still another answer is suggested by plotting out sinc($x$) near $x=0$ and reading the value off the graph: sinc(0) = 1.

```{r}
#| label: fig-sinc-near-zero
#| fig-cap: "To judge from this plot, $\\sin(0) = 1$."
slice_plot(sin(x) / x ~ x, domain(x=c(-10,10)), npts=500)
```
The graph of sinc() looks smooth and the shape makes sense. Even if we zoom in very close to $x=0$, the graph continues to look smooth. We call such functions ***well behaved***.

Compare the well-behaved sinc() to a very closely related function (which doesn't seem to be so important in applied work): $\frac{\sin(x)}{x^3}$.

Both $\sin(x)/x$ and $\sin(x) / x^3$, evaluated at $x=0$ involve a divide by zero. Both are indeterminate forms 0/0 at $x=0$. But the graph of $\sin(x) / x^3$ (see @fig-sinc2) is not we'll behaved.  $\sin(x) / x^3$ does not have any particular value at $x=0$; instead, it has an asymptote. 

```{r}
#| label: fig-sinc2
#| fig-cap: "Zooming in around the division by zero. Left: The graph of $\\sin(x)/x$ versus $x$. Right: The graph of $\\sin(x)/x^2$. The vertical scales on the two graphs are utterly different."
slice_plot(sin(x) / x ~ x, domain(x=c(-0.1, 0.1)), npts=500) %>%
  gf_refine(scale_y_log10())
slice_plot(sin(x) / x^3 ~ x, domain(x=c(-0.1, 0.1)), npts=500) %>%
  gf_refine(scale_y_log10())
```

Since both $\sin(x)/x\left.{\Large\strut}\right|_{x=0}$ and $\sin(x)/x^3\left. {\Large\strut}\right|_{x=0}$ involve a divide-by-zero, the answer to the utterly different behavior of the two functions is not to be found at zero. Instead, it's to be found ***near*** zero. For any non-zero value of $x$, the arithmetic to evaluate the functions is straight-forward. Note that $\sin(x) / x^3$ starts its mis-behavior away from zero. The slope of $\sin(x) / x^3$ is very large near $x=0$, while the slope of $\sin(x) / x$ smoothly approaches zero.

Since we're interested in behavior **near** $x=0$, a useful technique is to approximate the numerator and denominator of both functions by polynomial approximations.

- $\sin(x) \approx x - \frac{1}{6} x^3$ near $x=0$
- $x$ is already a polynomial.
- $x^3$ is already a polynomial.

Remember, these approximations are **exact** as $x$ goes to zero. So sufficiently close to zero, 

$$\frac{\sin(x)}{x} = \frac{x - \frac{1}{6} x^3}{x} = 1 + \frac{1}{6} x^2$$
Even at $x=0$, there's nothing indeterminant about $1 + x^2/6$; it's simply 1.

Compare this to the polynomial approximation to $\sin(x) / x^3$:
$$\frac{\sin(x)}{x^3} = \frac{x - \frac{1}{6} x^3}{x^3} = \frac{1}{x^2} - \frac{1}{6}$$

Evaluating this at $x=0$ involves division by zero. No wonder it's badly behaved.

The procedure for checking whether a function involving division by zero behaves well or poorly is described in the first-ever calculus textbook, published in 1697. The title (in English) is: *The analysis into the infinitely small for the understanding of curved lines*. In honor of the author, the Marquis de l'Hospital, the procedure is called ***l'Hopital's rule***.^[In many French words, the sequence "os" has been replaced by a single, accented letter, $\hat{\text{o}}$.]

Conventionally, the relationship is written
$$\lim_{x\rightarrow x_0} \frac{u(x)}{v(x)} = \lim_{x\rightarrow x_0} \frac{\partial_x u(x)}{\partial_x v(x)}$$

Let's try this out with our two example functions around $x=0$:

$$\lim_{x\rightarrow 0} \frac{\sin(x)}{x} = \frac{\lim_{x\rightarrow 0} \cos(x)}{\lim_{x \rightarrow 0} 1} = \frac{1}{1} = 1$$

$$\lim_{x\rightarrow 0} \frac{\sin(x)}{x^3} = \frac{\lim_{x\rightarrow 0} \cos(x)}{\lim_{x \rightarrow 0} 3x^2} = \frac{1}{0} \ \ \text{indeterminate}!$$

## Computing with indeterminate forms

In the early days of electronic computers, division by zero would cause a fault in the computer, often signaled by stopping the calculation and printing an error message to some display. This was inconvenient, since programmers did not always forsee division-by-zero situations and avoid them. 

As you've seen, modern computers have adopted a convention that simplifies programming considerably. Instead of stopping the calculation, the computer just carries on normally, but produces as a result one of two indeterminant forms: `Inf` and `NaN`.

`Inf` is the output for  the simple case of dividing zero into a non-zero number, for instance:

```{r}
17/0
```
`NaN`, standing for "not a number," is the output for more challenging cases: dividing zero into zero, or multiplying `Inf` by zero, or dividing `Inf` by `Inf`.

```{r}
0/0
0 * Inf
Inf / Inf
```

The brilliance of the idea is that any calculation  that involves `NaN` will return a value of `NaN`. This might seem to get us nowhere. But most programs are built out of other programs, usually written by other people interested in other applications. You can use those programs (mostly) without worrying about the implications of a divide by zero. If it's important to respond in some particularly way, you can always check the result for being `NaN` in your own programs. (Much the same is true for `Inf`, although dividing a non-`Inf` number by `Inf` will return 0.)

Plotting software will often treat `NaN` values as "don't plot this." That's why it's possible to make a sensible plot of $\sin(x)/x$ even when the plotting domain includes zero.


## Exercises

`r if (knitr::is_latex_output()) knitr::knit_exit()`

`r insert_calcZ_exercise("26.02", "3EdMBL", "Exercises/birch-lie-sheet.Rmd")`

`r insert_calcZ_exercise("26.04", "o6dP4k", "Exercises/pig-iron-closet.Rmd")`

`r insert_calcZ_exercise("26.06", "Co1Ekt", "Exercises/dog-feel-boat.Rmd")`

`r insert_calcZ_exercise("26.08", "llX7EF", "Exercises/calf-tear-fridge.Rmd")`

`r insert_calcZ_exercise("26.10", "ecdVKx", "Exercises/frog-throw-screen.Rmd")`

`r insert_calcZ_exercise("26.11", "pseisj", "Exercises/fish-hurt-ring.Rmd")`

`r insert_calcZ_exercise("26.12", "682lsB", "Exercises/local-shift.Rmd")`

`r insert_calcZ_exercise("26.14", "cQT2v6", "Exercises/dolphin-mean-linen.Rmd")`

`r insert_calcZ_exercise("26.16", "jAOmME", "Exercises/birch-dig-radio.Rmd")`



<!--
## Fitting polynomials

Taylor polynomials provide a means to approximate continuous and smooth functions around a center $x_0$. So long as $x$ is very close to $x_0$, the approximation will be excellent. But Taylor polynomials aren't a solution to every problem. Consider the piecewise continuous function, $ramp(x-1)$, in @fig-ramp-Taylor.

```{r echo=FALSE}
#| label: ramp-Taylor
#| fig-cap: "A piecewise continuous ramp function (gray) together with its Taylor polynomial (magenta) centered on $x_0 = 0$."
ramp <- makeFun(ifelse(x-1 < 0, 0, x-1) ~ x)
slice_plot(ramp(x) ~ x, size = 3, alpha = 0.25, domain(x=c(-2, 4))) %>%
  slice_plot(0 ~ x, color="magenta") %>%
  gf_text(0.25 ~ 3, label="Taylor poly.",color = "magenta")

```
The value of $ramp(x-1) \left.\Large\right|_{x=0}$ is zero, as is the value of the first, second, third, and every other derivative. Whatever we choose for the order $n$ of the Taylor polynomial, it will be $\text{Taylor(x) = 0}$. That's an excellent approximation to $ramp(x-1)$ around $x=0$! But it misses the point of $ramp()$ entirely.

Or consider the problem that introduced this chapter: finding an arithmetic process to evaluate $\sin(x)$. As it happens, we only need to be able to evaluate $\sin(x)$ on the interval $0 \leq x \leq \pi/2$.^[If $x$ is outside this range, add or subtract a multiple $k \in [\ldots, -2, -1, 0, 1, 2, \ldots]$ of $\pi$ so  $0 \leq x - k \pi \leq \pi$. Then, if $\pi/2 \leq (x - k \pi)$, calculate $\sin(\pi - (x - k \pi))$ 

FIT TO PART OF SINE

```{r}
Pts <- tibble(x = seq(0, pi/2, length=1000), y = sin(x))
mod <- lm(y ~ x + I(x*x) + I(x*x*x) - 1, data = Pts)
mod
fmod <- makeFun(mod)
slice_plot(sin(x) ~ x, domain(x=c(0, pi/2)), size=3, alpha=0.25) %>%
  slice_plot(fmod(x) ~ x, color="magenta")
slice_plot(sin(x) - fmod(x) ~ x, domain(x=c(0, pi/2))) %>%
  slice_plot(sin(x) - (x - x^3/6) ~ x, color="green")
```

-->

## Drill

`r Znotes:::MC_counter$reset(labels="roman")`



```{r drill-Polynomials-4, echo=FALSE, results='markup'}
Znotes::askMC(
  prompt = r"(Here is a function $f(x)$: <br><img src="https://raw.githubusercontent.com/dtkaplan/Zdrill/main/inst/Polynomials/poly-01.png"><br> In the Taylor polynomial approximation to $f(x)$ centered at $x=-2$, what will be the sign of the coefficient on the first-order term. Choose the best answer.  )",
r"(positive)" = r"( )",
  r"(+zero+)" = r"( )",
  r"(negative)" = r"( )",
  r"(no such coefficient exists in a Taylor polynomial)" = r"( )",
  random_answer_order=FALSE
)
```



```{r drill-Polynomials-5, echo=FALSE, results='markup'}
Znotes::askMC(
  prompt = r"(Here is a function $f(x)$: <br><img src="https://raw.githubusercontent.com/dtkaplan/Zdrill/main/inst/Polynomials/poly-01.png"><br> In the Taylor polynomial approximation to $f(x)$ centered at $x=1$, what will be the sign of the coefficient on the first-order term. Choose the best answer.  )",
r"(+positive+)" = r"( )",
  r"(zero)" = r"( )",
  r"(negative)" = r"( )",
  r"(no such coefficient exists in a Taylor polynomial)" = r"( )",
  random_answer_order=FALSE
)
```



```{r drill-Polynomials-6, echo=FALSE, results='markup'}
Znotes::askMC(
  prompt = r"(Here is a function $f(x)$: <br><img src="https://raw.githubusercontent.com/dtkaplan/Zdrill/main/inst/Polynomials/poly-01.png"><br> In the Taylor polynomial approximation to $f(x)$ centered at $x=1$, what will be the sign of the coefficient on the second-order term. Choose the best answer.  )",
r"(positive)" = r"( )",
  r"(zero)" = r"( )",
  r"(+negative+)" = r"( )",
  r"(no such coefficient exists in a Taylor polynomial)" = r"( )",
  random_answer_order=FALSE
)
```



```{r drill-Polynomials-7, echo=FALSE, results='markup'}
Znotes::askMC(
  prompt = r"(Here is a function $f(x)$: <br><img src="https://raw.githubusercontent.com/dtkaplan/Zdrill/main/inst/Polynomials/poly-01.png"><br> In the Taylor polynomial approximation to $f(x)$ centered at $x=-4$, what will be the sign of the coefficient on the first-order term. Choose the best answer.  )",
r"(positive)" = r"( )",
  r"(zero)" = r"( )",
  r"(+negative+)" = r"( )",
  r"(no such coefficient exists in a Taylor polynomial)" = r"( )",
  random_answer_order=FALSE
)
```



```{r drill-Polynomials-8, echo=FALSE, results='markup'}
Znotes::askMC(
  prompt = r"(Here is a function $f(x)$: <br><img src="https://raw.githubusercontent.com/dtkaplan/Zdrill/main/inst/Polynomials/poly-01.png"><br> In the Taylor polynomial approximation to $f(x)$ centered at $x=4$, what will be the sign of the coefficient on the second-order term. Choose the best answer.  )",
r"(+positive+)" = r"( )",
  r"(zero)" = r"( )",
  r"(negative)" = r"( )",
  r"(no such coefficient exists in a Taylor polynomial)" = r"( )",
  random_answer_order=FALSE
)
```



```{r drill-Polynomials-9, echo=FALSE, results='markup'}
Znotes::askMC(
  prompt = r"(Here is a function $f(x)$: <br><img src="https://raw.githubusercontent.com/dtkaplan/Zdrill/main/inst/Polynomials/poly-01.png"><br> In the Taylor polynomial approximation to $f(x)$ centered at $x=4$, what will be the sign of the coefficient on the constant (zeroth-order) term. Choose the best answer.  )",
r"(positive)" = r"( )",
  r"(zero)" = r"( )",
  r"(+negative+)" = r"( )",
  r"(no such coefficient exists in a Taylor polynomial)" = r"( )",
  random_answer_order=FALSE
)
```



```{r drill-Polynomials-10, echo=FALSE, results='markup'}
Znotes::askMC(
  prompt = r"(Here is a function $f(x)$: <br><img src="https://raw.githubusercontent.com/dtkaplan/Zdrill/main/inst/Polynomials/poly-01.png"><br> In the Taylor polynomial approximation to $f(x)$ centered at $x=3$, what will be the sign of the coefficient on the second-order term. Choose the best answer.  )",
r"(positive)" = r"( )",
  r"(+zero+)" = r"( )",
  r"(negative)" = r"( )",
  r"(no such term exists in a Taylor polynomial)" = r"( )",
  random_answer_order=FALSE
)
```



```{r drill-Polynomials-11, echo=FALSE, results='markup'}
Znotes::askMC(
  prompt = r"(Here is a function $f(x)$: <br><img src="https://raw.githubusercontent.com/dtkaplan/Zdrill/main/inst/Polynomials/poly-01.png"><br> In the Taylor polynomial approximation to $f(x)$ centered at $x=0$, what will be the sign of the coefficient on the reciprocal term. Choose the best answer.  )",
r"(positive)" = r"( )",
  r"(zero)" = r"( )",
  r"(negative)" = r"( )",
  r"(+no such term exists in a Taylor polynomial+)" = r"( )",
  random_answer_order=FALSE
)
```



```{r drill-Polynomials-12, echo=FALSE, results='markup'}
Znotes::askMC(
  prompt = r"(Here is a function $g(x)$: <br><img src="https://raw.githubusercontent.com/dtkaplan/Zdrill/main/inst/Polynomials/poly-03.png"><br> Two Taylor polynomials, centered on the same $x$ are shown. One is fifth-order, the other is third-order. Which is which?  )",
r"(+The third-order polynomial is brown.+)" = r"( )",
  r"(The third-order polynomial is magenta.)" = r"(The higher the order of the polynomial, the further away from the center it remains a good approximation to the function. The brown polynomial drifts away from the function before the magenta polynomial.)",
  random_answer_order=FALSE
)
```



```{r drill-Polynomials-13, echo=FALSE, results='markup'}
Znotes::askMC(
  prompt = r"(Here is a function $g(x)$: <br><img src="https://raw.githubusercontent.com/dtkaplan/Zdrill/main/inst/Polynomials/poly-02.png"><br> with a Taylor polynomial shown in magenta. What is the order of the polynomial?  )",
r"(zero)" = r"( )",
  r"(one)" = r"( )",
  r"(two)" = r"( )",
  r"(three)" = r"( )",
  r"(+four+)" = r"(the tails of the polynomial point point in the same direction and there are three wiggles clearly visible.)",
  random_answer_order=FALSE
)
```

