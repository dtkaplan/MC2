[
  {
    "objectID": "Preliminaries/01-quant-fun-space.html",
    "href": "Preliminaries/01-quant-fun-space.html",
    "title": "1  Quantity, function, space",
    "section": "",
    "text": "1.1 Quantity vs number\nA mathematical quantity is an amount. How we measure amounts depends on the kind of stuff we are measuring. The real-world stuff might be time, speed, force, price inflation, physiological and ecological systems … anything to which arithmetic can be applied!\nEveryone learns that arithmetic provides a set of patterns and rules for manipulating numbers. A number is a special, abstract kind of quantity. Among people who work with applied mathematics, experience has shown that numbers lack something that is important in many modeling situations: units. To fill the gap, we use “quantity” to indicate a structure with two parts: (1) a number and (2) the units of measurement. From this perspective, a number is merely a quantity that has no units.\nWe learn about units in elementary school, where we study the kinds of units appropriate for measuring tangible stuff. For example, the capacity of a water bottle might be presented in liters, cups, fluid ounces, gallons, bushels, and so on. The area of a field or apartment can be measured in square-meters, hectares, square-feet, square yards, acres, and so on. One of the things school-children learn about units is to identify what kind of stuff is measured by any given unit. For example, a gallon and a cup measure the same kind of stuff: volume. A meter and an inch measure the same kind of stuff: length. Some students also learn how to convert between units. For example, a meter is about 40 inches, a gallon is exactly 16 cups and a cup is exactly 48 teaspoons. But there is no way to convert, say, an inch into a cup. These lessons on units are useful in everyday life, even if most adults have only a limited recollection of them.\nMoving beyond stuff like length, area, volume, and time can be difficult. You probably recognize kilometers-per-hour (or miles-per-hour) as a unit of velocity, but many student struggle when they first encounter something like meters-per-second-per-second, which is a unit for acceleration.\nOne of the great uses of Calculus is to convert between different kinds of stuff. A simple, everyday example is relevant to transportation. You can convert from speed (say, miles-per-hour) to distance travelled. The conversion is accomplished by multiplying speed by the duration of movement. Or, for some purposes, you might need to convert into speed two different measurements: distance travelled and time taken. In later chapters, we will encounter many situations where such conversions between types of stuff is important.\nKeeping track of the type of stuff is an important habit when using Calculus for such conversions. In other words, you need to keep track of the units of each kind of stuff.\nWhen talking about the methods of Calculus, however, there is a shorthand that enables you to see what kind of conversion is being done. This shorthand can help you select the appropriate arithmetical or Calculus method for whatever model manipulation you need to do, and it is especially helpful for spotting and correcting errors. The shorthand involves explicitly noting the “dimension” of the quantity.\nUnits and dimensions are closely related. Knowing the units, you can readily figure out the dimension. But knowing the dimension makes it easier to design and check your calculations. Chapter 15 will describe units and dimension more thoroughly. For now, however, we will give just a hint, enough to understand why knowing and understanding dimensions will help tremendously in your calculation.\nIn particular, we want to emphasize the reason to think about using quantities rather than mere numbers. Whenever you see a quantity, you should expect to see units or their shorthand, dimension.\nAlthough there are hundreds of different kinds of units, we will need only four basic dimensions for most of the applications presented in this book. These are:\nConsider the familiar rules of arithmetic. The basic actions—addition, subtraction, multiplication, division—apply to any two numbers (although division by zero is not allowed). So 17 + 1.3 is 18.3, whatever those numbers are meant to represent. But the two quantities 17 inches and 1.3 seconds (L and T, respectively) cannot be meaningfully added. To apply addition and subtraction, the two quantities must be in the same unit, hence the same dimension. If you encounter someone adding quantities with different dimensions, you know you have spotted an error.\nOn the other hand, division and multiplication can work with any two quantities, regardless of their respective dimensions. In fact, division and multiplication are the basic arithmetic that enables us to construct new kinds of stuff from old kinds of stuff. For insight into the use of the word “dimension,” consider that a length times a length (L \\(\\times\\) L) gives an area (L2) and an area times a length (L2 \\(\\times\\) L) gives a volume (L^3). These correspond to one-, two- and three-dimensional objects respectively.\nThe mathematics of units and dimension are to the technical world what common sense is in our everyday world. For instance (and this may not make sense at this point), if people tell me they are taking the square root of 10 liters, I know immediately that either they are just mistaken or that they haven’t told me essential elements of the situation. It is just as if someone said, “I swam across the tennis court.” You know that person either used the wrong verb—walk or run would work—or that it wasn’t a tennis court, or that something important was unstated, perhaps, “During the flood, I swam across the tennis court.”",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quantity, function, space</span>"
    ]
  },
  {
    "objectID": "Preliminaries/01-quant-fun-space.html#sec-quant-vs-number",
    "href": "Preliminaries/01-quant-fun-space.html#sec-quant-vs-number",
    "title": "1  Quantity, function, space",
    "section": "",
    "text": "time, denoted T\nlength, denoted L\nmass, denoted M\nmoney, denoted V (for “value”)",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quantity, function, space</span>"
    ]
  },
  {
    "objectID": "Preliminaries/01-quant-fun-space.html#sec-functions",
    "href": "Preliminaries/01-quant-fun-space.html#sec-functions",
    "title": "1  Quantity, function, space",
    "section": "1.2 Functions",
    "text": "1.2 Functions\nFunctions, in their mathematical and computing sense, are central to calculus. The introduction to this Preliminaries Block states, “Calculus is about change, and change is about relationships.” The idea of a mathematical function gives a definite perspective on this. The relationship represented by a function is between the function’s input and the function’s output. The input might be day-of-year2 and the output cumulative rainfall up to that day. Every day it rains, the cumulative rainfall increases.\n\nApplication area 1.1 —Temperature as a function of height, brightness as a function of time past noon.\n\n\n\n\n\n\n\nApplication area 1.1 Two examples of input/output relationships\n\n\n\n\nThe input is the altitude on your hike up Pikes Peak to its peak elevation of 14,115 feet; the output is the air temperature. Typically, as you gain altitude the temperature goes down.\nThe input is the number of hours past noon; the output is the brightness of sunlight. As the afternoon progresses, the light grows dimmer, but only to a point.\n\n\n\nA function is a mathematical concept for taking one or more inputs and returning an output. In calculus, we will deal mainly with functions that take one or more quantities as inputs and return another quantity as output.\n\n\n\n\n\n\nvariable\n\n\n\nBe aware of our use of “input” and “output” in place of the vague, but commonly used, “variable.” Try to put the word “variable” out of mind for the present, until we get to discussing the nature of data.\n\n\nBut sometimes we will work with functions that take functions as input and return a quantity as output. And, perhaps surprisingly, there will be functions that take a function as an input and return a function as output.\nIn a definition like \\(f(x) \\equiv \\sqrt{\\strut x}\\), think of \\(x\\) as the name of an input. So far as the definition is concerned, \\(x\\) is just a name. We could have used any other name; it is only convention that leads us to choose \\(x\\). The definition could equally well have been \\(f(y) \\equiv \\sqrt{\\strut y}\\) or \\(f(\\text{zebra}) \\equiv \\sqrt{\\strut\\text{zebra}}\\).\nNotation like \\(f(x)\\) is also used for something completely different from a definition. In particular, \\(f(x)\\) can mean apply the function \\(f()\\) to a quantity named \\(x\\). You can always tell which is intended—function definition or applying a function—by whether the \\(\\equiv\\) sign is involved in the expression.\n\n\n\n\n\n\nComing attraction … the pattern-book functions\n\n\n\nLater in this Preliminaries Block, we will introduce the “pattern-book functions.” These always take a pure number as input and return a pure number as output. In the Modeling Block, we will turn to functions that take quantities—which generally have units—as input and return another quantity as output. The output quantity also generally has units.\n\n\nOne familiar sign of applying a function is when the contents of the parentheses are not a symbolic name but a numeral. For example, when we write \\(\\sin(7.3)\\) we give the numerical value \\(7.3\\) to the sine function. The sine function then does its calculation and returns the value 0.8504366. In other words, \\(\\sin(7.3)\\) is utterly equivalent to 0.8504366.\nIn contrast, using a name on it is own inside the parentheses indicates that the specific value for the input is being determined elsewhere. For example, when defining a function we often will be combining two or more functions, like this: \\[g(x) \\equiv \\exp(x) \\sin(x)\\] or \\[h(y,z) \\equiv \\ln(z) \\left(\\strut\\sin(z) - \\cos(y)\\right)\\ .\\] The \\(y\\) and \\(z\\) on the left side of the definition are the names of the inputs to \\(h()\\).3 The right side describes how to construct the output, which is being done by applying \\(\\ln()\\), \\(\\sin()\\) and \\(\\cos()\\) to the inputs. Using the names on the right side tells us which function is being applied to which input. We won’t know what the specific values those inputs will have until the function \\(h()\\) is being applied to inputs, as with \\[h(y=1.7, z=3.2)\\ .\\]\nOnce we have specific inputs, we (or the computer) can plug them into the right side of the definitionto determine the function output: \\[\\ln(3.2)\\left(\\sin(3.2) - \\strut \\cos(1.7)\\right) = 1.163(-0.0584 + 0.1288) =-0.2178\\ .\\]\n\n\nWe will introduce the idea of “spaces” in ?sec-spaces-intro. A function maps each point in the function’s input space into a single point in the function’s output space. The input and output spaces are also known respectively as the “domain” and “range” of the function.\n\nApplication area 1.2 —An early application of computing\n\n\n\n\n\n\n\nApplication area 1.2 Functional gunnery\n\n\n\nThe various mathematical functions that we will be studying in this book are in the service of practical problems. But there are so many such problems, often involving specialized knowledge of a domain or science, engineering, economics, and so on, that an abstract mathematical presentation can seem detached from reality.\nVideo 1.1 is a training cartoon from 1945 for gunners in B-29 bombers. The gunner tries to position the gun (the function output) so that a shell and the plane will intersect. There are many inputs to the function, which has been implemented by electronics. The function itself is literally a black box. The inputs are provided by a human gunner training a telescope on a target and setting control dials. The ultimate output is the deflection of the guns in a remote turret. The main function is composed of several others, such as a function that outputs target range given the target size based on knowledge of the size of the target and how large it appears in the telescopic sight.\n\n\n\n\n\n\n\nVideo 1.1: A training video from World War II: Gunnery in the B-29: How to Shoot.\n\n\n\nDividing the gunnery task into a set of inputs and a computed output allows for a division of labor. The gunner can provide the skills properly trained humans are good at, such as tracking a target visually. The computer provides the capabilities—mathematical calculation—to which electronics are well suited. Combining the inputs with the calculation provides an effective solution to a practical problem.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quantity, function, space</span>"
    ]
  },
  {
    "objectID": "Preliminaries/01-quant-fun-space.html#sec-space-intro",
    "href": "Preliminaries/01-quant-fun-space.html#sec-space-intro",
    "title": "1  Quantity, function, space",
    "section": "1.3 Spaces",
    "text": "1.3 Spaces\nCalculus is largely about change, and change involves movement. To move, as you know, means to change location. But we often use location as a metaphor for other things. Consider, for example, the everyday expression, “The temperature is getting higher.” This is not pointing to a thermometer rising up in the air, but to a quantity as it changes.\nTo represent a quantity as it changes we need to provide scope for movement. One way to do this is familiar from schooling: the number line. Each location on the number line corresponds to a possible value for a quantity. The particular value of that quantity at a specific moment in time is represented as a tiny bead on that line. The line as a whole encompasses many other locations. The changing quantity is the movement of the bead.\nFigure 1.1 shows a number line as conventionally drawn. I have placed two different colored dots on the line to correspond to two different quantities: \\(\\color{blue}{6.3^\\circ\\ \\text{C}}\\) and \\(\\color{magenta}{-4.5^\\circ \\text{C}}\\). (You only know that the line is about temperature because I told you.)\n\n\n\n\n\n\nFigure 1.1: A number line drawn in the conventional style suitable for representing temperatures near freezing.\n\n\n\nThe graphical element representing the set of possibilities is the horizontal line segment. The tick marks and labels are added to enable you to translate any given location into the corresponding quantity. In mathematics texts, it’s common to put arrowheads at the ends of the line segment, perhaps to remind you that a line is infinite in length. But in other disciplines, no infinity or arrowheads are needed: the picture is just a scale to help people translate location into quantity.\nI could have placed many dots on the line to represent many different particular quantitities. Of course, all the dots would be representing temperatures in \\(^\\circ\\text{C}\\) since that is the sort of quantity that the number line in Figure 1.1 represents.\nA more general way for representing pairs of quantities is the coordinate plane, which Figure 1.2 shows in the mathematics-text style. (In Chapter 4, we will switch to another style more commonly used across disciplines.)\n\n\n\n\n\n\nFigure 1.2: The coordinate plane drawn in the style common to mathematics texts. Source: Wikipedia\n\n\n\nEvery location in the coordinate plane is a possibility. A specific pair of quantities is displayed by placing a dot. Figure 1.2 has four such dots, corresponding to four distinct pairs of quantities.\nThe space annoted by the coordinate axes and grid is two-dimensional, analogous to a table-top or a piece of paper or a computer display’s surface. Two-dimensional space accommodates change in each of the two quantities.\nEveryday life acquaints us well with three-dimensional spaces, where each location corresponds to a possible value for each of three quantities. Displaying a three-dimensional space is difficult to do well because conventional displays show only two dimensions. Figure 1.3 shows one style that uses perspective and shading to create the impression of a 3-D scene.\n\n\n\n\n\n\nFigure 1.3: One of many styles for displaying a three-dimensional space. Source\n\n\n\nDrawings of one-dimensional space (Figure 1.1) or two-dimensional space (Figure 1.2) make it straightforward to read off the quantitative value corresponding to any location. Already in 3-dimensional space, reading quantitative coordinates is difficult and requires conscious mental effort. We will make only limited use of 3-D displays.\nConsider now what the different dimensional spaces permit in terms of movement, that is, how quantities can change. The number line permits one kind of movement, left-right in Figure 1.1. Even though we use two words to name the kind of movement—“left” and “right”—we still consider movement in either opposing direction as one kind of movement. Left is the opposite of right. The coordinate plane permits two kinds of movement: left-right and up-down. A three-dimensional space permits three kinds of movement: left-right, up-down, nearer-farther. We often say that each type of movement is along an axis. as is conventional for one- and two-dimensional spaces. The number line has one axis, the coordinate plane has two axes, and three-dimensional space has three.\nFigures 1.1, 1.2, and 1.3 are conventional drawings of one-, two-, and three-dimensional spaces respectively. What about four- or higher-dimensional spaces? Many people put their foot down here and refuse to accept such a thing. Some others will point to the Theory of Relativity where an essential concept is “space-time,” a four-dimensional space sometimes denoted as \\((x, y, z, t)\\). True though this be, it does not much appeal to intuition and for a good reason. We are free to move objects in their x-, y-, and z-coordinates, but we have no control over time.\nEngineers, statisticians, physicists, and others often use the phrase “degree of freedom” to refer to a type of movement. In English, we have many phrases for different types of movement: in-and-out, back-and-forth, clockwise-and-counter-clockwise, nearer-and-farther, up-and-down, left-and-right, north-and-south, east-and-west, and so on. When imagining time machines, or showing photos from our recent trip, we speak of going forward in time or going back in time: forward-and-back. (Note the word “going,” which emphasizes the idea of movement rather than position.)\nHealth professions learn additional names for kinds of movement: adduction-abduction, flexion-extension, internal-vs-external rotation.\nThinking in terms of movement, it’s easier to construct a depiction of even four-, five-, and higher-dimensional space. Video 1.2 shows the movement of a robotic hand with six degrees of freedom. Each of these is a different kind of movement.\n\n\n\nTable 1.1: Six degrees of freedom for the robot hand.\n\n\n\n\nswiveling of the base\n\n2-4. rotation around each of the three knuckles\n\nswiveling at the wrist\n\n\nfingers moving closer together or farther apart\n\n\n\n\n\n\n\n\n\n\n\nVideo 1.2: A robot hand with six degrees of freedom. Source\n\n\n\nThere can be multiple ways of forming coordinates for the same space. In terms of the mechanism for the robotic hand, it’s configuration can be specified by the six degrees of freedom enumerated in Table 1.1. But from the point of view of a person training the robot, it might be more convenient to think about the configuration in terms of the six quantities given in Table 1.2.\n\n\n\nTable 1.2: A different way of describing the configuration of the robot hand that is better suited for a user training the robot in some action.\n\n\n\n1-3. The location of the wrist in everyday three-dimensional space. (x,y,z)\n4-5. The direction that the fingers point in. (Two angles often called “azimuth” and “elevation.”)\n\nThe distance between the fingers (d).\n\n\n\n\n\nTo train the robot to perform a specific movement, the engineer specifies a sequence of configurations. Each of the configurations corresponds to a dot in the six-dimensional space. An important Calculus method, introduced in Chapter 49, is to create a smooth, continuous path connecting the dots called a trajectory which, in this case, depicts movement through six-dimensional space.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quantity, function, space</span>"
    ]
  },
  {
    "objectID": "Preliminaries/01-quant-fun-space.html#all-together-now",
    "href": "Preliminaries/01-quant-fun-space.html#all-together-now",
    "title": "1  Quantity, function, space",
    "section": "1.4 All together now",
    "text": "1.4 All together now\nThe three mathematical concepts we’ve been discussing—quantities, functions, spaces—are used together.\nA quantity can be a specific value, like 42.681\\(^\\circ\\)F. But you can also think of a quantity more broadly, for instance, “temperature.” Naturally, there are many possible values for temperature. The set of all possible values is a space. And, using the metaphor of space, the specific value 42.681\\(^\\circ\\)F is a single point in that space.\nFunctions relate input quantities to a corresponding output quantity. A way to think of this—which will be important in Chapter 4 —is that a function is a correspondence between each point in the input space and a corresponding point in the output space. By mathematical convention, the output space in Calculus is always one-dimensional.\nEvery function has a set of legitimate potential inputs, a region in the input space. This input-space region is called the domain of the function. For some functions, the possible output values occupy the whole of the one-dimensional output space. Other functions use only regions of the one-dimensional output space. The set of possible outputs is called the range of the function.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quantity, function, space</span>"
    ]
  },
  {
    "objectID": "Preliminaries/01-quant-fun-space.html#exercises",
    "href": "Preliminaries/01-quant-fun-space.html#exercises",
    "title": "1  Quantity, function, space",
    "section": "1.5 Exercises",
    "text": "1.5 Exercises",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quantity, function, space</span>"
    ]
  },
  {
    "objectID": "Preliminaries/01-quant-fun-space.html#footnotes",
    "href": "Preliminaries/01-quant-fun-space.html#footnotes",
    "title": "1  Quantity, function, space",
    "section": "",
    "text": "Source: Oxford Languages↩︎\n“Day-of-year” is a quantity with units “days.” It starts at 0 on midnight of New Year’s Eve and ends at 365 at the end of day on Dec. 31.↩︎\nSometimes, we will use both a name and a specific value, for instance \\(\\sin(x=7.3)\\) or \\(\\left.\\sin(x)\\Large\\strut\\right|_{x=7.3}\\)↩︎",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quantity, function, space</span>"
    ]
  },
  {
    "objectID": "Preliminaries/04-graphs-and-graphics.html",
    "href": "Preliminaries/04-graphs-and-graphics.html",
    "title": "4  Visualizing functions",
    "section": "",
    "text": "4.1 Drawing function graphs with R/mosaic\nIn technical vocabulary, the word “graph” refers to the kind of display in Figure 4.1. The word “graphic” is not technical and is more general than graph. A “graphic” is any kind of visual presentation of art or information. This is important to note, since the majority of graphics in this Calculus book will not be “graphs.”\nA “graph” is a suitable format for displaying a function with one input. (Remember, all functions in this book will have one output.) In modeling, it’s typical to use functions of multiple inputs. Consequently, graphics involving functions will typically not be “graphs.” But, the reader is undoubted familiar with graphs from high school, so we start there so that we can focus on the computer commands before we introduce other modes.\nActive R chunk 4.1 shows an R/mosaic graphics command that produces a graphic showing the graph of a function. (Make sure to press “Run code” to see the graphic.)\nYou see the typical components of an R/mosaic command: a function name (slice_plot()) followed by a pair of parentheses. Inside the parentheses are two arguments, both of which are required.\nA tilde expression suitable for slice_plot() must have a single input name on the right-hand side. The left-hand side is an expression of the sort you have already used in makeFun().\nThe second argument to slice_plot() will always be domain() with a named argument matching the name from the tilde expression RHS. The value given for the named argument contains the left- and right-bounds for the horizontal axis.\nRemember that a function’s domain is the space of all possible valid inputs to the function. The domain for most of the functions you studied in high school is infinite, for instance extending from \\(-\\infty\\) to \\(\\infty\\). A graphical domain, in contrast, is usually a finite part of the function domain.\nThe R/mosaic command you will use to graph a mathematical function is slice_plot(). (We will get into the reasons for this name later.) slice_plot() takes two arguments which are, as always, separated by a comma. Here are two examples producing graphs of different functions:\nThe first argument to slice_plot() tells what function is to be plotted. The argument is a tilde expression of the same sort you saw in Chapter 3.\nThe second element establishes the graphics domain. It will always be the name domain() with a named argument that matches the name for the input on the RHS of the tilde expression. In (a) the argument name is x while in (b) the argument name is y.\nNamed arguments always consist of a name followed by an equal sign. After the equal sign comes the value of the argument. In (a), the value is 0 : 5 which means “zero to five.” In (b), the value is -3.5 : 3 standing for “minus three-point-five to three.” Each of these values is written with a colon (:). This is a bit of punctuation meaning “to,” as in “zero to five.”1",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/04-graphs-and-graphics.html#drawing-function-graphs-with-rmosaic",
    "href": "Preliminaries/04-graphs-and-graphics.html#drawing-function-graphs-with-rmosaic",
    "title": "4  Visualizing functions",
    "section": "",
    "text": "Active R chunk 4.1\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\na tilde expression for the function you wish to graph\nthe graphical domain to be covered by the horizontal axis\n\n\n\n\n\n\nslice_plot(sin(x) ~ x, domain(x = 0 : 5)\nslice_plot(3 * y + b  ~ y, domain(y = -3.5 : 3)",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/04-graphs-and-graphics.html#sec-drawing-contour-plots",
    "href": "Preliminaries/04-graphs-and-graphics.html#sec-drawing-contour-plots",
    "title": "4  Visualizing functions",
    "section": "4.2 Drawing contour plots of functions",
    "text": "4.2 Drawing contour plots of functions\nIn general, the functions we use in modeling can have multiple inputs. So best not to get to fixated on the format in Active R chunk 4.1. That format is appropriate only for functions that have one input.\nOur preferred visual format for a function of two inputs is the “contour plot.” You might be familiar with the idea if you have ever had to use a topographic map for hiking. Here’s an example:\n\n\n\n\nh &lt;- makeFun(3*x - 2*x*y - y^2 ~ x & y)\ncontour_plot(h(x, y) ~ x & y, domain(x = -1:1, y = 0:2))\n\n\n\n\n\n\n\n\n\n\nFigure 4.2\n\n\n\nThe function being plotted is named h(). Since h() takes two inputs, the graphical domain needs to be a space with two dimensions. We signal this by giving two arguments to domain(), one for each dimension of the input space. Of course, the names used within domain() have to match the names used in the tilde expression. Note that we use only one domain() call but it has two arguments: one for the horizontal axis, the other for the vertical axis.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/04-graphs-and-graphics.html#spaces-and-graphs-of-functions",
    "href": "Preliminaries/04-graphs-and-graphics.html#spaces-and-graphs-of-functions",
    "title": "4  Visualizing functions",
    "section": "4.3 Spaces and graphs of functions",
    "text": "4.3 Spaces and graphs of functions\nWe are going to step back from the instructions for drawing graphics like Active R chunk 4.1 in order to give you a better perspective on the essentials of making graphical displays of functions. This will help in reading and interpreting graphics, especially graphics for functions of multiple inputs such as in Figure 4.2.\nAs you know, the domain of a function is the set of all possible valid inputs to that function. In Chapter 1 we defined a space to be a set of possibilities. Thus, a function domain can be seen as a space, the “input space” for the function.\nIn Active R chunk 4.1, the input space is a number line. Each point in the space is a possible input value to the function.\n\n\n\n\n\n\nFigure 4.3: The input space for the function graphed in Active R chunk 4.1.\n\n\n\nThere is also an output space for a function, the set of all possible outputs. Figure 4.4 shows a number line that is the output space for the function graphed in Active R chunk 4.1.\n\n\n\n\n\n\nFigure 4.4\n\n\n\nThe function itself tells us, for every point in the input space, what is the corresponding point in the output space. Figure 4.5 gives an example. The input and output spaces are shown as number lines, while the function is indicated by the thin colored lines.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.5: A function relates points from the input space to their corresponding location in output space.\n\n\n\nFor example, the function displayed in Figure 4.5 translates an input value of -4 (see the green line) into an output value of 3.7.\nThe display in Figure 4.5 is very hard to interpret. Also, to avoid the display being filled up with colored ink, we can show only a few of the input/output pairs.\nA function graph (like Active R chunk 4.1) is much easier to read. But let’s be clear about where the spaces are shown. In a function graph, the input space is shown horizontally and the output space is shown vertically as in Figure 4.6.\n\n\n\n\n\n\nFigure 4.6: A function graph shows the input space horizontally and the output space vertically.\n\n\n\nNow for every point in the input space the function specifies a corresponding point in the output space. We mark the correspondence with a dot. The horizontal coordinate tells us what is the input value. The vertical coordinate tells us what is the corresponding output value. To show the function as a whole—the output corresponding to every input—we would need a lot of dots! So many dots that they collectively give an appearance of a thin curve, the curve seen in Active R chunk 4.1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.7: The mapping from Figure 4.5 translated into the form of a graph. The input space is marked by the horizontal axis and the output space by the vertical axis. Each of the arrows in Figure 4.5 is represented by a point, whose x-coordinate is the position of the tail of the arrow in the input space and whose y-coordinate is the position of the head of the arrow in the output space. This is the graph of the function.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/04-graphs-and-graphics.html#input-and-output-spaces",
    "href": "Preliminaries/04-graphs-and-graphics.html#input-and-output-spaces",
    "title": "4  Visualizing functions",
    "section": "4.4 Input and output spaces",
    "text": "4.4 Input and output spaces\nThis background about spaces is important for understanding functions because, for functions with two or more inputs, the input space and the output space can be depicted in different ways. For example, in the contour plot of Figure 4.2, the input space is the two-dimensional space in the plane of the display (that is, the paper or screen depending on how you are reading this). The output space for the contour plot is depicted using curves, colors, and labels. There is one curve for each output value. Only a handful of output values are shown, but you can get pretty close to estimating the output value even for inputs not on the curve.\nThe space of all possibilities (y, z, output) is three-dimensional, but very few of those possibilities are consistent with the function to be graphed. You can imagine our putting dots at all of those consistent-with-the-function points, or our drawing lots and lots of continuous curves through those dots, but the cloud of dots forms a surface; a continuous cloud of points floating over the (y, z) input space.\nFigure 4.8 displays this surface. Since the image is drawn on a two-dimensional screen, we have to use painters’ techniques of perspective and shading. In the interactive version of the plot, you can move the viewpoint for the image which gives many people a more solid understanding of the surface.\nMath textbooks—but not so much this one!—often display functions with two inputs using a three-dimensional space. This space is made by laying the two-dimensional input space on a table, and sticking the one-dimensional output space perpendicular to the table. Each point in the input space has a corresponding value in the output space which could, in principle, be marked with a dot. The whole set of dots, one for each value in the input space, appears as a surface floating over the table.\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.8: A function g(x, y) ~ x & y drawn as a surface. The function output value, for any given \\((x, y)\\) pair, is the height of the surface above the \\((x, y)\\)-plane. Such graphs are best appreciated by interacting with them. When you move the cursor over the graph, a rectangular frame appears from which you can read out coordinates numerically. Dragging while pressing will rotate the graph. Press the  icon to return to the original view.\n\n\n\nA variety of drawing techniques such as transparency, color, and interactive annotation are used to help us perceive a two-dimensional surface embedded in a three-dimensional space. (Place your cursor within the space delimited by the x, y, z axes to see the annotations.)\nPretty as such surfaces are, a contour plot (Figure 4.9) provides a good view of the same function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.9: The function from Figure 4.8 shown in contour-plot format.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/04-graphs-and-graphics.html#why-slice_plot",
    "href": "Preliminaries/04-graphs-and-graphics.html#why-slice_plot",
    "title": "4  Visualizing functions",
    "section": "4.5 Why “slice_plot()”?",
    "text": "4.5 Why “slice_plot()”?\nSaying “graph” for a display of \\(f(x)\\) versus \\(x\\) is correct and reasonable. But in MOSAIC Calculus we have another point to make.\nAlmost always, when mathematically modeling a real-world situation or phenomenon, we do not try to capture every nuance of every relationship that might exist in the real world. We leave some things out. Such simplifications make modeling problems easier to deal with and encourage us to identify the most important features of the most important relationships.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.10: The function that we will be taking slices of in Figures 4.11 and 4.12.\n\n\n\nIn this spirit, it is useful always to assume that our models are leaving something out and that a more complete model involves a function with more inputs than the present candidate. The present candidate model should be considered as a slice of a more complete model. Our slice leaves out one or more of the input quantities in a more complete model.\nTo illustrate this, suppose that the actual system involves relationships among three quantities, which we represent in the form of a function of two inputs, as shown in Figure 4.10. (The third quantity in the relationship is the output of the function.)\nThe most common forms of slice involve constructing a simpler function that has one input but not the other. For example, our simpler function might ignore input 22. There are different ways of collapsing the function of two inputs into a function of one input. An especially useful way in calculus is to take the two-input function and set one of the inputs to a constant value.\nFor instance, suppose we set input 22 to the constant value 1.5. This means that we can consider any value of input 1, but input 2 has been replaced by 1.5. In Figure 4.11, we’ve marked in red the points in the contour plot that give the output of the simplified function.\n\n\n\n\n\n\n\n\n\n(a) Contour plot of a function with two inputs. The red path shows points in the input space where input_2 is held constant at 1.5.\n\n\n\n\n\n\n\n\n\n(b) Values of the function at the points along the red path in (a). Since there is effectively one input, the function can be presented as a graph.\n\n\n\n\n\n\n\nFigure 4.11: A slice through the function in Figure 4.10,\n\n\n\nEach point along the red line in Figure 4.11(a) corresponds to a specific value of input #1. From the contours, we can read the output corresponding to each of those values of input #1. This relationship, output versus input #1 can be drawn as a mathematical graph (to the right of the contour plot). Study that graph until you can see how the rising and falling parts of the graph correspond to the contours being crossed by the red line.\nSlices can be taken in any direction or even along a curved path! The blue line in Figure 4.12 shows the slice constructed by letting input 2 vary and holding input 1 at the constant value 0.\n\n\n\n\n\n\n\n\n\n(a) The same function as shown in Figure 4.11 but with a slicing path drawn by holding input_1 at zero.\n\n\n\n\n\n\n\n\n\n(b) A graph of the value of the function along the blue slice. Note the violation of convention: The graph has been flipped on its side so that the input axis aligns with the direction of the slice.\n\n\n\n\n\n\n\nFigure 4.12: Another one-dimensional slice through the graph of a function with two inputs.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/04-graphs-and-graphics.html#exercises",
    "href": "Preliminaries/04-graphs-and-graphics.html#exercises",
    "title": "4  Visualizing functions",
    "section": "4.6 Exercises",
    "text": "4.6 Exercises",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/04-graphs-and-graphics.html#footnotes",
    "href": "Preliminaries/04-graphs-and-graphics.html#footnotes",
    "title": "4  Visualizing functions",
    "section": "",
    "text": "R experts should note that the colon has a different meaning within domain() than it does generally in R.↩︎",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizing functions</span>"
    ]
  },
  {
    "objectID": "preliminaries-part.html",
    "href": "preliminaries-part.html",
    "title": "PRELIMINARIES",
    "section": "",
    "text": "Calculus is about change, and change is about relationships. Consider the complex and intricate network of relationships that determine climate: a changing climate implies that there is a relationship between, say, global average temperature and time. Scientists know temperature changes with levels of CO2 and methane which themselves change due to their production or elimination by atmospheric and geological processes. A change in one component of climate (e.g., ocean acidification or pH level) provokes change in others.\nTo describe and use the relationships we find in the natural or designed world, we build mathematical representations of them. We call these mathematical models. Models provide the link between the real world and the abstractions of mathematics.\nTo be concise …\n\nA model is “a representation for a purpose.”\n\nA blueprint describing the design of a building is an everyday example of a model. The blueprint represents the building but in a way that is utterly different from the building itself. Blueprints are much easier to construct or modify than buildings, they can be carried and shared easily. Two of the purposes of a blueprint is to aid in the design of buildings and to communicate that design to the people securing the necessary materials and putting them together into the building itself.\nDefining the purpose for your model is a crucial first step in building a mathematical representation that will serve that purpose. Useful models of the same real-world setting can be very different, depending on the purpose. For instance, one routine use for a model is to make a prediction. But other models are intended for exploring the connections among the components of the system being modeled.\n\nApplication area 1 Modeling is crucial to atmospheric science.\n\n\n\n\n\n\n\nApplication area 1 Modeling the climate\n\n\n\nAtmospheric scientists build climate models whose purpose is to explore scenarios for the future emission of greenhouse gasses. The model serves as a stand-in for the Earth, enabling predictions in a few hours of decades of future change in the climate. This is essential for the development of policies to stabilize the climate.\n\n\nDesigning a building or modeling the climate requires expertise and skill in a number of areas. Nonetheless, constructing and using a model is easy compared to the alternative of working directly with the object of interest. For instance, a blueprint gives a comprehensive overview of a building in a way that is hard to duplicate just by walking around an actual building.\nModels are easy to manipulate compared to reality, easy to implement (think “draw a blueprint” versus “construct a building”), and easy to extract information from. We can build multiple models and compare and contrast them to gain insight into the real-world situation behind the models.\nA mathematical model is a model made out of mathematical and computational stuff. Example: a bank’s account books are a model made mostly out of numbers. But in technical areas—science and engineering are obvious examples, but there are many other fields, too—numbers don’t get you very far. By learning calculus, you gain access to important mathematical and computational concepts and tools for building models and extracting information from them. The chapters in this Preliminaries section of this book introduce some of the fundamental mathematical entities that are the heart of modeling.",
    "crumbs": [
      "PRELIMINARIES"
    ]
  },
  {
    "objectID": "Modeling/16-modeling-scientific-method.html",
    "href": "Modeling/16-modeling-scientific-method.html",
    "title": "16  Modeling and the scientific method",
    "section": "",
    "text": "16.1 Example: Cooling water\nChapter 11 presented data on water cooling from near boiling to room temperature. (See Figure 11.2.) Prof. Stan Wagon of Macalester College collected the data with a purpose in mind: to see whether Newton’s Law of Cooling provides a good description of the actual physical process. In particular, Wagon knew that a simple application of Newton’s Law implies that the water temperature will decay exponentially.\nFigure 16.1 shows an exponential function fitted to the water-cooling data. The fitted function is stored as Mod1_fun() with the 1 identifying it as the first of possibly several models.\nAt first glance, the exponential function seems a good match to the data. But what do we mean by “good?” Is “good” good enough? And “good enough” for what? The experienced modeler should always have in mind the criterion by which to evaluate “good enough.” These criteria, in turn, are shaped by the purpose of building the model.\nBeware of general-purpose criteria. For instance, a general-purpose criterion is to compare the shape of the function with the data. The exponential model slopes and curves in a similar manner to the data. The deviation of the model output from the data is, at worst, just a few degrees. This is small compared to the 50\\(^\\circ\\)C change in temperature throughout the experiment.\nAnother widely used general-purpose statistical measure of the quality of the match is the R2 (R-squared) value. Mathematically, R2 is always between 0 and 1. (See ?sec-stat-modeling, where the mathematics of R2 is introduced.) When R2 = 1, the match is perfect. For the Mod1_fun() and the cooling water data, R2 = 0.991. Many scientists would interpret this as “almost perfect,” although things are sometimes more complicated … as here.\nWagon’s purpose was not simply to see if an exponential curve resembles the data. Instead, he wanted to know if Newton’s Law of Cooling was consistent with the observed cooling over time. The exponential shape of temperature versus time is just one consequence of Newton’s Law. Another is that the water temperature should eventually reach equilibrium at room temperature.\nThe fitted exponential model fails here, as you can see by zooming in on the right tail of Figure 16.1. In Figure 16.2, we zoom in on the data’s left and right tails.\nIn the scientific method, one takes a theory (Newton’s Law of Cooling), makes predictions from that theory, and compares the predictions to observed data. If the predictions do not match the theory—here, that the water should cool to room temperature—then a creative process is called for, replacing or modifying the theory. The same applies when a mathematical model fails to suit its original purpose.\nThe creative process of constructing or modifying a theory is not primarily a matter of mathematics, it usually involves expert knowledge about the system being modeled. In the case of a cooling mug of water, the “expertise” can be drawn from everyday experience. For instance, everybody knows that it is not entirely a matter of the water cooling off; the mug gets hotter and then cools in its own fashion to room temperature.\nWagon’s initial modification of the theory went like this: Newton’s Law still applies, but with the water in contact with both the room and, more strongly, the mug. To build this model, he needed to draw on the mathematics of dynamical systems (to be introduced in Block 5), producing a new parameterized formula of water temperature versus time.\nWagon went on to check the water-mug-room theory against the data. He found that the improved model did not completely capture the temperature-versus-time data. He applied more expertise, that is, he considered something we all observe: water vapor (“steam”) rises from the water in the mug. This prompted some experimental work where a drop of oil was placed on the water surface to block the vapor. This experiment convinced him that a new revision to the model was called for that would include the cooling due to evaporation.\nWithout being able to anticipate the settings and purposes for your model building, we can’t pretend to teach you the real-world expertise you will need. But we can do something to help you move forward in your work. The remaining sections of this chapter introduce some general-purpose mathematical approaches to refining models. For instance, rather than using a single exponential function, Wagon used a linear combination of exponentials for his modeling. We will also try to warn you of potential pitfalls and ways you can mislead yourself.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling and the scientific method</span>"
    ]
  },
  {
    "objectID": "Modeling/16-modeling-scientific-method.html#example-cooling-water",
    "href": "Modeling/16-modeling-scientific-method.html#example-cooling-water",
    "title": "16  Modeling and the scientific method",
    "section": "",
    "text": "Mod1_fun &lt;- mosaic::fitModel(temp ~ A*exp(-k*time) + C, \n                             data = CoolingWater,\n                             start = list(C = 30, k = 1/20, A = 70))\ngf_point(temp ~ time, data = CoolingWater, alpha = .15 ) |&gt;\n  slice_plot(Mod1_fun(time) ~ time, color = \"blue\") |&gt;\n  gf_labs(x = \"Time (minutes)\", y=\"Temperature (deg. C)\")\n\n\n\n\n\n\n\n\n\n\nFigure 16.1: An exponential model (blue) fitted to the CoolingWater data. \\[T(t) \\equiv 27.0 + 62.1\\ e^{-0.021 t}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.2: The exponential model does not capture the water’s initial almost-boiling temperature. To judge from the \\(A\\) coefficient presented in the caption of Figure 16.1, the room temperature is 27\\(^\\circ\\)C, while the data themselves indicate a room temperature a little less than 25\\(^\\circ\\)C.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling and the scientific method</span>"
    ]
  },
  {
    "objectID": "Modeling/16-modeling-scientific-method.html#identifying-systematic-discrepancies",
    "href": "Modeling/16-modeling-scientific-method.html#identifying-systematic-discrepancies",
    "title": "16  Modeling and the scientific method",
    "section": "16.2 Identifying systematic discrepancies",
    "text": "16.2 Identifying systematic discrepancies\nRefer to Figure 16.1, which shows the measured temperature data along with the initial fitted exponential model. The data systematically differ from the model in four places: the model underestimates the temperature early in the process, then overestimates for an interval, returns later to an underestimation, and finishes up with an overestimation.\nWhen assessing a model, it is a good practice to calculate the difference between the observed data and the model value. Figure 16.3 shows this difference, calculated simply by subtracting from each data point the model value at the corresponding time. In statistical nomenclature, such differences are called the “residuals” from the model.\n\n\n\n\ngf_point(temp - Mod1_fun(time) ~ time, data = CoolingWater) |&gt;\n  gf_labs(x = \"Time (minutes)\", \"Residual (deg C)\")\n\n\n\n\n\n\n\n\n\n\nFigure 16.3: Residuals versus time. The comparatively smooth structure indicates systematic deviations between the model and the data.\n\n\n\nThe presence of noise in any measurement is to be expected. You can see such noise in Figure 16.3 in the small, irregular, trembling fluctuations. Conversely, any smooth, slowly changing pattern in the residuals is considered systematic variation. Here, those smooth variations are much larger in amplitude than the irregular noise, indicating that we should investigate the systematic variation more closely.\nThis is a chance for the modeler to speculate on what might be causing the systematic deviations. A good place to start is with the largest residuals. In this case, that’s at the beginning of the temperature recording. Note that early in the recording, the recorded temperature falls much faster than in the model. This is perhaps clearer with reference to Figure 16.1.\nWhat physical process might lead to the initial fast cooling of the water? Answering this question requires both detailed knowledge of how the system works and some creativity. As mentioned above, Wagon speculated that the water might be cooling in two ways: i) heat moving from the water to the air in the room and ii) heat moving from the water into the adjacent mug. This second process can be fast, making it a candidate for explaining the residuals in the early times of the experiment.\nA quick test of the speculation is to construct a linear combination of two exponential processes, one fast and one slower. This would be difficult to do by eye using the techniques of Chapter 8, but fitModel() can accomplish the task so long as we can give a rough estimate of suitable numerical values for the parameters. Figure 16.4 shows the resulting function and the residuals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.4: The water-cooling model with two exponential processes, one fast and one slow, and the residuals from the observed data.\n\n\n\nFigure 16.4 shows a much-improved match between the model and the data. The residuals are about one-tenth as large as those from the original model.\nDepending on the purpose for which the modeling is being done, this might be the end of the story. The model is within a few tenths of a degree C from the data, good enough for purposes such as prediction. For Prof. Wagon, however, the purpose was to investigate how complete an explanation Newton’s Law of Cooling is for the physical process. He concluded that the residuals in Figure 16.4 still show systematic patterns.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling and the scientific method</span>"
    ]
  },
  {
    "objectID": "Modeling/16-modeling-scientific-method.html#refine-the-data-refine-the-experimental-technique",
    "href": "Modeling/16-modeling-scientific-method.html#refine-the-data-refine-the-experimental-technique",
    "title": "16  Modeling and the scientific method",
    "section": "16.3 Refine the data, refine the experimental technique",
    "text": "16.3 Refine the data, refine the experimental technique\nThe initial model building often suggests how new, informative data might be collected. It’s impossible to generalize this to all situations, but in Prof. Wagon’s work, two possibilities arise:\n\nMeasure the temperature of the mug directly, as well as the temperature of the water.\nStifle other possible physical processes, such as evaporation of water from the top surface of the liquid, and collect new data\n\nProf. Wagon managed (2) by putting a small drop of oil on the water right after it was poured into the mug. This created a thin layer that hindered evaporation. He expected that canceling this non-Newtonian process would make the Newton model a better fit to the new data, providing an estimate of the magnitude of the evaporation effect.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling and the scientific method</span>"
    ]
  },
  {
    "objectID": "Modeling/16-modeling-scientific-method.html#including-new-influences",
    "href": "Modeling/16-modeling-scientific-method.html#including-new-influences",
    "title": "16  Modeling and the scientific method",
    "section": "16.4 Including new influences",
    "text": "16.4 Including new influences\nIn many situations, there is data on more than one factor that might come into play in determining the response variable. The mathematical methods relating to linear combinations covered in Block 3 provide the basic toolkit for considering such new influences. In Block 5, you will see many examples of situations where the system’s behavior depends critically on the interaction between two competing factors.\nThe closely related topic of statistical modeling provides essential concepts and tools for deciding whether to incorporate an additional input to a model function. Part of the importance of a statistical approach comes from a mathematical paradox that will be discussed in Block 4. Whenever you have a model of a response variable in terms of explanatory variables there will be residuals. Adding in a new response variable, even if it is utterly unrelated to the system, will make the residuals smaller. Statistical method provides the means to avoid mistakenly including the new response variable by comparing the actual reduction in residuals to what would be expected from a generic random variable.\nAnother important statistical topic is causality: reasoning about what causes what. Often, the modeler’s understanding of how the system under study works can guide the choice of additional variables to include in a model.\nSpace precludes detailed consideration of the statistical phenomena in this book, although the techniques used are well within the capabilities of anyone who has completed this book through Block 3.\nIt’s worth point out a regretable fact of life in the academic world covering the quantitative science. At almost all institutions (as of 2024), calculus is taught in a manner that is totally disconnected from statistics and data science. In part, this is because the professional training of mathematics instructors does not include an emphasis on statistics. This is an artifact of history and is slowly being addressed. MOSAIC Calculus is the result of a successful program to integrate modeling, statistics, calculus, and computing—the M S C of MoSaiC. To fit in with the academic structure at most universities, it was necessarily to deliver the integrated material in a form that could be slotted into existing Calculus curricula and, separately, into existing statistics curricula. The statistics part of the integrated curriculum is packaged into another free, online textbook: Lessons in Statistical Thinking.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling and the scientific method</span>"
    ]
  },
  {
    "objectID": "Modeling/16-modeling-scientific-method.html#interpolation-and-extrapolation",
    "href": "Modeling/16-modeling-scientific-method.html#interpolation-and-extrapolation",
    "title": "16  Modeling and the scientific method",
    "section": "16.5 Interpolation and extrapolation",
    "text": "16.5 Interpolation and extrapolation\nModels are most reliable when the domain of the functions is considered to be the span of the data or observations underlying them. Evaluating a function inside the span of the data is called “interpolation.” Conversely, sometimes functions are evaluated at inputs far outside the span of the data used to construct the function. This is “extrapolation.”\nExtrapolation is unreliable. However, sometimes, the purpose of a model is to help us consider how a system might behave when inputs are outside of the span of already-observed data. To satisfy such a purpose, extrapolation is unavoidable.\nStill, it is possible to adopt mathematical methods that mitigate the impact of extrapolation. It’s key, for instance, to avoid high-order polynomials, as these produce some of the most extreme and misleading behavior. (See ?sec-polynomials.) More reliable alternatives include using only low-order polynomials (as already described in Chapter 12), localized functions such as the sigmoid or gaussian, or splines (?sec-splines).\n\n\n\n\n\n\nExample: Extrapolating gravitation\n\n\n\nIsaac Newton famously developed his universal theory of gravitation by examining the orbits of planets. Planets close to the sun feel a strong tug of the Sun’s gravity, and farther-out planets have receive a weaker tug. Similarly, the motion of satellites around the Earth is determined mainly by the Earth’s gravitational pull. Per Newton’s theory, the acceleration due to gravity goes as the inverse-square of distance from the center of the Earth. Far-away satellites have long orbits, close-in satellites orbit with far lower orbital duration.\nPlanets orbit outside the Sun. Satellites orbit outside the Earth. Application of the inverse-square law to forces outside the Sun or outside the Earth are interpolation. But there are no observations of orbits or forces inside the Sun or Earth. So using the inverse-square law for inside forces is an extrapolation. This extrapolation produces catastrophically misleading models. In reality, the gravitational force on the inside—as you might imagine in a tunnel going through the center of the Earth—increases linearly with distance from the center. (Newton’s theory explains the mechanics of this.)",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling and the scientific method</span>"
    ]
  },
  {
    "objectID": "Modeling/16-modeling-scientific-method.html#sec-mechanism-vs-curve",
    "href": "Modeling/16-modeling-scientific-method.html#sec-mechanism-vs-curve",
    "title": "16  Modeling and the scientific method",
    "section": "16.6 Mechanism versus curve",
    "text": "16.6 Mechanism versus curve\nIt is important to distinguish between two basic types of model:\n\nEmpirical models which are rooted in observation and data.\nMechanistic models such as those created by applying fundamental laws of physics, chemistry, etc.\n\nWe will put off mechanistic models for a while, for two reasons. First, the “fundamental laws of physics, chemistry, and such” are often expressed with the concepts and methods of calculus. We are heading there, but at this point you don’t yet know the core concepts and methods of calculus. Second, most students don’t make a careful study of the “fundamental laws of physics, chemistry, and such” until after they have studied calculus. So examples of mechanistic models will be a bit hollow at this point.\nEmpirical models (sometimes deprecatingly called “curve fitting”) are common in many vital areas such as the social sciences and economics, psychology, clinical medicine, etc. A solid understanding of statistics greatly enhances the reliability of conclusions drawn from empirical models.\nIn addition to physical law, geometry provides many examples of mechanistic connections between elements. An example is the geometric calculation of the forward force on an arrow being pulled back by a bow’s draw.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling and the scientific method</span>"
    ]
  },
  {
    "objectID": "Modeling/16-modeling-scientific-method.html#sec-polishing-parameters",
    "href": "Modeling/16-modeling-scientific-method.html#sec-polishing-parameters",
    "title": "16  Modeling and the scientific method",
    "section": "16.7 Polishing parameters",
    "text": "16.7 Polishing parameters\nIn Section 11.6, we referred to a way to improve parameters in order to better match patterns in data: polishing. Polishing can be carried out by software such as fitModel(). It is usually worthwhile because it is easy and mostly automatic.\nPolishing becomes harmful when the modeler misinterprets it as increasing a model’s reliability. Polishing can add a digit or two to a model parameter. Still, those digits may be irrelevant from a statistical perspective and do not provide any warranty that the fitted function is an appropriate approximation to the real-world situation.\nAn analogy might help. Polishing your teeth might make you marginally more attractive, and can be part of good hygiene, but it will not make you a better person.\n\nApplication area 16.1 We look at attemps in the 1970s to predict when oil production and consumption will reach its peak.\n\n\n\n\n\n\n\nApplication area 16.1 Peak Oil?\n\n\n\nNowadays, everybody knows about the problems with oil consumption and climate change; bringing world oil consumption down to zero by 2050 is an often-stated goal of countries throughout the world. The basic theory of climate change is that increased CO\\(_2\\)_ from the consumption of fossil fuels causes warming via the greenhouse effect. (There are other greenhouse gasses as well such as methane.) The CO\\(_2\\)_ greenhouse phenomenon was first modeled by Svante Arrhenius in 1896. His model indicated that if atmospheric CO\\(_2\\)_ doubled, global temperature would rise by 5-6\\(^\\circ\\)C. As of today, atmospheric CO\\(_2\\)_ has increased by 50% from historic levels at the start of the industrial revolution. A simple linear interpolation of Arrhenius’ prediction would put global temperature increase at about 2.5\\(^\\circ\\)C. This is eerily close to the present-day target of an increase by 2 degrees.\nDespite Arrhenius’s prescience, widespread understanding of greenhouse gas effects emerged only around 1990. Up until then, the possibility of concern was that the world might run out of oil and freeze the economy. Policy makers attempted to use past data to predict future oil. These were entirely empirical models: curve fitting. The curve selected was sensible: a sigmoidal function. The exhaustion of oil corresponds to reaching the sigmoid’s upper horizontal asymptote is reach.\nFigure 16.5 shows cumulative oil consumption globally. The value for each year corresponds to all consumption from 1900 up to the given year. It’s revealing to look at a fitted curve using data up to 1973. (In 1973, the world’s oil economy started to change rapidly due to embargoes and oligopolistic behavior by leading oil producers.) The blue curve in Figure 16.5 shows a sigmoidal function as might have been fitted in 1973.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.5: The total cumulative amount of oil produced (units: Terra-watt-hours) globally for each year since 1900. Blue: a sigmoidal model fitted to the data up to 1973. Green: sigmoidal model fitted with data through 2023.\n\n\n\nSuch curve fitting is a fool’s errand for several reasons. Although it might be sensible to imagine cumulative oil production over time as a sigmoid, there is no indication in the data that the curve is leveling off. Nor is there good reason to assume that the mechanisms that shaped the oil economy of 1920 were still in action in 1973, let alone in 2023. Not only is the model an extrapolation, but it depends on the details of polishing of sigmoidal parameters. To illustrate this sensitivity, consider the green curve in Figure 16.5 which is fitted to the data through 2023. The observed data in 2023 is only about one-third the level of the 1973 prediction for 2023.\nNow consider a different purpose for such modeling. Suppose we take at face value international claims of a “net-zero” CO\\(_2\\) economy by 2050 and we want to anticipate the total cumulative effect as of that time. Draw a smooth curve continuing the oil consumption data up through 2100, with a leveling off around 2050. There is no precise way to do this, since there is no reason to think that the growth part of the sigmoidal curve will be the same as the leveling-off part. There is a range of plausible scenarios, but since we are already close or even at the target of a 1.5 degree increase in global temperature, it seems entirely possible that the horizontal asymptote will be above 2 or even 2.5 degrees.\nFortunately, unlike in 1973, there are now detailed mechanistic climate models relating CO\\(_2\\) to global temperatures. We will have to pay careful attention to their predictions as we head toward the goal of net-zero emissions.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling and the scientific method</span>"
    ]
  },
  {
    "objectID": "Modeling/08-parameters.html",
    "href": "Modeling/08-parameters.html",
    "title": "8  Parameters",
    "section": "",
    "text": "8.1 Matching numbers to quantities\nThe coordinate axes in Figure 8.1 represent quantities. On the horizontal axis is time, measured in days. The vertical axis is denominated in “10000 cases,” meaning that the numbers on the vertical scale should be multiplied by 10000 to get the number of cases.\nThe exponential function takes as input a pure number and produces an output that is also a pure number. This is true for all the pattern-book functions. Since the graph axes don’t show pure numbers, it is no surprise then that the pattern-book exponential function doesn’t align with the COVID case data.\nIf we want the input to the model function \\(\\text{cases}(t)\\) to be denominated in days, we will have to convert \\(t\\) to a pure pure number (e.g. 10, not “10 days”) before the quantity is handed off as the argument to \\(\\exp()\\). We do this by introducing a parameter.\nThe standard parameterization for the exponential function is \\(e^{kt}\\). The parameter \\(k\\) will be a quantity with units of “per-day.” Suppose we set \\(k=0.2\\) per day. Then \\(k\\, t{\\LARGE\\left.\\right|}_{t=10 days} = 2\\). This “2” is a pure number because the units on the 0.2 (“per day”) and on the 10 (days) cancel out: \\[0.2\\, \\text{day}^{-1} \\cdot 10\\, \\text{days} = 2\\ .\\] The use of a parameter like \\(k\\) does more than handle the formality of converting input quantities into pure numbers. Having a choice for \\(k\\) allows us to stretch or compress the function to align with the data. Figure 8.2 plots the modeling version of the exponential function to the COVID-case data:\nFigure 8.2: Using the function form \\(A e^{kt}\\) with parameters \\(k=0.19\\) per day and \\(A = 0.0573\\) cases (in 10000s) matches the COVID-case data well.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parameters</span>"
    ]
  },
  {
    "objectID": "Modeling/08-parameters.html#matching-numbers-to-quantities",
    "href": "Modeling/08-parameters.html#matching-numbers-to-quantities",
    "title": "8  Parameters",
    "section": "",
    "text": "Recall that pure numbers, like 17.32, do not have units. Quantities, on the other hand, usually do have units, as in 17.3 days or 34 meters.\n\nIn every case, these parameters are arranged to translate a with-units quantity into a pure number suitable as an input to the pattern-book function. Similarly, parameters will translate the pure-number output from the pattern-book function into a quantity with units.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parameters</span>"
    ]
  },
  {
    "objectID": "Modeling/08-parameters.html#parallel-scales",
    "href": "Modeling/08-parameters.html#parallel-scales",
    "title": "8  Parameters",
    "section": "8.2 Parallel scales",
    "text": "8.2 Parallel scales\nAt the heart of how we use the pattern-book functions to model the relationship between quantities is the idea of conversion between one scale and another. Consider these everyday objects: a thermometer and a ruler.\n\n\n\n\n\n\n\n\n\n\n\n(a) A thermometer\n\n\n\n\n\n\n\n\n\n\n\n(b) A ruler\n\n\n\n\n\n\n\nFigure 8.3: Two everyday objects that facilitate conversion from one scale to another.\n\n\n\nEach object presents a read-out of what’s being measured—temperature or length—on two different scales. At the same time, the objects provide a way to convert one scale to another.\nA function gives the output for any given input. We represent the input value as a position on a number line—which we call an “axis”—and the output as a position on another output line, almost always drawn perpendicular to one another. But the two number lines can just as well be parallel to one another. To evaluate the function, find the input value on the input scale and read off the corresponding output.\nWe can translate the correspondance between one scale and the other into the form of a straight-line function. For instance, if we know the temperature in Fahrenheit (\\(^\\circ\\)F) and want to convert it to Celsius (\\(^\\circ C\\)) we have the following function: \\[C(F) \\equiv {\\small\\frac{5}{9}}(F-32)\\ .\\] Similarly, converting inches to centimeters can be accomplished with \\[\\text{cm(inches)} \\equiv 2.54 \\, (\\text{inches}-0)\\ .\\] Both of these scale conversion functions have the form of the straight-line function, which can be written as \\[f(x) \\equiv a x + b\\ \\ \\ \\text{or, equivalently as}\\ \\ \\ \\ f(x) \\equiv a(x-x_0)\\ ,\\] where \\(a\\), \\(b\\), and \\(x_0\\) are parameters.\nIn Section 8.3, we will use the \\(ax + b\\) form of scale conversion, to scale the input to pattern-book functions, but we could equally well have used \\(a(x-x_0)\\).\nIn Section 8.4 we will introduce a second scale conversion function, for the output from pattern-book functions. That scaling will also be in the form of a straight-line function: \\(A x + B\\). The use of the lower-case parameter names (\\(a\\), \\(b\\)) versus the upper-case parameter names (\\(A\\), \\(B\\)) will help us distinguish the two different uses for scale conversion, namely input scaling versus output scaling.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parameters</span>"
    ]
  },
  {
    "objectID": "Modeling/08-parameters.html#sec-input-scaling",
    "href": "Modeling/08-parameters.html#sec-input-scaling",
    "title": "8  Parameters",
    "section": "8.3 Input scaling",
    "text": "8.3 Input scaling\nFigure 8.4 is based on the data frame RI-tide, a minute-by-minute record of the tide level in Providence, Rhode Island (USA) for the period April 1 to 5, 2010. The level variable is measured in meters; the hour variable gives the time of the measurement in hours after midnight at the start of April 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.4: Tide levels oscillate up and down over time. This is analogous to the \\(\\\\sin(t)\\) pattern-book function.\n\n\n\nThe pattern-book \\(\\sin()\\) and the function \\(\\color{magenta}{\\text{level}}\\color{blue}{(hour)}\\) have similar shapes, so it seems reasonable to model the tide data as a sinusoid. However, the scale of the axes is different on the two graphs.\nTo model the tide with a sinusoid, we need to modify the sinusoid to change the scale of the input and output. First, let’s look at how to accomplish the input scaling. Specifically, we want the pure-number input \\(t\\) to the sinusoid be a function of the quantity \\(hour\\). Our framework for this re-scaling is the straight-line function. We will replace the pattern-book input \\(t\\) with a function \\[t(\\color{blue}{hour}) \\equiv a\\, \\color{blue}{hour} + b\\ .\\]\nThe challenge is to find values for the parameters \\(a\\) and \\(b\\) that will transform the \\(\\color{blue}{\\mathbf{\\text{blue}}}\\) horizontal axis into the black horizontal axis, like this:\n\n\n\n\n\n\n\n\n\nBy comparing the two axes, we can estimate that \\(\\color{blue}{10} \\rightarrow 4\\) and \\(\\color{blue}{100} \\rightarrow 49\\). With these two coordinate points, we can find the straight-line function that turns \\(\\color{blue}{\\mathbf{\\text{blue}}}\\) into black by plotting the coordinate pairs \\((\\color{blue}{0},1)\\) and \\((\\color{blue}{100}, 51)\\) and finding the straight-line function that connects the points.\n\n\n\n\n\n\n\n\nFigure 8.5: The input scaling function must transform 10 into 4 and transform 100 into 49 to properly arrange the time scale with the scale for the pattern-book function.\n\n\n\n\n\nYou can calculate for yourself that the function that relates \\(\\color{blue}{\\mathbf{\\text{blue}}}\\) to black is \\[t(\\color{blue}{time}) = \\underbrace{\\frac{1}{2}}_a \\color{blue}{time}  \\underbrace{-1\\LARGE\\strut}_b\\]\nReplacing the pure number \\(t\\) as the input to pattern-book \\(\\sin(t)\\) with the transformed \\(\\frac{1}{2} \\color{blue}{time}\\) we get a new function: \\[g(\\color{blue}{time}) \\equiv \\sin\\left(\\strut {\\small\\frac{1}{2}}\\color{blue}{time} - 1\\right)\\ .\\] Figure 11.6 plots \\(g()\\) along with the actual tide data.\n\n\n\n\n\n\n\n\nFigure 8.6: The sinusoid with input scaling (black) aligns nicely with the tide-level data.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parameters</span>"
    ]
  },
  {
    "objectID": "Modeling/08-parameters.html#sec-output-scaling",
    "href": "Modeling/08-parameters.html#sec-output-scaling",
    "title": "8  Parameters",
    "section": "8.4 Output scaling",
    "text": "8.4 Output scaling\nJust as the natural input needs to be scaled before it reaches the pattern-book function, so the output from the pattern-book function needs to be scaled before it presents a result suited for interpreting in the real world.\n\n\n\n\n\n\nFigure 8.7: Natural quantities must be scaled to pure numbers before being suited to the pattern-book functions. The output from the pattern-book function is a pure number which is scaled to the natural quantity of interest.\n\n\n\nThe overall result of input and output scaling is to tailor the pattern-book function so that it is ready to be used in the real world.\nLet’s return to Figure 11.6 which shows that the function \\(g(\\color{blue}{time})\\), which scales the input to the pattern-book sinusoid, has a much better alignment to the tide data. Still, the vertical axes of the two graphs in the figure are not the same.\nThis is the job for output scaling, which takes the output of \\(g(\\color{blue}{time})\\) (bottom graph) and scales it to match the \\(\\color{magenta}{level}\\) axis on the top graph. That is, we seek to align the black vertical scale with the \\(\\color{magenta}{\\mathbf{\\text{magenta}}}\\) vertical scale. To do this, we note that the range of the \\(g(\\color{blue}{time})\\) is -1 to 1, whereas the range of the tide-level is about 0.5 to 1.5. The output scaling will take the straight-line form \\[{\\color{magenta}{\\text{level}}}({\\color{blue}{time}}) = A\\, g({\\color{blue}{time}}) + B\\] or, in graphical terms\n\n\n\n\n\n\n\n\n\nWe can figure out parameters \\(A\\) and \\(B\\) by finding the straight-line function that connects the coordinate pairs \\((-1, \\color{magenta}{0.5})\\) and \\((1, \\color{magenta}{1.5})\\) as in Figure 8.8.\n\n\n\n\n\n\n\n\nFigure 8.8: Finding the straight-line function that converts \\(-1 \\rightarrow \\color{magenta}{0.5}\\) and converts \\(1 \\rightarrow \\color{magenta}{1.5}\\)\n\n\n\n\n\nYou can confirm for yourself that the function that does the job is \\[{\\color{magenta}{\\text{level}}} = 0.5 g({\\color{blue}{time}}) + 1\\ .\\]\nPutting everything together, that is, scaling both the input to pattern-book \\(\\sin()\\) and the output from pattern-book \\(\\sin()\\), we get\n\\[{\\color{magenta}{\\text{level}}}({\\color{blue}{time}}) = \\underbrace{0.5}_A \\sin\\left(\\underbrace{\\small\\frac{1}{2}}_a {\\color{blue}{time}}  \\underbrace{-1}_b\\right) + \\underbrace{1}_B\\]",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parameters</span>"
    ]
  },
  {
    "objectID": "Modeling/08-parameters.html#a-procedure-for-building-models",
    "href": "Modeling/08-parameters.html#a-procedure-for-building-models",
    "title": "8  Parameters",
    "section": "8.5 A procedure for building models",
    "text": "8.5 A procedure for building models\nWe’ve been using pattern-book functions as the intermediaries between input scaling and output scaling, using this format.\n\\[f(x) \\equiv A e^{ax + b} + B\\ .\\] We can use the other pattern-book functions—the gaussian, the sigmoid, the logarithm, the power-law functions—in the same way. That is, the basic framework for modeling is this:\n\\[\\text{model}(x) \\equiv A\\, {g_{pattern\\_book}}(ax + b) + B\\ ,\\] where \\(g_{pattern\\_book}()\\) is one of the pattern-book functions. To construct a basic model, you task has two parts:\n\nPick the specific pattern-book function whose shape resembles that of the relationship you are trying to model. For instance, we picked \\(e^x\\) for modeling COVID cases versus time (at the start of the pandemic). We picked \\(\\sin(x)\\) for modeling tide levels versus time.\nFind numerical values for the parameters \\(A\\), \\(B\\), \\(a\\), and \\(b\\). In Chapter 11 shows some ways to make this part of the task easier.\n\nIt is remarkable that models of a very wide range of real-world relationships between pairs of quantities can be constructed by picking one of a handful of functions, then scaling the input and the output. As we move on to other Blocks in MOSAIC Calculus, you will see how to generalize this to potentially complicated relationships among more than two quantities. That is a big part of the reason you’re studying calculus.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parameters</span>"
    ]
  },
  {
    "objectID": "Modeling/08-parameters.html#other-formats-for-scaling",
    "href": "Modeling/08-parameters.html#other-formats-for-scaling",
    "title": "8  Parameters",
    "section": "8.6 Other formats for scaling",
    "text": "8.6 Other formats for scaling\nOften, modelers choose to use input scaling in the form \\(a (x - x_0)\\) rather than \\(a x + b\\). The two are completely equivalent when \\(x_0 = - b/a\\). The choice between the two forms is largely a matter of convention. But almost always the output scaling is written in the format \\(A y + B\\).\n\nApplication area 8.1 —Shifting an exponential horizontally is the same as scaling vertically.\n\n\n\n\n\n\n\nApplication area 8.1 The start of COVID\n\n\n\nFor the COVID case-number data shown in Figure 8.2, we found that a reasonable match to the data can be had by input- and output-scaling the exponential: \\[\\text{cases}(t) \\equiv  \\underbrace{573}_A e^{\\underbrace{0.19}_a\\ t}\\ .\\]\nYou might wonder why the parameters \\(B\\) and \\(b\\) aren’t included in the model. One reason is that cases and the exponential function already have the same range: zero and upwards. So there is no need to shift the output with a parameter B.\nAnother reason has to do with the algebraic properties of the exponential function. Specifically, \\[e^{a x + b}= e^b e^{ax} = {\\cal A} e^{ax}\\] where \\({\\cal A} \\equiv e^b\\).\nIn the case of exponentials, writing the input scaling in the form \\(e^{a(x-x_0)}\\) can provide additional insight.\nA bit of symbolic manipulation of the model can provide some additional insight. As you know, the properties of exponentials and logarithms are such that \\[A e^{at} = e^{\\log(A)} e^{at} = e^{a t + \\log(A)} = e^{a\\left(\\strut t + \\log(A)/a\\right)} = e^{a(t-t_0)}\\ ,\\] where \\[t_0 = - \\log(A)/a = - \\log(593)/0.19 = -33.6\\ .\\] You can interpret \\(t_0\\) as the starting point of the pandemic. When \\(t = t_0\\), the model output is \\(e^{k 0} = 1\\): the first case. According to the parameters we matched to the data for March, the pandemic’s first case would have happened about 33 days before March 1, which is late January. We know from other sources of information, the outbreak began in late January. It is remarkable that even though the curve was constructed without any data from January or even February, the data from March, translated through the curve-fitting process, pointed to the start of the outbreak. This is a good indication that the exponential form for the model is fundamentally correct.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parameters</span>"
    ]
  },
  {
    "objectID": "Modeling/08-parameters.html#parameterization-conventions",
    "href": "Modeling/08-parameters.html#parameterization-conventions",
    "title": "8  Parameters",
    "section": "8.7 Parameterization conventions",
    "text": "8.7 Parameterization conventions\nThere are conventions for the symbols used for input-scaling parameterization of the pattern-book functions. Knowing these conventions makes it easier to read and assimilate mathematical formulas. In several cases, there is more than one conventional option. For instance, the sinusoid has a variety of parameterization forms that get used depending on which feature of the function is easiest to measure. Table 8.1 list several that are frequently used in practice.\n\n\n\nTable 8.1: Some standard forms of input scaling parameterizations\n\n\n\n\n\n\n\n\n\n\n\nFunction\nWritten form\nParameter 1\nParameter 2\n\n\n\n\nExponential\n\\(e^{kt}\\)\n\\(k\\)\nNot used\n\n\nExponential\n\\(e^{t/\\tau}\\)\n\\(\\tau\\) “time constant”\nNot used\n\n\nExponential\n\\(2^{t/\\tau_2}\\)\n\\(\\tau_2\\) “doubling time”\nNot used\n\n\nExponential\n\\(2^{-\\tau_{1/2}}\\)\n\\(-\\tau_{1/2}\\) “half life”\nNot used\n\n\nPower-law\n\\([x - x_0]^p\\)\n\\(x_0\\) x-intercept\nexponent\n\n\nSinusoid\n\\(\\sin\\left(\\frac{2 \\pi}{P} (t-t_0)\\right)\\)\n\\(P\\) “period”\n\\(t_0\\) “time shift”\n\n\nSinusoid\n\\(\\sin(\\omega t + \\phi)\\)\n\\(\\omega\\) “angular frequency”\n\\(\\phi\\) “phase shift”\n\n\nSinusoid\n\\(\\sin(2 \\pi \\omega t + \\phi)\\)\n\\(\\omega\\) “frequency”\n\\(\\phi\\) “phase shift”\n\n\nGaussian\ndnorm(x, mean, sd)\n“mean” (center)\nsd “standard deviation”\n\n\nSigmoid\npnorm(x, mean, sd)\n“mean” (center)\nsd “standard deviation”\n\n\nStraight-line\n\\(mx + b\\)\n\\(m\\) “slope”\n\\(b\\) “y-intercept”\n\n\nStraight-line\n\\(m (x-x_0)\\)\n\\(m\\) “slope”\n\\(x_0\\) “center”",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parameters</span>"
    ]
  },
  {
    "objectID": "Modeling/09-assembling-functions.html",
    "href": "Modeling/09-assembling-functions.html",
    "title": "9  Assembling functions",
    "section": "",
    "text": "9.1 Linear combination\nSection 8.4 introduced the notation of input and output scaling. For instance, Figure 11.6 illustrates how the pattern-book \\(\\sin()\\) function can be customized to model a particular situation. Recall that the input scaling involves inserting a straight-line function inside the parentheses, as in\n\\[f_1(t) \\equiv \\sin\\left( a t + b\\right) \\tag{9.1}\\]\nYou may recognize \\(at + b\\) as a straight-line function. Possibly, your recognition would be easier if we wrote \\(at + b\\) using different names for the input and the first parameter: \\(m x + b\\). You have been trained to pronounce \\(m\\) as the “slope” of the line and \\(b\\) as the “y-intercept.”\nUsing both input and output scaling gives a more general kind of function:\n\\[f_2(t) \\equiv A \\sin(a t + b) + B \\tag{9.2}\\]\nMath expression 9.2 is an example of a linear combination of two functions. The two functions are \\(g_1(t) \\equiv 1\\) and \\(g_2(t) \\equiv sin(a t + b)\\).\nIt might be easier to see Math expression 9.2 as a linear combination if the function were written explicitly using the two functions being combined, that is, as\n\\[f_2 \\equiv A g_2(t) + B g_1(t)\\]\nThe combination is made by scaling each of the functions involved then adding the scaled functions together. The two scaling factors, \\(A\\) and \\(B\\), could be called “parameters of \\(f_2()\\),” which indeed they are. It would be good to get used to another word that’s used specifically for the parameters in a linear combination: “coefficients.” The advantage of “coefficients” as a name lies in it marking the parameters as those involved in a linear combination, rather than any of the other ways parameters can be used.\nNote that “coefficients” always refers to parameters that are not inside the parentheses of a function. In contrast, often parameters are inside the parentheses as in Math expression 9.1. The parameters \\(a\\) and \\(b\\) in \\(\\sin(a t + b)\\) are inside the parentheses. Consequently, \\(a\\) and \\(b\\) shouldn’t be called “coefficients.” Sometimes, to emphasize this, parameters in parentheses are called “nonlinear parameters” to distinguish them from coefficients like \\(A\\) and \\(B\\).\nTo illustrate how linear combination is used to create new functions, consider polynomials, for instance, \\[f(x) \\equiv 3 x^2 + 5 x - 2\\ . \\tag{9.3}\\] In high school, polynomials are often presented as puzzles—factor them to find the roots! In calculus, however, polynomials are used as functions for modeling. They are a kind of modeling “clay,” which can be shaped as needed.\nThere are three pattern-book functions in Math expression 9.3. In polynomials the functions being combined are all power-law functions: \\(g_0(x) \\equiv 1\\), \\(g_1(x) \\equiv x\\), and \\(g_2(x) \\equiv x^2\\). With these functions defined, we can write the polynomial \\(f(x)\\) as \\[f(x) \\equiv 3 g_2(x) + 5 g_1(x) - 2 g_0(x)\\] Each of the functions is being scaled by a quantity: 3, 5, and -2 respectively. Then the scaled functions are added up. That is a linear combination; scale and add.\nLinear combination is an extremely important tactic that quantitative workers use throughout their careers. For instance, many physical systems are described by linear combinations. For instance, the motion of a vibrating molecule, a helicopter in flight, or a building shaken by an earthquake are described in terms of simple “modes” which are linearly combined to make up the entire motion. More down to Earth, the timbre of a musical instrument is set by the scalars in a linear combination of pure tones. And throughout work with data in science, commerce, government and other fields a primary data analysis method—called “regression”—is about finding the best linear combination of a set of explanatory variables to create a model function of the response variable.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assembling functions</span>"
    ]
  },
  {
    "objectID": "Modeling/09-assembling-functions.html#linear-combination",
    "href": "Modeling/09-assembling-functions.html#linear-combination",
    "title": "9  Assembling functions",
    "section": "",
    "text": "Identical vs proportional vs straight-line\n\n\n\nOne of the pattern book functions is very simple; the output is identical to the input:\n\\[\\text{identity}(x) \\equiv x\\] Multiplying the identity() function by a parameter gives a function that can well be called “proportional().”\n\\[\\text{proportional}(x) \\equiv a\\ \\text{identity}(x) = a\\ x\\] The parameter \\(a\\) is often called the “constant of proportionality.”\nIt’s common to call this closely related function the “linear” function, but a better name is the “straight-line” function. “Straight-line” is the name we shall use in this book.\n\\[\\text{straight\\_line}(x) \\equiv a\\ x + b\\]",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assembling functions</span>"
    ]
  },
  {
    "objectID": "Modeling/09-assembling-functions.html#sec-function-composition",
    "href": "Modeling/09-assembling-functions.html#sec-function-composition",
    "title": "9  Assembling functions",
    "section": "9.2 Function composition",
    "text": "9.2 Function composition\nFunction composition refers to combining functions by taking the input of one function and feeding it as input to another. “\\(g()\\) composed with \\(h()\\)” means \\(g(h(x))\\).\nTo illustrate, consider again the function defined in Math expression 9.2: \\[f(t) \\equiv A \\sin\\left( a t + b\\right) + B\\]\nYou’ve already seen how Math expression 9.2 is a linear combination of two functions \\(f_1(t) \\equiv 1\\) and \\(f_2(t) \\equiv sin(a t + b)\\). But \\(f_2()\\) is not a linear combination. Instead, it is a function composition. The two functions being composed are \\(sin(x)\\) and \\(a t + b\\), producing \\(sin(a t + b)\\). Here, \\(\\sin()\\) is the outer function in the composition and \\(at + b\\) is the inner function.\nIn function composition, the order of the functions matters: \\(f(g(x))\\) and \\(g(f(x))\\) are in general completely different functions.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assembling functions</span>"
    ]
  },
  {
    "objectID": "Modeling/09-assembling-functions.html#sec-function-multiplication",
    "href": "Modeling/09-assembling-functions.html#sec-function-multiplication",
    "title": "9  Assembling functions",
    "section": "9.3 Function multiplication",
    "text": "9.3 Function multiplication\nMultiplication is the third in our repertoire of methods for making new functions. To review, here are the first two methods involving combining a function \\(f()\\) with a function $g():\n\nLinear combinations. Example: \\(n_1(t) \\equiv 5 f(t) + 1.2 g(t)\\).\nFunction composition. There are two possibilities which produce two distinct functions:\n\n\\(c_1(t) \\equiv f(g(t))\\), that is, \\(g()\\) is the inner function.\n\\(c_2(t) \\equiv g(f(t))\\), that is, \\(g()\\) is the outer function.\n\n\nNow a new method:\n\nMultiplication of the output of two functions. Example: \\(m(t)\\equiv f(t) g(t)\\). This is perfectly ordinary multiplication. Multiplication is commutative, as illustrated by the equality of \\(5 \\times 2\\) and \\(2 \\times 5\\). Owing to the commutativity of multiplication, it doesn’t matter whether \\(f()\\) is first or \\(g()\\) is first.\n\n\\[\\underbrace{f(x) \\times g(x)}_\\text{multiplication}\\ \\ \\ \\ \\underbrace{{\\Large f(}g(x){\\Large)} \\ \\ \\text{or}\\ \\ \\ {\\Large g(}f(x){\\Large)}}_\\text{composition}\\]\nIn function composition, only one of the functions—the interior function is applied to the overall input, \\(x\\) in the above example. The exterior function is fed its input from the output of the interior function.\nIn multiplication, each of the functions is applied to the input individually. Then their outputs are multiplied to produce the overall output.\n\nApplication area 9.1 —What goes up may come down.\n\n\n\n\n\n\n\nApplication area 9.1 Modeling rise and fall\n\n\n\nThe initial rise in popularity of the social media platform Yik Yak was exponential. Then popularity leveled off, promising a steady, if static, business into the future. But, the internet being what it is, popularity collapsed to near zero and the company closed.\nOne way to model this pattern is by multiplying a sigmoid by an exponential. (See Figure 9.1.)\n\n\n\n\n\n\n\n\n\nA sigmoid (orange) and an exponential function, shifted to be centered in mid-2014\n\n\n\n\n\n\n\n\n\nMultiplying the sigmoid and exponential produces a hump.\n\n\n\n\n\n\n\nFigure 9.1: Building a model of a steep rise and gentler fall by multiplying a sigmoid by an exponential. Subscriptions to the web messaging service Yik Yak grew exponentially in 2013 and 2014, then collapsed. The company closed in 2017.\n\n\n\n\n\n\nApplication area 9.2 —Sounds of short duration can be modeled by multiplying sines by a local function such as the gaussian.\n\n\n\n\n\n\n\nApplication area 9.2 Transient vibration\n\n\n\nA guitar string is plucked to produce a note. The sound is, of course, vibrations of the air created by vibrations of the string.\nAfter plucking, the note fades away. An simple model of this is a sinusoid (of the correct period to correspond to the frequency of the note) times a gaussian.\n\n\n\n\n\n\n\n\n\nThe two components of the wave packet: an envelope and an oscillation.\n\n\n\n\n\n\n\n\n\nThe wave packet constructed by multiplication\n\n\n\n\n\n\n\nFigure 9.2: A wave packet constructed by multiplying a sinusoid and a gaussian function.\n\n\n\n\n\nFunction multiplication is used so often in modeling that you will see it in many modeling situations. Here’s one example that is important in physics and communication: the wave packet. Overall, the wave packet is a localized oscillation as in Figure 9.2. The packet can be modeled with the product of two pattern-book functions: a gaussian times a sinusoid.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assembling functions</span>"
    ]
  },
  {
    "objectID": "Modeling/09-assembling-functions.html#sec-piecewise-intro",
    "href": "Modeling/09-assembling-functions.html#sec-piecewise-intro",
    "title": "9  Assembling functions",
    "section": "9.4 Splitting the domain",
    "text": "9.4 Splitting the domain\nConsider the familiar absolute-value function, written \\[\\text{abs}(x) \\equiv \\left|x\\right|\\] in traditional notation. Written this way, the definition of abs() is a tautology: unless you already know what \\(\\left|x\\right|\\) means, you will have no clue what’s going on.\nCan we assemble abs() out of pattern-book functions? What’s distinctive about \\(abs(x)\\) is the break at \\(x=0\\). There is no similarly sharp transition in any of the pattern-book functions.\nDefining a function piecewise, that is, constructing it from two different functions on two non-overlapping domains, provides the means to create a sharp transition. For the absolute value function, one domain is the negative inputs: \\(x &lt; 0\\). The other domain is the positive inputs: \\(0 \\leq x\\). Note that the domains do not overlap.\n\n\nThe absolute value function has two pieces, one to the left of input 0 for the function \\(-x\\), the other to the right of 0 for the function \\(x\\). The two pieces meet at input 0. \\[\\text{abs}(x) \\equiv \\left\\{\n\\begin{array}{rl}  x & \\text{for}\\ 0 \\leq x \\\\\n-x & \\text{otherwise}\\\\\\end{array}\n\\right.\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.3: The absolute value function\n\n\n\n\n\nA function defined separately on different pieces of its domain is called a piecewise function. In the conventional mathematical notation, there is a large \\(\\LARGE\\left\\{\\right.\\) followed by two or more lines. Each line gives a formula for that part of the function and indicates to which interval the formula applies.\n\n\nAnother piecewise function widely used in technical work, but not as familiar as \\(abs()\\) is the Heaviside function, which has important uses in physics and engineering. \\[\\text{Heaviside}(x) \\equiv \\left\\{\n\\begin{array}{cl} 1 & \\text{for}\\ 0 \\leq x \\\\0 & \\text{otherwise}\\end{array}\n\\right.\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.4: The Heaviside function\n\n\n\n\n\nThe Heaviside function is defined on the same two pieces of the number line as \\(abs()\\). To the right of zero, Heaviside is identical to constant(). To the left, it is identical to 0 times constant\\(()\\).\nThe vertical gap between the two pieces of the Heaviside function is called a discontinuity. Intuitively, you cannot draw a discontinuous function without lifting the pencil from the paper. The Heaviside’s discontinuity occurs at input \\(x=0\\).\n\n\nThe ramp function is closely related to the Heaviside function. The ramp has output 0 when the input is negative. For positive inputs, the ramp is the identity function. \\[\\text{ramp}(x) \\equiv \\left\\{\n\\begin{array}{rl}  x & \\text{for}\\ 0 \\leq x \\\\\n0 & \\text{otherwise}\\\\\\end{array}\n\\right.\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.5: The ramp function\n\n\n\n\n\n\n9.4.1 Computing notation\nThe usual mathematical notation for piecewise functions, spread out over multiple lines that are connected with a tall brace, is an obvious non-candidate for computer notation. In R, the stitching together of the two pieces can be done with the function ifelse(). The name is remarkably descriptive. The ifelse() function takes three arguments. The first is a question to be asked, the second is the value to return if the answer is “yes,” and the third is the value to return for a “no” answer.\nTo define abs() or Heaviside() the relevant question is, “Is the input on the right or left side of zero on the number line?” In widely-used computing languages such as R, the format for asking a question does not involve a question mark. For example, to ask the question, “Is 3 less than 2?” use the expression:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn mathematics notation, \\(3 &lt; 2\\) is a declarative statement and is an impossibility. More familiar would be \\(x &lt; 2\\), which is again a declarative statement putting a restriction on the possible values of the quantity \\(x\\).\nIn computing notation, 3 &lt; 2 or x &lt; 2 is not a declaration, it is an imperative statement that directs the computer to do the calculation to find out if the statement is true or false for the particular values given.\nHere’s a definition of Heaviside() written with ifelse().\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nSince Heaviside() is a function, the question \\(0 \\leq x\\) does not get answered at the time the function is defined. Instead, when the function is evaluated we will know the value of input \\(x\\), can answer the \\(0 \\leq x\\) question, and use the answer to select either 1 or 0 as the output.\nTable 9.1 shows computer notation for some common sorts of questions.\n\n\n\nTable 9.1: Each of these imperative statements in R asks a question about numbers.\n\n\n\n\n\n\n\n\n\nR notation\nEnglish\n\n\n\n\nx &gt; 2\n“Is \\(x\\) greater than 2?”\n\n\ny &gt;= 3\n“Is \\(y\\) greater than or equal to 3?”\n\n\nx == 4\n“Is \\(x\\) exactly 4?”\n\n\n2 &lt; x & x &lt; 5\n“Is \\(x\\) between 2 and 5?” Literally, “Is \\(x\\) both greater than 2 and less than 5?”\n\n\nx &lt; 2 | x &gt; 6\n“Is \\(x\\) either less than 2 or greater than 6?”\n\n\nabs(x-5) &lt; 2\n“Is \\(x\\) within two units of 5?”\n\n\n\n\n\n\n\nApplication area 9.3 —Two uses of gas—heating and cooking—call for a piecewise function.\n\n\n\n\n\n\n\nApplication area 9.3 Heating with gas\n\n\n\nFigure 9.6 is a graph of monthly natural gas use in the author’s household versus average temperature during the month. (Natural gas is measured in cubic feet, abbreviated ccf.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.6: The amount of natural gas used for heating the author’s home varies with the outside temperature.\n\n\n\nThe graph looks somewhat like a hockey stick. A sloping straight-line dependence of ccf on temperature for temperatures below \\(60^\\circ\\)F and constant for higher temperatures. The shape originates from the dual uses of natural gas. Gas is used for cooking and domestic hot water, the demand for which is more or less independent of outdoor temperature at about 15 ccf per month. Gas is also used for heating the house, but that is needed only when the temperature is less than about \\(60^\\circ\\)F.\nWe can accomplish the hockey-stick shape with a linear combination of the ramp() function and a constant. The ramp function represents gas used for heating, the constant is the other uses of gas (which are modeled as not depending on temperature. Overall, the model is \\[\\text{gas}(x) \\equiv 4.3\\,  \\text{ramp}(62-x)  + 15\\ .\\]\nNote that the input to ramp() is 62 - \\(x\\). This input scaling (Section 8.3) turns the ramp around so that it rises to the left. The transition between the flat and ramp sections occurs at \\(x=62\\).",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assembling functions</span>"
    ]
  },
  {
    "objectID": "Modeling/09-assembling-functions.html#computing-outside-the-domain",
    "href": "Modeling/09-assembling-functions.html#computing-outside-the-domain",
    "title": "9  Assembling functions",
    "section": "9.5 Computing outside the domain",
    "text": "9.5 Computing outside the domain\nEach of our pattern-book functions, with two exceptions, has a domain that is the entire number line \\(-\\infty &lt; x &lt; \\infty\\). No matter how big or small is the value of the input, the function has an output. Such functions are particularly nice to work with since we never have to worry about the input going out of bounds.\nThe two exceptions are:\n\nthe logarithm function, which is defined only for \\(0 &lt; x\\).\nsome of the power-law functions: \\(x^p\\).\n\nWhen \\(p\\) is negative, the output of the function is undefined when \\(x=0\\). You can see why with a simple example: \\(g(x) \\equiv x^{-2}\\). Most students had it drilled into them that “division by zero is illegal,” and \\(g(0) = \\frac{1}{0} \\frac{1}{0}\\), a double law breaker.\nWhen \\(p\\) is not an integer, that is \\(p \\neq 1, 2, 3, \\cdots\\) the domain of the power-law function does not include negative inputs. To see why, consider the function \\(h(x) \\equiv x^{1/3}\\).\n\n\n\n\n\n\n\n\nDivision by zero on the computer\n\n\n\nIt can be tedious to make sure that you are on the right side of the law when dealing with functions whose domain is not the whole number line. The designers of the hardware that does computer arithmetic, after several decades of work, found a clever system to make it easier. It is a standard part of such hardware that whenever a function is handed an input that is not part of that function’s domain, one of two special “numbers” is returned. To illustrate:\n\nsqrt(-3)\n## [1] NaN\n(-2)^0.9999\n## [1] NaN\n1/0\n## [1] Inf\n\nNaN stands for “not a number.” Just about any calculation involving NaN will generate NaN as a result, even those involving multiplication by zero or cancellation by subtraction or division.1 For instance:\n\n0 * NaN\n## [1] NaN\nNaN - NaN\n## [1] NaN\nNaN / NaN\n## [1] NaN\n\nDivision by zero produces Inf, whose name is reminiscent of “infinity.” Inf infiltrates any calculation in which it takes part:\n\n3 * Inf\n## [1] Inf\nsqrt(Inf)\n## [1] Inf\n0 * Inf\n## [1] NaN\nInf + Inf\n## [1] Inf\nInf - Inf\n## [1] NaN\n1/Inf\n## [1] 0\n\nTo see the benefits of the NaN / Inf system let’s plot out the logarithm function over the graphics domain \\(-5 \\leq x \\leq 5\\). Of course, part of that graphics domain, \\(-5 \\leq x \\leq 0\\) is not in the domain of the logarithm function and the computer is entitled to give us a slap on the wrists. The NaN provides some room for politeness.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assembling functions</span>"
    ]
  },
  {
    "objectID": "Modeling/09-assembling-functions.html#footnotes",
    "href": "Modeling/09-assembling-functions.html#footnotes",
    "title": "9  Assembling functions",
    "section": "",
    "text": "One that does produce a number is NaN^0.↩︎",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assembling functions</span>"
    ]
  },
  {
    "objectID": "Modeling/10-functions-with-multiple-inputs.html",
    "href": "Modeling/10-functions-with-multiple-inputs.html",
    "title": "10  Functions with multiple inputs",
    "section": "",
    "text": "10.1 Linear combinations\nHousing prices are determined by several (or many!) factors. Translating the previous sentence into the language of functions, we can say that the price is a function of multiple inputs. Plausible inputs to the function include the amount of living area and the number of bedrooms and bathrooms. The inputs may also include quality of the neighborhood, length of commute, and so on.\nOften, the starting point for building a function with multiple inputs is a data frame whose variables include the function output (say, price) and the various inputs to the function.\nA large fraction of work in the data-oriented quantitative sciences uses just the identity function in the linear combination. Recall that the identify function merely echos as output whatever input is provided, that is, \\(\\text{identity}(x) \\equiv x\\).\nThere is no point in constructing a linear combination of identity functions that take the same input. For example, the linear combination\n\\[4.3\\ \\text{identity}(x) + 1.7\\ \\text{identity}(x) - 2.6\\ \\text{identity}(x)\\]\nis merely a long-winded way of saying \\(3.4\\ \\text{identity}(x)\\).\nWhere a linear combination of identity functions becomes useful is when the inputs to the various functions are different, for example, consider this function of three inputs, \\(x\\), \\(y\\), and \\(z\\).\n\\[4.3\\ \\text{identity}(x) + 1.7\\ \\text{identity}(y) - 2.6\\ \\text{identity}(z)\\]\nBy convention, we rarely write \\(\\text{identity}()\\) preferring instead just to write the name of the input, as with\n\\[4.3\\ x + 1.7\\ y - 2.6\\ z \\tag{10.3}\\]\nEven though there are no parentheses used in Math expression 10.3, it is still a linear combination of functions. Thinking of it this way prompts consideration of what other, non-identity function might have been included, for instance, \\(x^2\\) or \\(x\\times y\\).\nTo illustrate the use of linear combinations of identity functions (with different inputs), consider the following model of house prices:\n\\(\\text{price}(\\text{livingArea}, \\text{bedrooms}, \\text{bathrooms}) \\equiv\\)\n\\[\\ \\ \\ 21000 + 105\\,\\text{livingArea} - 13000\\, \\text{bedrooms} + 26000\\, \\text{bathrooms}\\]\nwhere each of the coefficients is in units of dollars. These coefficients are based on data in the SaratogaHouses data frame which records the sales price of 1728 houses in Saratoga County, New York, USA, in 2006\nThe model function is a simple linear combination, but it effectively quantifies how different aspects of a house contribute to its sales price. The model indicates that an additional square foot of living area is worth about 105 dollars per foot2. An extra bathroom is worth about $25,000. Bedrooms, strangely, are assigned a negative value by the model.\nPossibly you already understand what is meant by “an additional square foot” or “an extra bathroom.” These ideas can be intuitive, but they can be best understood with a grounding in calculus, which we turn to in Block II. For instance, the negative scalar on bedrooms will make sense when you understand “partial derivatives,” the subject of Chapter 25.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Functions with multiple inputs</span>"
    ]
  },
  {
    "objectID": "Modeling/10-functions-with-multiple-inputs.html#function-multiplication-fx-times-gt",
    "href": "Modeling/10-functions-with-multiple-inputs.html#function-multiplication-fx-times-gt",
    "title": "10  Functions with multiple inputs",
    "section": "10.2 Function multiplication: f(x) times g(t)",
    "text": "10.2 Function multiplication: f(x) times g(t)\nWhen a guitar string is at rest it forms a straight line connecting its two fixed ends: one set by finger pressure along the neck of the guitar and the other at the bridge near the center of the guitar body. When the string is plucked, its oscillations follow a sinusoid pattern of displacement. With the right camera and lighting setup, we can see these oscillations in action:\n\n\n\n\n\n\nVideo 10.1: The displacement of each guitar string is a function of position along the string and time. Figure 10.2 shows a single frame from the video, making it easier to see the string’s displacement as a function of position.\n\n\n\n\n\n\n\n\n\nFigure 10.2: The displacement of a vibrating guitar string is a function of both time and space. In the still picture taken from Video 10.1, you see a slice of that function taken at a fixed moment of time.\n\n\n\nFor a string of length \\(L\\), the string displacement is a function of position \\(x\\) along the string and is a linear combination of functions of the form \\[g_k(x) \\equiv \\sin(k \\pi x /L)\\] where \\(k\\) is an integer. A few of these functions are graphed in Figure 10.3 with \\(k=1\\), \\(k=2\\), and \\(k=3\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10.3: The overall shape of a guitar string at a fixed instant is complicated. It is a linear combination of three functions, each of which is called a “mode” of the string.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Functions with multiple inputs</span>"
    ]
  },
  {
    "objectID": "Modeling/11-fitting-features.html",
    "href": "Modeling/11-fitting-features.html",
    "title": "11  Fitting features",
    "section": "",
    "text": "11.1 Gaussian\nThe ability to perceive color comes from “cones”: specialized light-sensitive cells in the retina of the eye. Human color perception involves three sets of cones. The L cones are most sensitive to relatively long wavelengths of light near 570 nanometers. The M cones are sensitive to wavelengths near 540 nm, and the S cones to wavelengths near 430nm.\nThe current generation of Landsat satellites uses nine different wavelength-specific sensors. This makes it possible to distinguish features that would be undifferentiated by the human eye.\nBack toward Earth, birds have five sets of cones that cover a wider range of wavelengths than humans. (Figure 11.4) Does this give them a more powerful sense of the differences between natural features such as foliage or plumage? One way to answer this question is to take photographs of a scene using cameras that capture many narrow bands of wavelengths. Then, knowing the sensitivity spectrum of each set of cones, new “false-color” pictures can be synthesized recording the view from each set.1\nCreating the false-color pictures on the computer requires a mathematical model of the sensitivities of each type of cone. The graph of each sensitivity function resembles a Gaussian function.\nThe Gaussian has two parameters: the “mean” and the “sd” (short for standard deviation). It is straightforward to estimate values of the parameters from a graph, as in Figure 11.5.\nThe parameter “mean” is the location of the peak. The standard deviation is, roughly, half the width of the graph at a point halfway down from the peak.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting features</span>"
    ]
  },
  {
    "objectID": "Modeling/11-fitting-features.html#gaussian",
    "href": "Modeling/11-fitting-features.html#gaussian",
    "title": "11  Fitting features",
    "section": "",
    "text": "Figure 11.3: Two views of the same scene synthesized by combining the output of different types of cones. The top picture uses V, M, and S cones; the bottom only S, M, and L cones. The dark geen leaves revealed in the top picture are not distinguishable in the bottom picture. (Source: Tedore and Nilsson)\n\n\n\n\n\n\n\n\n\n\nFigure 11.4: Sensitivity to wavelength for each of the five types of bird cones. [Source: Tedore and Nilsson]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.5: A Gaussian function annotated to identify the parameters mean (location of peak of graph) and sd (half-width at half-height).",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting features</span>"
    ]
  },
  {
    "objectID": "Modeling/11-fitting-features.html#sinusoid",
    "href": "Modeling/11-fitting-features.html#sinusoid",
    "title": "11  Fitting features",
    "section": "11.2 Sinusoid",
    "text": "11.2 Sinusoid\nWe will use three parameters for fitting a sinusoid to data: \\[A \\sin\\left(\\frac{2\\pi}{P}\\right) + B\\] where\n\n\\(A\\) is the “amplitude”\n\\(B\\) is the “baseline”\n\\(P\\) is the period.\n\n\n\n\n\n\n\n\n\n\nFigure 11.6: A reproduction of the data originally shown in Figure 8.4. The baseline for the sinusoid is midway between the top of the oscillation and the bottom.\n\n\n\n\nThe baseline for the sinusoid is the value mid-way between the top of the oscillations and the bottom. For example, Figure 8.4 (reproduced in the margin) shows the sinusoidal-like pattern of tide levels. Dashed horizontal lines (\\(\\color{brown}{\\text{brown}}\\)) have been drawn roughly going through the top of the oscillation and the bottom of the oscillation. The baseline (\\(\\color{magenta}{\\text{magenta}}\\)) will be halfway between these top and bottom levels.\nThe amplitude is the vertical distance between the baseline and the top of the oscillations. Equivalently, the amplitude is half the vertical distance between the top and the bottom of the oscillations.\nIn a pure, perfect sinusoid, the top of the oscillation—the peaks—is the same for every cycle, and similarly with the bottom of the oscillation—the troughs. The data in Figure 8.4 is only approximately a sinusoid so the top and bottom have been set to be representative. In Figure 11.6, the top of the oscillations is marked at level 1.6, the bottom at level 0.5. The baseline is therefore \\(B \\approx = (1.6 + 0.5)/2 = 1.05\\). The amplitude is \\(A  = (1.6 - 0.5)/2 = 1.1/2 = 0.55\\).\nTo estimate the period from the data, mark the input for a distinct point such as a local maximum, then count off one or more cycles forward and mark the input for the corresponding distinct point for the last cycle. For instance, in Figure 11.6, the tide level reaches a local maximum at an input of about 6 hours, as marked by a black dotted line. Another local maximum occurs at about 106 hours, also marked with a black dotted line. In between those two local maxima you can count \\(n=8\\) cycles. Eight cycles in \\(106-6 = 100\\) hours gives a period of \\(P = 100/8 = 12.5\\) hours.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting features</span>"
    ]
  },
  {
    "objectID": "Modeling/11-fitting-features.html#sec-exponential-water",
    "href": "Modeling/11-fitting-features.html#sec-exponential-water",
    "title": "11  Fitting features",
    "section": "11.3 Exponential",
    "text": "11.3 Exponential\nTo fit an exponential function, such as the ones in ?fig-two-exponentials, we estimate the three parameters: \\(A\\), \\(B\\), and \\(k\\) in \\[A \\exp(kt)+ B\\]\n\n\n\n\n\n\n\n\nExp. growth\nExp. decay\n\n\n\n\n\n\n\n\n\nExponential decay is a left-to-right flip of exponential growth.\n\nThe data in Figure 11.2 illustrates the procedure. The first question to ask is whether the pattern shown by the data resembles an exponential function. After all, the exponential pattern book function grows in output as the input gets bigger, whereas the water temperature is getting smaller—the word decaying is used—as time increases. To model exponential decay, use \\(\\exp(-k t)\\), where the negative sign effectively flips the pattern-book exponential left to right.\nThe exponential function has a horizontal asymptote for negative inputs. The left-to-right flipped exponential \\(\\exp(-k t)\\) also has a horizontal asymptote, but for positive inputs.\nThe parameter \\(B\\), again called the “baseline,” is the location of the horizontal asymptote on the vertical axis. Figure 11.2 suggests the asymptote is located at about 25 deg. C. Consequently, the estimated value is \\(B \\approx 25\\) deg C.\n\n11.3.1 Estimating A\nThe parameter \\(A\\) can be estimated by finding the value of the data curve at \\(t=0\\). In Figure Figure 11.7 that is just under 100 deg C. From that, subtract off the baseline you estimated earlier: (\\(B = 25\\) deg C). The amplitude parameter \\(A\\) is the difference between these two: \\(A = 99 - 25 = 74\\) deg C.\n\n\n11.3.2 Estimating k\nThe exponential has a unique property of “doubling in constant time” as described in ?sec-doubling-time. We can exploit this to find the parameter \\(k\\) for the exponential function.\n\nThe procedure starts with your estimate of the baseline for the exponential function. In Figure 11.7 the baseline has been marked in \\(\\color{magenta}{\\text{magenta}}\\) with a value of 25 deg C.\nPick a convenient place along the horizontal axis. You want a place such that the distance of the data from the baseline to be pretty large. In Figure 11.7 the convenient place was selected at \\(t=25\\).\n\n\n\n\n\n\n\n\n\n\nFigure 11.7: Determining parameter \\(k\\) for the exponential function using the doubling time.\n\n\n\n\n\nMeasure the vertical distance from the baseline at the convenient place. In Figure 11.7 the data curve has a value of about 61 deg C at the convenient place. This is \\(61-25 = 36\\) deg C from the baseline.\nCalculate half of the value from (c). In Figure 11.7 this is \\(36/2=18\\) deg C. But you can just as well do the calculation visually, by marking half the distance from the baseline at the convenient place.\nScan horizontally along the graph to find an input where the vertical distance from the data curve to the baseline is the value from (d). In Figure 11.7 that half-the-vertical-distance input is at about \\(t=65\\). Then calculate the horizontal distance between the two vertical lines. In Figure 11.7 that is \\(65 - 25 = 40\\) minutes. This is the doubling time. Or, you might prefer to call it the “half-life” since the water temperature is decaying over time.\nCalculate the magnitude \\(\\|k\\|\\) as \\(\\ln(2)\\) divided by the doubling time from (e). That doubling time is 40 minutes, so \\(\\|k\\|= \\ln(2) / 40 = 0.0173\\). We already know that the sign of \\(k\\) is negative since the pattern shown by the data is exponential decay toward the baseline. So, \\(k=-0.0173\\).",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting features</span>"
    ]
  },
  {
    "objectID": "Modeling/11-fitting-features.html#graphics-layers",
    "href": "Modeling/11-fitting-features.html#graphics-layers",
    "title": "11  Fitting features",
    "section": "11.4 Graphics layers",
    "text": "11.4 Graphics layers\nWhen fitting a function to data, it is wise to plot out the resulting function on top of the data. This involves making graphics with two layers, as described in Chapter 7. As a reminder, here is an example comparing the cooling-water data to the exponential function we fitted in Section 11.3.\nThe fitted function we found was \\[T_{water}(t) \\equiv 74 \\exp(-0.0173 t) + 25\\] where \\(T\\) stands for “temperature.”\nTo compare \\(T_{water}()\\) to the data, we will first plot out the data with gf_point(), then add a slice plot of the function. We will also show a few bells and whistles of plotting: labels, colors, and such.\n\n\n\n\nT_water &lt;- makeFun(74*exp(-0.0173*t) + 25 ~ t)\ngf_point(temp ~ time, data = CoolingWater, alpha = 0.5 ) %&gt;%\n  slice_plot(T_water(time) ~ time, color = \"blue\") %&gt;%\n  gf_labs(x = \"Time (minutes)\", y = \"Temperature (deg. C)\")\n\n\n\n\n\n\n\n\n\n\nFigure 11.8: A graphic with two layers: one for the cooling-water data and the other with the exponential function fitted to the data.\n\n\n\nThe slice_plot() command inherited the domain interval from the gf_point() command. This happens only when the name of the input used in slice_plot() is the same as that in gf_point(). (it is time in both.) You can add additional data or function layers by extending the pipeline.\nBy the way, the fitted exponential function is far from a perfect match to the data. We will return to this mismatch in Chapter 16 when we explore the modeling cycle.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting features</span>"
    ]
  },
  {
    "objectID": "Modeling/11-fitting-features.html#fitting-other-pattern-book-functions",
    "href": "Modeling/11-fitting-features.html#fitting-other-pattern-book-functions",
    "title": "11  Fitting features",
    "section": "11.5 Fitting other pattern-book functions",
    "text": "11.5 Fitting other pattern-book functions\nThis chapter has looked at fitting the exponential, sinusoid, and Gaussian functions to data. Those are only three of the nine pattern-book functions. What about the others?\n\n\n\n\nTable 11.2: Shapes of the pattern-book functions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconst\nprop\nsquare\nrecip\ngaussian\nsigmoid\nsinusoid\nexp\nln\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn Blocks ?sec-differentiation-part and ?sec-accumulation-part, you will see how the Gaussian and the sigmoid are intimately related to one another. Once you see that relationship, it will be much easier to understand how to fit a sigmoid to data.\nThe remaining five pattern-book functions, the ones we haven’t discussed in terms of fitting, are the logarithm and the four power-law functions included in the pattern-book set. In Chapter 14 we will introduce a technique for estimating from data the exponent of a single power-law function.\nIn high school, you may have done exercises where you estimated the parameters of straight-line functions and other polynomials from graphs of those functions. In professional practice, such estimations are done with an entirely different and completely automated method called regression. We will introduce regression briefly in Chapter 16. However, the subject is so important that all of Block ?sec-vectors-linear-combinations is devoted to it and its background.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting features</span>"
    ]
  },
  {
    "objectID": "Modeling/11-fitting-features.html#sec-polishing-promise",
    "href": "Modeling/11-fitting-features.html#sec-polishing-promise",
    "title": "11  Fitting features",
    "section": "11.6 Polishing parameters",
    "text": "11.6 Polishing parameters\nOften, fitting parameters to match a pattern seen in data can be done automatically (or semi-automatically) by software. When there are multiple inputs to the function, practicality demands that automated techniques be used. And even when it’s easy to estimate parameters by eye, as with the examples in this chapter, they can be improved by use of function-fitting software. We call this improvement in estimated parameters “polishing.”\nAn example of such automated fitting is given in Active R chunk 7.2. We will return to the topic in more depth in Chapter 16.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting features</span>"
    ]
  },
  {
    "objectID": "Modeling/11-fitting-features.html#footnotes",
    "href": "Modeling/11-fitting-features.html#footnotes",
    "title": "11  Fitting features",
    "section": "",
    "text": "Cynthia Tedore & Dan-Eric Nilsson (2019) “Avian UV vision enhances leaf surface contrasts in forest environments”, Nature Communications 10:238 -Figure 11.3 and -Figure 11.4 ↩︎",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting features</span>"
    ]
  },
  {
    "objectID": "Modeling/12-low-order-polynomials.html",
    "href": "Modeling/12-low-order-polynomials.html",
    "title": "12  Low-order polynomials",
    "section": "",
    "text": "12.1 Polynomials\nRecall that the monomials are the power-law functions with non-negative, integer exponents: \\(x^0\\), \\(x^1\\), \\(x^2\\), \\(x^3\\), and so on. The “and so on” refers to even higher integer exponents such as \\(x^4\\) or \\(x^{51}\\) or \\(x^{213}\\), to name but a few. The more common name for a linear combination of monomials is polynomial.\nFor instance, a fifth-order polynomial consists of a linear combination of monomials up to order 5. That is, up to \\(x^5\\). This will have six terms because we count the order of the monomials starting with 0. \\[g(t) \\equiv a_0 + a_1 t + a_2 t^2 + a_3 t^3 + a_4 t^4 + a_5 t^5\\ .\\]\nThe challenge in shaping a polynomial is to find the scalar multipliers—usually called coefficients when it comes to polynomials—that give us the shape we want. This might seem to be a daunting task, and it is for a human. But it can easily be handled using volumes of arithmetic, too much arithmetic for a human to take on but ideally suited for computing machines.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Low-order polynomials</span>"
    ]
  },
  {
    "objectID": "Modeling/12-low-order-polynomials.html#polynomials",
    "href": "Modeling/12-low-order-polynomials.html#polynomials",
    "title": "12  Low-order polynomials",
    "section": "",
    "text": "The first two terms in the polynomial \\(g(t)\\) could be written using exponents, like this: \\[ g(t) \\equiv a_0 t^0 + a_1 t^1 + \\cdots\\] In practice, nobody writes out explicitly the \\(t^0\\) function. Instead, recognizing that \\(t^0 = 1\\), we write the first term simply as \\(a_0\\). Similarly, rather than writing \\(t^1\\) in the second term, we write \\(a_1 t\\), without the exponent. This practice makes the formulas for polynomials more concise but at the cost of failing to remind the reader that all the functions in the linear combination are monomials.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Low-order polynomials</span>"
    ]
  },
  {
    "objectID": "Modeling/12-low-order-polynomials.html#low-order-polynomial-models",
    "href": "Modeling/12-low-order-polynomials.html#low-order-polynomial-models",
    "title": "12  Low-order polynomials",
    "section": "12.2 Low-order polynomial models",
    "text": "12.2 Low-order polynomial models\nPolynomials in general can show a wide variety of snake-like patterns. A fifth-order polynomial can have up to four internal curves. A tenth-order polynomial can have 9 internal curves, and so. There is, however, rarely a need for generating functions with all those curves. Instead, a great deal of modeling work can be accomplished with just first-order polynomials (no internal curves) or second-order polynomials (one internal curve).\n\\[\\begin{eqnarray}\n\\textbf{First-order: }\\ \\ \\ \\ \\ & f_1(t) \\equiv b_0 + b_1 t\\\\\n\\textbf{Second-order: }\\ \\ \\ \\ \\ & f_2(t) \\equiv c_0 + c_1 t + c_2 t^2\n\\end{eqnarray}\\]\n\n\nNote that we are using different names for the coefficients in each of the polynomial examples. The only significance of this is a reminder that each of the coefficients can be any number at all and isn’t necessarily related to any of the other coefficients. In addition to the usual \\(a\\), \\(b\\), \\(c\\), we’ve used the Greek alpha, beta, and gamma, that is \\(\\alpha\\), \\(\\beta\\), and \\(\\gamma\\). The subscript on the coefficient name indicates which term it belongs to. For instance, the coefficient on the \\(y^2\\) term of the \\(h_c\\) polynomial is named \\(\\gamma_{yy}\\) while the coefficient on the \\(x y\\) term has the subscript \\(_{xy}\\). Always, the coefficients are constant quantities and not functions of \\(x\\) or any other input.\nIn high-school mathematics, polynomials are often written without subscript, for instance \\(a x^2 + b x + c\\). This can be fine when working with only one polynomial at a time, but in modeling we often need to compare multiple, related polynomials.\nYou may prefer to think about a first-order polynomial as a straight-line function. Similarly, a second-order polynomial is also known as a “quadratic” or even a “parabola.” Nonetheless, it is good to see them as polynomials distinguished by their order. This puts them into a general framework, all of which can be handled by the technology of linear combinations. And polynomials can also involve more than one input. For instance, here are three polynomial forms that involve inputs \\(x\\) and \\(y\\):\n\\[\\begin{eqnarray}\nh_a(x, y) &\\equiv & \\alpha_0 + \\alpha_x\\, x + \\alpha_y\\, y\\\\\nh_b(x, y) &\\equiv & \\beta_0 + \\beta_x\\, x + \\beta_y\\, y + \\beta_{xy}\\, x y\\\\\nh_c(x, y) &\\equiv & \\gamma_0 + \\gamma_x\\, x + \\gamma_y\\, y + \\gamma_{xy}\\, x y + \\gamma_{xx}\\, x^2 + \\gamma_{yy}\\, y^2\n\\end{eqnarray}\\]\nThe reason to work with first- and second-order polynomials is rooted in the experience of modelers. Second-order polynomials provide a useful amount of flexibility while remaining simple and avoiding pitfalls.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Low-order polynomials</span>"
    ]
  },
  {
    "objectID": "Modeling/12-low-order-polynomials.html#eight-simple-shapes",
    "href": "Modeling/12-low-order-polynomials.html#eight-simple-shapes",
    "title": "12  Low-order polynomials",
    "section": "12.3 Eight simple shapes",
    "text": "12.3 Eight simple shapes\nAn easy way to think about how to use low-order polynomials in modeling is to think about the shapes of their graphs. Figure 12.1 shows eight simple shapes for functions with a single input that occur often in modeling.\n\n\n\n\n\n\n\n\nFigure 12.1: The eight simple shapes of functions with one input.\n\n\n\n\n\nRecall that Chapter 6 introduced terms such as concavity, monotonicity, and slope for describing functions. To choose among these shapes, consider your modeling context:\n\nis the relationship positive (slopes up) or negative (slopes down)?\nis the relationship monotonic or not?\nis the relationship concave up, concave down, or neither?\n\nEach of the eight simple shapes corresponds to a particular set of answers to these equations. Consider these modeling contexts as examples:\n\nHow many minutes can you run as a function of speed? Concave down and downward sloping: Shape (F). In everyday terms, you wear out faster if you run at high speed.\nHow much fuel is consumed by an aircraft as a function of distance? For long flights, the function is concave up and positive sloping: Shape (D). In everyday terms: fuel use increases with distance, but the amount of fuel you have to carry also increases with distance. A heavy aircraft uses more fuel per mile.\nHow far can you walk as a function of time? Steep-then-shallow and concave down: Shape (E). Your pace slows as you get tired.\nHow does the stew taste as a function of saltiness? There is a local maximum: Shape (H). The taste improves as the amount of salt increases … up to a point. Too much salt and the stew is unpalatable.\nThe incidence of an out-of-control epidemic versus time is concave up, but shallow-then-steep. As the epidemic is brought under control, the decline is steep-then-shallow and concave up. Over the whole course of an epidemic, there is a maximum incidence. Experience shows that epidemics can have a phase where incidence reaches a local minimum: a decline as people practice social distancing followed by an increase as people become complacent.\nIn micro-economic theory there are production functions that describe how much of a good is produced at any given price, and demand functions that describe how much of the good will be purchased as a function of price. As a rule, production increases with price and demand decreases with price.\nIn the short term, production functions tend to be concave down, since it is hard to squeeze increased production out of existing facilities. Production functions are Shape (E).\n\nFor demand in the short term, functions will be concave up when there is some group of consumers who have no other choice than to buy the product. Downward sloping and concave up: Shape (C). In the long term, consumption functions can be concave down as consumers find alternatives to the high-priced good. For example, high prices of gasoline may, in the long term, prompt a switch to more efficient cars, hybrids, or electric vehicles. This will push demand down steeply.\n\n\nRemarkably, all the eight simple shapes can be generated by appropriate choices for the coefficients in a second-order polynomial: \\(g(x) = a_0 + a_1 x + a_2 x^2\\). So long as \\(a_2 \\neq 0\\), the graph of the second-order polynomial will be a parabola.\n\nThe parabola opens upward if \\(0 &lt; a_2\\). That is the shape of a local minimum.\nThe parabola opens downward if \\(a_2 &lt; 0\\). That is the shape of a local maximum\n\nConsider what happens if \\(a_2 = 0\\). The function becomes simply \\(a_0 + a_1\\, x\\), the straight-line function.\n\nWhen \\(0 &lt; a_1\\) the line slopes upward.\nWhen \\(a_1 &lt; 0\\) the line slopes downward.\n\nTo produce the steep-then-shallow or shallow-then-steep shapes, you also need to restrict the function domain to be on one side or another of the turning point of the parabola as shown in Figure 12.2.\n\n\n\n\n\n\n\n\nFigure 12.2: Four of the eight simple shapes correspond to the sides of the parabola. The labels refer to the graphs in Figure 12.1.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Low-order polynomials</span>"
    ]
  },
  {
    "objectID": "Modeling/12-low-order-polynomials.html#sec-low-order-two",
    "href": "Modeling/12-low-order-polynomials.html#sec-low-order-two",
    "title": "12  Low-order polynomials",
    "section": "12.4 Polynomials with two inputs",
    "text": "12.4 Polynomials with two inputs\nFor functions with two inputs, the low-order polynomial approximation looks like this:\n\\[g(x, y) \\equiv a_0 + a_x x + a_y y + a_{xy} x y + a_{yy} y^2 + a_{xx} x^2\\]\nIt helps to have different names for the various terms. It is not too bad to say something like, “the \\(a_{xy}\\) term.” (Pronunciation: “a sub x y” or “a x y”) But the proper names are: linear terms, quadratic terms, and interaction term. And a shout out to \\(a_0\\), the constant term.\n\\[g(x, y) \\equiv a_0 + \\underbrace{a_x x + a_y y}_\\text{linear terms} \\ \\ \\ +\n\\underbrace{a_{xy} x y}_\\text{interaction term} +\\ \\ \\  \\underbrace{a_{yy} y^2 + a_{xx} x^2}_\\text{quadratic terms}\\]\nThe interaction term arises in models of phenomena such as the spread of epidemics, the population dynamics of predator and prey animals, and the rates of chemical reactions. In each of these situations, one thing is interacting with another: a predator killing a prey animal, an infective individual meeting a person susceptible to the disease, one chemical compound reacting with another.\nUnder certain circumstances, modelers include one or both quadratic terms, as in \\[h_3(x, y) \\equiv c_0 + c_x\\, x + c_y\\, y + c_{xy}\\,x\\, y + \\underbrace{c_{yy}\\, y^2}_\\text{quadratic in y}\\] The skilled modeler can often deduce which terms to include from basic facts about the system being modeled. We will need some additional calculus concepts before we can explain this straightforwardly.\n\n## Loading required namespace: plotly\n\nA second-order polynomial with two inputs can take on any one of three shapes: a bowl, a hilltop, or a saddle.\n\n\n\n\n\n\n\n\nFigure 12.3: The three forms for a second-order polynomial with two inputs.\n\n\n\n\n\nOther shapes for modeling can be extracted from these three basic shapes. For example, the lower-right quadrant of the Saddle has the shape of seats in an amphitheater.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Low-order polynomials</span>"
    ]
  },
  {
    "objectID": "Modeling/12-low-order-polynomials.html#theory-out-of-a-hat",
    "href": "Modeling/12-low-order-polynomials.html#theory-out-of-a-hat",
    "title": "12  Low-order polynomials",
    "section": "12.5 Theory out of a hat",
    "text": "12.5 Theory out of a hat\nThe start of Chapter 11 introduced a little mystery. Newton introduced his Law of Cooling in the 17th century: The rate at which an object cools depends on the difference in temperature between the object and its ambient environment. But in the 17th century, there was no precise way to measure a rate of temperature change. So how did Newton do it?\nEven with primitive thermometers, one can confirm that a mug of hot water will cool and a glass of cold water will warm to room temperature and stay there. So Newton could deduce that the rate of temperature change is zero when the object’s temperature is the same as the environment. Similarly, it is easy to observe with a primitive thermometer that a big difference in temperature between an object and its environment produces a rapid change in temperature, even if you cannot measure the rate precisely. So the rate of cooling is a function of the temperature difference \\(\\Delta T\\) between object and environment.\nWhat kind of function?\nLow-order polynomials to the rescue! The simplest model is that the rate of cooling will be \\(a_0 + a_1 \\Delta T\\), a first-order polynomial. But we know that the rate of cooling is zero when \\(\\Delta T = 0\\), implying that \\(a_0=0\\). All that is left is the first-order term \\(\\Delta T\\), which you can recognize as the proportional() function.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Low-order polynomials</span>"
    ]
  },
  {
    "objectID": "Modeling/13-operations.html",
    "href": "Modeling/13-operations.html",
    "title": "13  Operations on functions",
    "section": "",
    "text": "13.1 Zero finding\nA function is a mechanism for turning any given input into an output. Zero finding is about going the other way: given an output value, find the corresponding input. As an example, consider the exponential function \\(e^x\\). Given a specific input, say \\(x=2.135\\) you can easily compute the corresponding output:\nexp(2.135)\n## [1] 8.457047\nBut suppose the information you have at hand is in the form of an output from the function, say \\(e^{x_0} = 4.93\\). We don’t (yet) know \\(x_0\\) but, whatever it might be, we know that \\(e^{x_0}\\) will produce the value 4.93.\nHow do you find the specific input \\(x_0\\) that will produce that output? The answer typically presented in high school is to apply another function, \\(\\ln()\\), to the output:\nlog(4.93)\n## [1] 1.595339\nTo confirm that the result 1.595339 is correct, apply the exponential function to it and check that the output is the same as the original, given output 4.93.\nexp(1.595339)\n## [1] 4.93\nThis process works because we happen to have a function at hand, the logarithm, that is perfectly set up to “undo” the action of the exponential function. In high school, you learned a handful of function/inverse pairs: exp() and log() as you’ve just seen, sin() and arcsin(), square and square root, etc.\nAnother situation that is usually addressed in high school is inverting low-order polynomial functions. For instance, suppose your modeling function is \\(g(x) \\equiv 1.7 - 0.85 x + 0.063 x^2\\) and you seek the \\(x_0\\) such that \\(g(x_0) = 3\\). High school students are taught to approach such problems in a process using the quadratic formula. to apply the quadratic formula, you need to place the problem into a standard format, not \\[1.7 - 0.85 x + 0.063 x^2 = 3\\] but \\[0.063\\, x^2 - 0.85\\, x - 1.4 = 0\\]\nOne reason that low-order polynomials are popular in modeling is that such operations are straightforward.\nIf none of the high-school approaches are suited to your modeling function, as is often the case, you can still carry out the zero-finding operation.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operations on functions</span>"
    ]
  },
  {
    "objectID": "Modeling/13-operations.html#zero-finding",
    "href": "Modeling/13-operations.html#zero-finding",
    "title": "13  Operations on functions",
    "section": "13.2 Optimization",
    "text": "The name “zero-finding” can be a little misleading. The objective is find \\(x_0\\) such that \\(h(x_0) = b\\). In this sense, “b-finding” would be a more appropriate name. Instead of chasing after honey as “b-finding” suggests, we reformat the problem into finding \\(x_0\\) such that \\(h(x_0) - b = 0\\). In other words, we look for zeros of the function \\(h(x) - b\\).\n\n\n\n13.1.1 Graphical zero-finding\n\n\n\n\n## Scale for y is already present.\n## Adding another scale for y, which will replace the existing scale.\n\n\n\n\n\n\n\n\n\n\nFigure 13.1: Finding an \\(x_0\\) such that \\(h(x_0) = 3\\)\n\n\n\nConsider any function \\(h(x)\\) that you constructed by linear combination and/or function multiplication and/or function composition. To illustrate, we will work with the function \\(h(x)\\) graphed in Figure 13.1.\nSuppose the output for which we want to find a corresponding input is 3, that is, we want to find \\(x_0\\) such that \\(h(x_0)=3\\).\nThe steps of the process are:\n\nGraph the function \\(h(x)\\) over a domain interval of interest.\nDraw a horizontal line located at the value on the right-hand side of the equation \\(h(x_0) = 3\\). (This is the \\(\\color{magenta}{\\text{magenta}}\\) line in Figure 13.1.)\nFind the places, if any, where the horizontal line intersects the graph of the function. In Figure 13.1, there are two such values: \\(x_0 = -3.5\\) or \\(x_0 = 2.75\\).\n\n\nThe graph shows a function \\(g(t)\\). Find a value \\(t_0\\) such that \\(g(t_0) = 5\\).\n\n\n\n\n\n\n\n\n\n\nDraw a horizontal line at output level 5.\nFind the t-value where the horizontal line intersects the function graph. There is only one such intersection and that is at about \\(t=1.2\\).\n\nConsequently, \\(t_0 = 1.2\\), at least to the precision possible when reading a graph.\n\nThe graphical approach to zero finding is limited by your ability to locate positions on the vertical and horizontal axis. If you need more precision than the graph provides, you have two options:\n\nTake a step-by-step, iterative approach. Use the graph to locate a rough value for the result. Then refine that answer by drawing another graph, zooming in on a small region around the result from the first step. You can iterate this process, repeatedly zooming in on the result you got from the previous step.\nUse software implementing a numerical zero-finding algorithm. Such software is available in many different computer languages and a variety of algorithms is available, each with its own merits and demerits.\n\n\n\n13.1.2 Numerical zero finding\nIn this book, for consistency with our notation, we use the R/mosaic Zeros() function. The first argument to Zeros() is a tilde expression and the second argument an interval of the domain over which to search.\nZeros() is set up to find inputs where the mathematical function defined in the tilde expression produces zero as an output. But suppose you are dealing with a problem like \\(f(x) = 10\\)? You can modify the tilde expression so that it implements a slightly different function: \\(f(x) - 10\\). If we can find \\(x_0\\) such that \\(f(x_0) - 10 = 0\\), that will also be the \\(x_0\\) satisfying \\(f(x_0) = 10\\).\n\nThe point of this example is to show how to use Zeros(), so we will define a function \\(f(x)\\) using doodle_fun() from R/mosaic. This constructs a function by taking a linear combination of other functions selected at random. The argument seed=579 determines which functions will be in the linear combination. The function is graphed in Figure 13.2.\n\nf &lt;- doodle_fun( ~ z, seed=579)\n\nWe want to find the zeros of the function \\(f(x) - 10\\) which corresponds to solving \\(f(x) = 10\\). The function Zeros() handles this for us.\n\nZeros(f(z) - 10 ~ z, domain(z=-4:4))\n## # A tibble: 2 × 2\n##         z      .output.\n##     &lt;dbl&gt;         &lt;dbl&gt;\n## 1 -2.92   -0.0000000344\n## 2  0.0795 -0.00000118\n\nThe output produced by Zeros() is a data frame with one row for each of the \\(x_0\\) found. Here, two values were found: \\(x_0 = -2.92\\) and \\(x_0 = 0.0795\\), shown as vertical lines in Figure 13.2. The .output column reports \\(f(x_0)\\). In principal, this should be exactly zero. However, computer arithmetic is not always exactly precise. Even so, numbers as small as those in the .output. column—\\(-3.4 \\times 10^{-8}\\) for example—are miniscule compared to the range of values seen in Figure 13.2.\n\n\n\n\nf &lt;- doodle_fun( ~ z, seed=579)\nslice_plot(f(x) ~ x, bounds(x=-4:4)) |&gt;\n  gf_hline(yintercept = ~ 10, color=\"magenta\") |&gt;\n  gf_vline(xintercept = c(-2.919, 0.0795), color = \"red\", alpha=0.25)\n## Warning: `geom_vline()`: Ignoring `mapping` because `xintercept` was provided.\n\n\n\n\n\n\n\n\n\n\nFigure 13.2\n\n\n\n\n13.2 Optimization\nOptimization problems consist of both a modeling phase and a solution phase (that is, an information extraction phase). We use our knowledge of how the world works for the modeling phase. Then we extract information in the form we want from the solution phase.\nIn this chapter we will deal only with the solution phase. In real work, the modeling phase is essential\n\n13.2.1 Graphical optimization\nLook for local peaks, then read off the input that generates the value at the peak.\n\n\n13.2.2 Numerical optimization\nWhen it comes to functions, maximization is the process of finding an input to the function that produces a larger output than any of the other, nearby inputs.\nTo illustrate, Figure 13.3 shows a function with two peaks.\n\n\n\n\n\n\n\n\n\n\n\n(a) a function with two peaks\n\n\n\n\n\n\n\nFigure 13.3: A function with two peaks\n\n\n\nJust as you can see a mountain top from a distance, so you can see where the function takes on its peak values. Draw a vertical line through each of the peaks. The input value corresponding to each vertical line is called an argmax, short for “the argument2 at which the function reaches a local maximum value.\nMinimization refers to the same technique, but where the vertical lines are drawn at the deepest point in each “valley” of the function graph. An input value located in one of those valleys is called an argmin.\nOptimization is a general term that covers both maximization and minimization.\n\n\n13.2.3 Numerical optimization\nThe R/mosaic argM() function finds a mathematical function’s argmax and argmin over a given domain. It works in exactly the same way as slice_plot(), but rather than drawing a graphic it returns a data frame giving the argmax in one row and the argmin in another. For instance, the function shown in Figure 13.3 is \\(h()\\), generated by doodle_fun():\n\nh &lt;- doodle_fun(~ x, seed=7293)\nargM(h(x) ~ x, domain(x=-5:5))\n## # A tibble: 2 × 3\n##        x .output. concavity\n##    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 -1.68      1.93         1\n## 2  0.173     8.25        -1\n\nThe x column holds the argmax and argmin, the .output. column gives the value of the function output for the input x. The concavity column tells whether the function’s concavity at x is positive or negative. Near a peak, the concavity will be negative; near a valley, the concavity is positive. Consequently, you can see that the first row of the data frame corresponds to a local minimum and the second row is a local maximum.\nargM() is set up to look for a single argmax and a single argmin in the domain interval given as the second argument. In Figure 13.3 there are two local peaks and two local valleys. argM() gives only the largest of the peaks and the deepest of the valleys.\n\n\n\n13.3 Iteration\nMany computations involve starting with a guess followed by a step-by-step process of refining the guess. A case in point is the process for calculating square roots. There isn’t an operational formula for a function that takes a number as an input and produces the square root of that number as the output. When we write \\(\\sqrt{\\strut x}\\) we aren’t saying how to calculate the output, just describing the sort of output we are looking for.\nThe function that is often used to calculate \\(\\sqrt{x}\\) is better():\n\\[\\text{better(guess)} = \\frac{1}{2}\\left( \\text{guess} + \\frac{x}{\\text{guess}}\\right)\\ .\\]\nIt may not be at all clear why this formula is related to finding a square root. Let’s put that matter off until the end of the section and concentrate our attention on how to use it.\nTo start, let’s define the function for the computer:\n\nbetter &lt;- makeFun((guess + x/guess)/2 ~ guess)\n\nNotice that \\(x\\) is cast in the role of a parameter of the function rather than an input to the function.\nSuppose we want to apply the square root function to the input 55, that is, calculate \\(\\sqrt{\\strut x=55}\\). The value we should assign to \\(x\\) is therefore 55.\nTo calculate better(guess) we need not only \\(x=55\\) but a value for the guess. What should be this value and what will we do with the quantity better(guess) when we’ve calculated it.\nWithout explanation, we will use guess = 1, regardless of the value of \\(x\\). Calculating the output …\n\nbetter(1, x=55)\n## [1] 28\n\nNeither our guess 1 nor the output 28 are \\(\\sqrt{\\strut x=55}\\). (Having long-ago memorized the squares of integers, we know \\(\\sqrt{\\strut x=55}\\) will be somewhere between 7 and 8. Neither 1 nor 28 are in that interval.)\nThe people—more than two thousand years ago—who invented the ideas behind the better() function were convinced that better() constructs a better guess for the answer we seek. It is not obvious why 28 should be a better guess than 1 for \\(\\sqrt{\\strut x=55}\\) but, out of respect, let’s accept their claim.\nThis is where iteration comes in. Even if 28 is a better guess than 1, 28 is still not a good guess. But we can use better() to find something better than 28:\n\nbetter(28, x=55)\n## [1] 14.98214\n\nTo iterate an action means to perform that action over and over again. (“Iterate” stems from the Latin word iterum, meaning “again.”) A bird iterates its call, singing it over and over again. In mathematics, “iterate” has a twist. When we repeat the mathematical action, we will draw on the results of the previous angle rather than simply repeating the earlier calculation.\nContinuing our iteration of better() …\n\nbetter(14.98214, x=55)\n## [1] 9.326589\nbetter(9.326589, x=55)\n## [1] 7.611854\nbetter(7.611854, x=55)\n## [1] 7.418713\nbetter(7.418713, x=55)\n## [1] 7.416199\nbetter(7.416199, x=55)\n## [1] 7.416198\n\nIn the last step, the output of better() is practically identical to the input, so no reason to continue. We can confirm that the last output is a good guess for \\(\\sqrt{\\strut x=55}\\):\n\n7.416198^2\n## [1] 54.99999\n\n\n13.3.1 Graphical iteration\nTo iterate graphically, we graph the function to be iterated and mark the initial guess on the horizontal axis. For each iteration step, trace vertically from the current point to the function, then horizontally to the line of identity (blue dots). The result will be the starting point for the next guess.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThree steps of iteration of better() starting with an initial guess of 1. ### Numerical iteration\nUse the R/mosaic Iterate() function. The first argument is a tilde expression defining the function to be iterated. The second is the starting guess. The third is the number of iteration steps. For instance:\n\nIterate(better(guess, x=55) ~ guess, x0=1, n=8)\n##   n     guess\n## 1 0  1.000000\n## 2 1 28.000000\n## 3 2 14.982143\n## 4 3  9.326590\n## 5 4  7.611854\n## 6 5  7.418713\n## 7 6  7.416199\n## 8 7  7.416198\n## 9 8  7.416198\n\nThe output produced by Iterate() is a data frame. The initial guess is in the row with \\(n=0\\). Successive rows give the output, step by step, with each new iteration step.\n\n\n\n\n\n\nWhere does better() come from?\n\n\n\nFor calculating square roots, we used the function \\[\\text{better}(y) = \\frac{1}{2}\\left( y + \\frac{x}{y}\\right)\\ .\\] Let’s suppose you happened on a guess that is exactly right, that is \\(y = \\sqrt{x}\\). There is no way to improve on a guess that is exactly right, so the best better() can do is to return the guess unaltered. Indeed it does: \\[\\text{better}\\left(y=\\!\\!\\sqrt{\\strut x}\\ \\right) = \\frac{1}{2}\\left( \\sqrt{\\strut x} + \\frac{x}{\\sqrt{x}} \\right)\\ = \\frac{1}{2}\\left(\\sqrt{\\strut x} + \\sqrt{\\strut x}\\right) = \\sqrt{\\strut x}.\\]\nOf course, the initial guess \\(y\\) might be wrong. There are two ways to be wrong:\n\nThe guess is too small, that is \\(y &lt; \\sqrt{\\strut x}\\).\nThe guess is too big, that is \\(\\sqrt{\\strut x} &lt; y\\).\n\nThe formula for better() is the average of the guess \\(y\\) and another quantity \\(x/y\\). If \\(y\\) is too small, then \\(x/y\\) must be too big. If \\(y\\) is too big, then \\(x/y\\) must be too small.\nAs guesses, the two quantities \\(y\\) and \\(x/y\\) are equivalent in the sense that \\(\\text{better}(y) = \\text{better}(x/y)\\). The average of \\(y\\) and \\(x/y\\) will be closer to the true result than the worst of \\(y\\) or \\(x/y\\); the average will be a better guess.\n\n\n\n\n\n\nFigure 13.4",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operations on functions</span>"
    ]
  },
  {
    "objectID": "Modeling/13-operations.html#optimization",
    "href": "Modeling/13-operations.html#optimization",
    "title": "13  Operations on functions",
    "section": "",
    "text": "Optimization problems consist of both a modeling phase and a solution phase (that is, an information extraction phase). We use our knowledge of how the world works for the modeling phase. Then we extract information in the form we want from the solution phase.\nIn this chapter we will deal only with the solution phase. In real work, the modeling phase is essential\n\n13.2.1 Graphical optimization\nLook for local peaks, then read off the input that generates the value at the peak.\n\n\n13.2.2 Numerical optimization\nWhen it comes to functions, maximization is the process of finding an input to the function that produces a larger output than any of the other, nearby inputs.\nTo illustrate, Figure 13.3 shows a function with two peaks.\n\n\n\n\n\n\n\n\n\n\n\n(a) a function with two peaks\n\n\n\n\n\n\n\nFigure 13.3: A function with two peaks\n\n\n\nJust as you can see a mountain top from a distance, so you can see where the function takes on its peak values. Draw a vertical line through each of the peaks. The input value corresponding to each vertical line is called an argmax, short for “the argument2 at which the function reaches a local maximum value.\nMinimization refers to the same technique, but where the vertical lines are drawn at the deepest point in each “valley” of the function graph. An input value located in one of those valleys is called an argmin.\nOptimization is a general term that covers both maximization and minimization.\n\n\n13.2.3 Numerical optimization\nThe R/mosaic argM() function finds a mathematical function’s argmax and argmin over a given domain. It works in exactly the same way as slice_plot(), but rather than drawing a graphic it returns a data frame giving the argmax in one row and the argmin in another. For instance, the function shown in Figure 13.3 is \\(h()\\), generated by doodle_fun():\n\nh &lt;- doodle_fun(~ x, seed=7293)\nargM(h(x) ~ x, domain(x=-5:5))\n## # A tibble: 2 × 3\n##        x .output. concavity\n##    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 -1.68      1.93         1\n## 2  0.173     8.25        -1\n\nThe x column holds the argmax and argmin, the .output. column gives the value of the function output for the input x. The concavity column tells whether the function’s concavity at x is positive or negative. Near a peak, the concavity will be negative; near a valley, the concavity is positive. Consequently, you can see that the first row of the data frame corresponds to a local minimum and the second row is a local maximum.\nargM() is set up to look for a single argmax and a single argmin in the domain interval given as the second argument. In Figure 13.3 there are two local peaks and two local valleys. argM() gives only the largest of the peaks and the deepest of the valleys.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operations on functions</span>"
    ]
  },
  {
    "objectID": "Modeling/13-operations.html#iteration",
    "href": "Modeling/13-operations.html#iteration",
    "title": "13  Operations on functions",
    "section": "13.3 Iteration",
    "text": "13.3 Iteration\nMany computations involve starting with a guess followed by a step-by-step process of refining the guess. A case in point is the process for calculating square roots. There isn’t an operational formula for a function that takes a number as an input and produces the square root of that number as the output. When we write \\(\\sqrt{\\strut x}\\) we aren’t saying how to calculate the output, just describing the sort of output we are looking for.\nThe function that is often used to calculate \\(\\sqrt{x}\\) is better():\n\\[\\text{better(guess)} = \\frac{1}{2}\\left( \\text{guess} + \\frac{x}{\\text{guess}}\\right)\\ .\\]\nIt may not be at all clear why this formula is related to finding a square root. Let’s put that matter off until the end of the section and concentrate our attention on how to use it.\nTo start, let’s define the function for the computer:\n\nbetter &lt;- makeFun((guess + x/guess)/2 ~ guess)\n\nNotice that \\(x\\) is cast in the role of a parameter of the function rather than an input to the function.\nSuppose we want to apply the square root function to the input 55, that is, calculate \\(\\sqrt{\\strut x=55}\\). The value we should assign to \\(x\\) is therefore 55.\nTo calculate better(guess) we need not only \\(x=55\\) but a value for the guess. What should be this value and what will we do with the quantity better(guess) when we’ve calculated it.\nWithout explanation, we will use guess = 1, regardless of the value of \\(x\\). Calculating the output …\n\nbetter(1, x=55)\n## [1] 28\n\nNeither our guess 1 nor the output 28 are \\(\\sqrt{\\strut x=55}\\). (Having long-ago memorized the squares of integers, we know \\(\\sqrt{\\strut x=55}\\) will be somewhere between 7 and 8. Neither 1 nor 28 are in that interval.)\nThe people—more than two thousand years ago—who invented the ideas behind the better() function were convinced that better() constructs a better guess for the answer we seek. It is not obvious why 28 should be a better guess than 1 for \\(\\sqrt{\\strut x=55}\\) but, out of respect, let’s accept their claim.\nThis is where iteration comes in. Even if 28 is a better guess than 1, 28 is still not a good guess. But we can use better() to find something better than 28:\n\nbetter(28, x=55)\n## [1] 14.98214\n\nTo iterate an action means to perform that action over and over again. (“Iterate” stems from the Latin word iterum, meaning “again.”) A bird iterates its call, singing it over and over again. In mathematics, “iterate” has a twist. When we repeat the mathematical action, we will draw on the results of the previous angle rather than simply repeating the earlier calculation.\nContinuing our iteration of better() …\n\nbetter(14.98214, x=55)\n## [1] 9.326589\nbetter(9.326589, x=55)\n## [1] 7.611854\nbetter(7.611854, x=55)\n## [1] 7.418713\nbetter(7.418713, x=55)\n## [1] 7.416199\nbetter(7.416199, x=55)\n## [1] 7.416198\n\nIn the last step, the output of better() is practically identical to the input, so no reason to continue. We can confirm that the last output is a good guess for \\(\\sqrt{\\strut x=55}\\):\n\n7.416198^2\n## [1] 54.99999\n\n\n13.3.1 Graphical iteration\nTo iterate graphically, we graph the function to be iterated and mark the initial guess on the horizontal axis. For each iteration step, trace vertically from the current point to the function, then horizontally to the line of identity (blue dots). The result will be the starting point for the next guess.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThree steps of iteration of better() starting with an initial guess of 1. ### Numerical iteration\nUse the R/mosaic Iterate() function. The first argument is a tilde expression defining the function to be iterated. The second is the starting guess. The third is the number of iteration steps. For instance:\n\nIterate(better(guess, x=55) ~ guess, x0=1, n=8)\n##   n     guess\n## 1 0  1.000000\n## 2 1 28.000000\n## 3 2 14.982143\n## 4 3  9.326590\n## 5 4  7.611854\n## 6 5  7.418713\n## 7 6  7.416199\n## 8 7  7.416198\n## 9 8  7.416198\n\nThe output produced by Iterate() is a data frame. The initial guess is in the row with \\(n=0\\). Successive rows give the output, step by step, with each new iteration step.\n\n\n\n\n\n\nWhere does better() come from?\n\n\n\nFor calculating square roots, we used the function \\[\\text{better}(y) = \\frac{1}{2}\\left( y + \\frac{x}{y}\\right)\\ .\\] Let’s suppose you happened on a guess that is exactly right, that is \\(y = \\sqrt{x}\\). There is no way to improve on a guess that is exactly right, so the best better() can do is to return the guess unaltered. Indeed it does: \\[\\text{better}\\left(y=\\!\\!\\sqrt{\\strut x}\\ \\right) = \\frac{1}{2}\\left( \\sqrt{\\strut x} + \\frac{x}{\\sqrt{x}} \\right)\\ = \\frac{1}{2}\\left(\\sqrt{\\strut x} + \\sqrt{\\strut x}\\right) = \\sqrt{\\strut x}.\\]\nOf course, the initial guess \\(y\\) might be wrong. There are two ways to be wrong:\n\nThe guess is too small, that is \\(y &lt; \\sqrt{\\strut x}\\).\nThe guess is too big, that is \\(\\sqrt{\\strut x} &lt; y\\).\n\nThe formula for better() is the average of the guess \\(y\\) and another quantity \\(x/y\\). If \\(y\\) is too small, then \\(x/y\\) must be too big. If \\(y\\) is too big, then \\(x/y\\) must be too small.\nAs guesses, the two quantities \\(y\\) and \\(x/y\\) are equivalent in the sense that \\(\\text{better}(y) = \\text{better}(x/y)\\). The average of \\(y\\) and \\(x/y\\) will be closer to the true result than the worst of \\(y\\) or \\(x/y\\); the average will be a better guess.\n\n\n\n\n\n\nFigure 13.4",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operations on functions</span>"
    ]
  },
  {
    "objectID": "Modeling/13-operations.html#footnotes",
    "href": "Modeling/13-operations.html#footnotes",
    "title": "13  Operations on functions",
    "section": "",
    "text": "We use the phrase “extract information” to reflect the purpose of using the operations discussed in this chapter. You may be more familiar with the phrase “find a solution.”↩︎\nAlso known as an input.↩︎",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operations on functions</span>"
    ]
  },
  {
    "objectID": "Modeling/14-magnitudes.html",
    "href": "Modeling/14-magnitudes.html",
    "title": "14  Magnitude",
    "section": "",
    "text": "14.1 Order of magnitude\nWe will refer to judging the size of numbers by their count of digits as reading the magnitude of the number. To get started, consider numbers that start with 1 followed by zeros, e.g. 100 or 1000. We will quantify the magnitude as the number of zeros: 100 has a magnitude of 2 and 1000 has a magnitude of 3. In comparing numbers by magnitude, we way things like, “1000 is an order of magnitude greater than 100,” or “1,000,000” is five orders of magnitude larger than 10.\nMany phenomena and quantities are better understood in terms of magnitude than in terms of number. An example: Animals, including humans, go about the world in varying states of illumination, from the bright sunlight of high noon to the dim shadows of a half-moon. To be able to see in such diverse conditions, the eye needs to respond to light intensity across many orders of magnitude.\nThe lux is the unit of illuminance in the Système international. This table1 shows the illumination in a range of familiar outdoor settings:\nFor a creature active both night and day, the eye needs to be sensitive over 7 orders of magnitude of illumination. To accomplish this, eyes use several mechanisms: contraction or dilation of the pupil accounts for about 1 order of magnitude, photopic (color, cones) versus scotopic (black-and-white, rods, nighttime) covers about 3 orders of magnitude, adaptation over minutes (1 order), squinting (1 order).\nMore impressively, human perception of sound spans more than 16 orders of magnitude in terms of the energy impinging on the eardrum. The energy density of perceptible sound ranges from the threshold of hearing at 0.000000000001 Watt per square meter to a conversational level of 0.000001 W/m2 to 0.1 W/m2 in the front rows of a rock concert. But in terms of our subjective perception of loudness, each order of magnitude change is perceived in the same way, whether it be from street traffic to vacuum cleaner or from whisper to normal conversation. (The unit of sound measurement is the decibel (dB), with 10 decibels corresponding to an order of magnitude in the energy density of sound.)\n6, 60, 600, and 6000 miles-per-hour are quantities that differ in size by orders of magnitude. Such differences often point to a substantial change in context. A jog is 6 mph, a car on a highway goes 60 mph, a cruising commercial jet goes 600 mph, and a rocket passes through 6000 mph on its way to orbital velocity. From an infant’s crawl to highway cruising is 3 orders of magnitude in speed.\nOf course, many phenomena are not well represented in terms of orders of magnitudes. For example, the difference between normal body temperature and high fever is 0.01 orders of magnitude in temperature.2 An increase of 1 order of magnitude in blood pressure from the normal level would cause instant death! The difference between a very tall adult and a very short adult is about 1/4 of an order of magnitude.\nOrders of magnitude are used when the relevant comparison is a ratio. “A car is 10 times faster than a person,” refers to the ratio of speeds. In contrast, quantities such as body temperature, blood pressure, and adult height are compared using a difference. Fever is 2\\(^\\circ\\)C higher in temperature than normal. A 30 mmHg increase in blood pressure will likely correspond to developing hypertension. A very tall and a very short adult differ by about 2 feet.\nOne clue that thinking in terms of orders of magnitude is appropriate is when you are working with a set of objects whose range of sizes spans one or many factors of 2. Comparing baseball and basketball players? Probably no need for orders of magnitudes. Comparing infants, children, and adults in terms of height or weight? Orders of magnitude may be useful. Comparing bicycles? Mostly they fit within a range of 2 in terms of size, weight, and speed (but not expense!). Comparing cars, SUVs, and trucks? Differences by a factor of 2 are routine, so thinking in terms of order of magnitude is likely to be appropriate.\nAnother clue is whether “zero” means “nothing.” Daily temperatures in the winter are often near “zero” on the Fahrenheit or Celcius scales, but that in no way means there is a complete absence of heat. Those scales are arbitrary. Another way to think about this clue is whether negative values are meaningful. If so, thinking in terms of orders of magnitude is not likely to be useful.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Magnitude</span>"
    ]
  },
  {
    "objectID": "Modeling/14-magnitudes.html#order-of-magnitude",
    "href": "Modeling/14-magnitudes.html#order-of-magnitude",
    "title": "14  Magnitude",
    "section": "",
    "text": "Illuminance\nCondition\n\n\n\n\n110,000 lux\nBright sunlight\n\n\n20,000 lux\nShade illuminated by entire clear blue sky, midday\n\n\n1,000 lux\nTypical overcast day, midday\n\n\n400 lux\nSunrise or sunset on a clear day (ambient illumination)\n\n\n0.25 lux\nA full Moon, clear night sky\n\n\n0.01 lux\nA quarter Moon, clear night sky\n\n\n\n\n\n\n\n\n\n\nTable 14.1: Energy density of sound in various situations. Sound at 85 dB, for extended periods, can cause permanent hearing loss. Exposure to sound at 120 dB over 30 seconds is dangerous.\n\n\n\n\n\nSituation\nEnergy level (dB)\n\n\n\n\nRustling leaves\n10 dB\n\n\nWhisper\n20 dB\n\n\nMosquito buzz\n40 dB\n\n\nNormal conversation\n60 dB\n\n\nBusy street traffic\n70 dB\n\n\nVacuum cleaner\n80 dB\n\n\nLarge orchestra\n98 dB\n\n\nEarphones (high level)\n100 dB\n\n\nRock concert\n110 dB\n\n\nJackhammer\n130 dB\n\n\nMilitary jet takeoff\n140 dB",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Magnitude</span>"
    ]
  },
  {
    "objectID": "Modeling/14-magnitudes.html#sec-counting-digits",
    "href": "Modeling/14-magnitudes.html#sec-counting-digits",
    "title": "14  Magnitude",
    "section": "14.2 Counting digits",
    "text": "14.2 Counting digits\nImagine having a digit counting function called digits(). It takes a number as input and produces a number as output. We have not yet presented a formula for digits(), but for some inputs, the output can be calculated just by counting. For numbers like 0.01 or 10 or 100000, we will define the number of digits to be the count of zeros before For example:\ndigits(10) \\(\\equiv\\) 1\ndigits(100) \\(\\equiv\\) 2\ndigits(1000) \\(\\equiv\\) 3\n… and so on …\ndigits(1,000,000) \\(\\equiv\\) 6\n… and on.\nFor numbers smaller than 1, like 0.01 or 0.0001, we define the number of digits to be the negative of the number of zeros before the 1.\ndigits(0.1) \\(\\equiv\\) -1\ndigits(0.01) \\(\\equiv\\) -2\ndigits(0.0001) \\(\\equiv\\) -4\nThe digits() function easily can be applied to the product of two numbers. For instance:\n\ndigits(1000 \\(\\times\\) 100) = digits(1000) + digits(100) = 3 + 2 = 5.\n\nSimilarly, applying digits() to a ratio gives the difference of the digits of the numerator and denominator, like this:\n\ndigits(1,000,000 \\(\\div\\) 10) = digits(1,000,000) - digits(10) = 6 - 1 = 4\n\nIn practice, digits() is so useful that it could well have been one of our basic modeling functions. Actually, this is very nearly the case: the logarithm is proportional to the number of digits.\nTo illustrate, consider these three calculations of logarithms:\n\n\n\nActive R chunk 14.1: The output is the order of magnitude of the number given as an argument to log().\n\n\n\n0.4342945 * log(100) \n## [1] 2\n0.4342945 * log(1000) \n## [1] 3\n0.4342945 * log(10000)\n## [1] 4\n\n\n\n\nHere is a formula definition of the digits() function.\n\\[\\text{digits}(x) \\equiv \\ln(x) / \\ln(10) \\tag{14.1}\\]\nIn R/mosaic, the analogous definition is:\n\n\n\nActive R chunk 14.2\n\n\n\ndigits &lt;- makeFun(log(x) / log(10) ~ x)\n\n\n\n\nYou may have guessed that digits() is handy for computing differences in terms of orders of magnitude. Here’s how:\n\nMake sure that the quantities are expressed in the same units.\nCalculate the difference between the digits() of the numerical part of the quantity.\n\n\nWhat is the order-of-magnitude difference in velocity between a snail and a walking human? A snail slides at about 1 mm/sec, a human walks at about 5 km per hour. Putting human speed in the same units as snail speed: \\[\\begin{eqnarray}5 \\frac{km}{hr} = \\left[\\frac{1}{3600} \\frac{hr}{sec}\\right] 5 \\frac{km}{hr} &=& \\\\\n\\left[10^6 \\frac{mm}{km}\\right] \\left[\\frac{1}{3600} \\frac{hr}{sec}\\right] 5 \\frac{km}{hr} &=& 1390 \\frac{mm}{sec}\n\\end{eqnarray}\\] Calculating the difference in digits() between 1 and 1390:\n\nlog10(1390) - log10(1)\n## [1] 3.143015\n\nSo, about 3 orders of magnitude difference in speed. To a snail, we walking humans must seem like rockets on their way to orbit!\n\nThe use of factors of 10 in counting orders of magnitude is arbitrary. A person walking and a person jogging are on the edge of being qualitatively different, although their speeds differ by a factor of only 2. Aircraft that cruise at 600 mph and 1200 mph are qualitatively different in design, although the speeds are only a factor of 2 apart. A professional basketball player (height 2 meters or more) is qualitatively different from a third grader (height about 1 meter).\n\n\n\n\n\n\nCalculus history—The “natural” logarithm\n\n\n\nYou may have noticed in Math expression 14.1 or Active R chunk 14.2 the terms \\(1/\\ln(10)\\) and / log(10) are used as a conversion factor. Similarly, in Active R chunk 14.1 the conversion factor 0.4342945 is used. Actually, 0.4342945 and 1 / log(10) are the same number:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nBut why is a conversion factor needed for the digits() calculation?\nThe point of the conversion factor is to have the “units” of the output of digits() correspond to a factor of 10. Such units are called “decades.” The unit of decade is dimensionless, just as the unit for angles—rads or degrees, as you preferr—is dimensionless.\nNaturally, we use decades because of our human habit of writing numbers in base 10, using the digits 0 to 9. I say “naturally” because base 10 is familiar to us. But different people have different notions of what is “natural.”\nWe could have used \\(1/\\ln(2)\\) as the conversion factor in digits() in which case each multiple of 2 in the input corresponds to a change in the output of 1. Or, I should say, “1 bit” because “bit” is the name given to the unit when \\(1/\\ln(2)\\) (that is, 1.442695) is used as the conversion factor.\nAn aesthetic widely admired in the field of mathematics is that having any such conversion factor at all is “unnatural.” The only mathematically pretty conversion factor is 1, that is, no conversion factor at all. If we had used 1 instead of \\(1/\\ln(10)\\) in digits(), the output would be in different units, not in decades.\nThe name mathematicians have given to the version of digits() where the conversion factor is 1 is the natural logarithm. It’s hard to understand the advantages of the natural logarithm until we get further into Calculus. For the natural logarithm, each increase in the input by a factor of 2.7182818281828… leads to an increase in the output by 1. To illustrate, run the code in Active R chunk 14.3.\n\n\n\nActive R chunk 14.3\n\n\n\nlog(63)\n## [1] 4.143135\nlog(2.718281828 * 63)\n## [1] 5.143135",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Magnitude</span>"
    ]
  },
  {
    "objectID": "Modeling/14-magnitudes.html#sec-magnitude-graphics",
    "href": "Modeling/14-magnitudes.html#sec-magnitude-graphics",
    "title": "14  Magnitude",
    "section": "14.3 Magnitude graphics",
    "text": "14.3 Magnitude graphics\nTo display a variable from data that varies over multiple orders of magnitude, it helps to plot the logarithm rather than the variable itself. Let’s illustrate using the Engine data frame, which contains measurements of many different internal combustion engines of widely varying sizes. For instance, we can graph engine RPM (revolutions per second) versus engine mass, as in Figure 14.1.\n\ngf_point(RPM ~ mass, data = Engines)\n\n\n\n\n\n\n\n\nFigure 14.1: Engine RPM versus mass for 39 different enginges plotted on the standard linear axis.\n\n\n\n\nIn the graph, most of the engines have a mass that is … zero. At least that is what it appears to be. The horizontal scale is dominated by the two huge 100,000-pound monster engines plotted at the right end of the graph.\nPlotting the logarithm of the engine mass spreads things out, as in Figure 14.2.\n\ngf_point(RPM ~ mass, data = Engines) %&gt;%\n  gf_refine(scale_x_log10())\n\n\n\n\n\n\n\n\nFigure 14.2: Engine RPM versus mass on semi-log axes.\n\n\n\n\nNote that the horizontal axis has been labeled with the actual mass (in pounds), with the labels evenly spaced in terms of their logarithm. This presentation, with the horizontal axis constructed this way, is called a semi-log plot.\nWhen both axes are labeled this way, we have a log-log plot, as shown in Figure 14.3.\n\ngf_point(RPM ~ mass, data = Engines) %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n    )\n\n\n\n\n\n\n\n\nFigure 14.3: Engine RPM versus mass on log-log axes.\n\n\n\n\nSemi-log and log-log axes are widely used in science and economics, whenever data spanning several orders of magnitude need to be displayed. In the case of the engine RPM and mass, the log-log axis shows that there is a graphically simple relationship between the variables. Such axes are very useful for displaying data but can be hard for the newcomer to read quantitatively. For example, calculating the slope of the evident straight-line relationship in Figure 14.3 is extremely difficult for a human reader and requires translating the labels into their logarithms.\n\n\n\n\n\n\nCalculus history—Boyle’s Law\n\n\n\nRobert Boyle (1627-1691) was a founder of modern chemistry and the scientific method in general. As any chemistry student already knows, Boyle sought to understand the properties of gasses. His results are summarized in Boyle’s Law.\nThe data frame Boyle contains two variables from one of Boyle’s experiments as reported in his lab notebook: pressure in a bag of air and volume of the bag. The units of pressure are mmHg and the units of volume are cubic inches.3\nFamously, Boyle’s Law states that, at a constant temperature, the pressure of a constant mass of gas is inversely proportional to the volume occupied by the gas. Figure 14.4 shows a cartoon of the relationship.\n\n\n\n\n\n\nFigure 14.4: A cartoon illustrating Boyle’s Law. Source: NASA Glenn Research Center”\n\n\n\nFigure 14.5 plots out Boyle’s actual experimental data. I\n\n\n\n\ngf_point(pressure ~ volume, data = Boyle) %&gt;%\n  gf_lm()\n## Warning: Using the `size` aesthetic with geom_line was deprecated in ggplot2 3.4.0.\n## ℹ Please use the `linewidth` aesthetic instead.\n\n\n\n\n\n\n\n\n\n\nFigure 14.5: A plot of Boyle’s pressure vs volume data on linear axes. The straight line model is a poor representation of the pattern seen in the data.\n\n\n\nYou can see a clear relationship between pressure and volume, but it is hardly a linear relationship.\nPlotting Boyle’s data on log-log axes reveals that, in terms of the logarithm of pressure and the logarithm of volume, the relationship is linear.\n\ngf_point(log(pressure) ~ log(volume), data = Boyle) %&gt;%\n  gf_lm()\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.6: Plotting the logarithm of pressure against the logarithm of volume reveals a straight-line relationship.\n\n\n\nFigure 14.6 shows that Boyle’s log-pressure and log-volume data are a straight-line function. In other words:\n\\[\\ln(\\text{Pressure}) = a + b \\ln(\\text{Volume})\\]\nYou can find the slope \\(b\\) and intercept \\(a\\) from the graph. For now, we want to point out the consequences of the straight-line relationship between logarithms.\nExponentiating both sides gives \\[e^{\\ln(\\text{Pressure})} = \\text{Pressure} = e^{a + b \\ln(\\text{Volume})} = e^a\\  \\left[e^{ \\ln(\\text{Volume})}\\right]^b = e^a\\, \\text{Volume}^b\\] or, more simply (and writing the number \\(e^a\\) as \\(A\\))\n\\[\\text{Pressure} = A\\,  \\text{Volume}^b\\] A power-law relationship!",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Magnitude</span>"
    ]
  },
  {
    "objectID": "Modeling/14-magnitudes.html#sec-reading-log-axes",
    "href": "Modeling/14-magnitudes.html#sec-reading-log-axes",
    "title": "14  Magnitude",
    "section": "14.4 Reading logarithmic scales",
    "text": "14.4 Reading logarithmic scales\nPlotting the logarithm of a quantity gives a visual display of the magnitude of the quantity and labels the axis as that magnitude. A useful graphical technique is to label the axis with the original quantity, letting the position on the axis show the magnitude.\nTo illustrate, Figure 14.7(left) is a log-log graph of horsepower versus displacement for the internal combustion engines reported in the Engines data frame. The points are admirably evenly spaced, but it is hard to translate the scales to the physical quantity. The right panel in Figure 14.7 shows the same data points, but now the scales are labeled using the original quantity.\n\n\n\n\n\n\n\n\nFigure 14.7: Horsepower versus displacement from the Engines data.frame plotted with log-log scales.\n\n\n\n\n\nThe tick marks on the vertical axis in the left pane are labeled for 0, 1, 2, 3, and 4. These numbers do not refer to the horsepower itself, but to the logarithm (base 10) of the horsepower. The right pane has tick labels that are in horsepower at positions marked 1, 10, 100, 1000, and 10000.\nSuch even splits of a 0-100 scale are not appropriate for logarithmic scales. One reason is that 0 cannot be on a logarithmic scale in the first place since \\(\\log(0) = -\\infty\\).\nAnother reason is that 1, 3, and 10 are pretty close to an even split of a logarithmic scale running from 1 to 10. It is something like this:\n1              2            3          5            10     x\n|----------------------------------------------------|\n0               1/3         1/2        7/10          1     log(x)\nIt is nice to have the labels show round numbers. It is also nice for them to be evenly spaced along the axis. The 1-2-3-5-10 convention is a good compromise; almost evenly separated in space yet showing simple round numbers.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Magnitude</span>"
    ]
  },
  {
    "objectID": "Modeling/14-magnitudes.html#mini-project",
    "href": "Modeling/14-magnitudes.html#mini-project",
    "title": "14  Magnitude",
    "section": "14.5 Mini-project",
    "text": "14.5 Mini-project\n\nExercise COVID-19 pandemic\nYou have likely heard the phrase “exponential growth” used to describe the COVID-19 pandemic. Let’s explore this idea using actual data.\nThe COVID-19 Data Hub is a collaborative effort of universities, government agencies, and non-governmental organizaions (NGOs) to provide up-to-date information about the pandemic. We will use the data about the US at the whole-country level. (There is also data at state and county levels. Documentation is available via the link above.)\nPerhaps the simplest display is to show the number of cumulative cases (the confirmed variable) and deaths as a function of time. We will focus on the data up to June 30, 2020.\nCopy the R/mosaic commands below into your R console to produce a graph of confirmed cases in blue and deaths in tan.\n\ngf_line(\n  deaths ~ date, \n  data = Covid_US |&gt; \n    filter(date &lt; as.Date(\"2020-06-30\")), \n  color = \"orange3\") %&gt;%\n  gf_line(confirmed ~ date, color = \"blue\")\n\n\n\n\n\n\n\n\n\n\n**Part A**  As of mid June, 2020 about how many confirmed cases were there? (Note that the labeled tick marks refer to the beginning of the month, so the point labeled `Feb` is February 1.) \nabout 50,000 about 200,000 about 500,000 about 1,000,000 about 2,000,000 about 5,000,000\n\n\nThis code makes the same graphic as above, but taking the logarithm (base 10) of the number of cases (that is, confirmed) and of the number of deaths. Since we are taking the logarithm of only the y-variable, this is called a “semi-log” plot.\n\ngf_point(\n  log10(confirmed) ~ date, \n  data = Covid_US |&gt; \n    filter(date &lt; as.Date(\"2020-06-30\")), \n  color = \"dodgerblue\") %&gt;%\n  gf_point(log10(deaths) ~ date, color = \"orange3\") \n\n\n\n\n\n\n\n\nUp through the beginning of March in the US, it is thought that most US cases were in people traveling into the US from hot spots such as China and Italy and the UK, as opposed to contagion between people within the US. (Such contagion is called “community spread.”) So let’s look at the data representing community spread, from the start of March onward.\nExponential growth appears as a straight-line on a semi-log plot. Obviously, the overall pattern of the curves is not a straight line. The explanation for this is that the exponential growth rate changes over time, perhaps due to public health measures (like business closures, mask mandates, etc.)\nThe first (official) US death from Covid-19 was recorded was recorded on Feb. 29, 2020. Five more deaths occurred two days later, bringing the cumulative number to 6.\n\n\n**Part B**  The tan data points for Feb 29/March 1 show up at zero on the vertical scale for the semi-log plot. The tan data point for March 2 is at around 2 on the vertical scale. Is this consistent with the facts stated above? \n\nNo. The data contradict the facts.\nYes. The vertical scale is in log (base 10) units, so 0 corresponds to 1 death, since \\(\\log_{10} 1 = 0\\).\nNo. The vertical scale does not mean anything.\n\n\n\nOne of the purposes of making a semi-log plot is to enable you to compare very large numbers with very small numbers on the same graph. For instance, in the semi-log plot, you can easily see when the first death occurred, a fact that is invisible in the plot of the raw counts (the first plot in this exercise).\nAnother feature of semi-log plots is that they preserve proportionality. Look at the linear plot of raw counts and note that the curve for the number of deaths is much shallower than the curve for the number of (confirmed) cases. Yet on the semi-log plot, the two curves are practically parallel.\nOn a semi-log plot, the arithmetic difference between the two curves tells you what the proportion is between those curves. The parallel curves mean that the proportion is practically constant. Calculate what the proportion between deaths and cases was in the month of May. Here’s a mathematical hint: \\(\\log_{10} \\frac{a}{b} == \\log_{10} a - \\log_{10} b\\). We are interested in \\(\\frac{a}{b}\\).\n\n\n**Part C**  What is the proportion of deaths to cases during the month of May? \nabout 1% about 2% about 5% about 25% about 75%\n\n\nIn many applications, people use semi-log plots to see whether a pattern is exponential or to compare very small and very large numbers. Often, people find it easier if the vertical scale is written in the original units rather than the log units. To accomplish both, the vertical scale can be ruled with raw units spaced logarithmically, like this:\n\ngf_point(confirmed ~ date, \n         data = Covid_US |&gt; filter(date &lt; as.Date(\"2020-06-30\")), \n         color = \"dodgerblue\") %&gt;%\n  gf_point(deaths ~ date, color = \"orange3\") %&gt;%\n  gf_refine(scale_y_log10())\n\n\n\n\n\n\n\n\nThe labels on the vertical axis show the raw numbers, while the position shows the logarithm of those numbers.\nThe next question has to do with the meaning of the interval between grid lines on the vertical axis. Note that on the horizontal axis, the spacing between adjacent grid lines is half a month.\n\n\n**Part D**  What is the numerical spacing (in terms of raw counts) between adjacent grid lines on the vertical axis? (Note: Two numbers are different by a \"factor of 10\" when one number is 10 times the other.\" Similarly, \"a factor of 100\" means that one number is 100 times the other. \n10 cases 100 cases A factor of 10. A factor of 100.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Magnitude</span>"
    ]
  },
  {
    "objectID": "Modeling/14-magnitudes.html#footnotes",
    "href": "Modeling/14-magnitudes.html#footnotes",
    "title": "14  Magnitude",
    "section": "",
    "text": "Source: https://en.wikipedia.org/wiki/Daylight↩︎\nwe are using the Kelvin scale, which is the only meaningful scale for a ratio of temperatures.↩︎\nBoyle’s notebooks are preserved at the Royal Society in London. The data in the Boyle data frame have been copied from this source.)↩︎",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Magnitude</span>"
    ]
  },
  {
    "objectID": "Modeling/15-dimensions.html",
    "href": "Modeling/15-dimensions.html",
    "title": "15  Dimensions and units",
    "section": "",
    "text": "15.1 Mathematics of quantity\nThe first step in understanding the mathematics of quantity is to make an absolute distinction between two concepts that, in everyday life, are used interchangeably: dimension and unit.\nLength is a dimension. Meters is a unit of length. We also measure length in microns, mm, cm, inches, feet, yards, kilometers, and miles, to say nothing of furlongs, fathoms, astronomical units (AU), and parsecs.\nTime is a dimension. Seconds is a unit of time. We also measure time in micro-seconds, milliseconds, minutes, hours, days, weeks, months, years, decades, centuries, and millenia.\nMass is a dimension. Kilograms is a unit of mass.\nLength, time, and mass are called fundamental dimensions. This is not because length is more important than area or volume. It is because you can construct area and volume by multiplying lengths together. This is evident when you consider units of area like square inches or cubic centimeters, but obscured in the names of units like acre, liter, gallon.\nWe use the notation L, T, and M to refer to the fundamental dimensions. (Electrical current Q is also a fundamental dimension, but we won’t have much use for it in our examples. Also useful are \\(\\Theta\\) (“theta”) for temperature, S for money, and P for a count of organisms such as the population of the US or the size of a sheep herd.)\nBrackets translate between a quantity and the dimension. For instance, [1 yard] = L, [1000 kg] = M, [3 years] = T, [10 \\(\\mu\\) (microns)] = L,",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dimensions and units</span>"
    ]
  },
  {
    "objectID": "Modeling/15-dimensions.html#mathematics-of-quantity",
    "href": "Modeling/15-dimensions.html#mathematics-of-quantity",
    "title": "15  Dimensions and units",
    "section": "",
    "text": "We will write the few basic fundamental dimensions using capital letters: L, T, M, P, S, \\(\\Theta\\). Dimensions are never expressed in terms of units. In contrast, quantities are a number value combined with the unit that value refers to.\nThe square brackets \\([\\) and \\(]\\) signify that we are looking at the dimension of the quantity inside the brackets. For instance, the population of the US state Colorado is about 5.8 million people. Surround that with square brackets and you get [5.8 million people] which is a dimension, namely, P.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dimensions and units</span>"
    ]
  },
  {
    "objectID": "Modeling/15-dimensions.html#compound-dimensions",
    "href": "Modeling/15-dimensions.html#compound-dimensions",
    "title": "15  Dimensions and units",
    "section": "15.2 Compound dimensions",
    "text": "15.2 Compound dimensions\nThere are other dimensions: volume, force, pressure, energy, torque, velocity, acceleration, and such. These are called compound dimensions because we represent them as combinations of the fundamental dimensions, L, T, and M. The notation for these combinations involves multiplication and division. For instance:\n\nVolume is L \\(\\times\\) L \\(\\times\\) L \\(=\\) L\\(^3\\), as in “cubic centimeters”\nVelocity is L/T, as in “miles per hour”\nForce is M L/T\\(^2\\), which is obscure unless you remember Newton’s Second Law that \\(\\text{F} = \\text{m}\\,\\text{a}\\): “force equals mass times acceleration.” In terms of dimension, mass is M, acceleration is L/T\\(^2\\). Multiply the two together and you get the dimension “force.”\n\nMultiplication and division are used to construct a compound dimension from the fundamental dimensions L, T, and M.\nAddition and subtraction are never used to form a compound dimension.\nMuch of the work in understanding dimensions involves overcoming the looseness of everyday speech. Remember the weight scale graduated in pounds and kilograms. The unit kilograms is a way of measuring M, but the unit of pounds is a way of measuring force: M L/T\\(^2\\).\nWeight is not the same as mass. This makes no sense to most people and does not matter in everyday life. It is only when you venture off the surface of the Earth that the difference shows up. The Mars rover Perseverance weighs 1000 kg on Earth. It was weightless for most of its journey to Mars. After landing on Mars, Perseverence weighed just 380 kg. But the rover’s mass didn’t change at all.\nAnother source of confusion carried over from everyday life is that sometimes we measure the same quantity using different dimensions. You can measure a volume by weighing water; a gallon of water weighs 8 pounds; a liter of water has a mass of 1 kg. Serious bakers measure flour by weight; a casual baker uses a measuring cup. We can measure water volume with length because water has a (more-or-less) constant mass density. But 8 pounds of gasoline is considerably more than a gallon. It turns out that the density of flour varies substantially depending on how it is packed, humidity, etc. This is why it matters whether you weigh flour for baking or measure it by volume. You can measure time by the swing of a pendulum. To measure the same time successfully with different pendula they need to have the same length, not the same mass.\nA unit is a conventional amount of a quantity of a given dimension. All lengths are the same dimensionally, but they can be measured with different conventions: inches, yards, meters, … Units for the same dimension can all be converted unambiguously one into the other. A meter is the same quantity of length as 39.3701 inches, a mile is the same length as 1609.34 meters. Liters and gallons are both units of volume (L\\(^3\\)): a gallon is the same as 3.78541 liters.\nYou will hear it said that a kilogram is 2.2 pounds. That is not strictly true. A kilogram has dimension M and a pound has dimension ML/T\\(^2\\). Quantities with different dimensions cannot be “equal” or even legitimately compared to one another. Unless you bring something else into the game that physically changes the situation, for instance, gravity (dimension of acceleration due to gravity (dimension \\(\\text{L}/\\text{T}^2\\)). The weight of a kilogram on the surface of the Earth is 2.2 pounds because gravitational acceleration is (almost) the same everywhere on the surface of the Earth.\nIt is also potentially confusing that sometimes different dimensions are used to get at the same idea. For instance, the same car that gets 35 miles / gallon in the US (dimension \\(\\text{L}/\\text{L}^3 = 1/\\text{L}^2\\)) will use 6.7 liters per 100 kilometers (\\(\\text{L}^3 / L = \\text{L}^2\\)) in Europe. Same car. Same fuel. Different conventions using different dimensions.\nKeeping track of the various compound dimensions can be tricky. For many people, it is easier to keep track of the physical relationships involved and use that knowledge to put together the dimensions appropriately. Often, the relationship can be described using specific calculus operations, so knowing dimensions and units helps you use calculus successfully.\nEasy compound dimensions that you likely already know:\n\n[Area] \\(= \\text{L}^2\\). Some corresponding units to remind you: “square feet”, “square miles”, “square centimeters.”\n[Volume] \\(= \\text{L}^3\\). Units to remind you: “cubic centimeters”, “cubic feet”, “cubic yards.” (What landscapers informally call a “yard,” for instance “10 yards of topsoil” should properly be called “10 cubic-yards of topsoil.”)\n[Velocity] \\(= \\text{L}/\\text{T}\\). Units: “miles per hour,” “inches per second.”\n[Momentum] \\(= \\text{M}\\text{L}/\\text{T}\\). Units: “kilogram meters per second.”\n\nAnticipating that you will return to this section for reference, we’ve also added some dimensions that can be understood through the relevant calculus operations.\n\n[Acceleration] \\(= \\text{L}/\\text{T}^2\\). Units: “meters per second squared,” In calculus, acceleration is the derivative of velocity with respect to time, or, equivalently, the 2nd derivative of position with respect to time.\n[Force] \\(=  \\text{M}\\, \\text{L}/\\text{T}^2\\) In calculus: force is the derivative of momentum with respect to time.\n[Energy] or [Work] \\(=   \\text{M}\\, \\text{L}^2/\\text{T}^2\\) In calculus, energy is the integral of force with respect to length.\n[Power] \\(=  \\text{M}\\, \\text{L}^2/\\text{T}^3\\) In the language of calculus, power is the derivative of energy with respect to time.\n\n\nApplication area 15.1 —With respect to what? The several different meanings of density.\n\n\n\n\n\n\n\nApplication area 15.1 Density\n\n\n\nDensity sounds like a specific concept, but there are many different kinds of densities. These have in common that they are a ratio of a physical amount to a geometric extent:\n\na physical amount: which might be mass, charge, people, etc.\na geometric extent: which might be length, area, or volume.\n\nSome examples:\n\n“paper weight” is the mass per area, typically grams-per-square-meter\n“charge density” is the amount of electrical charge, usually per area or volume\n“lineal density of red blood cells” is the number of cells in a capillary divided by the length of the capillary. (Capillaries are narrow. Red blood cells go through one after the other.)\n“population density” is people per area of ground.\n\n\n\n\nApplication area 15.2 —The units for people.\n\n\n\n\n\n\n\nApplication area 15.2: A person as a unit\n\n\n\nThe theory of dimensions and units was developed for the physical sciences. Consequently, the fundamental dimensions are those of physics: length, mass, time, electrical current, and luminous intensity.\nSince proper use of units is important even outside the physical sciences, it is helpful to recognize the dimension of several other kinds of quantity.\n\n“people” / “passengers” / “customers” / “patients” / “cases” / “passenger deaths”: these are different different ways to refer to people. we will consider such quantities to have dimension P, for population.\n“money”: Units are dollars (in many varieties: US, Canadian, Australian, New Zealand), euros, yuan (synonym: renminbi), yen, pounds (many varieties: UK, Egypt, Syria, Lebanon, Sudan, and South Sudan), pesos (many varieties), dinar, franc (Swiss, CFA), rand, riyal, rupee, won, and many others. Conversion rates depend on the situation and national policy, but we will consider money a dimension, denoted by S (from the name of the first coinage, the Mesopotamian Shekel).\n\nExamples:\n\nPassenger-miles is a standard unit of transport.\nPassenger-miles-per-dollar is an appropriate unit of the economic efficiency of transport.\nPassenger-deaths per million passenger-mile is one way to describe the risk of transport.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dimensions and units</span>"
    ]
  },
  {
    "objectID": "Modeling/15-dimensions.html#arithmetic-with-dimensions",
    "href": "Modeling/15-dimensions.html#arithmetic-with-dimensions",
    "title": "15  Dimensions and units",
    "section": "15.3 Arithmetic with dimensions",
    "text": "15.3 Arithmetic with dimensions\nRecall the rules for arithmetic dimensioned quantities. We restate them briefly with the square-bracket notation for “the dimension of.” For instance, “the dimension of \\(b\\)” is written \\([b]\\). We also write \\([1]\\) to stand for the dimension of a pure number, that is, a quantity without dimension.\n\n\n\n\nTable 15.1: Conditions under which functions can be applied to dimensionful quantitities. Note that \\([a] = [b]\\) means that the dimension of \\(a\\) and of \\(b\\) are the same. For instance, even though 1 mm and 500 miles are very different distances, [1 mm]\\(=\\)[500 miles]. Both [1 mm] and [500 miles] are dimension L.\n\n\n\n\n\n\n\n\n\n\n\nOperation\nResult\nOnly if satisfies\nMetaphor\n\n\n\n\nMultiplication\n\\([a \\times b] = [a]  \\times [b]\\)\nanything goes\npromiscuous\n\n\nDivision\n\\([a \\div b] = [a] \\div  [b]\\)\nanything goes\npromiscuous\n\n\nAddition\n\\([a + b] = [a]\\)\n\\([a] = [b]\\)\nmonogomous\n\n\nSubtraction\n\\([a - b] = [a]\\)\n\\([a] = [b]\\)\nmonogomous\n\n\nTrigonometric\n\\([\\sin(a)] = [1]\\)\n\\([a] = [1]\\)\ncelibate\n\n\nExponential\n\\([e^a] = [1]\\)\n\\([a] = [1]\\) (of course, \\([e] = [1]\\))\ncelibate\n\n\nPower-law\n\\([b  ^  a] = \\underbrace{[b]\\times[b]\\times ...\\times [b]}_{a\\ \\ \\text{times}}\\)\n\\([a]  = [1]\\) with \\(a\\) an integer\nexponent celibate\n\n\nSquare root\n\\([\\sqrt{b}] = [c]\\)\n\\([b] = [c\\times c]\\)\nidiosyncratic\n\n\nCube root\n\\([\\sqrt[3]{b}] = [c]\\)\n\\([b] = [c \\times c \\times  c]\\)\nidiosyncratic\n\n\nBump\n\\([\\text{bump}(a)] = [1]\\)\n\\([a] =  [1]\\)\ncelibate\n\n\nSigmoid\n\\([\\text{sigmoid}(a)] =  [1]\\)\n\\([a] = [1]\\)\ncelibate\n\n\n\n\n\n\n\n15.4 Example: Dimensional analysis\nWe want to relate the period (in T) of a pendulum to its length and mass. Acceleration due to gravity also plays a role; that has dimension \\(\\text{L}\\cdot \\text{T}^{-2}\\). For simplicity, we will assume that only the bob at the end of the pendulum cable or rod has mass.\nThe analysis strategy is to combine the four quantities we think play a role into one total quantity that is dimensionless. Since it is dimensionless, it can be constant regardless of the mass, length, period, or gravity of each situation.\n\\[\\text{[Period]}^a \\cdot \\text{[Mass]}^b \\cdot \\text{[Length]}^c \\cdot \\text{[Gravity]}^d = T^a \\cdot M^b \\cdot L^c \\cdot L^d \\cdot T^{-2d} = [1]\\] To be dimensionless:\n\n\\(c = -d\\), cancel out the L\n\\(a = 2d\\), cancel out the T\n\\(b=0\\), there is no other mass term, and we need to cancel out the M\n\nAll of the exponents can be put in terms of \\(d\\). That does not tell us what \\(d\\) should be, but whatever value for \\(d\\) we decide to choose, we get a ratio that is equivalent to:\n\\[ \\frac{[\\text{Gravity}]\\cdot [\\text{Period}]^2}{[\\text{Length}]} = [1]\\]\nThis is a relationship between dimensions of quantities. To render it into a formula involving the quantities themselves we need to take into account the units.\n\\[ \\frac{\\text{Gravity}\\cdot \\text{Period}^2}{\\text{Length}} = B\\]\nWe can experimentally determine the numerical value of the dimensionless constant \\(B\\) by measuring the period and length of a pendulum and (on Earth) recognizing that gravitational acceleration on Earth’s surface is 9.8 meters-per-second-squared. Such experiments and mathematical models using differential equations give \\(B = (2\\pi)^2\\).\n\n\n15.5 Conversion: Flavors of 1\nNumbers are dimensionless but not necessarily unitless. Failure to accept this distinction is one of the prime reasons people have trouble figuring out how to convert from one unit to another.\nThe number one is a favorite of elementary school students because its multiplication and division tables are completely simple. Anything times one, or anything divided by one, is simply that thing. Addition and subtraction are pretty simple, too, a matter of counting up or down.\nWhen it comes to quantities, there is not just one one but many. And often they look nothing like the numeral 1. Some examples of 1 as a quantity:\n\n\\(\\frac{180}{\\pi} \\frac{\\text{degrees}}{\\text{radians}}\\)\n\\(0.621371 \\frac{\\text{mile}}{\\text{kilometer}}\\)\n\\(3.78541 \\frac{\\text{liter}}{\\text{gallon}}\\)\n\\(\\frac{9}{5} \\frac{^\\circ F}{^\\circ C}\\)\n\\(\\frac{1}{12} \\frac{\\text{dozen}}{\\text{item}}\\)\n\nI like to call these and others different flavors of one.\nIn every one of the above examples, the dimension of the numerator matches the dimension of the denominator. The same is true when comparing feet and meters ([feet / meter] is L/L = [1]), or comparing cups and pints ([cups / pint] is \\(\\text{L}^3/\\text{L}^3 = [1]\\)) or comparing miles per hour and feet per second ([miles/hour / ft per sec] = L/T / L/T = [1]). Each of these quantities has units but it has no dimension.\nIt is helpful to think about conversion between units as a matter of multiplying by the appropriate flavor of 1. Such conversion will not change the dimension of the quantity but will render it in new units.\n\nExample: Convert 100 feet-per-second into miles-per-hour. First, write the quantity to be converted as a fraction and alongside it, write the desired units after the conversion. In this case that will be \\[100 \\frac{\\text{feet}}{\\text{second}} \\ \\ \\ \\text{into} \\ \\ \\ \\frac{\\text{miles}}{\\text{hour}}\\]\nFirst, we will change feet into miles. This can be accomplished by multiplying by the flavor of one that has units miles-per-foot. Second, we will change seconds into hours. Again, a flavor of 1 is involved.\nWhat number will give a flavor of one? One mile is 5280 feet, so \\[\\frac{1}{5280} \\frac{\\text{miles}}{\\text{foot}}\\] is a flavor of one.\nNext, we need a flavor of one that will turn \\(\\frac{1}{\\text{second}}\\) into \\(\\frac{\\text{1}}{\\text{hour}}\\). We can make use of a minute being 60 seconds, and an hour being 60 minutes. \\[\\underbrace{\\frac{60\\  \\text{s}}{\\text{minute}}}_\\text{flavor of 1}\\  \\underbrace{\\frac{60\\ \\text{minutes}}{\\text{hour}}}_\\text{flavor of 1} = \\underbrace{3600\\frac{\\text{s}}{ \\text{hour}}}_\\text{flavor of 1}\\]\nMultiplying our carefully selected flavors of one by the initial quantity, we get \\[\n\\underbrace{\\frac{1}{5280} \\frac{\\text{mile}}{\\text{foot}}}_\\text{flavor of 1} \\times \\underbrace{3600 \\frac{\\text{s}}{\\text{hour}}}_\\text{flavor of 1} \\times \\underbrace{100 \\frac{\\text{feet}}{\\text{s}}}_\\text{original quantity} = 100 \\frac{3600}{5280} \\frac{\\text{miles}}{\\text{hour}} = 68.18 \\frac{\\text{miles}}{\\text{hour}}\\]\n\n\n\n15.6 Dimensions and linear combinations\nLow-order polynomials are a useful way of constructing model functions. For instance, suppose we want a model of the yield of corn in a field per inch of rain over the growing season, will call it corn(rain). The output will have units of bushels (of corn). The input will have units of inches (of rain). A second-order polynomial will be appropriate for reasons to be discussed in Chapter 24.\n\\[\\text{corn(rain)} \\equiv a_0 + a_1\\, \\text{rain} + \\frac{1}{2} a_2\\, \\text{rain}^2\\] Of course, the addition in the linear combination will only make sense if all three terms \\(a_0\\), \\(a_1\\,\\text{rain}\\), and \\(\\frac{1}{2}\\, a_2\\, \\text{rain}^2/2\\) have the same dimension. But \\([\\text{rain}] \\neq [\\text{rain}^2]\\). In order for things to work out, the coefficients must themselves have dimension. We know the output of the function will have dimension \\([\\text{volume}] = \\text{L}^3\\). Thus, \\([a_0] = \\text{L}^3\\).\n\\([a_1]\\) must be different, because it has to combine with the \\([\\text{rain}] = \\text{L}\\) and produce \\(\\text{L}^3\\). Thus, \\([a_1] = \\text{L}^2\\).\nFinally, \\([a_2] = \\text{L}\\). Multiplying that by \\([\\text{rain}]^2\\) will give the required \\(\\text{L}^3\\)\n\n\n\n\n\n\n\nThink in degrees, compute in radians\n\n\n\nIn everyday communication as well as in most domains such as construction, geography, navigation, and astronomy we measure angles in degrees. 90 degrees is a right angle. But in mathematics, the unit of angle is radians where a right angle is 1.5708 radians. (1.5708 is the decimal version of \\(\\pi/2\\).) The conversion function, which we will call raddeg(), is \\[\\text{raddeg}(r) \\equiv \\frac{180}{\\pi} r\\] The function that converts degrees to radians, which we will call degrad() is very similar: \\[\\text{degrad}(d) \\equiv \\frac{\\pi}{180} d\\] (Incidentally, \\(\\frac{180}{\\pi} = 57.296\\) while \\(\\frac{\\pi}{180} = 0.017453\\).)\nIn traditional notation, the trigonometric functions such as \\(\\sin()\\) and \\(\\tan()\\) can be written with an argument either in degrees or radians. For instance, \\(\\sin(90^\\circ) = \\sin\\left(\\frac{\\pi}{2}\\right)\\). Similarly, for the inverse functions like \\(\\arccos()\\) the units of the output are not specified. This works because there is always a human to intervene between the written expression and the eventual computation.\nIn R, as in many other computer languages, there an expression like sin(90 deg) generates an error. In these languages, 90 deg is not a valid expression (although it might be good if it were valid!). In these and many other languages, angles are always given in radians. Such consistency is admirable, but people are not always so consistent. It is a common source of computer bugs that angles in degrees are handed off to functions like \\(\\sin()\\) and that the output of \\(\\arccos()\\) is (wrongly) interpreted as degrees rather than radians.\nFunction composition to the rescue!\nConsider this function given in the Wikipedia article on the position of the sun as seen from Earth.1 \\[\\delta_\\odot(n) \\equiv - 23.44^\\circ \\cdot \\cos \\left [ \\frac{360^\\circ}{365\\, \\text{days}} \\cdot \\left ( n + 10 \\right ) \\right ]\\] Where \\(n\\) is zero at the midnight marking New Years and increases by 1 per day. (The \\(n+10\\) has units of days and translates New Years back 10 days, to the day of the winter solstice.) \\(\\delta_\\odot()\\) gives the declination of the sun: the latitude pieced by an imagined line connecting the centers of the earth and the sun.\nThe Wikipedia formula is well written in that it uses some familiar numbers to help the reader see where the formula comes from. 365 is recognizably the length of the year in days. \\(360^\\circ\\) is the angle traversed when making a full cycle around a circle. \\(23.44^\\circ\\) is less familiar, but the student of geography might recognize it as the latitude of the Tropic of Cancer, the latitude farthest north where the sun is directly overhead at noon (on the day of the summer solstice).\nBut there is a world of trouble for the programmer who implements the formula as\n\ndec_sun &lt;- makeFun(-23.44 * cos((360/365)*(n+10)) ~ n)\n\nFor instance, the equinoxes are around March 21 (n=81) and Sept 21 (n=264). On an equinox, the declination of the sun is zero degrees. But let’s plug \\(n=81\\) and \\(n=264\\) into the formula and see what we get.\n\ndec_sun(81)\n## [1] 5.070321\ndec_sun(264)\n## [1] -23.38324\n\nThe equinoxes aren’t even equal! And they are not close to zero. Does this mean astronomy is wrong?\nThe Wikipedia formula should have been programmed this way, using 2 \\(\\pi\\) radians instead of 360 degrees in the argument to the cosine function:\n\ndec_sun_right &lt;- \n  makeFun(-23.44 * cos(( 2*pi/365)*(n+10)) ~ n)\ndec_sun_right(81)\n## [1] -0.1008749\ndec_sun_right(264)\n## [1] -0.1008749\n\nThe deviation of one-tenth of a degree reflects rounding off the time of the equinox to the nearest day.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dimensions and units</span>"
    ]
  },
  {
    "objectID": "Modeling/15-dimensions.html#sec-pendulum-dimensions",
    "href": "Modeling/15-dimensions.html#sec-pendulum-dimensions",
    "title": "15  Dimensions and units",
    "section": "15.4 Example: Dimensional analysis",
    "text": "15.4 Example: Dimensional analysis\nWe want to relate the period (in T) of a pendulum to its length and mass. Acceleration due to gravity also plays a role; that has dimension \\(\\text{L}\\cdot \\text{T}^{-2}\\). For simplicity, we will assume that only the bob at the end of the pendulum cable or rod has mass.\nThe analysis strategy is to combine the four quantities we think play a role into one total quantity that is dimensionless. Since it is dimensionless, it can be constant regardless of the mass, length, period, or gravity of each situation.\n\\[\\text{[Period]}^a \\cdot \\text{[Mass]}^b \\cdot \\text{[Length]}^c \\cdot \\text{[Gravity]}^d = T^a \\cdot M^b \\cdot L^c \\cdot L^d \\cdot T^{-2d} = [1]\\] To be dimensionless:\n\n\\(c = -d\\), cancel out the L\n\\(a = 2d\\), cancel out the T\n\\(b=0\\), there is no other mass term, and we need to cancel out the M\n\nAll of the exponents can be put in terms of \\(d\\). That does not tell us what \\(d\\) should be, but whatever value for \\(d\\) we decide to choose, we get a ratio that is equivalent to:\n\\[ \\frac{[\\text{Gravity}]\\cdot [\\text{Period}]^2}{[\\text{Length}]} = [1]\\]\nThis is a relationship between dimensions of quantities. To render it into a formula involving the quantities themselves we need to take into account the units.\n\\[ \\frac{\\text{Gravity}\\cdot \\text{Period}^2}{\\text{Length}} = B\\]\nWe can experimentally determine the numerical value of the dimensionless constant \\(B\\) by measuring the period and length of a pendulum and (on Earth) recognizing that gravitational acceleration on Earth’s surface is 9.8 meters-per-second-squared. Such experiments and mathematical models using differential equations give \\(B = (2\\pi)^2\\).",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dimensions and units</span>"
    ]
  },
  {
    "objectID": "Modeling/15-dimensions.html#conversion-flavors-of-1",
    "href": "Modeling/15-dimensions.html#conversion-flavors-of-1",
    "title": "15  Dimensions and units",
    "section": "15.5 Conversion: Flavors of 1",
    "text": "15.5 Conversion: Flavors of 1\nNumbers are dimensionless but not necessarily unitless. Failure to accept this distinction is one of the prime reasons people have trouble figuring out how to convert from one unit to another.\nThe number one is a favorite of elementary school students because its multiplication and division tables are completely simple. Anything times one, or anything divided by one, is simply that thing. Addition and subtraction are pretty simple, too, a matter of counting up or down.\nWhen it comes to quantities, there is not just one one but many. And often they look nothing like the numeral 1. Some examples of 1 as a quantity:\n\n\\(\\frac{180}{\\pi} \\frac{\\text{degrees}}{\\text{radians}}\\)\n\\(0.621371 \\frac{\\text{mile}}{\\text{kilometer}}\\)\n\\(3.78541 \\frac{\\text{liter}}{\\text{gallon}}\\)\n\\(\\frac{9}{5} \\frac{^\\circ F}{^\\circ C}\\)\n\\(\\frac{1}{12} \\frac{\\text{dozen}}{\\text{item}}\\)\n\nI like to call these and others different flavors of one.\nIn every one of the above examples, the dimension of the numerator matches the dimension of the denominator. The same is true when comparing feet and meters ([feet / meter] is L/L = [1]), or comparing cups and pints ([cups / pint] is \\(\\text{L}^3/\\text{L}^3 = [1]\\)) or comparing miles per hour and feet per second ([miles/hour / ft per sec] = L/T / L/T = [1]). Each of these quantities has units but it has no dimension.\nIt is helpful to think about conversion between units as a matter of multiplying by the appropriate flavor of 1. Such conversion will not change the dimension of the quantity but will render it in new units.\n\nExample: Convert 100 feet-per-second into miles-per-hour. First, write the quantity to be converted as a fraction and alongside it, write the desired units after the conversion. In this case that will be \\[100 \\frac{\\text{feet}}{\\text{second}} \\ \\ \\ \\text{into} \\ \\ \\ \\frac{\\text{miles}}{\\text{hour}}\\]\nFirst, we will change feet into miles. This can be accomplished by multiplying by the flavor of one that has units miles-per-foot. Second, we will change seconds into hours. Again, a flavor of 1 is involved.\nWhat number will give a flavor of one? One mile is 5280 feet, so \\[\\frac{1}{5280} \\frac{\\text{miles}}{\\text{foot}}\\] is a flavor of one.\nNext, we need a flavor of one that will turn \\(\\frac{1}{\\text{second}}\\) into \\(\\frac{\\text{1}}{\\text{hour}}\\). We can make use of a minute being 60 seconds, and an hour being 60 minutes. \\[\\underbrace{\\frac{60\\  \\text{s}}{\\text{minute}}}_\\text{flavor of 1}\\  \\underbrace{\\frac{60\\ \\text{minutes}}{\\text{hour}}}_\\text{flavor of 1} = \\underbrace{3600\\frac{\\text{s}}{ \\text{hour}}}_\\text{flavor of 1}\\]\nMultiplying our carefully selected flavors of one by the initial quantity, we get \\[\n\\underbrace{\\frac{1}{5280} \\frac{\\text{mile}}{\\text{foot}}}_\\text{flavor of 1} \\times \\underbrace{3600 \\frac{\\text{s}}{\\text{hour}}}_\\text{flavor of 1} \\times \\underbrace{100 \\frac{\\text{feet}}{\\text{s}}}_\\text{original quantity} = 100 \\frac{3600}{5280} \\frac{\\text{miles}}{\\text{hour}} = 68.18 \\frac{\\text{miles}}{\\text{hour}}\\]",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dimensions and units</span>"
    ]
  },
  {
    "objectID": "Modeling/15-dimensions.html#dimensions-and-linear-combinations",
    "href": "Modeling/15-dimensions.html#dimensions-and-linear-combinations",
    "title": "15  Dimensions and units",
    "section": "15.6 Dimensions and linear combinations",
    "text": "15.6 Dimensions and linear combinations\nLow-order polynomials are a useful way of constructing model functions. For instance, suppose we want a model of the yield of corn in a field per inch of rain over the growing season, will call it corn(rain). The output will have units of bushels (of corn). The input will have units of inches (of rain). A second-order polynomial will be appropriate for reasons to be discussed in Chapter 24.\n\\[\\text{corn(rain)} \\equiv a_0 + a_1\\, \\text{rain} + \\frac{1}{2} a_2\\, \\text{rain}^2\\] Of course, the addition in the linear combination will only make sense if all three terms \\(a_0\\), \\(a_1\\,\\text{rain}\\), and \\(\\frac{1}{2}\\, a_2\\, \\text{rain}^2/2\\) have the same dimension. But \\([\\text{rain}] \\neq [\\text{rain}^2]\\). In order for things to work out, the coefficients must themselves have dimension. We know the output of the function will have dimension \\([\\text{volume}] = \\text{L}^3\\). Thus, \\([a_0] = \\text{L}^3\\).\n\\([a_1]\\) must be different, because it has to combine with the \\([\\text{rain}] = \\text{L}\\) and produce \\(\\text{L}^3\\). Thus, \\([a_1] = \\text{L}^2\\).\nFinally, \\([a_2] = \\text{L}\\). Multiplying that by \\([\\text{rain}]^2\\) will give the required \\(\\text{L}^3\\)\n\n\n\n\n\n\n\nThink in degrees, compute in radians\n\n\n\nIn everyday communication as well as in most domains such as construction, geography, navigation, and astronomy we measure angles in degrees. 90 degrees is a right angle. But in mathematics, the unit of angle is radians where a right angle is 1.5708 radians. (1.5708 is the decimal version of \\(\\pi/2\\).) The conversion function, which we will call raddeg(), is \\[\\text{raddeg}(r) \\equiv \\frac{180}{\\pi} r\\] The function that converts degrees to radians, which we will call degrad() is very similar: \\[\\text{degrad}(d) \\equiv \\frac{\\pi}{180} d\\] (Incidentally, \\(\\frac{180}{\\pi} = 57.296\\) while \\(\\frac{\\pi}{180} = 0.017453\\).)\nIn traditional notation, the trigonometric functions such as \\(\\sin()\\) and \\(\\tan()\\) can be written with an argument either in degrees or radians. For instance, \\(\\sin(90^\\circ) = \\sin\\left(\\frac{\\pi}{2}\\right)\\). Similarly, for the inverse functions like \\(\\arccos()\\) the units of the output are not specified. This works because there is always a human to intervene between the written expression and the eventual computation.\nIn R, as in many other computer languages, there an expression like sin(90 deg) generates an error. In these languages, 90 deg is not a valid expression (although it might be good if it were valid!). In these and many other languages, angles are always given in radians. Such consistency is admirable, but people are not always so consistent. It is a common source of computer bugs that angles in degrees are handed off to functions like \\(\\sin()\\) and that the output of \\(\\arccos()\\) is (wrongly) interpreted as degrees rather than radians.\nFunction composition to the rescue!\nConsider this function given in the Wikipedia article on the position of the sun as seen from Earth.1 \\[\\delta_\\odot(n) \\equiv - 23.44^\\circ \\cdot \\cos \\left [ \\frac{360^\\circ}{365\\, \\text{days}} \\cdot \\left ( n + 10 \\right ) \\right ]\\] Where \\(n\\) is zero at the midnight marking New Years and increases by 1 per day. (The \\(n+10\\) has units of days and translates New Years back 10 days, to the day of the winter solstice.) \\(\\delta_\\odot()\\) gives the declination of the sun: the latitude pieced by an imagined line connecting the centers of the earth and the sun.\nThe Wikipedia formula is well written in that it uses some familiar numbers to help the reader see where the formula comes from. 365 is recognizably the length of the year in days. \\(360^\\circ\\) is the angle traversed when making a full cycle around a circle. \\(23.44^\\circ\\) is less familiar, but the student of geography might recognize it as the latitude of the Tropic of Cancer, the latitude farthest north where the sun is directly overhead at noon (on the day of the summer solstice).\nBut there is a world of trouble for the programmer who implements the formula as\n\ndec_sun &lt;- makeFun(-23.44 * cos((360/365)*(n+10)) ~ n)\n\nFor instance, the equinoxes are around March 21 (n=81) and Sept 21 (n=264). On an equinox, the declination of the sun is zero degrees. But let’s plug \\(n=81\\) and \\(n=264\\) into the formula and see what we get.\n\ndec_sun(81)\n## [1] 5.070321\ndec_sun(264)\n## [1] -23.38324\n\nThe equinoxes aren’t even equal! And they are not close to zero. Does this mean astronomy is wrong?\nThe Wikipedia formula should have been programmed this way, using 2 \\(\\pi\\) radians instead of 360 degrees in the argument to the cosine function:\n\ndec_sun_right &lt;- \n  makeFun(-23.44 * cos(( 2*pi/365)*(n+10)) ~ n)\ndec_sun_right(81)\n## [1] -0.1008749\ndec_sun_right(264)\n## [1] -0.1008749\n\nThe deviation of one-tenth of a degree reflects rounding off the time of the equinox to the nearest day.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dimensions and units</span>"
    ]
  },
  {
    "objectID": "Modeling/15-dimensions.html#footnotes",
    "href": "Modeling/15-dimensions.html#footnotes",
    "title": "15  Dimensions and units",
    "section": "",
    "text": "Article accessed on May 30, 2021↩︎",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dimensions and units</span>"
    ]
  },
  {
    "objectID": "Modeling/Modeling-projects.html",
    "href": "Modeling/Modeling-projects.html",
    "title": "Projects",
    "section": "",
    "text": "Block 1 Project 1: Ebola in Guinea, part I\nIn December 2013, an 18-month-old boy from a village in Guinea suffered fatal diarrhea. Over the next months a broader outbreak was discovered, and in mid-March 2014, the Pasteur Institute in France confirmed the illness as Ebola-Virus Disease caused by the Zaire ebolavirus.\nAlthough the outbreak was first recognized in Guinea, it eventually encompassed Liberia and Sierra Leone as well. By July 2014, the outbreak spread to the capitals of all three countries. We will examine the time course of the epidemic using reports that were issued by the World Health Organization along with some concepts and techniques we have been studying in the first block of Math 141Z. Data are provided by the US Centers for Disease Control (CDC).",
    "crumbs": [
      "BLOCK I. Modeling",
      "Projects"
    ]
  },
  {
    "objectID": "Modeling/Modeling-projects.html#block-1-project-1-ebola-in-guinea-part-i",
    "href": "Modeling/Modeling-projects.html#block-1-project-1-ebola-in-guinea-part-i",
    "title": "Projects",
    "section": "",
    "text": "Data\nThe CDC data are provided to you as a dataframe named EbolaAll. The dataframe consists of 182 reports spread out over 18 months during 2014 and 2015. Each report is represented by a single row in the dataframe . Each report gives the number of new cases and disease-related deaths since the last report in each or three countries: Sierra Leon, Liberia, and Guinea. These values have been calculated from the raw, cumulative data. The data have been scrubbed to remove obvious errors.\n\n\nExploring the Data\nWe will begin by looking at some data. Use head(EbolaAll) to see the column headers and first 6 rows of data. As you can see, the EbolaAl dataframe is structured like a table, and each row contains multiple columns of data. The table below lists the column names found in EbolaAll dataframe along with a brief description of what the column records.\n\nDate: Date when the World Health Organization issued the report\nGcases: Number of new cases in Guinea\nGdeaths: Number of new deaths in Guinea\nLcases: Number of new cases in Liberia\nLdeaths: Number of new deaths in Liberia\nSLcases: Number of new cases in Sierra Leone\nSLdeaths: Number of new deaths in Sierra Leone\nTotCases: Total number of cases across all three countries\nTotDeaths: Total number of deaths across all three countries\n\nWe will focus on the Guinea data. As we have done throughout this book, we model this data using the pattern-book functions.\n\nIn a SANDBOX, use gf_point() to plot the number of new cases in Guinea (variables Date and Gcases in dataframe EbolaAll).\n\n**Question 1**: Look at the shape of the graph. Of the functions we have studied, which would be most appropriate as a model for the new cases in Guinea? If none of the functions we have studied best matches this data, select \"None of the above.\" \nAlthough we are tempted to regard data sets as definitive, they are the culmination of an imperfect process of data collection in the field and imperfect processing/correction/amendment by people. For instance, most of the cases (and deaths) in the epidemic were never confirmed by viral testing and are considered “suspected cases.” Cases and deaths reported on one day might possibly be from previous days, and some cases and deaths were, no doubt, not reported.\nIn the Sandbox, run the following command to look at the data:\n\nDT::datatable(EbolaAll)\n\nThe resulting display is interactive; you can page through the many rows of data.\n\nQuestion 2: Look through the data printout and find a data point form one of the World Health Organization reports that seems strange or inconsistent when compared with nearby reports. To receive full credit for this question, you must state the country and date of the report and explain your reasons for identifying this report as inconsistent.\n\n\n\nData Wrangling\nAn essential part of all projects involving data is to prepare the data for analysis: a process often called data wrangling. Although data wrangling is an important skill, it is not the topic of this book. So we will take care of the programming and present you with the results in the form of a data frame EbolaGuinea.\nThe wrangling accomplished two things: averaging the data over 7-report windows and extracting a numerical “day number” from the Date of each report.\nFirst, rather than working with the year:month:day format of Date, it is convenient to use a purely numerical quantity to represent time. So, we will translate the day when reports were issued into number of days from the initial report, presenting the result in the column Days. This makes the data-analysis programming easier, since all our mathematical software knows how to handle numbers, but not necessarily calendar dates.\nSecond, we will smooth the number of cases and the number of deaths. We did this by averaging each day’s number-of-cases and number-of deaths over several report. As you can see in the data, the interval between reports is not constant. Some reports occur just one day after the previous report; sometimes there is a week between reports. The widely spaced reports tend to have much higher cases and deaths than reports that come every day. This is for the simple reason that each report gives the number of new cases and deaths since the previous report; there is more time for the numbers to accumulate when there is a wide spacing between reports. The data reflect not just the process of the Ebola epidemic, but also the way the data were collected and reported, which is not directly of interest to us. To reduce this report-to-report fluctuation, we will average the number of new cases in each report with the three reports before and three reports after: a moving average over seven reports.\nThe result of our wrangling—the dataframe EbolaGuinea—includes three new columns:\n\nDays: When the report was issued in terms of a count of days from the initial report.\nG7Rcases: Number of new cases in Guinea averaged across 7 reports\nG7Rdeaths: Number of new deaths in Guinea averaged across 7 reports\n\n\n\nAnalysis of the outbreak\nIn your Sandbox, use gf_point() to plot the smoothed data in EbolaGuinea to show the time course of the epidemic. The variables you want to plot are G7Rcases versus Days. averaged number of new cases in Guinea (variables Days and G7Rcases in dataframe EbolaGuinea).\n\nQuestion 3 Look at the shape of the graph. Of the functions we have studied, which would be most appropriate as a model for the new cases in Guinea? If none of the functions we have studied best matches this data, select “None of the above.”\n\nIt is well known that the infection rate during an outbreak follows a Gaussian pattern when the population interacts consistently. Ebola initially spreads exponentially as people pass the virus to one another. As people are exposed to the virus, there are fewer and fewer people who are still susceptible. The result is that the rate of exponential growth slows and, when the fraction of the population that is susceptible becomes small, the epidemic slows down and the number of new cases decays exponentially. Overall, the pattern of new cases day-by-day looks like a gaussian: zero cases per day before the outputbreak, exponential growth at first after the virus is introduced to the population, leveling out for a time, and exponential decay back to zero new cases.\nThe graph of G7Rcases versus Days looks only vaguely like a gaussian bump. One possible explanation is that the data summarize more than one outbreak, as the virus spreads from one region to another. We will treat each individual output as a gaussian and try to figure out how many of them make up the whole data. The individual outbreaks will be modelled as a gaussian function with it is own center and width. The center for each gaussian corresponds to the peak of the hypothesized outbreak in one particular region.\nWe will combine the several gaussians in a linear combination. The scalar multiplying each gaussian is related to the size of the population exposed in each outbreak.\nHere is an R/mosaic definition of a linear combination of two gaussian outbreaks.\n\nmy_mod &lt;- makeFun(15800*(A*dnorm(t,m1,sd1) + \n                            (1-A)*dnorm(t,m2,sd2)) ~ t) \n\nThere are five parameters in the function. m1 the time when the first hypothesized outbreak peaked, m2 is the peak time of the second hypothesized outbreaks, and sd1 and sd2 reflect the duration of each outbreak. The parameter A represents the relative population sizes of the two regions. The number 15,800 reflects the total number of cases, which we know retrospectively. (The 15,800 includes a correction for the average day spacing between reports, but that detail need not concern us here.)\nNote that selecting A sets the amplitude for both Gaussians, i.e. A and 1−A. The amplitudes A and 1-A sum up to 1. Arranging things this way effectively make A the size of the one outbreak, leaving 1-A to be the size of the other outbreak.\nTo see how these Gaussians work together, start with the following values: A = 0.5, m1 = 150, sd1 = 50, m2 = 350, sd2 = 50. Do not change the 15800 number which reflects the overall size of the whole outbreak, as opposed to the relative size of the hypothesized individual outbreaks: A and 1-A.\nUse gf_point(), the pipe operator %&gt;%, and slice_plot() to overlay your model on top of the data. Discrepancies between the model and the data can lead you to improve the model by adjusting the parameters. It is to be expected that you will need several cycles of such parameter adjustments until you find a model that seems to agree with the data in a satisfactory way. to you Guinea cases along with your model.\nOnce you have adjusted your model to match the data as closely as you can, write down the parameters you used in your report, as well as the graph comparing the data to your final model.\nQuestion 3: Find the longest time interval during which your model systematically overstates the number of cases. What’s the start and end of that interval? (Your answer can be rough, just so long as it points the reader of your report to the interval you mean.)\n\n\nMore data smoothing\nAccurately modeling the Guinea case data with two gaussian functions is difficult. Despite the smoothing, the numbers in G7Rcases fluctuate irregularly and substantially from report to report.\nAnother way to smooth the data, that is, to reduce the irregular report-to-report fluctuations, is to accumulate the number of new cases to get the total number of cases up through each report. (This accumulation, at the end of the epidemic, will be the total number of people who came down with Ebola illness.)\nSuch a sum of new cases from the beginning of the outbreak through the day of each report is called a cumulative sum or “running total.” Keep in mind that this is not a number, but a new column of data giving the number of cases to date for each of the dates in the data.\nTo illustrate, the small set of numbers: [1 2 3 4 5 6]. The cumulative sum of this set shows the running total: [1 3 6 10 15 21]. Make sure you are comfortable with how the second set of numbers is related to the first.\nThe R function cumsum() calculates the cumulative sum on a column of data. You will need to do a little data wrangling; we cannot do it for you because we don’t know what model you decided to settle on.\nThere are many computer systems for data wrangling. You may have heard of one called Structured Query Language (SQL) which is a required skill listed in many job listings and a multi-billion dollar component of the economy. We will use a system called dplyr specially designed for the R language. In the dplyr system, the function mutate() is used to create a new column in a data frame by carrying out calculations on the existing columns.\nHere is an R/dplyr command to generate the running total of cases. (we are using Gcases instead of the smoothed G7Rcases because the cumulative summing will do the smoothing for us.)\n\nEbolaGuinea &lt;- EbolaGuinea |&gt; mutate(GcasesTotal = cumsum(Gcases)) \n\nIt is worthwhile to parse that command carefully. The part to the right of &lt;- is an instruction for taking the EbolaGuinea data frame and adding a new column. The new column will be called GcasesTotal. The values in the new column will be generated by applying cumsum() to the Gcases column. Altogether, the right-hand side of the statement creates a new data frame that includes the new column. The left-hand side of the statement stores this new data frame under a name. For convenience, we are using EbolaGuinea as that name. Effectively, the command as a whole, including the name assignment EbolaGuinea &lt;- can be seen as updating the data frame with the new column.\nYou can verify that the new column is in place by showing the first several lines of the updated data frame:\n\nhead(EbolaGuinea) \n\nUse gf_point() to plot GCasesTotal versus Days. The graph will show how the number of cases accumulated over time to to the overall total for the epidemic as a whole.\n\nQuestion 5: Consider which of the functions we have studied could be fitted to represent the GCasesTotal versus Days curve in your graph? it is likely that none of the functions we have studied fits the data particularly well, but one of them will be better than the others.\n\nWe haven’t forgotten that you already created a model of the new cases by a linear combination of gaussian functions. What we want to do now is translate your model for the number of new cases each day into a model of the cumulative number of cases to date. In other words, we want to perform the same action that cumsum() does, but applied to your model rather than data.\nTo do this, you will replace the gaussian dnorm() function in your model with the sigmoid pnorm(). No other changes are needed. This works because the gaussian and sigmoid functions are related to one another in just the same way as cumsum() relates the GCases column to the GCasesTotal column.\nRemember that the Gaussian and sigmoid functions are related to one another. The sigmoid is the accumulation of the Gaussian, i.e. It is the cumulative sum of the Gaussian. We will use this relationship to improve your double Gaussian model.\nCreate a new function in your sandbox called my_mod_cumulative(). Plot this new function over the GCasesTotal versus Dates data. As before, you can do this with gf_point(), the pipe operator %&gt;%, and slice_plot().\n\nQuestion 6: Observe the rate of change (slope) of your model. The rate of change represents the number of new infections per day. Find the day on which your rate of change is greatest. Describe how this is related to the argmax of your model from Question 4.\n\nTry adjusting the parameters in my_mod_cumulative() to better match the function to the GCasesTotal data. Some aspects of the outbreak can be better seen from the cumulative number of cases to date, and other aspects may be better seen with the-newcases-each-day data.\nNow we will leverage the relationship between the Gaussian and sigmoid functions. Adjust your double Gaussian model and use the Question6 cumulative sums graph to evaluate your modifications. The procedure is outlined here:\n\n\nFinal thoughts\nThe gaussian as a model for the time course of new cases, and the sigmoid for the time course of accumulated cases to date, are well established. But here we’ve used two gaussians (or, equivalently, two sigmoids). So was there one outbreak or two?\n\nQuestion 7: Think about why the two-gaussian model matches the data better than the one-gaussian model. What might this mean in terms of the structure of the Ebola outbreak in Guinea? Don’t be afraid to speculate and frame your answer in terms that a layman might understand.\n\nThe modeling cycle is all about using your current model to identify ways that you might be able to improve the model. Of course, in practice, you need to present your model for use, so you have to exit the cycle at some point. You’re at that point now, but we ask you to reflect a bit more.\n\nQuestion 8: Given the results of your modeling efforts and your answer to Question 7, might it be better to model the outbreak using 3 or 4 Gaussians in our linear combination? What are the challenges associated with using more Gaussians?\n\n\nQuestion 9: The EbolaAll data frame records the Sierra Leone and Liberia outbreaks as well as the outbreak in Guinea. How might you use that additional data to explore the validity of your modeling process?\n\nAuthors: Prof. Robert Wolverson, USAFA & Daniel Kaplan, Macalester College and USAFA.",
    "crumbs": [
      "BLOCK I. Modeling",
      "Projects"
    ]
  },
  {
    "objectID": "Modeling/Modeling-projects.html#block-1-project-2-orbit-dimensions",
    "href": "Modeling/Modeling-projects.html#block-1-project-2-orbit-dimensions",
    "title": "Projects",
    "section": "Block 1 Project 2: Orbit dimensions",
    "text": "Block 1 Project 2: Orbit dimensions\nThis activity will apply some of the concepts and techniques you’re learning to answer the following question:\n\nHow fast does a satellite move along its orbit?\n\nAs you can imagine, the answer is already known and you could look it up. The point of our reconstructing what is already known is to see the totality of a modeling project, even if it is a very simple one.\nIn textbooks and in-class demonstrations, students are often shown complete, flawless models. In reality, model construction is a matter of trial and error. Whoops! we are supposed to say “modeling cycle.” That phrase does not suggest anything about “error.” But in reality, modelers make mistakes, operate under misconceptions, collect erroneous data, misunderstand the purpose of building a model, and make all sorts of mistakes. To cope with this unhappy situation, good modelers are constantly checking and testing their models for inconsistencies.\nTo start, you should have\n\nA good idea of what the eventual answer will be. Often, that idea comes from somewhat vague and imprecise knowledge. For example, you may have heard that it takes a satellite in low orbit about 90 minutes to complete one circuit of the Earth. You may also know that the length of the equator is roughly 40,000 kilometers. (This is the original definition of the meter.) A velocity is distance traveled over time, so a satellite in low orbit has a velocity of roughly \\(40000 / 90\\) km/minute, which comes out to 7400 meters/second.\nA theory that relates what you want to know to what you already know. For our purposes, that theory comes directly from Isaac Newton in the 1680s: his laws of motion and his theory of universal gravitation.\n\n\nThe theory\nWe won’t assume that you have anything more than a vague understanding of Newton’s laws and theory of gravitation. The diagram shows the situation schematically.\n\n\n\n\n\n\nFigure 1: Gravitational force pulls the satellite into an orbit.\n\n\n\nThe satellite is traveling clockwise along a curved trajectory encircling the Earth. The position of the satellite is shown at several times by the numbered blue dots. Let’s focus on the satellite at time 1.\nThe satellite is an object in motion. Newton’s First Law (“Lex I”) is stated in his 1687 book, Philosophiae Naturis Principia Mathematica (Mathematical principles of natural philosophy) on p.12\n ```\nTranslating into English, this is\n\nLaw I: Every body persists in its state of rest or uniform motion in a straight line, unless compelled to change that motion by forces impressed upon it.\n\nThe dashed line connecting the points labeled 1 and 2’ shows the path that the satellite would follow if there were no forces impressed upon it.\nYet there is a force impressed on the satellite: the gravitational attraction between the Earth and the satellite. This force accelerates the satellite perpendicular to its orbit (toward the center of the Earth) causing the satellite to follow a curved path rather than a straight path off into deep space. The acceleration of the satellite traveling at constant speed in orbit depends on both the velocity \\(v\\) of the satellite and the radius \\(r\\) of its orbit.\nTask #1: Let \\(A_1\\) be the acceleration needed to keep the satellite in a circular orbit. Find a plausible relationship between \\(A_1\\), \\(r\\), and \\(v\\). One possibility is that the relationship is a general product of the form \\[A_1 = v^n\\ r^m .\\] Use dimensional analysis to find \\(n\\) and \\(m\\). Recall that acceleration has dimension L/T\\(^{2}\\), velocity has dimension \\(L/T\\) and radius has dimension L.\nOnce you determine \\(n\\) and \\(m\\), write down the relationship \\(A_1\\) as a function of \\(r\\) and \\(v\\).\n\nAs we all know, gravity pulls all objects toward the center of the Earth. The acceleration \\(A_2\\) due to gravity on an object a distance \\(r\\) from the enter of the Earth is proportional to the mass of the Earth and is known to be \\[A_2  = G\\ M_e/r^2\\] where \\(G\\) is a constant of proportionality and \\(M_e\\) is the mass of Earth.\nIn order for the satellite to stay in orbit, the two accelerations \\(A_1\\) (what’s needed to stay in orbit) and \\(A_2\\) (what the Earth’s gravity provides) must be equal.\nTask #2: Set your expression for \\(A_1\\) equal to the expression for \\(A_2\\) and solve for the velocity \\(v\\) of the satellite (our original objective for this exercise). Your answer will involve \\(G\\), \\(M_e\\), and \\(r\\).\nUse the known numerical values for \\(G\\) and \\(M_e\\) given in the next section to check that your answer makes sense.\n\n\nThe data\nThe data here come from scientific observations made over centuries that give us numerical values (with units) of \\(M_e\\) and \\(G\\) in the theory.\n\\(G\\) is a universal constant (according to Newton’s theory of gravitation). The quantity is given by several sources as\n\\[G = 6.674 \\times 10^{-11} m^3 /(s^2 kg).\\]\nSimilarly, the mass of the Earth is given as\n\\[M_e  = 5.972 × 10^{24} kg\\]\nThese reported facts seem plausible, but it is a good practice to check. Toward that end, check\n\nThe dimension and units of \\(A_2(v, r)\\) are consistent.\nThe value of \\(A_2\\) at the Earth’s surface is consistent with the famous value 9.8 m/s\\(^2\\).\n\nTask #3: Finishing up.\nUse the formula you derived for \\(v\\) as a function of \\(r\\), \\(G\\), and \\(M_e\\) to find \\(v\\) for a satellite in low orbit around the Earth. The official extent of the “Low Earth Orbit Region” is up to 2000 km. If you were using the altitude of the International Space Station (400 km), you would set \\(r = r_e + 400km\\), where \\(r_e\\) is the radius of the earth: 6, 378.1 km.\nAs always, you want to do the calculation in a way that helps you to spot possible errors. Here are two good practices:\n\nYou have already confirmed (or should have) that your formula for \\(v\\) as a function of \\(r\\), \\(G\\), and \\(M_e\\) is dimensionally consistent. As you plug in numerical values for \\(r\\), \\(G\\), and \\(M_e\\), make sure to keep track of the units explicitly and that the result you get has proper units for velocity.\nCompare your result to the rough estimate of \\(v\\) for satellites in low orbit that you made at the beginning of this activity. If there is a discrepancy, review both your initial rough estimate as well as your gravity-based derivation of \\(v\\) to figure out where the inconsistency comes from. Then fix it.",
    "crumbs": [
      "BLOCK I. Modeling",
      "Projects"
    ]
  },
  {
    "objectID": "modeling-part.html",
    "href": "modeling-part.html",
    "title": "BLOCK I. Modeling",
    "section": "",
    "text": "A model is a representation of something—for instance, a building—for a specific purpose. A model of a building might take several different forms, depending on the purpose. For example, a blueprint plan shows the layout of rooms, corridors, windows, etc. for the purpose of exploring the design and guiding the construction. A three-dimensional balsa-wood model of a building helps to examine how the building appears from different perspectives. A list of components in the building is a model whose purpose is to facilitate ordering those components and checking whether everything needed is at hand during construction.\nA mathematical model is a model made up of mathematical stuff. Balsa-wood and blueprint paper are not mathematical stuff. This Block introduces some of the different kinds of mathematical stuff that we will use in constructing mathematical models. Of primary importance will be mathematical functions. The block describes various aspects of functions: parameters and the ways to set their values, ways to construct complicated functions out of simpler components, a particularly useful type of function—polynomials—that are a type of modeling “clay.”\nUsing mathematical models often involves performing a mathematical operation on a model. One type of operation emphasized in high-school math is solving which takes information in one form (e.g., a function) and gives information in another form (e.g., the inputs that will cause the function output to take on a specific value). Other kinds of operations are optimization and iteration, which will be introduced in this Block.\nAlthough functions are fundamental to mathematical modeling, there are other mathematical and scientific concepts that make it much easier to think about how a model relates to the real world. Two of these are the mathematics of magnitude and the units and dimension of quantities.\nFinally, the block considers the process of building models and techniques that help a human modeler to get started and then refine the model until it can serve its purpose.",
    "crumbs": [
      "BLOCK I. Modeling"
    ]
  },
  {
    "objectID": "Modeling/16-modeling-scientific-method.html#enr-peak-oil-peak-oil",
    "href": "Modeling/16-modeling-scientific-method.html#enr-peak-oil-peak-oil",
    "title": "16  Modeling and the scientific method",
    "section": "16.8 Enrichment topic 16.1: Peak Oil",
    "text": "16.8 Enrichment topic 16.1: Peak Oil",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling and the scientific method</span>"
    ]
  },
  {
    "objectID": "Differentiation/26-approximation.html",
    "href": "Differentiation/26-approximation.html",
    "title": "26  Local approximations",
    "section": "",
    "text": "26.1 Eight simple, local shapes\nIn many modeling situations with a single input, selecting one of eight simple shapes, those shown in Figure 26.1, can get you far.\nTo choose among these shapes, consider your modeling context:\nConsider these historical examples:\nSome other examples:",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Local approximations</span>"
    ]
  },
  {
    "objectID": "Differentiation/26-approximation.html#eight-simple-local-shapes",
    "href": "Differentiation/26-approximation.html#eight-simple-local-shapes",
    "title": "26  Local approximations",
    "section": "",
    "text": "Figure 26.1: The eight simple shapes of local functions with one input.\n\n\n\n\n\nis the relationship positive (slopes up) or negative (slopes down)\nis the relationship monotonic or not\nis the relationship concave up, concave down, or neither\n\n\n\nNewton’s Law of Cooling. Let the input be the difference in temperature between an object and its environment. Let the output be the rate at which the object’s temperature changes. Newton’s Law of Cooling amounts to a choice of shape (B).\nHooke’s Law, describing the force supplied by a compressed or stretched object such as a spring. Let the input be the how much the object is compressed or stretched: negative for compression, positive for stretching. Let the output be the force supplied by the string, with a positive force meaning away from the spring and negative towards the spring. Hooke’s Law is shape (A).\nInverse square law for gravitational force. The input is the distance between the masses, the output is the force, with a negative force corresponding to attraction. The inverse square law corresponds to shape (C).\nChemistry’s Law of Mass Action for an element or molecule reacting with itself. The input is the concentration of the substance, the output is the rate of production of the compound. Shape (D). (For the Law of Mass Action involving two different substances, we need shapes of functions with two inputs. See Section 26.3.)\n\n\n\nThe incidence of an out-of-control epidemic versus time is concave up, but shallow-then-steep. Shape D. As the epidemic is brought under control, the decline is steep-then-shallow and concave up. Shape C. Notice that in each case we are describing only the local behavior of the function.\nHow many minutes can you run as a function of speed? Concave down and shallow-then-steep; you wear out faster if you run at high speed. How far can you walk as a function of time? Steep-then-shallow and concave down; your pace slows as you get tired. Shape (E).\nHow does the stew taste as a function of saltiness. The taste improves as the amount of salt increases … up to a point. Too much salt and the stew is unpalatable. Shape (G).\nHow much fuel is consumed by an aircraft as a function of distance? For long flights the function is concave up and shallow-then-steep; fuel use increases with distance, but the amount of fuel you have to carry also increases with distance and heavy aircraft use more fuel per mile. Shape (E).\nIn micro-economic theory there are production functions that describe how much of a good is produced at any given price, and demand functions that describe how much of the good will be purchased as a function of price.\n\nAs a rule, production increases with price and demand decreases with price. In the short term, production functions tend to be concave down, since it is hard to squeeze increased production out of existing facilities. Shape (F).\nFor demand in the short term, functions will be concave up when there is some group of consumers who have no other choice than to buy the product. An example is the consumption of gasoline versus price: it is hard in the short term to find another way to get to work. Shape (C). In the long term, consumption functions can be concave down as consumers find alternatives to the high-priced good. For example, high prices for gasoline may, in the long term, prompt a switch to more efficient cars, hybrids, or electric vehicles. This will push demand down steeply. Shape (E).",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Local approximations</span>"
    ]
  },
  {
    "objectID": "Differentiation/26-approximation.html#low-order-polynomials",
    "href": "Differentiation/26-approximation.html#low-order-polynomials",
    "title": "26  Local approximations",
    "section": "26.2 Low-order polynomials",
    "text": "26.2 Low-order polynomials\nThere is a simple, familiar functional form that, by selecting parameters appropriately, can take on each of the eight simple shapes: the second-order polynomial. \\[g(x) \\equiv a + b x + c x^2\\] As you know, the graph of \\(g(x)\\) is a parabola.\n\nThe parabola opens upward if \\(0 &lt; c\\). That is the shape of a local minimum.\nThe parabola opens downward if \\(c &lt; 0\\). That is the shape of a local maximum\n\nConsider what happens if \\(c = 0\\). The function becomes simply \\(a + bx\\), the straight-line function.\n\nWhen \\(0 &lt; b\\) the line slopes upward.\nWhen \\(b &lt; 0\\) the line slopes downward.\n\nWith the appropriate choice of parameters, the form \\(a + bx + cx^2\\) is capable of representing four of the eight simple shapes. What about the remaining four? This is where the idea of local becomes important. Those remaining four shapes are the sides of parabolas, as in Figure 26.2.\n\n\n\n\n\n\n\n\nFigure 26.2: Four of the eight simple shapes correspond to the sides of the parabola. The labels refer to the graphs in Figure 26.1.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Local approximations</span>"
    ]
  },
  {
    "objectID": "Differentiation/26-approximation.html#sec-low-order-two",
    "href": "Differentiation/26-approximation.html#sec-low-order-two",
    "title": "26  Local approximations",
    "section": "26.3 The low-order polynomial with two inputs",
    "text": "26.3 The low-order polynomial with two inputs\nFor functions with two inputs, the low-order polynomial approximation looks like this:\n\\[g(x, y) \\equiv a_0 + a_x x + a_y y + a_{xy} x y + a_{yy} y^2 + a_{xx} x^2\\] In reading this form, note the system being used to name the polynomial’s coefficients. First, we’ve used \\(a\\) as the root name of all the coefficients. Sometimes we might want to compare two or more low-order polynomials, so it is convenient to be able to use \\(a\\) for one, \\(b\\) for another, and so on.\nThe subscripts on the coefficients describes exactly which term in the polynomial involves each coefficient. For instance, the \\(a_{yy}\\) coefficient applies to the \\(y^2\\) term, while \\(a_x\\) applies to the \\(x\\) term.\nEach of \\(a_0, a_x,\\) \\(a_y,\\) \\(a_{xy}, a_{yy}\\), and \\(a_{xx}\\) will, in the final model, be a constant quantity. Don’t be confused by the use of \\(x\\) or \\(y\\) in the name of the coefficients. Each coefficient is a constant and not a function of the inputs. Often, your prior knowledge of the system being modeled will tell you something about one or more of the coefficients, for example, whether it is positive or negative. Finding a precise value is often based on quantitative data about the system.\nIt helps to have different names for the various terms. It is not too bad to say something like, “the \\(a_{xy}\\) term.” (Pronounciation: “a sub x y” or “a x y”) But the proper names are: linear terms, quadratic terms, and interaction term. And a shout out to \\(a_0\\), the constant term.\n\\[g(x, y) \\equiv a_0 + \\underbrace{a_x x + a_y y}_\\text{linear terms} \\ \\ \\ +\n\\underbrace{a_{xy} x y}_\\text{interaction term} +\\ \\ \\  \\underbrace{a_{yy} y^2 + a_{xx} x^2}_\\text{quadratic terms}\\]\n\n## Loading required namespace: plotly\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 26.3: A saddle",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Local approximations</span>"
    ]
  },
  {
    "objectID": "Differentiation/26-approximation.html#sec-partial-thought",
    "href": "Differentiation/26-approximation.html#sec-partial-thought",
    "title": "26  Local approximations",
    "section": "26.4 Thinking partially",
    "text": "26.4 Thinking partially\nThe expression for a general low-order polynomial in two inputs can be daunting to think about all at once: \\[g(x, y) \\equiv a_0 + a_x x + a_y y + a_{xy} x y + a_{xx} x^2 + a_{yy} y^2\\] As with many complicated settings, a good approach can be to split things up into simpler pieces. With a low-order polynomial, one such splitting up involves partial derivatives. There are six potentially non-zero partial derivatives for a low-order polynomial, of which two are the same; so only five quantities to consider.\n\n\\(\\partial_x g(x,y) = a_x + a_{xy}y + 2 a_{xx} x\\)\n\\(\\partial_y g(x,y) = a_y + a_{xy}x + 2 a_{yy} y\\)\n\\(\\partial_{xy} g(x,y) = \\partial_{yx} g(x,y) = a_{xy}\\). These are the so-called mixed partial derivatives. It does not matter whether you differentiate by \\(x\\) first or by \\(y\\) first. The result will always be the same for any smooth function.\n\\(\\partial_{xx} g(x,y) = 2 a_{xx}\\)\n\\(\\partial_{yy} g(x,y) = 2 a_{yy}\\)\n\nThe above list states neutral mathematical facts that apply generally to any low-order polynomial whatsoever.1 Those facts, however, shape a way of asking questions of yourself that can help you shape the model of a given phenomenon based on what you already know about how things work.\nTo illustrate, consider the situation of modeling the effect of study \\(S\\) and of tutoring \\(T\\) (a.k.a. office hours, extended instruction) on performance \\(P(S,T)\\) on an exam. In the spirit of partial derivatives, we will assume that all other factors (student aptitude, workload, etc.) are held constant.\nTo start, pick fiducial values for \\(S\\) and \\(T\\) to define the local domain for the model. Since \\(S=0\\) and \\(T=0\\) are easy to envision, we will use those for the fiducial values.\nNext, ask five questions, in this order, about the system being modeled.\n\nDoes performance increase with study time? Don’t over-think this. Remember that the approximation is around a fiducial point. Here, a reasonable answer is, “yes.” we will take\\(\\partial_S P(S, T) &gt; 0\\) to imply that \\(a_S &gt; 0\\). This is appropriate because close to the fiducial point, the other contributors to \\(\\partial_S P(S, T)\\), namely \\(a_{ST}T + 2 a_{SS} S\\) will be vanishingly small.\nDoes performance increase with time spent being tutored? Again, don’t over-think this. Don’t worry (yet) that your social life is collapsing because of the time spent studying and being tutored, and the consequent emotional depression will cause you to fail the exam. We are building a model here and the heuristic being used is to consider factors in isolation. Since (as we expect you will agree) \\(\\partial_T P(S, T) &gt; 0\\), we have that \\(a_T &gt; 0\\).\n\nNow the questions get a little bit harder and will exercise your calculus-intuition since you will have to think about changes in the rates of change.\n\nThis question has to do with the mixed partial derivative, which we’ve written variously as \\(\\partial_{ST} P(S,T)\\) or \\(\\partial_{TS} P(S,T)\\) and which it might be better to think about as \\(\\partial_S \\left[\\partial_T P(S,T) \\right]\\) or \\(\\partial_T \\left[\\partial S P(S,T)\\right]\\). Although these are mathematically equal, often your intuition will favor one form or the other. Recall that we are working on the premise that \\(\\partial_S P(S,T) &gt; 0\\), or, in other words, study will help you do better on the exam. Now for \\(\\partial_T \\left[\\partial S P(S,T)\\right]\\). This is a the matter of whether some tutoring will make your study more effective. Let’s say yes here, since tutoring can help you overcome a misconception that is a roadblock to effective study. So \\(\\partial_{TS} P(S,T) &gt; 0\\) which implies \\(a_{ST} &gt; 0\\).\n\nThe other way round, \\(\\partial_S \\left[\\partial_T P(S,T) \\right]\\) is a matter of whether increasing study will enhance the positive effect of tutoring. We will say yes here again, because a better knowledge of the material from studying will help you follow what the tutor is saying and doing. From pure mathematics, we already know that the two forms of mixed partials are equivalent, but to the human mind they sometimes (and incorrectly) appear to be different in some subtle, ineffable way.\nIn some modeling contexts, there might be no clear answer to the question of \\(\\partial_{xy}\\, g(x,y)\\). That is also a useful result, since it tells us that the \\(a_{xy}\\) term may not be important to understanding that system.\n\nOn to the question of \\(\\partial_{SS} P(S,T)\\), that is, whether \\(a_{SS}\\) is positive, negative, or negligible. We know that \\(a_{SS} S^2\\) will be small whenever \\(S\\) is small, so this is our opportunity to think about bigger \\(S\\). So does the impact of a unit of additional study increase or decrease the more you study? One point of view is that there is some moment when “it all comes together” and you understand the topic well. But after that epiphany, more study might not accomplish as much as before the epiphany. Another bit of experience is that “cramming” is not an effective study strategy. And then there is your social life … So let’s say, provisionally, that there is an argmax to study, beyond which point you’re not helping yourself. This means that \\(a_{SS} &lt; 0\\).\nFinally, consider \\(\\partial_{TT} P(S, T)\\). Reasonable people might disagree here, which is itself a reason to suspect that \\(a_{TT}\\) is negligible.\n\nAnswering these questions does not provide a numerical value for the coefficients on the low-order polynomial, and says nothing at all about \\(a_0\\), since all the questions are about change.\nAnother step forward in extracting what you know about the system you are modeling is to construct the polynomial informed by questions 1 through 5. Since you don’t know the numerical values for the coefficients, this might seem impossible. But there is a another modeler’s trick that might help.\nLet’s imagine that the domain of both \\(S\\) and \\(T\\) or the interval zero to one. This is not to say that we think one hour of study is the most possible but simply to defer the question of what are appropriate units for \\(S\\) and \\(T\\). Very much in this spirit, for the coefficients we will use \\(+0.5\\) when are previous answers indicated that the coefficient should be greater than zero, \\(-0.5\\) when the answers pointed to a negative coefficient, and zero if we don’t know. Using this technique, here is the model, which mainly serves as a basis for checking whether our previous answers are in line with our broader intuition before we move on quantitatively.\n\n\n\n\nP &lt;- makeFun(0.5*S + 0.5*T + 0.5*S*T - 0.5*S^2 ~ S & T)\ncontour_plot(P(S, T) ~ S & T, bounds(S=0:1, T=0:1))\n\n\n\n\n\n\n\n\n\n\nFigure 26.4: The result of our intuitive investigation of the effects of study and tutoring on exam performance. The units are not yet assigned.\n\n\n\nNotice that for small values of \\(T\\), the horizontal spacing between adjacent contours is large. That is, it takes a lot of study to improve performance a little. At large values of \\(T\\) the horizontal spacing between contours is smaller.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Local approximations</span>"
    ]
  },
  {
    "objectID": "Differentiation/26-approximation.html#finding-coefficients-from-data",
    "href": "Differentiation/26-approximation.html#finding-coefficients-from-data",
    "title": "26  Local approximations",
    "section": "26.5 Finding coefficients from data",
    "text": "26.5 Finding coefficients from data\nLow-order polynomials are often used for constructing functions from data. In this section, I’ll demonstrate briefly how this can be done. The full theory will be introduced in Block 5 of this text.\nThe data I’ll use for the demonstration is a set of physical measurements of height, weight, abdominal circumference, etc. on 252 human subjects. These are contained in the Body_fat data frame, shown below.\n\n\n\n\n\n\nOne of the variables records the body-fat percentage, that is, the fraction of the body’s mass that is fat. This is thought to be an indicator of fitness and health, but it is extremely hard to measure and involves weighing the person when they are fully submerged in water. This difficulty motivates the development of a method to approximation body-fat percentage from other, easier to make measurements such as height, weight, and so on.\nFor the purpose of this demonstration, we will build a local polynomial model of body-fat percentage as a function of height (in inches) and weight (in pounds).\nThe polynomial we choose will omit the quadratic terms. It will contain the constant, linear, and interaction terms only. That is \\[\\text{body.fat}(h, w) \\equiv c_0 + c_h h + c_w w + c_{hw} h w\\] The process of finding the best coefficients in the polynomial is called linear regression. Without going into the details, we will use linear regression to build the body-fat model and then display the model function as a contour plot.\n\nmod &lt;- lm(bodyfat ~ height + weight + height*weight,\n          data = Body_fat)\nbody_fat_fun &lt;- makeFun(mod)\ncontour_plot(body_fat_fun(height, weight) ~ height + weight,\n             bounds(weight=c(100, 250), height = c(60, 80))) %&gt;%\n  gf_labs(title = \"Body fat percentage\")\n\n\n\n\n\n\n\nFigure 26.5: A low order polynomial model of body fat percentage as a function of height (inches) and weight (lbs).\n\n\n\n\n\nBlock 3 looks at linear regression in more detail.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Local approximations</span>"
    ]
  },
  {
    "objectID": "Differentiation/26-approximation.html#footnotes",
    "href": "Differentiation/26-approximation.html#footnotes",
    "title": "26  Local approximations",
    "section": "",
    "text": "Note that any other derivative you construct, for instance \\(\\partial_{xxy} g(x,y)\\) must always be zero.↩︎",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Local approximations</span>"
    ]
  },
  {
    "objectID": "Differentiation/17-continuous-change.html",
    "href": "Differentiation/17-continuous-change.html",
    "title": "17  Continuous change",
    "section": "",
    "text": "17.1 Mathematics in motion\nThe questions that started it all had to do with motion of planets and marbles. In more technical language, “ballistics,” the science of balls. There were words to describe speed: fast and slow. There were words to describe force: strong and weak, heavy and light. And there were words to describe location and distance: far and near, long and short, here and there. But what were the relationships among these things? And how did time fit in, an intangible quantity that had aspects of location (long and short) and speed (quick and slow)?\nGalileo (1564-1642) started the ball rolling.1 As the son of a musician and music theorist, he had a sense of musical time, a steady beat of intervals. When a student of medicine in Pisa, he noted that swinging pendulums kept reliable time, regardless of the amplitude of their swing. After unintentionally attending a geometry lecture, he turned to mathematics and natural philosophy.\nUsing his newly developed apparatus, the telescope, Galileo’s observations put him on a collision course with the accepted classical truth about the nature of the planets. Seeking to understand gravity, he built an apparatus that enabled him accurately to measure the position in time of a ball rolling down a straight ramp. The belled gates he set up to mark the ball’s passage were spaced evenly in musical time: 1, 2, 3, 4, …. To get this even spacing in time, Galileo found he had to position the gates unevenly. Defining as 1 the distance of the first gate from the ball’s release point, the gates were at positions 1, 4, 9, 16, ….\nA re-enactment of Galileo’s rolling-ball experiment. The frets on the ramp are at positions 2 cm, 8 cm, 18 cm, 32 cm, 50 cm, …, that is, 2 cm times 1, 4, 9, 16, 25. ::: ```\nAnyone familiar with the squares of the integers can see the pattern in 1, 4, 9, 16, …. To demonstrate the pattern, Galileo took the difference between the successive positions, what we will call the “first increment.”\n\\[\\underbrace{1 - 0}_1 \\ \\ \\ \\ \\ \\underbrace{4 - 1}_3\\ \\ \\ \\ \\ \\underbrace{9 - 4}_{5}\\ \\ \\ \\ \\ \\underbrace{16-9}_7\\ \\ \\ \\underset{{\\Large\\strut}\\text{first increment}}{\\text{}}\\] Next, Galileo repeated the differencing process on the first increment to produce a “second increment.”\n\\[\\underbrace{3 - 1}_2 \\ \\ \\ \\ \\ \\underbrace{5 - 3}_2\\ \\ \\ \\ \\ \\underbrace{7 - 5}_{2}\\ \\ \\ \\underset{{\\Large\\strut}\\text{second increment}}{\\text{}}\\] ::: {.column-margin} For more about Galileo’s measurements, see Stillman Drake (1986) “Galileo’s physical measurements” American Journal of Physics 54, 302-305 https://doi.org/10.1119/1.14634 :::\nThe rule established by Galileo’s observations for the motion of a ball rolling down the ramp:",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Continuous change</span>"
    ]
  },
  {
    "objectID": "Differentiation/17-continuous-change.html#mathematics-in-motion",
    "href": "Differentiation/17-continuous-change.html#mathematics-in-motion",
    "title": "17  Continuous change",
    "section": "",
    "text": "Table 17.1: Galileo’s observations and their first & second increments.\n\n\n\n\n\n\\(t\\)\n\\(x(t)\\)\nfirst increment\nsecond increment\n\n\n\n\n0\n0\n1\n2\n\n\n1\n1\n3\n2\n\n\n2\n4\n5\n2\n\n\n3\n9\n7\n\n\n\n4\n16\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe second increment of position is constant.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Continuous change</span>"
    ]
  },
  {
    "objectID": "Differentiation/17-continuous-change.html#continuous-time",
    "href": "Differentiation/17-continuous-change.html#continuous-time",
    "title": "17  Continuous change",
    "section": "17.2 Continuous time",
    "text": "17.2 Continuous time\nGalileo’s mathematics of first and second increments was suited to the discrete-time measurements he was able to make. It would be for Newton to develop the continuous-time analog of increments.\nTo start, we can imagine a function \\(x(t)\\) that gives the position of the ball at any instant \\(t\\). With this notation, Galileo’s measured positions were \\(x(0), x(1), x(2), x(3), x(4), \\ldots\\), and the first increments were \\(x(1) - x(0)\\), \\(x(2) - x(1)\\), \\(x(3) - x(2)\\), and so on.\nBut just as position \\(x(t)\\) is a continuous function of time \\(t\\), the first increment can also be written as a continous function: \\[y(t) \\equiv x(t+1) - x(t)\\ .\\] Similarly, there is a second increment function: \\[\\begin{eqnarray}z(t) & \\equiv&  y(t+1) - y(t)\\\\ & = & \\left[x(t+2) - x(t+1)\\right] - \\left[x(t+1) - x(t)\\right] \\\\ &=& x(t+2) - 2 x(t+1) + x(t)\\ .\n\\end{eqnarray}\\]\nThe \\(+1\\) and \\(+2\\) in the first and second increment functions correspond to the time elapsed from one belled gate to the next. More generally, rather than using Galileo’s unit of rhythmic time, we can define the increment functions using a time quantity of our own choice; we will call it \\(h\\).\nRe-written using \\(h\\), the first increment becomes \\[y(t) \\equiv x(t+h) - x(t)\\ .\\] The second increment function is \\[\\begin{eqnarray}z(t) & \\equiv&  y(t+h) - y(t)\\\\ & = & \\left[x(t+2h) - x(t+h)\\right] - \\left[x(t+h) - x(t)\\right] \\\\ &=& x(t+2h) - 2 x(t+h) + x(t)\\ .\n\\end{eqnarray}\\]\nEvidently, the numerical values (dimension L) of the first and second increments depend on \\(h\\), which is a choice made by the experimenter, not a fact of nature. If the experimenter selects a large \\(h\\), the first and second increments will be large.\nIt would be nice to frame the ballistics theory so that \\(h\\) does not appear. Newton’s insight amounts to taking two steps:\n\nReplace the simple difference \\(x(t+h) - x(t)\\) with a rate of change, that is: ::: {.column-margin} Note that we are using the symbol \\({\\cal D}\\_t\\) and naming the rate of change function \\({\\cal D}_t y(t)\\). Read \\({\\cal D}_t\\) as “the rate of change with respect to \\(t\\). ::: \\[\\text{Rate of change of } x(t): \\ \\ \\ \\ {\\cal D}_t y(t) \\equiv \\frac{x(t+h) - x(t)}{h}\\]\n\nLikewise, the second increment will become a “rate of change of a rate of change,” a phrase that is easier to understood when written as a formula:\n\n\nRead \\({\\cal D}_t {\\cal D}_t y(t)\\) as “the rate of change of the rate of change of \\(y(t)\\).”\n\\[\\begin{eqnarray}\n{\\cal D}_t {\\cal D}_t y(t)  &\\equiv&   {\\cal D}_t \\left(\\strut \\frac{{\\cal D}_t y(t+h) - {\\cal D}_t y(h)}{h}\\right) \\\\\n&=& \\frac{y(t+h) - y(t)}{h} \\\\\n&=& \\frac{\\frac{x(t+2h) - x(t+h)}{h}- \\frac{x(t+h) - x(t)}{h}}{h}\\\\\n&=& \\frac{x(t+2h) - 2 x(t+h) + x(t)}{h^2}\\ .\n\\end{eqnarray}\\]\nAdmittedly, this complicated expression for the rate-of-change equivalent of Galileo’s second increment hardly looks like an improvement! And it still depends on \\(h\\).\nThis is where the second step of Newton’s insight comes in.\n\nMake \\(h\\) vanishingly small.\n\nIn the next chapters, we will look at how these two steps—use rate of change rather than change and make \\(h\\) vanishingly small—create mathematical entities that allowed Newton to extend Galileo’s work to become a universal theory of motion.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Continuous change</span>"
    ]
  },
  {
    "objectID": "Differentiation/17-continuous-change.html#change-relationships",
    "href": "Differentiation/17-continuous-change.html#change-relationships",
    "title": "17  Continuous change",
    "section": "17.3 Change relationships",
    "text": "17.3 Change relationships\nAs you know, function is a mathematical idea used to represent a relationship between quantities. For instance, the water volume of a reservoir behind a dam varies with the seasons and over the years. As a function, water volume is a relationship between water volume (one quantity) and time (another quantity). Similarly, the flow in a river feeding the reservoir has its own relationship with time. In spring, the river may be rushing with snow-melt, in late summer the river may be dry, but after a summer downpour the river flow again rises briefly. In other words, river flow is a function of time.\nDifferentiation is a way of describing a relationship between relationships. The water volume in the reservoir has a relationship with time. The river flow has a relationship with time. Those two relationships are themselves related: the river flow feeds the reservoir and thereby influences the water volume.\nIt is not easy to keep straight what’s going on in a “relationship between relationships.” Consequently, we need tools such as differentiation to aid our understanding. For instance, Johannes Kepler (1572-1630) spent years analyzing the data collected by astronomer Tycho Brahe (1546-1601). The data showed a relationship between time and the speed of a planet across the sky. Long-standing wisdom claimed that there is also a specific relationship between a planet’s position and time. From antiquity, it had been claimed that planets moved in circular orbits. Kepler worked hard to find the relationship between the two relationships: speed versus time and position versus time. He was unsuccessful until he dropped the assumption that planetary orbits are circular. Testing the hypothesis that orbits are elliptical, Kepler was able to find a simple relationship between speed vs. time and position vs. time.\nBuilding on Kepler’s earlier work, Newton hypothesized that planets might be influenced by the same gravity that pulls an apple to the ground. It was evident from human experience that gravity has the most trivial relationship with time: gravity is constant! But Newton could not find a link between this notion of gravity as a constant and Kepler’s planetary motion as a function of time. Success came when Newton hypothesized—without any direct evidence from experience—that gravity is a function of distance. Newton’s formulation of the relationship between relationships— gravity-as-a-function-of-distance and orbital-position-as-a-function-of time—became the foundation of modern science. Newton’s theories of gravity, force, and motion created an extremely complicated chain or reasoning that is still hard to grasp. Or, more precisely, it is hard to grasp until you have the language for describing relationships between relationships. Newton invented this language: differentiation. As you learn this language, you will find it easier to express and understand relationships between relationships, that is, the mechanisms that account for the ever-changing quantities around us.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Continuous change</span>"
    ]
  },
  {
    "objectID": "Differentiation/17-continuous-change.html#with-respect-to",
    "href": "Differentiation/17-continuous-change.html#with-respect-to",
    "title": "17  Continuous change",
    "section": "17.4 With respect to …",
    "text": "17.4 With respect to …\nWe’ve introduced a bit of new notation in the previous section, \\({\\cal D}_t\\).  As mentioned previously, \\({\\cal D}\\) stands for “the rate of change of ____.” In use, you put a function in the slot indicated by ____. Which function depends on what you want to describe. For instance, the position of a rolling ball is a function of time: \\(x(t)\\). “The rate of change of \\(x(t)\\)” is written \\({\\cal D}_t x(t)\\). This object \\({\\cal D}_t x(t)\\) is itself a function of time.In Chapter 19, when we carry out the second step of Newton’s program by making \\(h\\) vanishingly small, we will switch from the big \\({\\cal D}\\) to a smaller one, \\(\\partial\\), to remind us that \\(h\\) has vanished from the picture.\nAnother example: consider a water reservoir fed by a spring and drained by the water utility to serve its customers. Suppose \\(w(t)\\) is the volume of water in the reservoir, a quantity that changes over time. Then \\({\\cal D}_t w(t)\\) is the rate of change of water volume in the reservoir. Common sense suggests that the rate of change in water volume will be positive during a wet season and negative in a drought.\nThe subscript on \\({\\cal D}_t\\) is the with-respect-to input. To illustrate, suppose that \\(h(v, w)\\) is the volume of the harvest from a field as a function of the amount of irrigation water \\(w\\) and the amount of fertilizer used during the growing season.  As will be described in Chapter 25, there are two different rate-of-change functions associated with \\(h()\\). One is the rate of change in harvest volume with respect to \\(w\\), the other is the rate change in harvest volume with respect to \\(v\\). In everyday language, \\({\\cal D}_w h(v, w)\\) can be used to predict how much the harvest will change if, next year, the farmer uses less irrigation water. Similarly, \\({\\cal D}_v h(v, w)\\) can inform a farmer’s decision to reduce costs by using less fertilizer.Constructing such a function could be done by collecting data over many years of the harvest, along with the amount of water and fertilizer used each year.\nStrictly speaking, for functions with just one input the subscript on \\({\\cal D}\\) isn’t needed. Even so, we will always include a subscript, if only for the sake of forming good habits to serve us when we do examine rates of change in functions of multiple inputs.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Continuous change</span>"
    ]
  },
  {
    "objectID": "Differentiation/17-continuous-change.html#footnotes",
    "href": "Differentiation/17-continuous-change.html#footnotes",
    "title": "17  Continuous change",
    "section": "",
    "text": "Galileo was not aware of Kepler’s elliptical theory, even though they lived at the same time.↩︎",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Continuous change</span>"
    ]
  },
  {
    "objectID": "Differentiation/18-rate-of-change.html",
    "href": "Differentiation/18-rate-of-change.html",
    "title": "18  Rate of change",
    "section": "",
    "text": "18.1 Outputs versus rates of change\nUsing functions to describe the car-trip situation, we can say that position is a function of time. We will call it \\(p(t)\\). The input to the function is time and the output is position.\nA rate of change for the function can be calculated by choosing two different values for time and evaluating the function at those times. The evaluation produces two different values for the output position. Calling the two times \\(t_0\\) and \\(t_1\\), the corresponding outputs are \\(p(t_0)\\) and \\(p(t_1)\\).\nThe average rate of change of \\(p(t)\\) over the interval \\(t_0 \\leq t \\leq t_1\\) is \\[\\frac{p(t_1) - p(t_0)}{t_1 - t_0}\\ .\\] In ?sec-dimension-and-units we saw that subtraction is legitimate only when the two quantities involved have the same dimension and units. That is the case here. \\(p(t_1)\\) and \\(p(t_2)\\) both have dimension L and miles as the unit. \\(t_1\\) and \\(t_0\\) both have dimension T and hours as the unit.\nThe division of \\(p(t_1) - p(t_0)\\) (dimension L) by \\(t_1 - t_0\\) (dimension T) is also dimensionally legitimate. The simple reason is that division of one quantity by another is always dimensionally legitimate. The division produces a quantity with dimension L/T.\nA quantity with dimension L/T is utterly different than a quantity of dimension L or a quantity of dimension T. In other words, “25 miles per hour” is neither a position nor a time, it is a velocity.\nOne way to see that velocity is a different kind of quantity than position or time is that you measure the quantities in different ways. You might measure position by noting the passage of a mile marker along the side of the road. You can measure time by reference, say, to your level of boredom or by checking a clock or watch. Divide change in position by change in time to get velocity. But you can also sense velocity directly, by the level of noise in the car or the blurring of nearby objects along the road.\nOn a graph, you also measure in different ways changes in the input to a function and the corresponding changes in output. As always, start by picking the endpoints of an interval in the domain of the function. As an example, ?fig-stop-and-go-b marks the endpoints of an interval with \\(\\color{magenta}{magenta}\\) dots.\nDraw a rectangle connecting the function values at the start and end of the interval. The change in input is the horizontal extent of the rectangle. The change in output is the vertical extent of the rectangle. If “vertical” and “horizontal” are enough to point out that the two measures are of different kinds of things, you will be reminded by your having to use two different scales for the two measurements.\nFigure 18.2: Adding a scale for slope to the graph.\nOver the interval marked, the average rate of change of the function is still another kind of perceived quantity, the “slope” of the diagonal of the rectangle. Unfortunately, graphs do not typically include a scale for slope, but we have added a scale to Figure 18.2. From the slope scale, you can easily see that the average rate of change is a little less than 30 miles per hour.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Rate of change</span>"
    ]
  },
  {
    "objectID": "Differentiation/18-rate-of-change.html#slope-at-a-point",
    "href": "Differentiation/18-rate-of-change.html#slope-at-a-point",
    "title": "18  Rate of change",
    "section": "18.2 Slope at a point",
    "text": "18.2 Slope at a point\nWith a slope scale, you can dispense with the laborious process shown in ?fig-stop-and-go-b: marking an interval, drawing a rectangle, measuring the vertical change, etc. The slope scale lets you read off the rate of change at a glance: pick a point in the domain, look at the slope of the function at that point, and compare it to the slope scale.\nPerhaps you can see that formally defining an interval isn’t an absolute necessity for defining a slope. Instead, you can perceive slope directly from a graph, even if it is hard to quantify without a special scale.\n\nUsing the slope scale in Figure 18.3, estimate the function’s slope at input \\(t=0.2\\). How does it compare to the slope at \\(t=0.4\\)?\n\n\n\n\n\n\n\n\n\nFigure 18.3: Adding a scale for slope to the graph.\n\n\n\n\nPlace a ruler on the function graph so that the rule touches the graph at \\(t=0.2\\). Keeping that point of contact, vary the slope of the ruler until it neatly aligns with the curve. Now, without changing the slope of the ruler, slide it over to the slope scale and read the ruler slope off that scale. The slope is a bit more than 20 miles per hour.\nAt \\(t=0.4\\), the function slope is considerably steeper than at \\(t=0.2\\), about 60 miles per hour.\n\nThe function’s slope at a specific input like \\(t=0.2\\) is called the instantaneous slope and corresponds to the instantaneous velocity of the car. , you do not have to measure the car’s velocity by reading the change in position over the interval between two distinct moments in time; you can simply look at the speedometer to get an instantaneous read-out of the velocity. We will translate instantaneous rate of change into the language of functions in Chapter 19.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Rate of change</span>"
    ]
  },
  {
    "objectID": "Differentiation/18-rate-of-change.html#sec-slope-function",
    "href": "Differentiation/18-rate-of-change.html#sec-slope-function",
    "title": "18  Rate of change",
    "section": "18.3 Slope function",
    "text": "18.3 Slope function\nFigure -Figure 18.3 shows that the rate of change of the car-position function changes during the trip. In other words, the rate of change of position is itself a function of time.\nIn general, for a function \\(p(t)\\) the rate of change function will be \\[{\\cal D}_t p(t) \\equiv \\frac{p(t+h) - p(t)}{h}\\] where \\(h\\) is the length of the interval used to compute the rate of change. We will call this the slope function of \\(f(t)\\).\n\n\n\n\n\n\n\n\n\nFigure 18.4: Showing the changing slope of \\(p(t)\\) as a series of segments. The slope scale (in red) indicates the numerical value of each segment.\n\n\n\n\nA fun but unconventional way to display a slope function is to show the literal slope of \\(p(t)\\) as a function of \\(t\\) as in Figure 18.4. It represents the value of \\({\\cal D}_t p(t)\\) as the slope of a little line segment. To read off the numerical value of the slope, refer to the slope scale drawn in red. A picture like Figure 18.4 is a good reminder that the slope function \\({\\cal D}_t p(t)\\) is all about the slope of \\(p(t)\\) and not at all about the actual value of \\(p(t)\\).\nThe conventional way to display a slope function is to show the numerical value of the slope by the position on the vertical axis, as in Figure 18.5. Such a graph is easy to read, but provides nothing but the axis label to remind you that the scale on the vertical axis is the slope of another function.\n\n\n\n\n\n\n\n\n\nFigure 18.5: Graphing the slope function \\({\\cal D}_t p(t)\\). The value of the slope of \\(p(t)\\) can be read from the vertical axis scale.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Rate of change</span>"
    ]
  },
  {
    "objectID": "Differentiation/18-rate-of-change.html#sec-tree-harvest-example",
    "href": "Differentiation/18-rate-of-change.html#sec-tree-harvest-example",
    "title": "18  Rate of change",
    "section": "18.4 Average rate of change",
    "text": "18.4 Average rate of change\nIn Chapter 19, we will start working with the instantaneous rate of change of a function. That concept is so important that you will tend to forget there was any such thing as the “average rate of change” over an interval.\nNevertheless, average rate of change can be a useful concept in many circumstances. To illustrate, Figure 18.6 shows a simplified model of the amount of usable wood harvestable from a typical tree in a managed forest of Ponderosa Pine. (You can see some actual forestry research models here.) Such a model, even if simplified, can provide useful insight for forestry planning.\n\n\n\n\n\n\n\n\nFigure 18.6: A model, somewhat realistic, of the amount of useful wood from a Ponderosa Pine as a function of the number of years from planting to harvest.\n\n\n\n\n\nThe overall pattern in ?fig-over-tree is that the tree continues to grow until year 50, when it seems to have reached an equilibrium: perhaps growth goes to zero, or rot balances growth.\nIf managing a forest for wood production, it seems sensible to try to get as much wood out of the tree as possible. The maximum volume of wood occurs, in ?fig-over-tree, at about year 50. Does that mean that harvesting at year 50 is optimal? If not, when is the best time? Spend a moment thinking about ?fig-over-tree and draw your own conclusions. Right or wrong, this will help you understand the argument we are about to make.\nGood forestry practices are “sustainable.” Forests are managed to be continually productive rather than subject to a one-time extraction of value followed by desolation. For sustainability, it is important to consider the life cycle of the forest. After all, continual productivity implies that the forest will continue to produce value into the indefinite future.\nOne implication of managing for sustainability is that the quantity to optimize is not the volume of wood from a one-time harvest. Rather, it is the rate (per year) at which wood can be sustainably extracted from the forest.\nAround year 25, the tree adds usable wood at the fastest instantaneous rate. This might suggest to some that a good time to harvest is near year 25. But, in fact, it makes no sense to harvest at the time of maximum rate of growth; why kill the tree when it is being most productive?\nA better quantity to look at for deciding when to harvest is the average rate of growth in the volume of wood. Remember that “average rate of change” is the rate of change over an extended interval. For wood harvesting, the relevant interval is the time from planting until harvest.\nHarvesting at year 25 will give a total change of 600 board feet over 25 years, corresponding to an average rate of change of \\(600 \\div 25 = 24\\ \\text{board-feet-per-year}\\). But if you wait until year 35, you will have about 900 board feet, giving an average rate of change of \\(900 \\div 35 = 25.7\\) board-feet-per-year (L3 T-1).\nIt is easy to construct a diagram that indicates whether year 35 is best for the harvest. Recall that our fundamental model of change is the straight-line function. So we will model the model of tree growth as a straight-line function. Like the model in Figure 18.6, our straight-line model will start with zero wood when planted. Furthermore, to be faithful to Figure 18.6, we will insist that the straight-line intersect or touch that curve.\nFigure 18.7 reiterates the Figure 18.6 model of the tree annotated with several straight-line models that all give zero harvest-able wood at planting time. Each green line represents a scenario where harvest occurs at \\(t_1\\), \\(t_2\\), etc. From the perspective of representing the rate of growth per year from planting to harvest, the straight-line green models do not need to replicate the actual growth curve. The complexities of the curve are not relevant to the growth rate. Instead, what’s relevant is the slope of a straight-line model connecting the output at planting time to the output at harvest time. In contrast, the \\(\\color{magenta}{\\text{magenta}}\\) curve is not a suitable model because it does not match the situation at any harvest time; it does not touch the curve anywhere after planting!\n\n\n\n\n\n\n\n\nFigure 18.7: Modeling the tree-growth model with straight lines connecting planting time to various harvest times. The slope of each line is the average rate of growth for that planting time.\n\n\n\n\n\nChoose a harvest time that produces the steepest possible green segment to maximize average lumber volume per year. From Figure 18.7, that steepest line glances the growth curve near year 31 (shown as \\(t_3\\) in the diagram).\nIt is best to find the argmax by creating a function that shows explicitly what one is trying to optimize. (In Chapter 24, we will use the name objective function to identify such function.) Here, the objective function is \\(\\text{ave.growth(year)} \\equiv \\text{volume(year)} / \\text{year}\\). See Figure 18.8.\n\n\n\n\n\n\n\n\nFigure 18.8: Graph of the average-growth function ave_growth(year), constructed by dividing volume(year) by year.\n\n\n\n\n\nThe graph of ave_growth(year) makes clear the maximum average growth from planting to harvest will occur at about year 32.\nThere is no point waiting until after year 50.\nAt year 25, the tree is growing as fast as ever. You will get about 600 board feet of lumber.1 Should you harvest at year 25? No! That the tree is growing so fast means that you will have a lot more wood in years 26, 27, etc. The time to harvest is when the growth is getting smaller so that it is not worth waiting an extra year.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Rate of change</span>"
    ]
  },
  {
    "objectID": "Differentiation/18-rate-of-change.html#sec-dimension-rate-change",
    "href": "Differentiation/18-rate-of-change.html#sec-dimension-rate-change",
    "title": "18  Rate of change",
    "section": "18.5 Dimension of a rate of change",
    "text": "18.5 Dimension of a rate of change\nThe function named \\(\\partial_t f(t)\\) which is the derivative of \\(f(t)\\) takes the same input as \\(f(t)\\); the notation makes that pretty clear. Let’s suppose that \\(t\\) is time and so the dimension of the input is \\([t] = \\text{T}\\).\nThe outputs of the two functions, \\(\\partial_t f(t)\\) and \\(f(t)\\) will not, in general, have the same dimension. Why not? Recall that a derivative is a special case of a slope function, the instantaneous slope function. It is easy to calculate a slope function:\n\\[{\\cal D}_t f(t) \\equiv \\frac{f(t+h) - f(t)}{h}\\] The dimension of the quantity \\(f(t+h) - f(t)\\) must be the same as the dimension of \\(f(t)\\); the subtraction would not be possible otherwise. Likewise, the dimension of \\(h\\) must be the same as the dimension of \\(t\\); the addition \\(t+h\\) wouldn’t make sense otherwise.\n\n\nKeep in mind that the dimension \\([f(t+h) - f(t)]\\) will be the same as \\([f(t)]\\). Why? The result of addition and subtraction will always have the same dimension as the quantities being combined.\nWhereas the dimension of the output \\(f(t)\\) is simply \\(\\left[f(t)\\right]\\), the dimension of the quotient \\(\\frac{f(t+h) - f(t)}{h}\\) will be different. The output of the derivative function \\(\\partial_t f(t)\\) will be \\[\\left[\\partial_t f(t)\\right] = \\left[f(t)\\right] / \\left[t\\right] .\\]\nSuppose \\(x(t)\\) is the position of a car as a function of time \\(t\\). Position has dimension L. Time has dimension T. The function \\(\\partial_t x(t)\\) will have dimension L/T. Familiar units for L/T are miles-per-hour, which you can recognize as velocity.\nAnother example: Imagine a function pressure() with that takes altitude above sea level (in km) and output pressure (in kPa, “kiloPascal”).2 The derivative function, let’s call it \\(\\partial_\\text{altitude} \\text{pressure}()\\), also takes an input in km, but produces an output in kPA per km: a rate.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Rate of change</span>"
    ]
  },
  {
    "objectID": "Differentiation/18-rate-of-change.html#footnotes",
    "href": "Differentiation/18-rate-of-change.html#footnotes",
    "title": "18  Rate of change",
    "section": "",
    "text": "A “board foot” is a volume, dimension L3. It is a square foot (L^2) times an inch (L).↩︎\nAir pressure at sea level is about 100 kiloPascal.↩︎",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Rate of change</span>"
    ]
  },
  {
    "objectID": "Differentiation/19-evanescent-h.html",
    "href": "Differentiation/19-evanescent-h.html",
    "title": "19  Evanescent h",
    "section": "",
    "text": "19.1 Evanescence algebraically\nLet’s look at a slope function using evanescent \\(h\\). To start, we will analyze \\(f(t) \\equiv t^2\\), one of our pattern-book functions. By definition, \\[{\\cal D}_t f(t) \\equiv \\frac{f(t+h) - f(t)}{h}\\ .\\] We can easily evaluate \\(f(t+h)\\) symbolically: \\[f(t+h) \\equiv (t+h)^2 = t^2 + 2 t h + h^2\\] Similarly, we can find the difference \\(f(t+h) - f(t)\\). It is \\[f(t+h) - f(t) = f(t+h) - t^2 = 2 t h + h^2\\ .\\] Notice that there is still some liquid (that is, \\(h\\)) in the difference. Now we let the difference start to dry, taking out the \\(h\\) by dividing the difference by \\(h\\): \\[\\frac{f(t+h) - f(t)}{h} = \\frac{2 t h + h^2}{h} = 2 t + h\\ .\\] The rate of chaange of \\(f(t)\\) has something solid—\\(2 t\\)—along with a little bit of liquid \\(h\\) that we can leave to evaporate to nothing.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Evanescent h</span>"
    ]
  },
  {
    "objectID": "Differentiation/19-evanescent-h.html#differentiation",
    "href": "Differentiation/19-evanescent-h.html#differentiation",
    "title": "19  Evanescent h",
    "section": "19.2 Differentiation",
    "text": "19.2 Differentiation\nBy this point you should be familiar with the definition of the average rate of change of \\(f(t)\\) over an interval from \\(t\\) to \\(t+h\\):\n\\[{\\cal D}_t f(t) \\equiv \\frac{f(t+h) - f(t)}{h}\\] To indicate that we want a rate of change with evanescent \\(h\\), we add a statement to that effect:\n\\[\\partial_t f(t) \\equiv \\lim_{h\\rightarrow 0} {\\cal D}_t f(t) = \\lim_{h\\rightarrow0}\\frac{f(t+h) - f(t)}{h}\\ .\\] A proper mathematical phrasing of \\(\\lim_{h\\rightarrow 0}\\) is, “the limit as \\(h\\) goes to zero.” In terms of the paint metaphor, read \\(\\lim_{h\\rightarrow 0}\\) as “once applied to the wall, let the paint dry.”\nTo save space, write \\(\\lim_{h\\rightarrow 0} {\\cal D}_t f(t)\\)a more compact way: \\(\\partial_t f(t)\\). We use the small symbol \\(\\partial\\) as a reminiscence of the role that small \\(h\\) played in the construction of \\(\\partial_t f(t)\\).\nThe function \\(\\partial_t f(t)\\) is called the derivative of the function \\(f(t)\\). The process of constructing the derivative of a function is called differentiation. The roots of these two words are not the same. “Differentiation” comes from “difference,” a nod to subtraction as in “the difference between 4 and 3 is 1.” In contrast, “derivative” comes from “derive,” whose dictionary definition is “obtain something from a specified source,” as in deriving butter from cream. “Derive” is a general term. But “derivative” and “differentiation” always refers to a specific form of related to rates of change.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Evanescent h</span>"
    ]
  },
  {
    "objectID": "Differentiation/19-evanescent-h.html#sec-d-pattern-book",
    "href": "Differentiation/19-evanescent-h.html#sec-d-pattern-book",
    "title": "19  Evanescent h",
    "section": "19.3 Derivatives of the pattern book functions",
    "text": "19.3 Derivatives of the pattern book functions\nThe pattern-book functions are so widely used that it is helpful to memorize facts about their derivatives. Remember that, as always, the derivative of a function is another function. For every pattern-book function, the derivative is itself built from a pattern-book functions. To emphasize this, the list below states the rules in terms of the names of the functions, rather than formulas.\n\n\\(\\partial_x\\) one(x) \\(=\\) zero(x)\n\\(\\partial_x\\) identity(x) \\(=\\) one(x)\n\\(\\partial_x\\) square(x) \\(=\\) 2 identity(x)\n\\(\\partial_x\\) reciprocal(x) \\(=\\) -1/square(x)\n\\(\\partial_x\\) log(x) \\(=\\) reciprocal(x)\n\\(\\partial_x\\) sin(x) \\(=\\) cos(x)\n\\(\\partial_x\\) exp(x) \\(=\\) exp(x)\n\\(\\partial_x\\) sigmoid(x) \\(=\\) gaussian(x)\n\\(\\partial_x\\) gaussian(x) \\(=\\) - x gaussian(x)\n\n\n\nIn applications, the pattern-book functions are parameterized, e.g. \\(\\sin\\left(\\frac{2\\pi}{P} t\\right)\\). Chapter 23 introduces the derivatives of the parameterized functions.\nNotice that \\(h\\) does not appear at all in the table of derivatives. Instead, to use the derivatives of the pattern-book functions we need only refer to a list of facts, not the process for discovering those facts.\n\n\n\n\n\n\n\n\nFigure 19.3: A diagram showing how differentiation connects the pattern-book functions to one another.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Evanescent h</span>"
    ]
  },
  {
    "objectID": "Differentiation/19-evanescent-h.html#notations-for-differentiation",
    "href": "Differentiation/19-evanescent-h.html#notations-for-differentiation",
    "title": "19  Evanescent h",
    "section": "19.4 Notations for differentiation",
    "text": "19.4 Notations for differentiation\nThere are many notations in wide use for differentiation. In this book, we will denote differentiation in a manner has a close analogy in computer notation.\nWe will write the derivative of \\(f(x)\\) as \\(\\partial_x f(x)\\). If we had a function \\(g(t)\\), with \\(t\\) being the input name, the derivative would be \\(\\partial_t g(t)\\). Since there is nothing special about the name of the input in functions with one input, we could just as well write the one-input function that is the derivative of \\(g()\\) with respect to its input as \\(\\partial_x g(x)\\) or \\(\\partial_z g(z)\\) or even \\(\\partial_{zebra} g(zebra)\\). For functions with just one input, the notation skeptic might argue that there is no need for a subscript on \\(\\partial\\), since it will always match the name of the input to the function being differentiated.\nEarly in the history of calculus, mathematician Joseph-Louis Lagrange (1736-1813) proposed a more compact notation for the derivative of a function with a single input. Rather than \\(\\partial_x f(x)\\), Lagrange wrote \\(f'\\). We pronounce this “f-prime.” This notation is still widely used in calculus textbooks because it is compact. But it is not a viable notation for functions used in modeling since those functions often have more than one input.\nEarlier than Lagrange, Newton used a very compact notation. The historian needs to be careful, because Newton did not use the term “derivative” nor the term “function.” Instead, Newton wrote of “flowing quantities,” that is, quantities that change in time. For Newton, typical names for such flowing quantities were \\(x\\) and \\(y\\). He didn’t use the parentheses that we now associate with functions, just the bare name. Newton used “fluent” to name such flowing quantities. Newton’s fluents were more or less what we call today “functions of time.” What we now call “derivatives,” Newton called “fluxions.” If \\(x\\) is a fluent, then Newton wrote \\(\\dot{x}\\) to stand for the fluxion. This is pronounced “x-dot.” Like Lagrange’s compact prime notation, Newton’s dot notation is still used, particularly in physics.\nThe mathematician Gottfried Wilhelm Leibniz (1646-1716) was a contemporary of Newton. Leibniz developed his own notation for calculus, which was easier to understand than Newton’s. In Leibniz’s notation, the derivative (with respect to \\(x\\)) of \\(f(x)\\) was written \\[\\frac{df}{dx}\\ .\\] The little \\(d\\) stands for “a little bit of” or “a little change in,” so \\(\\frac{df}{dx}\\) makes clear that the derivative is a ratio of two little bits. In the denominator, \\(dx\\) refers to an infinitesimal change in the value of the input \\(x\\). In the numerator, the \\(df\\) names the corresponding change in the output of \\(f()\\) when the input is changed.\nLeibniz’s notation is by far the most widely used in introductory calculus. It has many advantages compared to Newton’s or Lagrange’s notations. For example, it provides an opportunity to name the with-respect-to input. It also provides a nice notation for an operation called “anti-differentiation” which we will meet in Part ?sec-accumulation-part. And many a physics or engineering student has been taught to treat \\(dx\\) as if it were a number when doing algebraic manipulations.\nThe problem with Leibniz’s notation, from the perspective of this book, is that it does not translate well into computer notation. A statement like:\ndf/dx &lt;- x^2 + 3*x\nis a non-starter since the character / is not allowed in a name in most computer languages, including R.\nFor functions with multiple inputs, for instance, \\(h(x,y,z)\\), differentiation can be done with respect to any input. Leibniz’s notation might possibly be used to indicate which is the with-respect-to input; the three derivatives of \\(h()\\) would be written \\(dh/dx\\) and \\(dh/dy\\) and \\(dh/dz\\). However, mathematical notation did not go in this direction. Instead, for functions with multiple inputs, the three derivatives are most usually written \\(\\partial h/\\partial x\\) and \\(\\partial h/partial y\\), and \\(\\partial h/\\partial z\\). In the expression, \\(\\partial h/\\partial y\\), the symbol \\(\\partial\\) is pronounced “partial,” The three different derivatives \\(\\partial h/\\partial x\\), \\(\\partial y /\\partial y\\), and \\(\\partial h/\\partial z\\) are called “partial derivatives” and are the subject of Chapter 25.\nThis book uses \\(\\partial_x h\\), \\(\\partial_y h\\), and \\(\\partial_z h\\) to denote partial derivatives. This adequately identifies the with-respect-to input and has a close analog in computer notation. For instance, if f(x,y,z) has been defined already, the following statements are entirely valid:\ndz_f &lt;- D(f(x,y,z) ~ z)\ndy_f &lt;- D(f(x,y,z) ~ y)\nIt has been more than 300 years since Leibniz’s death. At this point calculus is so we will established that we don’t need the notation \\(df/dx\\) to remind us that a derivative is “a little bit of \\(f\\) divided by a little bit of \\(x\\).”\nThere are several traditional notations for differentiation of a single-input function named \\(f()\\). Here’s a list of some of them, along with the name associated with each:\n\nLeibnitz: \\(\\frac{df}{dx}\\)\nPartial: \\(\\frac{\\partial f}{\\partial x}\\)\nNewton (or “dot”): \\(\\dot{f}\\)\nLagrange (or “prime”): \\(f'\\)\nOne-line (used in this book): \\(\\partial_x f\\)\n\nTo read calculus fluently, you will have to recognize each of these notations. For functions with one input, they all mean the same thing. But when functions have multiple inputs, the choice is between the styles \\(\\partial f / \\partial x\\) and \\(\\partial_x f\\). We use the later because it can easily be incorporated into computer commands.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Evanescent h</span>"
    ]
  },
  {
    "objectID": "Differentiation/19-evanescent-h.html#footnotes",
    "href": "Differentiation/19-evanescent-h.html#footnotes",
    "title": "19  Evanescent h",
    "section": "",
    "text": "https://www.merriam-webster.com/dictionary/evanescent↩︎\nFull title: The Analyst: A Discourse Addressed to an Infidel Mathematician: Wherein It Is Examined Whether the Object, Principles, and Inferences of the Modern Analysis Are More Distinctly Conceived, or More Evidently Deduced, Than Religious Mysteries and Points of Faith↩︎",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Evanescent h</span>"
    ]
  },
  {
    "objectID": "Differentiation/20-computing.html",
    "href": "Differentiation/20-computing.html",
    "title": "20  Constructing derivatives",
    "section": "",
    "text": "20.1 Why differentiate?\nBefore showing the easy computer-based methods for constructing the derivative of a function, it is good to provide some motivation: Why is differentiation so frequently in so many fields of study and application?\nA primary reason lies in the laws of physics. Newton’s Second Law of Motion reads:\nNewton defined used position \\(x(t)\\) as the basis for velocity \\(v(t) = \\partial_t x(t)\\). “Change in motion,” which we call “acceleration,” is in turn the derivative \\(\\partial v(t)\\). Derivatives are also central to the expression of more modern forms of physics such as quantum theory and general relativity.\nMany relationships encountered in the everyday or technical worlds are more understandable if framed in terms of derivatives. For instance,\nOften, we know one member in such function-and-derivative pairs, but to need to calculate the other. Many modeling situations call for putting together different components of change to reveal how some other quantity of interest will change. For example, modeling the financial viability of retirement programs such as the US Social Security involves looking at the changing age structure of the population, the returns on investment, the changing cost of living, and so on. In Block V, we will use derivatives explicitly to construct models of systems, such as an outbreak of disease, with many changing parts.\nDerivatives also play an important role in design. They play an important role in the construction and representation of smooth curves, such as a robot’s track or the body of a car. (See Chapter 49.) Control systems that work to stabilize a airplane’s flight or regulate the speed and spacing of cars are based on derivatives. The notion of “stability” itself is defined in terms of derivatives. (See Chapter 45.) Algorithms for optimizing design choices also often make use of derivatives. (See Chapter 50.)",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Constructing derivatives</span>"
    ]
  },
  {
    "objectID": "Differentiation/20-computing.html#why-differentiate",
    "href": "Differentiation/20-computing.html#why-differentiate",
    "title": "20  Constructing derivatives",
    "section": "",
    "text": "“The change of motion of an object is proportional to the force impressed; and is made in the direction of the straight line in which the force is impressed.”\n\n\n\n\nElectrical power is the rate of change with respect to time of electrical energy.\nBirth rate is one component of the rate of change with respect to time of population. (The others are the death rate and the rates immigration and emigration.)\nInterest, as in bank interest or credit card interest, is the rate of change with respect to time of assets.\nInflation is the rate of change with respect to time of prices.\nDisease incidence is one component of the rate of change with respect to time of disease prevalence. (The other components are death or recovery from disease.)\nForce is the rate of change with respect to position of energy.\nDeficit (as in spending deficits) is the change with respect to time of debt.\n\n\n\n\n\n\n\n\n\nCalculus history—The Wealth of Nations\n\n\n\nEconomics as a field makes considerable use of concepts of calculus—particularly first and second derivatives, the subjects of this Block—although the names used are peculiar to economics, for instance, “elasticity”, “marginal returns” and “diminishing marginal returns.”\nThe origins of modern economics, especially the theory of the free market, are attributed to a book published in 1776, The Wealth of Nations. The author, Adam Smith (1723-1790), lays out dozens of relationships between different quantities — wages, labor, stock, interest, prices, profits, and coinage among others. Yet despite the invention of calculus a century before Wealth of Nations, the book uses no calculus.\nConsider this characteristic statement in Wealth of Nations:\n\nThe market price of every particular commodity is regulated by the proportion between the quantity which is brought to market, and the demand of those who are willing to pay the natural price of the commodity.\n\nWithout calculus and the ideas of functions and their derivatives, Smith was not able to think about prices in a modern way where price is shaped by demand and supply. Instead, for Smith, each item has a “natural price”: a fixed quantity that depends on the amount of labor used to produce the item. Nowadays, we understand that productivity changes as new methods of production and new inventions are introduced. But Smith lived near the end of a centuries-long period of static economies. Transportation, agriculture, manufacture, and population size were all much as they had been for the past 500 years or longer. James Watt’s steam engine was introduced only in 1776 and it would be decades before being adapted to the myriad uses of steam power characteristic of the 19th century. The cotton gin (1793), labor-saving agricultural machines such as the McCormick reaper (1831), the assembly line (1901), and the many other innovations of industry all lay in the future when Smith was writing Wealth of Nations.\n\n\n\nIt took the industrial revolution and nearly a century of intellectual development before economics had to and could embrace the rapid changes in the production process. In this dynamical view, supply and demand are not mere quantities, but functions of which price is the primary input. The tradition in economics is to use the word “curve” instead of “function,” giving us the phrases “supply curve” and “demand curve.” Making the transition from quantity to function, that is, between a single amount and a relationship between amounts, is a core challenge to those learning economics.\n\n\n\n\n\n\nDemand as a function of price, as first published by Antoine-Augustin Cournot in 1836.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Constructing derivatives</span>"
    ]
  },
  {
    "objectID": "Differentiation/20-computing.html#symbolic-differentiation",
    "href": "Differentiation/20-computing.html#symbolic-differentiation",
    "title": "20  Constructing derivatives",
    "section": "20.2 Symbolic differentiation",
    "text": "20.2 Symbolic differentiation\nThe R/mosaic function D() takes a formula for a function and produces the derivative. It uses the same sort of tilde expression used by makeFun() or contour_plot() or the other R/mosaic tools. For instance,\n\nD(t * sin(t) ~ t)\n## function (t) \n## sin(t) + t * cos(t)\n\nIf you prefer, you can use makeFun() to define a function, then hand that function to D() for differentiation.\n\nmyf &lt;- makeFun(sqrt(y * pnorm(1 + x^2, mean=2, sd=3)) ~ x & y)\ndx_myf &lt;- D(myf(x, y) ~ x, y=3)\ndx_myf\n## function (x, y = 3) \n## {\n##     .e1 &lt;- 1 + x^2\n##     x * y * dnorm(.e1, 2, 3)/sqrt(y * pnorm(.e1, mean = 2, sd = 3))\n## }\n\nIn the right side of the tilde expression handed off to D() names the with-respect-to input. This is similar to the tilde expressions used in plotting, which name the inputs that form the graphics domain. But it contrasts with the tilde expressions in makeFun(), where the right-hand side specifies the order in which you want the inputs to appear.\n\nNeedless to say, D() knows the rules for the derivatives of the pattern-book functions introduced in Section 19.3. For instance,\n\nD(sin(t) ~ t)\n## function (t) \n## cos(t)\nD(log(x) ~ x)\n## function (x) \n## 1/x\nD(exp(x) ~ x)\n## function (x) \n## exp(x)\nD(x^2 ~ x)\n## function (x) \n## 2 * x",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Constructing derivatives</span>"
    ]
  },
  {
    "objectID": "Differentiation/20-computing.html#finite-difference-derivatives",
    "href": "Differentiation/20-computing.html#finite-difference-derivatives",
    "title": "20  Constructing derivatives",
    "section": "20.3 Finite-difference derivatives",
    "text": "20.3 Finite-difference derivatives\nWhenever you have a formula amenable to the construction of a symbolic derivative, that is what you should use. Finite-difference derivatives are useful in those situation where you don’t have such a formula. The calculation is simple but has a weakness that points out the advantages of the evanescent-\\(h\\) approach.\nFor a function \\(f(x)\\) and a “small,” non-zero number \\(h\\), the finite-difference approximates the derivative with this formula:\n\\[\\partial_x f(x) \\approx \\frac{f(x+h) - f(x-h)}{2h}\\ .\\] To demonstrate, let’s construct the finite-difference approximation to \\(\\partial_x \\sin(x)\\). Since we already know the symbolic derivative—it is \\(\\partial_x \\sin(x) = \\cos(x)\\)—there is no genuinely practical purpose for this demonstration. Still, it can serve to confirm the symbolic rule.\nWe will call the finite-difference approximation fq_sin() and use makeFun() to construct it:\n\nfq_sin &lt;- makeFun((sin(x+h)- sin(x-h))/(2*h) ~ x, h=0.01)\n\nNotice that fq_sin() has a parameter, h whose default value is being set to 0.01. Whether 0.01 is “small” or not depends on the context. Operationally, we define “small” to be a value that gives practically the same result even if it is made smaller by a factor of 2 or 10.\nAs a demonstration that fq_sin() with \\(h=0.01\\) approximates the genuine \\(\\partial_x \\sin(x)\\), we exploit our knowledge that \\(\\partial_x \\sin(x) = \\cos(x)\\). Figure 20.1 plots out the difference between the the \\(h=0.01\\) approximation and the genuine derivative.\n\nslice_plot(fq_sin(x, h=0.01) - cos(x) ~ x, bounds(x=-10:10)) %&gt;%\n  slice_plot(fq_sin(x, h=0.001) - cos(x) ~ x, color=\"magenta\") %&gt;%\n  gf_labs(y=\"Error from true value.\")\n\n\n\n\n\n\n\nFigure 20.1: Comparing fq_sin() to \\(\\partial_x \\sin(x)\\) for two values of \\(h\\).\n\n\n\n\n\nYou will need to look carefully at the vertical axis scale in Figure 20.1 to see what’s happening. For \\(h=0.01\\), fq_sin() is not exactly the same as cos(), but it is close, always being within $$0.00017. For many purposes, this would be accurate enough. But not for all purposes. We can make the approximation better by using a smaller \\(h\\). For instance, the \\(h=0.001\\) version of fq_sin() is accurate to within $$0.0000017.\nIn practical use, one employs the finite-difference method in those cases where one does not already know the exact derivative function. This would be the case, for example, if the function is a sound wave recorded in the form of an MP3 audio file.\nIn such situations, a practical way to determine what is a small \\(h\\) is to pick one based on your understanding of the situation. For example, much of what we perceive of sound involves mixtures of sinusoids with periods longer than one-two-thousandth of a second, so you might start with \\(h\\) of 0.002 seconds. Use this guess about \\(h\\) to construct a candidate finite-difference approximation. Then, construct another candidate using a smaller h, say, 0.0002 seconds. If the two candidates are a close match to one another, then you have confirmed that your choice of \\(h\\) is adequate.\nIt is tempting to think that the approximation gets better and better as h is made even smaller. But that is not necessarily true for computer calculations. The reason is that quantities on the computer have only a limited precision: about 15 digits. To illustrate, let’s calculate a simple quantity, \\((\\sqrt{3})^2 - 3\\). Mathematically, this quantity is exactly zero. On the computer, however it is not quite zero:\n\nsqrt(3)^2 - 3\n## [1] -4.440892e-16\n\nWe can see this loss of precision at work if we make h very small in the finite-difference approximation to \\(\\partial_x \\sin(x)\\). In Figure 20.2 we are using h = 0.000000000001. The result is unsatisfactory.\n\nslice_plot(  fq_sin(x, h=0.000000000001) - cos(x) ~ x, \n           bounds(x=-10:10)) %&gt;%\n  slice_plot(fq_sin(x, h=0.0000000000001) - cos(x) ~ x,\n             color=\"magenta\") %&gt;%\n  gf_labs(y=\"Error from true value.\")\n\n\n\n\n\n\n\nFigure 20.2: In computer calculations, using too small an h leads to a loss of accuracy in the finite-difference approximation.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Constructing derivatives</span>"
    ]
  },
  {
    "objectID": "Differentiation/20-computing.html#second-and-higher-order-derivatives",
    "href": "Differentiation/20-computing.html#second-and-higher-order-derivatives",
    "title": "20  Constructing derivatives",
    "section": "20.4 Second and higher-order derivatives",
    "text": "20.4 Second and higher-order derivatives\nMany applications call for differentiating a derivative or even differentiating the derivative of a derivative. In English, such phrases are hard to read. They are much simpler using mathematical notation.\n\n\\(f(x)\\) a function\n\\(\\partial_x f(x)\\) the derivative of \\(f(x)\\)\n\\(\\partial_x \\partial_x f(x)\\), the second derivative of \\(f(x)\\), usually written even more concisely as \\(\\partial_{xx}f f(x)\\).\n\nThere are third-order derivatives, fourth-order, and on up, although they are not often used.\nTo compute a second-order derivative \\(\\partial_{xx} f(x)\\), first differentiate \\(f(x)\\) to produce \\(\\partial_x f(x)\\). Then, still using the techniques described earlier in this chapter, differentiate \\(\\partial_x f(x)\\).\nThere is a shortcut for constructing high-order derivatives using D() in a single step. On the right-hand side of the tilde expression, list the with-respect-to name repeatedly. For instance:\n\nThe second derivative \\(\\partial_{xx} \\sin(x)\\):\n\n\nD(sin(x) ~ x & x)\n## function (x) \n## -sin(x)\n\n\nThe third derivative \\(\\partial_{xxx} \\ln(x)\\):\n\n\nD(log(x) ~ x & x & x)\n## function (x) \n## 2/x^3\n\n\nPhysics students learn a formula for the position of an object in free fall dropped from a height \\(x_0\\) and at an initial velocity \\(v_0\\): \\[ x(t) \\equiv -\\frac{1}{2} g t^2 + v_0 t + x_0\\ .\\] The acceleration of the object is the second derivative \\(\\partial_{tt} x(t)\\). Use D() to find the object’s acceleration.\nThe second derivative of \\(x(t)\\) with respect to \\(t\\) is:\n\nD(0.5*g*t^2 + v0*t + x0 ~ t & t)\n## function (t, g, v0, x0) \n## g\n\nThe acceleration does not depend on \\(t\\); it is the constant \\(g\\). No wonder \\(g\\) is called “gravitational acceleration.”",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Constructing derivatives</span>"
    ]
  },
  {
    "objectID": "Differentiation/21-concavity.html",
    "href": "Differentiation/21-concavity.html",
    "title": "21  Concavity and curvature",
    "section": "",
    "text": "21.1 Quantifying concavity and curvature\nIt often happens in building models that the modeler (you!) knows something about the concavity or the curvature of a function. For example, concavity is essential in classical economics; the curve for supply as a function of price is concave down while the curve for demand as a function of price is concave up. For a train, car, or plane, sideways forces depend on the curvature of the track, road, or trajectory. Road designers need to calculate the curvature to know if the road is safe at the indicated speed.\nIt turns out that quantifying these properties of functions or shapes is naturally done by calculating derivatives.\nWe will frame the calculations in terms of a function \\(f(x)\\). Depending on the setting, \\(x\\) might be the price of a product and \\(f(x)\\) the demand for that product. Alternatively, the graph of \\(f(x)\\) might represent the path of a road drawn in \\((x,y)\\) coordinates or the reach of a robot arm as a function of time.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Concavity and curvature</span>"
    ]
  },
  {
    "objectID": "Differentiation/21-concavity.html#quantifying-concavity-and-curvature",
    "href": "Differentiation/21-concavity.html#quantifying-concavity-and-curvature",
    "title": "21  Concavity and curvature",
    "section": "",
    "text": "Application area 21.1 —Concavity on the highway.\n\n\n\n\n\n\n\nApplication area 21.1 Designing a highway curve\n\n\n\nImagine designing a highway. Due to the terrain, part of the road is oriented east-west and another part north-south. Those two parts need to be connected together for vehicles to use the road! (In math-speak, we might say that the road has to be continuous, but this is just common sense.)\nExperience with highways shows that the connection will be a smooth curve. If the curve is part of a circle, the design needs to specify the radius of curvature. Too tight a radius and the traffic will not be able to handle the centrifugal force; vehicles will drift or skid off the road. A big radius provides safety, but making the radius bigger than required adds road construction costs.\nReal-world highway on- and off-ramps are usually not precisely sections of a circle, so specifying the shape of the ramp is not as simple as setting the radius of the curve. Instead, the radius changes at the entry and exit of the curve. The American Association of State Highway and Transportation Officials Policy on Geometric Design of Highways and Streets (1994) explains why:\nAny motor vehicle follows a transition path as it enters or leaves a circular horizontal curve. The steering change and the consequent gain or loss of centrifugal force cannot be effected instantly. For most curves the average driver can effect a suitable transition path within the limits of normal lane width. However, with combinations of high speed and sharp curvature the resultant longer transition can result in crowding and sometimes actual occupation of adjoining lanes. In such instances transition curves would be appropriate because they make it easier for a driver to confine the vehicle to his or her own lane. The employment of transition curves between tangents and sharp circular curves and between circular curves of substantially different radii warrants consideration.\n\n\n\n\nRemember that \\(f()\\) is just a pronoun so that we can refer to a particular function without naming it. Likewise with \\(g()\\), \\(h()\\), etc.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Concavity and curvature</span>"
    ]
  },
  {
    "objectID": "Differentiation/21-concavity.html#sec-concavity-deriv",
    "href": "Differentiation/21-concavity.html#sec-concavity-deriv",
    "title": "21  Concavity and curvature",
    "section": "21.2 Concavity",
    "text": "21.2 Concavity\nRecall that to find the slope of a function \\(f(x)\\) at any input \\(x\\), you compute the derivative of that function, which we’ve been writing \\(\\partial_x\\,f(x)\\). Plug in some value for the input \\(x\\) and the output of \\(\\partial_x\\, f(x)\\) will be the slope of \\(f(x)\\) at that input. (Chapter 20 introduced some techniques for computing the derivative of any given function.)\nNow we want to show how differentiation can quantify the concavity of a function. First, remember that when we speak of the “derivative” of a function, we mean the first derivative of the function. That full name naturally suggests that there will be a second derivative, a third derivative, and higher-order derivatives.\nFigure 21.1 shows a simple function that is concave down.\n\n\n\n\n\n\n\n\n\nFigure 21.1: A function that is concave down.\n\n\n\n\nNotice that the concavity is not about the slope. The curve in Figure 21.1 is concave down everywhere in the domain \\(0 \\leq x \\leq 4\\), but the slope is positive for \\(0 \\leq x \\leq 1\\) and negative for larger \\(x\\). Slope and concavity are two different aspects of a function.\nAs introduced in ?sec-fun-describing, the concavity of a function describes not the slope but the change in the slope. Figure 21.2 adds some annotations on top of the graph in Figure 21.1. In the subdomain marked A, the function slope is positive, while in the subdomain B, the function slope is negative. This transition from the slope at A to the slope at B corresponds to the concavity of the function between A and B.\n\n\n\n\n\n\n\n\n\nFigure 21.2: Concavity is about how the slope changes from one place in the domain to another.\n\n\n\n\nSimilarly, the function’s concavity in the interval B to C reflects the transition in the instantaneous slope at B to the different instantaneous slope at C.\nLet’s look at this using symbolic notation. Keep in mind that the function graphed is \\(f(x)\\) while the slope is the function \\(\\partial_x\\,f(x)\\). We’ve seen that the concavity is indicated by the change in slope of \\(f()\\), that is, the change in \\(\\partial_x\\, f(x)\\). We will go back to our standard way of describing the rate of change near an input \\(x\\):\n\\[\\text{concavity.of.f}(x) \\equiv\\ \\text{rate of change in}\\ \\partial_x\\, f(x) = \\partial_x [\\partial_x f(x)] \\\\\n\\\\\n= \\lim_{h\\rightarrow 0}\\frac{\\partial_x f(x+h) - \\partial_x f(x)}{h}\\] We are defining the concavity of a function \\(f()\\) at any input \\(x\\) to be \\(\\partial_x [\\partial_x f(x)]\\). We create the concavity_of_f(x) function by applying differentiation twice to the function \\(f()\\).\nSuch a double differentiation of a function \\(f(x)\\) is called the second derivative of \\(f(x)\\). The second derivative is so important in applications that it has its own compact notation: \\[\\text{second derivative of}\\ f()\\ \\text{is written}\\ \\partial_{xx} f(x)\\] Look carefully to see the difference between the first derivative \\(\\partial_x f(x)\\) and the second derivative \\(\\partial_{xx} f(x)\\): it is all in the double subscript \\(_{xx}\\).\nComputing the second derivative is merely a matter of computing the first derivative \\(\\partial_x f(x)\\) and then computing the (first) derivative of \\(\\partial_x f(x)\\). In R this process looks like:\n\ndx_f  &lt;- D(   f(x) ~ x)   # First deriv. of f()\ndxx_f &lt;- D(dx_f(x) ~ x)   # Second deriv. of f()\n\n\n\nA notation shortcut for the two-step process above: double up on the x on the right-hand side of the tilde: dxx_f &lt;- D(f(x) ~ x & x)",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Concavity and curvature</span>"
    ]
  },
  {
    "objectID": "Differentiation/21-concavity.html#sec-curvature-definition",
    "href": "Differentiation/21-concavity.html#sec-curvature-definition",
    "title": "21  Concavity and curvature",
    "section": "21.3 Curvature",
    "text": "21.3 Curvature\nAs you see from Section 21.2, it is easy to quantify the concavity of a function \\(f(x)\\): just evaluate the second derivative \\(\\partial_{xx} f(x)\\). However, it turns out that people cannot do a good job of estimating the quantitative value of concavity by eye.\nTo illustrate, consider the square function, \\(f(x) \\equiv x^2\\). (See Figure 21.3.)\n\n\n\n\n\n\n\n\n\nFigure 21.3: Does the concavity of the square function vary with \\(x\\)?\n\n\n\n\nThe square function is concave up. Now a test: Looking at the graph of the square function, where is the concavity the largest? Don’t read on until you’ve pointed where you think the concavity is largest.\nWith the answer to the test question in mind, we can calculate the concavity of the square function using derivatives.\n\\[f(x) \\equiv x^2\\ \\text{      so     }\\\n\\partial_x f(x) = 2 x\\ \\text{     and therefore     }\\ \\partial_{xx} f(x) = 2\\]\nThe second derivative of \\(f(x)\\) is positive, as expected for a function that is concave up. Surprisingly, however, the second derivative is constant.\nThe concavity-related property that the human eye reads from the function graph is not the concavity itself but the curvature of the function. The curvature of \\(f(x)\\) at \\(x_0\\) is defined to be the radius of the circle tangent to the function at \\(x_0\\).\nFigure 21.4 illustrates the changing curvature of \\(f(x) \\equiv x^2\\) by inscribing tangent circles at several points on the function graph, marked with dots. That the function’s thin black line goes right down the middle of the broader lines used to draw the circles shows the tangency of the circle to the function graph.\n\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\nFigure 21.4: At any point on the graph of a smooth function, there is a circle tangent to the graph. The radius of this circle is \\(1/{\\cal K}\\).\n\n\n\n\n\nBlack dots along the graph at the points indicate where the function graph is tangent to the inscribed circle. The visual sign of tangency is that the function graph goes right down the circle’s center.\nThe inscribed circle at \\(x=0\\) is tightest, the circle at \\(x=1\\) larger and the radius of the circle at \\(x=-1.5\\) is the largest of all. Whereas the concavity is the same at all points on the graph, the visual impression that the function is most highly curved near \\(x=0\\) is better captured by the radius of the inscribed circle. The radius of the inscribed circle at any point is the reciprocal of a quantity \\({\\cal K}\\) called the curvature.\nThe curvature \\({\\cal K}\\) of a function \\(f(x)\\) depends on both the first and second derivative. The formula for curvature \\(K\\) is somewhat off-putting; you are not expected to memorize it. But you can see where \\(\\partial x f()\\) and \\(\\partial_{xx}f()\\) come into play.\n\\[{\\cal K}_f  \\equiv \\frac{\\left|\\partial_{xx} f(x)\\right|}{\\ \\ \\ \\ \\left|1 + \\left[\\strut\\partial_x f(x)\\right]^2\\right|^{3/2}}\\]\nMathematically, the curvature \\(\\cal K\\) corresponds to the reciprocal of the radius of the tangent circle. When the tangent circle is tight, \\(\\cal K\\) is large. When radius of the tangent circle is large, that is, when the function is very close to approximating a straight line, \\(\\cal K\\) is very small.\n\nApplication area 21.2 —Centripetal force on the highway\n\n\n\n\n\n\n\nApplication area 21.2 Back to the highway\n\n\n\nReturning to the highway design example in Application area 21.1 … The Policy on geometric design of highways and streets called for the curvature of a road to change gently, giving the driver time to adjust the steering and accommodate the centrifugal force of the car going around the curve.\nChanging curvature implies that \\(\\partial_x {\\cal K}\\) is non-zero. Since \\({\\cal K}\\) depends on the first and second derivatives of \\(f(x)\\), the Policy on gradual change means that the third derivative of \\(f(x)\\) is non-zero.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Concavity and curvature</span>"
    ]
  },
  {
    "objectID": "Differentiation/22-cont-and-smooth.html",
    "href": "Differentiation/22-cont-and-smooth.html",
    "title": "22  Continuity and smoothness",
    "section": "",
    "text": "22.1 Continuity\nThe intuition behind continuity is simple: If you can draw the graph of a function without lifting the pencil from the paper, the function is continuous.\nContinuity can be an important attribute of a modeling function. Often, we expect that a small change in input produces a small change in output. For instance, if your income changes by one penny, you would expect your lifestyle not to change by much. If the temperature of an oven changes by 1 degree, you don’t expect the quality of the cake you are baking to change in any noticeable way.\nFigure 22.1: The Heaviside function is piecewise constant with a discontiuity at \\(x=0\\).\nAll of our basic modeling functions are continuous over their entire input domain.1 To illustrate discontinuity we will consider piecewise functions, as introduced in Section 9.4. The Heaviside function, graphed in Figure 22.1 is discontinuous.\nDrawing the graph of the Heaviside function \\(H(x)\\) involves lifting the pencil at \\(x=0\\).\nIn contrast, the piecewise ramp function (Figure 22.2) is continuous; you don’t need to lift the pencil from the paper to draw the ramp function.\nFigure 22.2: The ramp function is a continuous piecewise function.\nImagine that you were constructing a model of plant growth as a function of the amount of water (in cc) provided each day. The plant needs about 20 cc of water to thrive. You use the Heaviside function for the model, say \\(H(W-20)\\), where an output of 1 means the plant thrives and a output 0 means the plant does not. The model implies that with 20.001 cc of water, the plant will thrive. But providing only 19.999 cc of water, the plant will die. In other words, a very small change in the input can lead to a large change in the output.\nCommon sense suggests that a change of 0.002 cc in the amount of water—a small fraction of a drop, 2 cubic millimeters of volume—is not going to lead to a qualitative change in output. So you might prefer to use a sigmoid function as your model rather than a Heaviside function.\nOn the other hand, sometimes a very small change in input does lead to a large change in output. For instance, a sensible model of the hardness of water as a function of temperature would include a discontinuity at \\(32^\\circ\\)F, the temperature at which water turns to ice.\nOne of author Charles Dickens’s famous characters described the relationship between income, expenditure, and happiness this way:\nMacawber referred to the common situation in pre-20th century England of putting debtors in prison, regardless of the size of their debt. Macawber’s statement suggests he would model happiness as a Heaviside function \\(H(\\text{income}- \\text{expenditure})\\).\nWhenever the output of a function is a binary (yes-or-no) value, you can anticipate that a model will involve a discontinuous function.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Continuity and smoothness</span>"
    ]
  },
  {
    "objectID": "Differentiation/22-cont-and-smooth.html#continuity",
    "href": "Differentiation/22-cont-and-smooth.html#continuity",
    "title": "22  Continuity and smoothness",
    "section": "",
    "text": "“Annual income 20 pounds, annual expenditure 19 [pounds] 19 [shillings] and six [pence], result happiness. Annual income 20 pounds, annual expenditure 20 pounds ought and six, result misery.” — the character Wilkins Micawber in David Copperfield",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Continuity and smoothness</span>"
    ]
  },
  {
    "objectID": "Differentiation/22-cont-and-smooth.html#discontinuity",
    "href": "Differentiation/22-cont-and-smooth.html#discontinuity",
    "title": "22  Continuity and smoothness",
    "section": "22.2 Discontinuity",
    "text": "22.2 Discontinuity\nRecall the logical path that led us to the idea of the derivative of a function. We started with the differencing operator, which takes as input a function and a “small” value of \\(h\\): \\[{\\cal D}_x f(x) \\equiv \\frac{f(x+h) - f(x)}{h}\\] Then, through algebraic manipulation and numerical experiments we found that, once \\(h\\) is small enough, the graph of the slope function \\({\\cal D}_x f(x)\\) does not depend on \\(h\\). And so we defined a function \\(\\partial_x f(x)\\) where \\(h\\) does not play a role, writing \\(\\lim_{h\\rightarrow 0}\\) to remember our care to never divide by zero. \\[\\partial_x f(x) \\equiv \\lim_{h\\rightarrow 0} \\frac{f(x+h) - f(x)}{h}\\ .\\] Conveniently, we found that the derivatives of the pattern-book functions can be written in terms of the pattern-book functions without making any reference to \\(h\\). For instance:\n\n\\(\\partial_x \\ln(x) = 1/x\\) No \\(h\\) appears.\n\\(\\partial_x e^x = e^x\\) No \\(h\\) appears\n\\(\\partial_x x^p = p\\, x^{p-1}\\) No \\(h\\) appears.\nand so on.\n\nWith discontinuous functions, we have no such luck. Figure 22.3 shows what happens if we compute \\({\\cal D}_x H(x)\\), the derivative of the Heaviside function, for smaller and smaller \\(h\\).\n\nH &lt;- makeFun(ifelse(x &gt;=0, 1, 0) ~ x)\nDH01   &lt;- makeFun((H(x + 0.1) - H(x))/0.1 ~ x)\nDH001  &lt;- makeFun((H(x + 0.01) - H(x))/0.01 ~ x)\nDH0001 &lt;- makeFun((H(x + 0.001) - H(x))/0.001 ~ x)\nslice_plot(DH01(x) ~ x, bounds(x=-0.02:0.02),\n           npts=500, color=\"red\", size=2) %&gt;%\n  slice_plot(DH001(x) ~ x,\n           color=\"darkgreen\", npts=500, size=3, alpha=0.5) %&gt;%\n  slice_plot(DH0001(x) ~ x,\n           color=\"blue\", npts=500, alpha=0.5, size=2) \n\n\n\n\n\n\n\nFigure 22.3: \\({\\cal D}_x H(x)\\), the slope function of the discontinuous Heaviside, function, depends on the value of \\(h\\) used for the slope function. (Red: \\(h=0.1\\); Green: \\(h=0.01\\); Blue \\(h=0.001\\))\n\n\n\n\n\nDifferencing the Heaviside function produces very different functions depending on the value of \\(h\\). The bump near \\(x=0\\) gets taller and taller as \\(h\\) gets smaller. Mathematicians would describe this situation as \\[\\lim_{h\\rightarrow0}{\\cal D}_x H(x=0) \\equiv \\lim_{h\\rightarrow 0} \\frac{H(0+h) - H(0)}{h}\\ \\ \\ \\text{does not exist}.\\] Of course, for any given value of \\(h\\), e.g. \\(h=0.000001\\), the function \\({\\cal D}_x H(x)\\) has a definite shape. But that shape keeps changing as \\(h \\rightarrow 0\\), so we cannot point to any specific shape as the “limit as \\(h \\rightarrow 0\\).”\nSince there is no convergence in the shape of \\({\\cal D}_x H(0)\\) as \\(h\\) gets smaller, it is fair to say that the Heaviside function does not have a derivative at \\(x=0\\). But away from \\(x=0\\), the Heaviside function has a perfectly sensible derivative: \\(\\partial_x H(x) = 0\\) for \\(x\\neq 0\\). But there is no derivative at \\(x=0\\).",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Continuity and smoothness</span>"
    ]
  },
  {
    "objectID": "Differentiation/22-cont-and-smooth.html#smoothness",
    "href": "Differentiation/22-cont-and-smooth.html#smoothness",
    "title": "22  Continuity and smoothness",
    "section": "22.3 Smoothness",
    "text": "22.3 Smoothness\nSmoothness is a different concept than continuity, although the two are related. Most simply, any discontinuous function is not smooth at any input where a discontinuity occurs. But even the continuous ramp function is not smooth at the start of the ramp. Intuitively, imagine you were sliding your hand along the ramp function. You would feel the crease at \\(x=0\\).\nA function is not smooth if the derivative of that function is discontinuous. For instance, the derivative of the ramp function is the Heaviside function, so the ramp is not smooth at \\(x=0\\).\nAll of our basic modeling functions are smooth everywhere in their domain. In particular, the derivatives of the basic modeling functions are continuous, as are the second derivative, third derivative, and so on down the line. Such functions are called C-infinity, written \\(C^\\infty\\). The superscript \\(\\infty\\) means that every order of derivative is continuous.\n\n\n\nYou cannot tell from the plot that the second derivative is discontinuous. But if you were in a plane flying along that trajectory, you would feel a jerk as you crossed \\(x=0\\).\nMathematicians quantify the “smoothness” of a function by looking at the function’s continuity and the continuity of its derivatives. Smoothness is assessed on a scale \\(C^0, C^1, C^2, \\ldots, C^\\infty\\).\n\n\\(C^0\\): the function \\(f()\\) is continuous. Intuitively, this means that the function’s graph can be drawn without lifting the pencil from the paper.\n\\(C^1\\): the function \\(f()\\) has a derivative over its entire domain and that derivative \\(\\partial_x f(x)\\) is continuous. (See ?fig-c1-function for an example.)\n\\(C^2\\): the function \\(\\partial_x f(x)\\) has a derivative over its entire domain and that derivative is continuous. In other words, \\(\\partial_{xx} f(x)\\) exists and is continuous.\n\\(C^n\\): Like \\(C^2\\), but we are talking about the \\(n\\)th-derivative of \\(f(x)\\) existing and being continuous.\n\\(C^\\infty\\): Usually when we denote a sequence with an infinite number of terms, we write down something like \\(C^0, C^1, C^2, \\ldots\\). It would be entirely valid to do this in talking about the \\(C^n\\) sequence. But many of the mathematical functions we work with are infinitely differentiable, that is \\(C^\\infty\\).\n\nExamples of \\(C^\\infty\\) functions:\n\n\\(\\sin(x)\\): the derivatives are \\(\\partial_x \\sin(x) = \\cos(x)\\), \\(\\partial_{xx} \\sin(x) = -\\sin(x)\\), \\(\\partial_{xxx} \\sin(x) =-\\cos(x)\\), \\(\\partial_{xxxx} \\sin(x) =\\sin(x)\\), … You can keep going infinitely.\n\\(e^x\\): the derivatives are \\(\\partial_x e^x = e^x\\), \\(\\partial_{xx} e^x = e^x\\), and so on.\n\\(x^2\\): the derivatives are \\(\\partial_x x^2 = 2 x\\), \\(\\partial_{xx} x^2 = 2\\), \\(\\partial_{xxx} x^2 = 0\\), … Higher order derivatives are all simply 0. Boring, but still existing.\n\nExample of non-\\(C^2\\) functions: We see these often when we take two or more different \\(C^\\infty\\) functions and split their domain, using one function for one subdomain and the other(s) for other subbounds(s).\n\n\\(|x|\\), the absolute value function. \\(|x|\\) is a pasting together of two \\(C^\\infty\\) functions: \\[|x| \\equiv \\left\\{\\begin{array}{rcl}+x & \\text{for} & 0 \\leq x\\\\-x&\\text{for}& \\text{otherwise}\\end{array} \\right.\\ .\\] The domain is split at \\(x=0\\).\n\n\nFor engineering and design problems, smoothness means something substantially different than described by the mathematical concepts above. In ?sec-splines we will introduce cubic splines which are continuous functions defined by a finite set of coordinate pairs: two variables of a data frame. Each line of the data frame specifies a “knot point.” The spline consists of ordinary cubic polynomials drawn piecewise between consecutive knot points. At a knot point, the cubics on either side have been arranged to have their first and second derivatives match. Thus, the first two derivatives are continuous. The function is at least \\(C^2\\). The second derivative of a cubic is a straight-line function, so the second derivative of a cubic spline is a series of straight-line functions connected at the knot points. The second derivative does not itself have a derivative at the knot points. So, a cubic spline cannot satisfy the requirements for \\(C^3\\); it is \\(C^2\\).",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Continuity and smoothness</span>"
    ]
  },
  {
    "objectID": "Differentiation/22-cont-and-smooth.html#footnotes",
    "href": "Differentiation/22-cont-and-smooth.html#footnotes",
    "title": "22  Continuity and smoothness",
    "section": "",
    "text": "The domain of the function \\(1/x\\) is the whole number line, except 0, where the positive and negative branches fail to meet up.↩︎",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Continuity and smoothness</span>"
    ]
  },
  {
    "objectID": "Differentiation/23-rules.html",
    "href": "Differentiation/23-rules.html",
    "title": "23  Derivatives of assembled functions",
    "section": "",
    "text": "23.1 Using the rules\nWhen you encounter a function that you want to differentiate, you first have to examine the function to decide which rule you want to apply. In the following, we will to use the names \\(f()\\) and \\(g()\\), but in practice the functions will often be basic modeling functions, for instance \\(e^{kx}\\) or \\(\\sin\\left(\\frac{2\\pi}{P}t\\right)\\), etc.\nStep 1: Identify f() and g()\nWe will write the rules in terms of two function names, \\(f()\\) and \\(g()\\), which can stand for any functions whatsoever. It is rare to see the product or the composition written explicitly as \\(f(x)g(x)\\) of \\(f(g(x))\\). Instead, you are given something like \\(e^x \\ln(x)\\). The first step in differentiating the product or composition is to identify what are \\(f()\\) and \\(g()\\) individually.\nIn general, \\(f()\\) and \\(g()\\) might be complicated functions, themselves involving linear combinations, products, and composition. But to get started, we will practice with cases where they are simple, pattern-book functions.\nStep 2: Find f’() and g’()\nFor differentiating either products or compositions, you will need to identify both \\(f()\\) and \\(g()\\) (the first step) and then compute the derivatives \\(\\partial_x f()\\) and \\(\\partial_x g()\\). That is, you will write down four functions.\nStep 3: Apply the relevant rule\nRecall from ?sec-fun-assembling that will will be working with three important forms for creating new functions out of existing functions:",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Derivatives of assembled functions</span>"
    ]
  },
  {
    "objectID": "Differentiation/23-rules.html#sec-using-the-rules",
    "href": "Differentiation/23-rules.html#sec-using-the-rules",
    "title": "23  Derivatives of assembled functions",
    "section": "",
    "text": "Linear combinations, e.g. \\(a f(x) + bg(x)\\)\nProducts of functions, e.g. \\(f(x) g(x)\\)\nCompositions of functions, e.g. \\(f\\left(g(x)\\right)\\)",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Derivatives of assembled functions</span>"
    ]
  },
  {
    "objectID": "Differentiation/23-rules.html#differentiating-linear-combinations",
    "href": "Differentiation/23-rules.html#differentiating-linear-combinations",
    "title": "23  Derivatives of assembled functions",
    "section": "23.2 Differentiating linear combinations",
    "text": "23.2 Differentiating linear combinations\nLinear combination is one of the ways in which we make new functions from existing functions. As you recall, linear combination involves scaling functions and then adding the scaled functions as in \\(a f(x) + b g(x)\\), alinear combination of \\(f(x)\\) and \\(g(x)\\). We can easily use \\(h\\) to show what is the result of differentiating a linear combination of functions. First, let’s figure out what is \\(\\partial_x\\, a f(x)\\), Going back to writing \\(\\partial_x\\) in terms of a slope function: \\[\\partial_x\\, a\\,f(x) = \\frac{a\\, f(x + h) - a\\,f(x)}{h}\\\\\n\\ \\\\\n= a \\frac{f(x+h) - f(x)}{h} = a\\, \\partial_x f(x)\\] In other words, if we know the derivative \\(\\partial_x\\, f(x)\\), we can easily find the derivative of \\(a\\, f()\\). Notice that even though \\(h\\) was used in the derivation, it appears nowhere in the result \\(\\partial_x\\, b\\,f(x) = b\\, \\partial_x\\, f(x)\\). The \\(h\\) is solvent to get the paint on the wall and evaporates once its job is done.\nNow consider the derivative of the sum of two functions, \\(f(x)\\) and \\(g(x)\\): \\[\\begin{eqnarray}\n\\partial_x\\, \\left[f(x) + g(x)\\right] & =\\frac{\\left[f(x + h) + g(x + h)\\right] - \\left[f(x) + g(x)\\right]}{h} \\\\\n\\ \\\\\n&= \\frac{\\left[f(x+h) -f(x)\\right] + \\left[g(x+h) - g(x)\\right]}{h}\\\\\n\\ \\\\\n&= \\frac{\\left[f(x+h) -f(x)\\right]}{h} + \\frac{\\left[g(x+h) - g(x)\\right]}{h}\\\\\n\\ \\\\\n&= \\partial_x\\, f(x) + \\partial_x\\, g(x)\n\\end{eqnarray}\\]\nBecause of how \\(\\partial_x\\) can be “passed through” a linear combination, mathematicians say that differentiation is a linear operator. Consider this new fact about differentiation as a down payment on what will eventually become a complete theory telling us how to differentiate a product of two functions or the composition of two functions. We will lay out the \\(h\\)-theory based algebra of this in the next two sections.\nWe can summarize the h-theory result for linear combinations this way:\n\nThe derivative of a linear combination is the linear combination of the derivatives.\n\nThat is:\n\\[\\partial_x \\left[\\strut \\color{magenta}{a} \\color{brown}{f(x)} + \\color{magenta}{b} \\color{brown}{g(x)}\\right] = \\color{magenta}{a} {\\large\\color{brown}{f'(x)}} + \\color{magenta}{b} {\\large\\color{brown}{g'(x)}}\\] as well as \\[\\partial_x \\left[\\strut \\color{magenta}{a}\\, \\color{brown}{f(x)} + \\color{magenta}{b}\\, \\color{brown}{g(x)}  + \\color{magenta}{c}\\, \\color{brown}{h(x)} + \\cdots\\right] = \\color{magenta}{a}\\, {\\large\\color{brown}{f'(x)}} + \\color{magenta}{b}\\, {\\large\\color{brown}{g'(x)}} + \\color{magenta}{c}\\, {\\large\\color{brown}{h'(x)}} + \\cdots\\]\n\nThe derivative of a polynomial is a polynomial of a lower order.\nConsider the polynomial \\[h(x) = \\color{magenta}{a}\\color{brown}{x^0}  + \\color{magenta}{b} \\color{brown}{x^1} + \\color{magenta}{c} \\color{brown}{x^2}\\] The derivative is \\[\\partial_x h(x) = \\color{brown}{0}\\, \\color{magenta}{a}  + \\color{brown}{1}\\, \\color{magenta}{b}  + \\color{magenta}{c}\\, \\color{brown}{2 x} = \\color{magenta}{b} +  \\color{magenta}{2 c}\\  x\\]",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Derivatives of assembled functions</span>"
    ]
  },
  {
    "objectID": "Differentiation/23-rules.html#product-rule-for-multiplied-functions",
    "href": "Differentiation/23-rules.html#product-rule-for-multiplied-functions",
    "title": "23  Derivatives of assembled functions",
    "section": "23.3 Product rule for multiplied functions",
    "text": "23.3 Product rule for multiplied functions\nThe question at hand is how to compute the derivative \\(\\partial_x f(x) g(x)\\). Of course, you can always use numerical differentiation. But let’s look at the problem from the point of view of symbolic differentiation. And since \\(f(x)\\) and \\(g(x)\\) are just pronoun functions, we will assume you are starting out already knowing the derivatives \\(\\partial_x f(x)\\) and \\(\\partial_x g(x)\\).\nThis situation arises particularly when \\(f(x)\\) and \\(g(x)\\) are pattern-book functions for which you already have memorized \\(\\partial_x f(x)\\) and \\(\\partial_x g(x)\\) or are basic modeling functions whose derivatives you will memorize in Section @ref(basic-derivs).\nThe purpose of this section is to derive the formula for \\(\\partial_x f(x) g(x)\\) in terms of \\(f(x)\\), \\(g(x)\\), \\(\\partial_x f(x)\\) and \\(\\partial_x g(x)\\). This formula is called the product rule. The point of showing a derivation of the product rule is to let you see how the logic of evanescent \\(h\\) plays a role. In practice, everyone simply memorizes the rule, which has a beautiful, symmetric form:\n\\[\\text{Product rule:}\\ \\ \\ \\ \\partial_x \\left[\\strut f(x)g(x)\\right] = \\left[\\strut \\partial_x f(x)\\right]\\, g(x) + f(x)\\, \\left[\\strut\\partial_x g(x)\\right]\\] and is even prettier in Lagrange notation (where \\(\\partial_x f(x)\\) is written \\(f'\\)): \\[ \\left[\\strut f g\\right]' = f' g + g' f\\]\nAs with all derivatives, the product rule is based on the instantaneous rate of change \\[F'(x) \\equiv \\lim_{h\\rightarrow 0} \\frac{F(x+h) - F(x)}{h}\\] introduced in ?sec-instantaneous-rate-of-change.\nWe also need two other statements about \\(h\\) and functions:\n\nThe derivative \\(F'(x)\\) is the slope of of \\(F()\\) at input \\(x\\). Taking a step of size \\(h\\) from \\(x\\) will induce a change of output of \\(h F'(x)\\), so \\[F(x+h) = f(x) + h F'(x)\\ .\\]\nAny result of the form \\(h F(x)\\), where \\(F(x)\\) is finite, gives 0. More precisely, \\(\\lim_{h\\rightarrow 0} h F(x) = 0\\)\n\nAs before, we will put the standard \\(\\lim_{h\\rightarrow 0}\\) disclaimer against dividing by \\(h\\) until there are no such divisions at all, at which point we can safely use the equality \\(h = 0\\).\nSuppose the function \\(F(x) \\equiv f(x) g(x)\\), a product of the two functions \\(f(x)\\) and \\(g(x)\\).\n\\[F'(x) = \\partial_x \\left[\\strut f(x) g(x) \\right] \\equiv \\lim_{h\\rightarrow 0}\\frac{f(x+h) g(x+h) - f(x) g(x)}{h}\\] We will replace \\(g(x_h)\\) with its equivalent \\(g(x) + h g'(x)\\) giving\n\\[= \\lim_{h\\rightarrow 0} \\frac{f(x+h) \\left[\\strut g(x) + h g'(x) \\right] - f(x) g(x)}{h} \\] \\(g(x)\\) appears in both terms in the numerator, once multiplied by \\(f(x+h)\\) and once by \\(f(x)\\). Collecting those terms give:\n\\[=\\lim_{h\\rightarrow 0}\\frac{\\left[\\strut f(x+ h) - f(x)\\right]  g(x) + \\left[\\strut f(x+h) h\\, g'(x)\\right]}{h}\\] This has two bracketed terms added together over a common denominator. Let’s split them into separate terms:\n\\[=\\lim_{h\\rightarrow 0}\\underbrace{\\left[\\strut \\frac{f(x+h) - f(x)}{h}\\right]}_{f'(x)} g(x) + \\lim_{h\\rightarrow 0}\\frac{\\left[\\strut f(x) + h f'(x)\\right]h\\,g'(x)}{h}\\]\nThe first term is \\(g(x)\\) multiplied by the familiar form for the derivative of \\(f(x)\\) \\[= f'(x) g(x) + \\lim_{h\\rightarrow 0}\\frac{f(x) h g'(x)}{h} + \\lim_{h\\rightarrow 0}\\frac{h f'(x) h g'(x)}{h}\\] In each of the last two terms there is an \\(h/h\\) involved. This is safely set to 1, since the \\(\\lim_{h\\rightarrow 0}\\) implies that \\(h\\) will not be exactly zero. There remain no divisions by \\(h\\) so we can drop the \\(\\lim_{h\\rightarrow 0}\\) in favor of \\(h=0\\): \\[= f'(x) g(x) + f(x) g'(x) + \\cancel{h f'(x) g'(x)}\\]\n\\[=f'(x) g(x) + g'(x) f(x)\\]\nThe last step relies on statement (2) above.\nSome people find it easier to read the rule in Lagrange shorthand, where \\(f\\) and \\(g\\) stand for \\(f(x)\\) and \\(g(x)\\) respectivly, and \\(f'\\) (“f-prime”) and \\(g'\\) (“g-prime”) stand for \\(\\partial f()\\) and \\(\\partial g()\\).\n\\[\\large\\text{Lagrange shorthand:}\\ \\   \\partial[\\color{magenta}f \\times \\color{brown}g] = [\\color{magenta}f \\times \\color{brown}g]' = \\color{magenta}{f'}\\color{brown}g + \\color{brown}{g'}\\color{magenta}f\\]\n\nThe expression \\(\\partial_x x^3\\) is the same as \\(\\partial_x \\left[\\strut x\\  x^2\\right]\\). Since we already know \\(\\partial_x x\\) (it is 1) and \\(\\partial_x x^2\\) (it is \\(2x\\)) let’s apply the product rule to find \\(\\partial_x x^3\\): \\[\\large\\partial [\\color{magenta}x \\times \\color{brown}{x^2}] = \\color{magenta}{[\\partial x]} \\times \\color{brown}{x^2} \\ +\\  \\color{brown}{[\\partial x^2]} \\times \\color{magenta}x =\\color{magenta}1\\times \\color{brown}{x^2} + \\color{brown}{2x} \\times \\color{magenta}x = 3 x^2\\]\n\n\nOccasionally, mathematics gives us a situation where being more general produces simplicity.\nIn the case of function products, the generalization is from products of two functions \\(f(x)\\cdot g(x)\\) to products of more than two functions, e.g. \\(u(x) \\cdot v(x) \\cdot w(x)\\).\nThe chain rule here takes a form that makes the overall structure much clearer:\n\\[\\begin{eqnarray}\n\\partial_x \\left[\\strut u(x) \\cdot v(x) \\cdot w(x)\\right] = \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\\\\n\\color{blue}{\\partial_x u(x)} \\cdot v(x) \\cdot w(x)\\ + \\\\\nu(x) \\cdot \\color{blue}{\\partial_x v(x)} \\cdot w(x)\\ + \\\\\nu(x) \\cdot v(x) \\cdot \\color{blue}{\\partial_x w(x)}\\ \\  \\ \\\n\\end{eqnarray}\\]\\end{eqnarray}\nIn the Lagrange shorthand, the pattern is even more evident: \\[\\left[ u\\cdot v\\cdot w\\right]' = \\color{blue}{u'}\\cdot v\\cdot w\\ +\\ u\\cdot \\color{blue}{v'}\\cdot w\\ +\\ u\\cdot v\\cdot \\color{blue}{w}'\\]",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Derivatives of assembled functions</span>"
    ]
  },
  {
    "objectID": "Differentiation/23-rules.html#chain-rule-for-function-composition",
    "href": "Differentiation/23-rules.html#chain-rule-for-function-composition",
    "title": "23  Derivatives of assembled functions",
    "section": "23.4 Chain rule for function composition",
    "text": "23.4 Chain rule for function composition\nA function composition, as described in Section 9.2, involves inserting the output of one function (the “interior function”) as the input of the other function (the “exterior function”). As we so often do, we will be using pronouns a lot. A list might help keep things straight:\n\nThere are two functions involved in a composition. Generically, we call them \\(f(y)\\) and \\(g(x)\\). In the composition \\(f(g(x))\\), the exterior function is \\(f()\\) and the interior function is \\(g()\\).\nEach of the two functions \\(f()\\) and \\(g()\\) has an input. In our examples, we use \\(y\\) to stand for the input to the exterior function and \\(x\\) for the input to the interior function.\nAs with all rules for differentiation, we will need to compute the derivatives of the functions involved, each with respect to its own input. So these will be \\(\\partial_y f(y)\\) and \\(\\partial_x g(x)\\).\n\nA reason to use different pronouns for the inputs to \\(f()\\) and \\(g()\\) is to remind us that the output \\(g(x)\\) is in general not the same kind of quantity as the input \\(x\\). In a function composition, the \\(f()\\) function will take the output \\(g(x)\\) as input. But since \\(g(x)\\) is not necessarily the same kind of thing as \\(x\\), why would we want to use the same name for the input to \\(f()\\) as we use for the input to \\(g()\\).\nWith this distinction between the names of the inputs, we can be even more explicit about the composition, writing \\(f(y=g(x))\\) instead of \\(f(g(x))\\). Had we used the pronound \\(x\\) for the input to \\(f()\\) but our explicit statement, although technically correct, would be confusing: \\(f(x = g(x))\\)!\nWith all these pronouns in mind, here is the chain rule for the derivative \\(\\partial_x f(g(x))\\):\n\\[\\large\\partial_x \\left[\\strut \\color{magenta}{f\\left(\\strut\\right.}\\strut \\color{brown}{g(x)}\\color{magenta}{\\left.\\right)}\\right] = [\\color{magenta}{\\partial_y f}](\\color{brown}{g(x)}) \\times [\\color{brown}{\\partial_xg(x)}]\\] Or, using the Lagrange prime notation, where \\('\\) stands for the derivative of a function with respect to its input, we have \\[\\large\\text{Lagrange shorthand:}\\ \\   [\\color{magenta}f(\\color{brown}g)]' = \\color{magenta}{f'} (\\color{brown}g) \\times \\color{brown}{g}'\\]",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Derivatives of assembled functions</span>"
    ]
  },
  {
    "objectID": "Differentiation/23-rules.html#rates-per-time",
    "href": "Differentiation/23-rules.html#rates-per-time",
    "title": "23  Derivatives of assembled functions",
    "section": "23.5 Rates per time",
    "text": "23.5 Rates per time\nIn news and policy discussions, you will often hear about “inflation rate” or “birth rate” or “interest rate” or “investment rate of return.” In each case, there is a function of time combined with a derivative of that function: with the general form \\[\\frac{\\partial_t f(t)}{f(t)}\\ .\\]\n\nInflation rate: The function is cost_of_living(\\(t\\)). The derivative is the rate of change with respect to time in the cost of living: \\(\\partial_t\\,\\)cost_of_living(\\(t\\)).\nBirth rate: The function is population(\\(t\\)). The derivative is \\(\\partial_t\\,\\)population(\\(t\\)), or at least that component of the overall \\(\\partial_t\\,\\)population(\\(t\\)) that is related to births. (Other components are deaths and the balance of in-migration and out-migration.)\nInterest rate: The function is account_balance(\\(t\\)) and the derivative is \\(\\partial_t\\,\\)account_balance(\\(t\\)).\nInvestment returns: The function is net_worth(\\(t\\)) and the derivative is \\(\\partial_t\\,\\)net_worth(\\(t\\)).\n\nIn all these cases, The “rate” is not merely “per time” as would be the case for \\(\\partial_t f(t)\\). Instead the rate is “per unit of the whole per time.” For birth rate, the “whole” is the population. The birth rate is the number of births in a year divided by the population itself. Birth rates are often stated with the phrase is “per capita per year.”“Per capita” is Latin. It translates to “by head.” Its modern sense is “per unit of population.” Of course, the “unit of population” is a person.\nNotice the two uses of “per” in the phrase: “births per capita per year.” A proportional rate is two rates in one. Births per capita is a proportion of the population. Births per year is an average rate with respect to time. But “births per capita per year” is a rate in the proportion with respect to time.\nThe rate word “per” also appears as part of “percent,” which literally means “per hundred.” A “percentage change” is the amount of change divided by the base amount. Confusingly, perhaps, “percentage change” is often truncated to the shorter “percent.” This is the case with inflation rates, interest rates, and rates of return on investment. The interest rate on a credit-card debt is stated as a proportion of the current debt; all that is packed into the word “percent.” The interest rate itself is the “proportion of the current debt per year”: two rates in one.\nSimilarly for an inflation rate. “Inflation” is stated as the change in prices divided by the current price: a proportional change. “Inflation rate” is the proportional change per unit of time, where the “whole” is current prices and the rate is change in current prices per year divided by current prices.\nThanks to the chain rule, there is a shortcut way of writing proportional rates per time. Exactly equivalent to the ratio \\(\\frac{\\partial_t f(t)}{f(t)}\\) is \\[\\partial_t \\ln(f(t))\\ .\\]\nDerivatives of logarithms appear often in fields such as economics or finance, where it is common to consider the logarithm of the economic quantity to render changes as percent of the whole.\n\nApplication area 23.1 —Tips on reading a graph\n\n\n\n\n\n\n\nApplication area 23.1 Linear or logarithmic axes?\n\n\n\nIt always pays to look carefully at the axes in a graph. For instance, consider Figure 23.1 which shows the cumulative number of COVID during a period in 2020, early in the pandemic.\n\n\n\n\n\n\n\n\nFigure 23.1: Growth in the number of Coronavirus cases in Italy and the US early in the pandemic. Source\n\n\n\n\n\nThe two panels in Figure 23.1 show the same data about growing numbers of coronavirus cases, the left graph on linear axes, the right on the now-familiar semi-log axes.\nMost people are excellent at comparing slopes, even if they find it difficult or tedious to quantify a slope with a number and units. For instance, a glance suffices to show that in the left graph, well through mid-March the red curve (Italy) is steeper on any given date than the blue curve (US). Correspondingly, the number of people with coronavirus was growing faster (per day) in Italy.\nThe right graph tells a different story: up until about March 1, the Italian cases were increasing faster than the US cases. Afterwards, the US sees a larger growth rate than Italy until, around March 19, the US growth rate is substantially larger than the Italy growth rate.\nThe previous two paragraphs and their corresponding graphs seem to contradict one another. But they are both accurate, truthful depictions of the same events. What’s different between the two graphs is that the left shows one kind of rate and the right shows another kind of rate. In the left, the slope is new-cases-per-day, the output of the derivative function\nleft graph: \\(\\ \\ \\ \\  \\partial\\_t\\, \\text{daily\\_new\\_cases}(t)\\).\nOn the right, the slope is the proportional increase in cases per day, that is,\nright graph: \\(\\ \\ \\ \\ \\frac{\\partial_t\\, \\text{daily\\_new\\_cases}(t)}{\\text{daily\\_new\\_cases}(t)}\\).\nFrom the chain rule, we know that\n\\[\\partial_t \\left[\\strut\\ln(f(t))\\right] = \\frac{\\partial_t f(t)}{f(t)}\\ .\\]\nSince the right graph is on semi-log axes, the slope we perceive visually is \\(\\partial_t \\left[\\strut\\ln(f(t))\\right]\\). That is an obscure-looking bunch of notation until the chain rule reveals it to be the rate of change at time \\(t\\) divided by the value at time \\(t\\).\n\n\nThe derivation of the chain rule relies on two closely related statements which are expressions of the idea that near any value \\(x\\) a function can be expressed as a linear approximation with the slope equal to the derivative of the function :\n\n\\(g(x + h) = g(x) + h g'(x)\\)\n\\(f(y + \\epsilon) = f(y) + \\epsilon f'(y)\\), which is the same thing as (1) but uses \\(y\\) as the argument name and \\(\\epsilon\\) to stand for the small quantity we usually write with an \\(h\\).\n\nWe will now look at \\(\\partial_x f\\left({\\large\\strut} g(x)\\right)\\) by writing down the fundamental definition of the derivative. This, of course, involves the disclaimer \\(\\lim_{h\\rightarrow 0}\\) until we are sure that there is no division by \\(h\\) involved.\n\\[\\partial_x \\left[{\\large\\strut} f\\left(\\strut g(x)\\right)\\right]  \\equiv \\lim_{h\\rightarrow 0}\\frac{\\color{magenta}{f(g(x+h))} - f(g(x))}{h}\\]\nLet’s examine closely the expression \\(\\color{magenta}{f\\left(\\strut g(x+h)\\right)}\\). Applying rule (1) above turns it into \\[\\lim_{h\\rightarrow 0} f\\left(\\strut g(x) + \\color{blue}{h g'(x)}\\right)\\] Now apply rule (2) but substituting in \\(g(x)\\) for \\(y\\) and \\(\\color{blue}{h g'(x)}\\) for \\(\\epsilon\\), giving\n\\[\\lim_{h\\rightarrow 0} \\color{magenta}{f\\left(\\strut g(x+h)\\right)} = \\lim_{h\\rightarrow 0} \\color{brown}{\\left[{\\large\\strut} f\\left(g(x)\\right) + \\color{blue}{h g'(x)}f'\\left(g(x)\\right)\\right]}\\] We will substitute the \\(\\color{blue}{blue}\\) and \\(\\color{brown}{brown}\\) expression for the \\(\\color{magenta}{magenta}\\) expression in \\[\\partial_x f\\left(\\strut g(x)\\right)  \\equiv \\lim_{h\\rightarrow 0}\\frac{\\color{magenta}{f(g(x+h))} - f(g(x))}{h}\\] giving \\[\\partial_x f\\left(\\strut g(x)\\right)  \\equiv \\lim_{h\\rightarrow 0}\\frac{\\color{brown}{f\\left(g(x)\\right) + \\color{blue}{h g'(x)}f'\\left(g(x)\\right)} - f\\left(g(x)\\right)}{h}\\] In the denominator, \\(f\\left(g(x)\\right)\\) appears twice and cancels itself out. That leaves a single term with an \\(h\\) in the numerator and an \\(h\\) in the denominator. Those \\(h\\)’s cancel out, at the same time obviating the need for \\(\\lim_{h\\rightarrow 0}\\) and leaving us with the chain rule: \\[\\partial_x f\\left(\\strut g(x)\\right)  \\equiv \\lim_{h\\rightarrow 0}\\frac{\\color{brown}{ \\color{blue}{h g'(x)} f'\\left(g(x)\\right)}}{h} = f'\\left(g(x)\\right)\\ g'(x)\\]\n\nUse the chain rule to find the derivative \\(\\partial_x e^{2x}\\).\nRecognize that \\(g(x) \\equiv 2x\\) is the interior function in \\(e^{2x}\\) and \\(f(x) \\equiv \\exp(x)\\) is the exterior function. Thus \\[\\partial_x e^{2x} = f'(g(x)) g'(x) = \\exp(g(x)) 2 = 2 e^{2x}\\ .\\] Happily, this is the same result as we got from using the product rule to find \\(\\partial_x e^{2x}\\).\nRecognizing \\(e^{2x}\\) as \\(e^x \\times e^x\\), we can apply the product rule.\n\n\nThe chain rule can be used in a clever way to find a formula for \\(\\partial_x \\ln(x)\\).\nWe’ve already seen that the logarithm is the inverse function to the exponential, and vice versa. That is: \\[e^{\\ln(y)} = y \\ \\ \\ \\text{and}\\ \\ \\ \\ln(e^y) = x\\] Since \\(\\ln(e^y)\\) is the same function as \\(y\\), the derivative \\(\\partial_y \\ln(e^y) = \\partial_y y = 1\\).\nLet’s differentiate the second form using the chain rule: \\[\\partial_y \\ln(e^y) = \\left[\\partial_y \\ln\\right](e^y)\\, e^x = 1\\] giving \\[\\left[\\partial_y \\ln\\right](e^y) = \\frac{1}{e^y} = \\recip(e^y)\\] Whatever the function \\(\\partial_x \\ln()\\) might be, it takes its input and produces as output the reciprocal of that input. In other words: \\[\\partial_x \\ln(x) = \\frac{1}{x}\\ .\\]\n\n\nKnowing that \\(\\partial_x \\ln(x) = 1/x\\) and the chain rule, we are in a position to demonstrate the power-law rule \\(\\partial_x x^p = p\\, x^{p-1}\\). The key is to use the identity \\(e^{\\ln(x)} = x\\).\n\\[\\partial_x x^p = \\partial_x \\left[e^{\\ln(x)}\\right]^p\\] The rules of exponents allow us to recognize \\[\\left[e^{\\ln(x)}\\right]^p = e^{p \\ln(x)}\\] Thus, \\(x^p\\) can be seen as a composition of the exponential function onto the logarithm function.\nApplying the chain rule to this composition gives \\[\\partial_x e^{p \\ln(x)} = e^{p\\ln(x)}\\partial_x [p \\ln(x)] =\ne^{p\\ln(x)} \\frac{p}{x}\\ .\\] Of course, we already know that \\(e^{p \\ln(x)} = x^p\\), so we have \\[\\partial_x x^p = x^p \\frac{p}{x} = p x^{p-1}\\ .\\]\n\n\n\\(\\large\\partial_x [\\color{brown}\\sin(\\color{magenta}{a x + b})] = [\\partial_x \\color{brown}{\\sin}](\\color{magenta}{a x + b}) \\times \\partial_x [\\color{magenta}{ax + b}] = \\color{brown}{\\cos}(\\color{magenta}{ax + b}) \\times \\color{magenta}a\\).",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Derivatives of assembled functions</span>"
    ]
  },
  {
    "objectID": "Differentiation/23-rules.html#sec-basic-derivs",
    "href": "Differentiation/23-rules.html#sec-basic-derivs",
    "title": "23  Derivatives of assembled functions",
    "section": "23.6 Derivatives of the basic modeling functions",
    "text": "23.6 Derivatives of the basic modeling functions\nThe basic modeling functions are the same as the pattern-book functions, but with bare \\(x\\) replaced by \\(\\line(x)\\). In other words, each of the basic modeling functions is a composition of the corresponding pattern-book function with \\(\\line(x)\\). Consequently, the derivatives of the basic modeling functions can be found using the chain rule.\nSuppose \\(f()\\) is one of our pattern-book functions. Then \\[\\large\\partial_x f(\\color{magenta}{ax + b}) = \\color{brown}{a} f'(\\color{magenta}{ax + b})\\] where \\(\\color{brown}{a}\\) is the derivative with respect to \\(x\\) of \\(\\color{magenta}{ax + b}\\).\nHere are the steps for differentiating a basic modeling function \\(\\color{brown}{f}(\\color{magenta}{a x + b})\\) where \\(f()\\) is one of the pattern-book functions:\n\nStep 1: Identify the particular pattern-book function \\(\\color{brown}{f}()\\) and write down its derivative \\(\\color{brown}{f'}\\). For example, if \\(f()\\) is \\(\\sin()\\), then \\(f'()\\) is \\(\\cos()\\).\nStep 2: Find the derivative of the linear interior function. If the function is \\(\\color{magenta}{ax + b}\\), then the derivative is \\(\\color{magenta}{a}\\). If the interior function is \\(\\frac{2\\pi}{P}(t-t_0)\\), the derivative is \\(\\frac{2 \\pi}{P}\\).\nStep 3: Write down the original function \\(\\large\\color{brown}{f}(\\color{magenta}{a x + b})\\) but replace \\(\\large\\color{brown}{f}\\) with \\(\\large \\color{brown}{f'}\\) and pre-multiply by the derivative of the interior function. For instance, \\[\\partial_x f(\\color{magenta}{ax + b}) = {\\large \\color{magenta}{a}}{\\large f'}(\\color{magenta}{ax + b})\\] Another example: \\[\\partial_t \\color{brown}{\\sin}\\left(\\color{magenta}{\\frac{2 \\pi}{P}(t-t_0)} \\right) = {\\large \\color{magenta}{\\frac{2 \\pi}{P}}}\\color{brown}{\\large\\cos}\\left(\\color{magenta}{\\frac{2 \\pi}{P}(t-t_0) }\\right) \\]\n\nBy convention, there are different ways of writing \\(\\line(x)\\) for the different pattern-book functions, for instance:\n\n\n\n\n\n\n\nPattern-book function \\(\\longrightarrow\\)\nBasic modeling\n\n\n\n\n\\(\\sin(x)\\ \\ \\ \\longrightarrow\\)\n\\(\\sin\\left(\\strut2 \\pi \\left[x-x_0\\right]/P\\right)\\)\n\n\n\\(\\exp(x)\\ \\ \\ \\longrightarrow\\)\n\\(\\exp(k x)\\)\n\n\n\\(x^2 \\ \\ \\ \\longrightarrow\\)\n\\(\\left[mx + b\\right]^2\\)\n\n\n\\(1/x \\ \\ \\ \\longrightarrow\\)\n\\(1/\\left[mx + b\\right]\\)\n\n\n\\(\\ln(x) \\ \\ \\ \\longrightarrow\\)\n\\(\\ln(a x + b)\\)\n\n\n\nThe rule for the derivative of any basic modeling function \\(f(\\line(x))\\) is \\[\\partial_x f(\\line(x)) = \\partial_x \\line(x) \\times \\partial_x f\\left(\\strut\\line(x)\\right)\\]\nTo illustrate:\n\n\\(\\partial_x e^{\\color{magenta}{kx}} = {\\large\\color{magenta}{k}}\\, e^{\\color{magenta}{kx}}\\) where \\(\\line(x) = kx\\).\n\\(\\partial_x \\sin(2\\pi (x-x_0)/P) = \\frac{2\\pi}{P}  \\sin(2\\pi (x-x_0)/P)\\) where \\(\\line(x) = 2\\pi (x-x_0)/P)\\).\n\\(\\partial_x (mx + b)^2 = m\\, 2 (m x + b) = 2 m^2 x + m^2 b\\) where \\(\\line(x) = mx + b\\).\n\\(\\partial_x \\text{reciprocal}(mx + b) = \\partial_x \\frac{1}{mx + b} = - \\frac{m}{(mx + b)^2}\\) where \\(\\line(x) = mx + b\\) and we use the fact that \\(\\partial_x \\text{reciprocal}(x) = - 1/x^2\\)\n\\(\\partial_x \\ln(a x + b) = a/(ax+b)\\)\n\\(\\partial_x \\pnorm(x, \\text{mean}, \\text{sd}) = dnorm(x, \\text{mean}, \\text{sd})\\).\n\\(\\partial_x \\dnorm(x, \\text{mean}, \\text{sd}) =  - \\frac{x-m}{\\text{sd}^2} \\dnorm(x, \\text{mean}, \\text{sd})\\)\n\nYou will be using the derivatives of the basic modeling functions so often, that you should practice and practice until you can write the derivative at a glance.\n\nThere are many possible implementations of the general concept of bump functions and sigmoid functions. This book uses \\(\\dnorm()\\) for the bump and \\(\\pnorm()\\) for the sigmoid.\nThe names \\(\\dnorm\\) and \\(\\pnorm\\) are worth remarking on. As we’ve said before, \\(\\dnorm()\\) is called the gaussian function in many fields of science and engineering. It is also a centrally important function in statistics, where it is called the normal function. (that is how important it is: it is just “normal.”) You may also have heard the normal function described as a “bell-shaped curve.”\nIn statistical nomenclature, \\(\\dnorm()\\) is called the “normal probability density function (PDF)” and \\(\\pnorm()\\) is called the “normal cumulative density function (CDF).” that is way too wordy for our purposes. For brevity, we have adopted the R name for those functions: dnorm() and pnorm().\nOwing to the origin of the names \\(\\dnorm\\) and \\(\\pnorm\\), we are writing the parameters of the functions—mean and sd—using the computer language notation. The pattern-book functions are just \\(\\dnorm(x)\\) and \\(\\pnorm(x)\\), without listing the parameters. But the basic modeling functions, with parameters, are written \\(\\dnorm(x, \\text{mean}, \\text{sd})\\) and \\(\\dnorm(x, \\text{mean}, \\text{sd})\\). This violates the convention that the basic modeling functions are the composition of the pattern-book functions with \\(\\line(x)\\). But \\(\\dnorm()\\) does not work this way because, by convention, the amplitude of the peak of \\(\\dnorm()\\) changes with the input parameter sd. That is not true for any other basic modeling function.\n\n\nComposition or product?\nThere is one family of functions for which function composition accomplishes same thing as multiplying functions: the power-law family.\nConsider, for instance, the function \\(h(x) \\equiv \\left[3x\\right]^4\\). Let’s let \\(g(x) \\equiv 3x\\) and \\(f(y) \\equiv y^4\\). With these definitions, \\(h(x) = f(g(x))\\).\nRecognizing that \\(\\partial_y f(y) = 4 y^3\\) and \\(\\partial_x g(x) = 3\\), the chain rule gives \\[\\partial_x h(x) =\n\\underbrace{4 g(x)^3}_{f'(g(x))} \\times \\underbrace{3}_{g'(x)} = \\underbrace{4 (3 x)^3}_{f'(g(x))} \\times 3 = 4\\cdot 3^4 \\times x^3 = 324\\ x^3\\] Another way to look at the same function is \\(g(x)\\) multiplied by itself 3 times: \\[h(x) = g(x)\\cdot g(x) \\cdot g(x) \\cdot g(x)\\] This is a product of 4 terms. Applying the product rule gives \\[\\begin{eqnarray}\n\\partial_x h(x) &=& \\color{blue}{g'(x)}\\cdot g(x)\\cdot g(x) \\cdot g(x) +\\\\\n&\\ & g(x)\\cdot \\color{blue}{g(x)}'\\cdot g(x) \\cdot g(x) +\\\\\n&\\ & g(x)\\cdot g(x)\\cdot \\color{blue}{g(x)'} \\cdot g(x) +\\\\\n&\\ & g(x)\\cdot g(x)\\cdot g(x) \\cdot \\color{blue}{g'(x)}\n\\end{eqnarray}\\] Since multiplication is commutative, all four terms are the same, each being \\(3^4 x^3\\). The sum of all four is therefore \\(4 \\times 3^4 x^3 = 324 x^3\\).\nThese are two long-winded ways of getting to the result. For most people, differentiating power-law functions algebraically is simplified by using the rules of exponentiation rather than the product or chain rule. Here, \\[h(x) \\equiv \\left[3x\\right]^4 = 3^4 x^4\\]so \\(\\partial_x h(x)\\) is easily handled as a scalar (\\(3^4\\)) times a function \\(x^4\\). Consequently, applying the rule for differentiating power laws, \\[\\partial_x h(x) = 3^4 \\times \\partial_x x^4 = 3^4 \\times 4 x^3 = 324 x^3\\] As another example, take \\(h(x) \\equiv \\sqrt[4]{\\strut x^3}\\). This is, of course, the composition \\(f(g(x))\\) where \\(f(y) \\equiv y^{1/4}\\) and \\(g(x) \\equiv x^3\\). Applying the chain rule to find \\(\\partial_x h(x)\\) will work (of course!), but is more work than applying the rules of exponentiation followed by a simple power-law differentiation. \\[h(x) = \\sqrt[4]{\\strut x^3} = x^{3/4}\\ \\ \\text{so}\\ \\  \\partial_x h(x) = \\frac{3}{4} x^{(3/4 - 1)} = \\frac{3}{4} x^{-1/4}\\]",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Derivatives of assembled functions</span>"
    ]
  },
  {
    "objectID": "Differentiation/23-rules.html#exponentials-and-logarithms-optional",
    "href": "Differentiation/23-rules.html#exponentials-and-logarithms-optional",
    "title": "23  Derivatives of assembled functions",
    "section": "23.7 Exponentials and logarithms (optional)",
    "text": "23.7 Exponentials and logarithms (optional)\nThe natural logarithm function, \\(\\ln(x)\\), is one of our basic modeling functions. As you know, there are other logarithmic functions. The one most often used is the logarithm-base-10, written \\(\\log_{10}(x)\\) or log10(x). Ten is an integer, and a nice number to use in arithmetic. So in practice, it is sensible to use \\(\\log_{10}()\\). (Indeed, \\(\\log_{10}()\\) is the digit() function, introduced in ?sec-magnitudes).\nThe “natural” in the “natural logarithm” means something different.\nThe base of the natural logarithm is the number called Euler’s constant and written \\(e\\). As a celebrity number, \\(e\\) is right up there with \\(\\pi\\) and \\(i\\). Just as \\(\\pi\\) has a decimal expansion that is infinitely long—the familiar \\(\\pi = 3.14159265358979...\\)—Euler’s constant has an infinitely long decimal representation: \\(e = 2.71828182845905...\\)\nIt is not obvious why \\(e = 2.71828182845905...\\) should be called “natural” by mathematicians. The reasons are:\n\n\\(\\ln(x)\\) is the inverse of \\(e^x\\), which is special for being invariant under differentiation: \\(\\partial_x e^x = e^x\\).\nThe derivative \\(\\partial_x \\ln(x)\\) which has a particularly simple form, namely, \\(1/x\\).\n\nLet’s look at the log-base-10 and its computer-savvy cousin log-base-2. The very definition of logarithms means that both 10 and 2 can be written \\[10 = e^{\\ln(10)}\\ \\ \\ \\text{and}\\ \\ \\ 2 = e^{\\ln(2)}\\] This implies that the base-10 and base-2 exponential functions can be written in terms of Euler’s constant \\(e\\):\n\\[10^x = \\left[\\strut e^{\\strut\\ln(10)}\\right]^x = e^{\\ln(10)x} \\ \\ \\ \\text{and}\\ \\ \\ 2^x = \\left[\\strut e^{\\strut\\ln(2)}\\right]^x = e^{\\ln(2) x}\\] Calculating \\(\\partial_x 10^x\\) or \\(\\partial_x 2^x\\) is a matter of applying the chain rule:\n\\[\\partial_x [10^x] = \\partial_x [e^{\\ln(10)x}] = e^{\\ln(10)x} \\times \\ln(10) \\ =\\  10^x \\times 2.3026\\] and \\[\\partial_x [2^x] = \\partial_x [e^{\\ln(2)x}] = e^{\\ln(2)x} \\times \\ln(2) \\ = \\ 2^x \\times 0.6931\\] Like \\(e^x\\), the derivatives of \\(10^x\\) and \\(2^x\\) are proportional to themselves. For \\(e^x\\) the constant of proportionality is 1, a very natural number indeed.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Derivatives of assembled functions</span>"
    ]
  },
  {
    "objectID": "Differentiation/24-optim.html",
    "href": "Differentiation/24-optim.html",
    "title": "24  Optimization",
    "section": "",
    "text": "24.1 Structure of the problem\nIn an optimization problem, there is one or more input quantities whose value you have to choose. The amount of salt; the years to wait from planting to harvesting a tree; the angle of the trail with respect to the slope. We will call this the decision quantity.\nSimilarly, there is one or more output quantity that you value and want to make as good as possible. The taste of the stew; the amount of usable wood harvested; the time it takes to walk up the hill. The output quantities are called the objectives.\nThis chapter deals with optimization problems that involve only a single objective. Problems with multiple objectives are among the most interesting and important in real-world decision making. Single-objective optimization techniques are a component of the more complex decision making, but they are a good place to get started.\nThe model that relates inputs to the objective output is the objective function. Solving an optimization problem—once the modeling phase is complete—amounts to finding a value for the decision quantity (the input to the objective function) that produces the best output from the objective function.\nSometimes the objective is something that you want to minimize, make as small as possible. For instance, in the hiking trail problem, we seek to minimize the time it takes to walk up the trail. Sometimes you want to maximize the objective, as in the wood-harvest problem where the objective is to harvest the most wood per year.\nRecall from Section 6.7 that there are two components to the task of maximization or minimization. The argmax is the input to the objective function which produces the largest output. The maximum is the value of that output.1 Argmin and minimum are the words used in a situation where you seek the smallest value of the objective function.\nOnce you have found the argmax you can plug that value into the objective function to find the output value. That value is the maximum.\nTo illustrate the setup of an optimization problem, imagine yourself in the situation of a contest to see who can shoot a tennis ball the farthest into a field with a slingshot. During the contest, you will adjust the vertical angle of launch, place the ball into the slingshot’s cradle, pull back as far as possible, and let go. To win the contest, you need to optimize how you launch the ball.\nThe objective is to maximize the distance traveled by the ball. The objective function models the distance travelled as a function of the quantities you can control, for instance the vertical angle of launch or the amount by which you pull back the slingshot. For simplicity, we will imagine that the slingshot is pulled back by a standard amount, producing a velocity of the ball at release of \\(v_0\\). You will win or lose based on the angle of launch you choose.\nBefore you head out into the field to experiment, let’s prepare by constructing the objective function. Using some principles of physics and mathematics (which you may not yet understand), we will model how far the ball will travel (horizontally) as a function of the angle of launch \\(\\theta\\) and the initial velocity \\(v_0\\).\nThe mathematics of such problems involves an area called differential equations, an important part of calculus which we will come to later in the course. Since you don’t have the tools yet, we will just state a simple model of how long the ball stays in the air. \\[\\text{duration}(v_0, \\theta) = 2 v_0 \\sin(\\theta)/g\\] \\(g\\) is the acceleration due to gravity, which is about \\(9.8 \\text{m}\\text{s}^{-2}\\), assuming that the contest is being held on Earth.\nThe horizontal distance travelled by the tennis ball will be \\[\\text{hdist}(v_0, \\theta) = \\cos(\\theta) v_0\\, \\text{duration}(v_0, \\theta) = 2 v_0^2 \\cos(\\theta)\\sin(\\theta) / g\\] Our objective function is hdist(), and we seek to find the argmax. The input \\(v_0\\) is (we have assumed) fixed, so the only decision quantity is the angle \\(\\theta\\).\nThe best choice of \\(\\theta\\) will make the quantity \\(\\cos(\\theta)\\sin(\\theta)\\) as large as possible. So in finding the argmax, we don’t need to be concerned with \\(v_0\\) or \\(g\\).\nFinding the argmax can be accomplished simply by plotting the function \\(\\cos(\\theta)\\sin(\\theta)\\). We will implement the function so that the input is in units of degrees.\nFigure 24.1: In the simple model of a tennis ball launched at an angle \\(\\theta\\) from the horizontal, the distance travelled is \\(2 v_0^2 / g\\) times \\(\\cos(\\theta)\\sin(\\theta)\\).\nYou can see that the maximum value is about 0.5 and that this occurs at an argmax \\(\\theta\\) that is a little bit less than 50\\(^\\circ\\).\nZooming in on the \\(\\theta\\) axis let’s you find the argmax with more precision:\nFigure 24.2: Zooming in on the argmax of the objective function. It is important to look at the scale of the vertical axis. Any value of \\(\\theta\\) between about 40 and 50 gives a close approximation to the maximum.\nFrom the graph, especially the zoomed-in version, you can read off the argmax as \\(\\theta = 45^\\circ\\).\nFinding the argmax solves the problem. You may also want to present your solution by reporting the value of hdist() when the argmax is given as input. You can read off the graph that the maximum of \\(\\cos(\\theta)\\sin(\\theta)\\) is 0.5 at \\(\\theta = 45^\\circ\\), so overall the distance will be \\(v_0^2 / g\\)",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "Differentiation/24-optim.html#structure-of-the-problem",
    "href": "Differentiation/24-optim.html#structure-of-the-problem",
    "title": "24  Optimization",
    "section": "",
    "text": "Mathematically, maximization and minimization are the same thing. Every minimization problem can be turned into a maximization problem by putting a negative sign in front of the objective function. To simplify the discussion, in talking about finding the solution to an optimization problem we will imagine that the goal is to maximize. But keep in mind that many circumstances in the real world, “best” can mean minimization.\n\n\n\nPeople often talk about “finding the maximum.” This is misleading. Instead, the idea is to find the input to the objective function—that is, the argmax—that produces the maximum output.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "Differentiation/24-optim.html#interpreting-the-argmax",
    "href": "Differentiation/24-optim.html#interpreting-the-argmax",
    "title": "24  Optimization",
    "section": "24.2 Interpreting the argmax",
    "text": "24.2 Interpreting the argmax\nThe graphical solution given to the slingshot problem is entirely satisfactory. Whether that solution will win the contest depends on whether the model we built for the objective function is correct. We have left out, for instance, air resistence, which is potentially important.\nSolving the optimization problem has prepared us to test the result in the field. Perhaps we will find that the real-world optimum angle is somewhat steeper or shallower than \\(\\theta = 45^\\circ\\).\nBesides the argmax, another important quantity to read from the graph in Figure 24.1 is the precision of the argmax. In strict mathematical terms, the argmax for the tennis-ball problem is exactly 45 degrees at which point \\(\\cos(\\theta)\\sin(\\theta) = 0.5\\). Suppose, however, that the ball were launched at only 40 degrees. Five degrees difference is apparent to the eye, but the result will be essentially the same as for 45 degrees: \\(\\cos(\\theta)\\sin(\\theta) = 0.492\\). The same is true for a launch angle of 50 degrees. For both “sub-optimal” launch angles, the output is within 2 percent of the 45-degree result. It is easy to imagine that a factor outside the scope of the simple model—the wind, for instance—could change the result by as much or more than 2 percent, so a practical report of the argmax should reasonable be “40 to 50 degrees” rather than “exactly 45 degrees.”\nContests are won or lost by margins of less than 1%, so you should not casually deviate from the argmax. On the other hand, \\(45^\\circ\\) is the argmax of the model. Reality may deviate from the model. For instance, suppose that air resistance or wind might might affect distance by 1%. That is. the real-world result might deviate by as much as 1% of the model value. If so, we shouldn’t expect the real-world argmax to be any closer to 45\\(^\\circ\\) than \\(\\pm 5^\\circ\\); anywhere in that domain interval generates an output that is within 1% of the maximum output for the model.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "Differentiation/24-optim.html#derivatives-and-optimization",
    "href": "Differentiation/24-optim.html#derivatives-and-optimization",
    "title": "24  Optimization",
    "section": "24.3 Derivatives and optimization",
    "text": "24.3 Derivatives and optimization\nWe are now going to reframe the search for the argmax and its interpretation in terms of derivatives of the objective function with respect to the decision quantity (\\(\\theta\\) in the slingshot problem). For a function with one input, this will not be an improvement from the look-at-the-graph technique to find the argmax. However, a genuine reason to use derivatives is to set us up in the future to solve problems with more than one input, where it is hard to draw or interpret a graph. Also, describing functions in the language of derivatives can help us think more clearly about aspects of the problem, such as the precision of the argmax.\nWith a graph such as Figure 24.1, it is easy to find the argmax; common sense carries the day. So it won’t be obvious at first why we will take the following approach:\nLet’s denote an argmax of the objective function \\(f(x)\\) by \\(x^\\star\\). Let’s look at the derivative \\(\\partial_x f(x)\\) in the neighborhood of \\(x^\\star\\). Referring to Figure 24.1, where \\(x^\\star = 45^\\circ\\), you may be able to see that \\(\\partial_x f(x^\\star)\\) is zero; the line tangent to the function’s graph at \\(x^\\star\\) is horizontal.\nSeen another way, the slope of \\(f(x)\\) to the left of \\(x^\\star\\) is positive. Moving a tiny bit to the right (that is, increasing \\(x\\) by a very small amount) increases the output \\(f(x)\\). On the other hand, just to the right of \\(x^\\star\\), the slope of \\(f(x)\\) is negative; as you reach the top of a hill and continue on, you will be going downhill. So the derivative function is positive on one side of \\(x^\\star\\) and negative on the other, suggesting that it crosses zero at the argmax.\nCommon sense is correct: Walk uphill to get to the peak, walk downhill to move away from the peak. At the top of a smooth hill, the terrain is level. (Since our modeling functions are smooth, so must be the hills that we visualize the functions with.)\nInputs \\(x^\\star\\) such that \\(\\partial_x f(x^\\star) = 0\\) are called critical points. Why not call them simply argmaxes? Because a the slope will also be zero at an argmin. And it is even possible to have the slope be zero at a point that is neither an argmin or an argmax.\nAt this point, we know that values \\(x^\\star\\) that give \\(\\partial_x f(x^\\star) = 0\\) are “critical points,” but we haven’t said how to figure out whether a given critical point is an argmax, an argmin, or neither. This is where the behavior of \\(\\partial_x f(x)\\) near \\(x=x^\\star\\) is important. If \\(x^\\star\\) is an argmax, then \\(\\partial_x f(x)\\) will be positive to the left of \\(x^\\star\\) and negative to the right of \\(x^\\star\\); walk up the hill to get to \\(x^\\star\\), at the top the hill is flat, and just past the top the hill has a negative slope.\nFor an argmin, changing \\(x\\) from less than \\(x^\\star\\) to greater than \\(x^\\star\\); you will be walking down into the valley, then level at the very bottom \\(x=x^\\star\\), then back up the other side of the valley after you pass \\(x=x^\\star\\). Figure 24.3 shows the situation.\n\n\n\n\n\n\n\n\nFigure 24.3: Top row: An objective function near an argmax (left) and an argmin (right). Bottom row: The derivative of the objective function. A horizontal line (orange) marks zero on the vertical axis.\n\n\n\n\n\nThe bottom row of graphs in Figure 24.3 shows the derivative of the objective function \\(f(x)\\), that is, \\(\\partial_x f(x)\\). You can see that for the argmax of \\(f(x)\\), the derivative \\(\\partial_x f(x)\\) is positive to the left and negative to the right. Similarly, near the argmin of \\(f(x)\\), the derivative \\(\\partial_x f(x)\\) is negative to the left and positive to the right.\nStated another way, the derivative \\(\\partial_x f(x)\\) has a negative slope just to the left of an argmin and a positive slope to the left of an argmax.\nThe second derivative of the objective function \\(f(x)\\) at a critical point \\(x^\\star\\) is what tells us whether the critical point is an argmax, an argmin, or neither.\n\n\n\n\n\n\n\n\n\n\nCritical point \\(x^\\star\\)\n\\(\\partial_x f(x^\\star)\\)\n\\(\\partial_{xx} f(x^\\star)\\)\n\n\n\n\nargmax\n0\nnegative\n\n\nargmin\n0\npositive\n\n\nneither\n0\n0\n\n\n\n\nThroughout Block 2, we have translated features of functions that are evident on a graph into the language of derivatives:\n\nThe slope of a function \\(f(x)\\) at any input \\(x\\) is the value of the derivative function \\(\\partial_x f(x)\\) at that same \\(x\\).\nThe concavity of a function \\(f(x)\\) at any input is the slope of the derivative function, that is, \\(\\partial_{xx} f(x)\\).\nPutting (i) and (ii) together, we get that the concavity of a function \\(f(x)\\) at any input \\(x\\) is the value of the second derivative function, that is, \\(\\partial_{xx} f(x)\\).\nAt an argmax \\(x^\\star\\) of \\(f(x)\\), the value of the derivative function \\(\\partial_x f(x^\\star)\\) is zero and the value of the second derivative function \\(\\partial_{xx} f(x^\\star)\\) is negative. The situation at an argmin is along the same lines, the derivative of the objective function is zero and the second derivative is positive.\n\n\n\nWhat’s the critical point?\nYou’re familiar with the quadratic polynomial: \\[g(x) = a_0 + a_1 x + a_2 x^2\\] The graph of a quadratic polynomial is a parabola, which might be concave up or concave down. A parabola has only one critical point, which might be an argmin or an argmax.\nLet’s find the critical point. We know that the critical point is \\(x^\\star\\) such that \\(\\partial_x g(x^\\star) = 0\\). Since we know how to differentiate a power law, we can see that \\[\\partial_x g(x) = a_1 + 2 a_2 x\\] and, more specifically, at the critical point \\(x^\\star\\) the derivative will be \\[a_1 + 2 a_2 x^\\star = 0\\] The above is an equation, not a definition. It says that whatever \\(x^\\star\\) happens to be, the quantity \\(a_1 + 2 a_2 x^\\star\\) must be zero. Using plain old algebra, we can find the location of the critical point \\[x^\\star = -\\frac{a_1}{2 a_2}\\]\n\n\nApplication area 24.1 —In economics, demand for a good is a function of the good’s price (and other things). This function is called the “demand curve.”\n\n\n\n\n\n\n\nApplication area 24.1 The demand “curve”\n\n\n\nIn economics, a monopoly or similar arrangement can set the price for a good or commodity. Monopolists can set the price at a level that generates the most income for themselves.\n\n\n\n\n\n\n\n\nFigure 24.4: Demand as a function of price, as first published by Antoine-Augustin Cournot in 1836. Source)\n\n\n\n\n\nIn 1836, early economist Antoine-Augustin Cournot published a theory of revenue versus demand based on his conception that demand will be a monotonically decreasing function of price. (That is, higher price means lower demand.) we will write as \\(\\text{Demand}(p)\\) demand as a function of price.\nThe revenue generated at price \\(p\\) is \\(R(p) \\equiv p \\text{Demand}(p)\\): price times demand.\nTo find the revenue-maximizing demand, differentiate \\(R(p)\\) with respect to \\(p\\) and find the argmax \\(p^\\star\\) at with \\(\\partial_p R(p^\\star) = 0).\\) This can be done with the product rule.\n\\[\\partial_p R(p) = p \\ \\partial_p \\text{Demand}(p) + \\text{Demand}(p)\\] At the argmax \\(p^\\star\\) we have: \\[p^\\star \\partial_p \\text{Demand}(p^\\star) + \\text{Demand}(p^\\star) = 0 \\ \\ \\stackrel{\\text{solving for}\\ p^\\star}{\\Longrightarrow} \\ \\ p^\\star = - \\frac{\\text{Demand}(p^\\star)}{\\partial_p \\text{Demand}(p^\\star)}\\]\nIf the monopolist knows the demand function \\(D(p)\\), finding the revenue maximizing price is a simple matter. But in general, the monopolist does not know the demand function in advance. Instead, an informed guess is made to set the initial price \\(p_0\\). Measuring sales \\(D(p_0)\\) gives one point on the demand curve. Then, try another price \\(p_1\\). This gives another point on the demand curve as well as an estimate \\[\\partial_p D(p_0) = \\frac{D(p_1) - D(p_0)}{p_1 - p_0}\\] Now the monopolist is set to model the demand curve as a straight-line function and easily to find \\(p^\\star\\) for the model. For instance, if the demand function is modeled as \\(D_1 (p) = a + b p\\), the optimal price will be \\(p^\\star_1 = - \\frac{a + b p^\\star}{b}\\) which can be solved as \\(p^\\star_1 = - a/2b\\).\n\\(p^\\star_1\\) is just an estimate of the optimum price. Still, the monopolist can try out that price, giving a third data point for the demand function. The new data can lead to a better model of the demand function. With the better estimate, find a new a argmax \\(p^\\star_2\\). This sort of iterative process for finding an argmax of a real-world function is very common in practice.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "Differentiation/24-optim.html#sec-flat-on-top",
    "href": "Differentiation/24-optim.html#sec-flat-on-top",
    "title": "24  Optimization",
    "section": "24.4 Be practical!",
    "text": "24.4 Be practical!\nDecision making is about choosing among alternatives. In some engineering or policy contexts, this can mean finding a value for an input that will produce the “best” outcome. For those who have studied calculus, it is natural to believe that calculus-based techniques for optimization are the route to making the decision.\nWe emphasize that the optimization techniques covered in this chapter are only part of a broader set of techniques for real-world decision-making problems. In particular, most policy contexts involve multiple objectives. For example, in designing a car one goal is to make it cheap to manufacture, another to make it attractive, and still another to make it safe. These different objectives are often at odds with one another. In Block 4 of this text, we will discuss some calculus techniques that help policy-makers in multi-objective settings.\nFor now, sticking with the idealized (and often unrealistic) setting of maximizing a single objective, with one or more inputs. Recall the setting for calculus-type maximization. You have a function with one or more inputs, say, \\(f(x)\\) or \\(g(x,y)\\) or, often, \\(h(x, y, z, \\ldots)\\) where \\(\\ldots\\) might be standing for tens or hundreds or thousands of inputs or more.\nIf you can graph the function (feasible for one- or two-input functions), you can often easily scan the graph by eye to find the peak. The basis of the calculus techniques is the observation that, at the argmax of a smooth function, the derivative of the function is 0.\nFor example, consider a style problem that often appears in calculus textbooks. Suppose you have been tasked to design a container for a large volume V of liquid. The design specifications call for the weight of the container to be as little as possible. (This is a minimization problem, then.) In classical textbook fashion, the specifications might also stipulate that the container is to be a cylinder made out of a particular metal of a particular thickness.\nThe above is a lovely geometry/calculus problem. Whether it is relevant to any genuine, real-world problem is another question.\n\n\n\n\n\n\n\n\n\nUsing the notation in the diagram, the volume and surface area of the cylinder is \\[V(r, h) \\equiv \\pi r^2 h \\ \\ \\ \\text{and}\\ \\ \\ A(r, h) \\equiv 2 \\pi r^2 + 2 \\pi r h\\]\nMinimizing the weight of the cylinder is our objective (according to the problem statement) and the weight is proportional to the surface area. Since the volume \\(V\\) is given (according to the problem statement), we want to re-write the area function to use volume:\n\\[h(r, V) \\equiv V / \\pi r^2 \\ \\ \\ \\implies\\ \\ \\ A(r, V) = 2 \\pi r^2 + 2 \\pi r V/\\pi r^2 = 2 \\pi r^2 + 2 V / r\\] Suppose \\(V\\) is specified as 1000 liters. A good first step is to choose appropriate units for \\(r\\) to make sure the formula for \\(A(r, V)\\) is dimensionally consistent. Suppose we choose \\(r\\) in cm. Then we want \\(V\\) in cubic centimeters (cc). 1000 liters is 1,000,000 cc. Now we can plot a slice of the area function:\n\nA &lt;- makeFun(2*pi*r^2 + 2*V/r ~ r, V=1000000)\nslice_plot(A(r) ~ r, bounds(r=c(10, 100))) %&gt;%\n  gf_labs(x = \"radius (cm)\", y = \"Surface area of container (square cm)\")\n\n\n\n\n\n\n\n\nAs always, the function’s derivative is zero at the optimal \\(r\\). In the graph, the argmin is near \\(r=50\\) cm at which point the minimum is about 50,000 cm\\(^2\\). Since \\(h(r,V) = V/\\pi r^2\\), the required height of cylinder will be near \\(10^6 / \\pi 50^2 = 127\\)cm.\nIn calculus courses, the goal is often to find a formula for the optimal radius as a function of \\(V\\). So we differentiate the objective function—that is, the area function for any \\(V\\) and \\(r\\) with respect to \\(r\\), \\[\\partial_r A(r, V) = 4 \\pi r - 2 V / r^2\\] Setting this to zero (which will be true at the optimal \\(r^\\star\\)) we can solve for \\(r^\\star\\) in terms of \\(V\\): \\[4 \\pi r^\\star - 2 \\frac{V}{\\left[r^\\star\\right]^2} = 0 \\ \\ \\ \\Longrightarrow\\ \\ \\ 4\\pi r^\\star = 2\\frac{V}{\\left[r^\\star\\right]^2} \\Longrightarrow\\ \\ \\ \\left[r^\\star\\right]^3 = \\frac{1}{2\\pi} V \\ \\ \\ \\Longrightarrow\\ \\ \\  r^\\star = \\sqrt[3]{V/2\\pi}\\]\nFor \\(V = 1,000,000\\) cm\\(^3\\), this gives \\(r^\\star = 54.1926\\) cm which in turn implies that the corresponding height \\(h^\\star = V/\\pi (r^\\star)^2 = 108.3852\\) cm.\nWe’ve presented the optimum \\(r^\\star\\) and \\(h^\\star\\) to the nearest micron. (Does that make sense to you? Think about it for a moment before reading on.)\nIn modeling, a good rule of thumb is this: “If you don’t know what a sensible precision is for reporting your result, you don’t yet have a complete grasp of the problem.” Here are two reasonable ways to sort out a suitable precision.\n\nSolve a closely related problem that would have been equivalent for many practical purposes.\nCalculate how much the input can deviate from the argmax while producing a trivial change in the output of the objective function.\n\nApproach (2) is always at hand, since you already know the objective function. Let’s graph the objective function near \\(r = 54.1926\\) …\n\n\n\n\n\n\n\n\n\nLook carefully at the axes scales. Deviating from the mathematical optimum by about 5cm (that is, 50,000 microns) produces a change in the output of the objective function by about 400 units out of 55,000. In other words, about 0.7%.\nIt is true that \\(r^\\star = 54.1926\\) cm gives the “best” outcome. And sometimes such precision is warranted. For example, improving the speed of an elite marathon racer by even 0.1% would give her a 7 second advantage: often the difference between silver and gold!\nWhat’s different is that you know exactly the ultimate objective of a marathon: finish faster. But you may not know the ultimate objective of the system your “optimal” tank will be a part of. For instance, your tank may be part of an external fuel pod on an aircraft. Certainly the aircraft designers want the tank to be as light as possible. But they also want to reduce drag as much as possible. A 54 cm diameter tube has about 17% more drag than a 50 cm tube. It is probably well worth increasing weight by 0.7% to save that much drag.\nIn reporting the results from an optimization problem, you ought to give the decision maker all relevant information. That might be as simple as including the above graph in your report.\nWe mentioned another technique for getting a handle on what precision is meaningful: (1) solve a closely related problem. It can requires some insight and creativity to frame the new problem. For instance, large capacity tanks often are shaped like a lozenge: a cylinder with hemi-spherical ends.\n\n\n\n\n\n\n\n\n\nUsing \\(h\\) for the length of the cylindrical portion of the tank, and \\(r\\) for the radius, the volume and surface area are: \\[V(r, h) = \\pi r^2 h + \\frac{4}{3} \\pi r^3 \\ \\ \\ \\text{and}\\ \\ \\ A(r,h) = 2 \\pi r h + 4 \\pi r^2\\] Again, \\(V\\) is specified as 1000 liters. As detailed in Exercise 24.18, the surface area of this 1000-liter tank is about 48,400 cm\\(^2\\). This is more than 10% less than for the cylindrical tank.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "Differentiation/24-optim.html#footnotes",
    "href": "Differentiation/24-optim.html#footnotes",
    "title": "24  Optimization",
    "section": "",
    "text": "Another word for an “input” is “argument.” Argmax is the contraction of argument producing the maximum output.↩︎",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "Differentiation/25-partial.html",
    "href": "Differentiation/25-partial.html",
    "title": "25  Partial change and the gradient vector",
    "section": "",
    "text": "25.1 Calculus on two inputs\nAlthough we use contour plots for good practical reasons, the graph of a function \\(g(x,y)\\) with two inputs is a surface, as described in Section @ref(surface-plot). The derivative of \\(g(x,y)\\) should encode the information needed to approximate the surface at any input \\((x,y)\\). In particular, we want the derivative of \\(g(x,y)\\) to tell us the orientation of the tangent plane to the surface.\nA tangent plane is infinite in extent. Let’s use the word facet to refer to a little patch of the tangent plane centered at the point of contact. Each facet is flat. (it is part of a plane!) Figure 25.2 shows some facets tangent to a familiar curved surface. No two of the facets are oriented the same way.\nFigure 25.2: A melon as a model of a curved surface such as the graph of a function of two inputs. Each tangent facet has its own orientation. (Disregard the slight curvature of the small pieces of paper. Summer humidity has interfered with my attempt to model a flat facet with a piece of Post-It paper!\nBetter than a picture of a summer melon, pick up a hardcover book and place it on a curved surface such as a basketball. The book cover is a flat surface: a facet. The orientation of the cover will match the orientation of the surface at the point of tangency. Change the orientation of the cover and you will find that the point of tangency will change correspondingly.\nIf melons and basketballs are not your style, you can play the same game on an interactive graph of a function with two inputs. The snapshot below is a link to an applet that shows the graph of a function as a blue surface. You can specify a point on the surface by setting the value of the (x, y) input using the sliders. Display the tangent plane (which will be green) at that point by check-marking the “Tangent plane” input. (Acknowledgments to Alfredo Sánchez Alberca who wrote the applet using the GeoGebra math visualization system.)\nFor the purposes of computation by eye, a contour graph of a surface can be easier to deal with. Figure 25.3 shows the contour graph of a smoothly varying function. Three points have been labeled A, B, and C.\nFigure 25.3: A function of 2 inputs with 3 specific inputs marked A, B, and C\nZooming in on each of the marked points presents a simpler picture for each of them, although one that is different for each point. Each zoomed-in plot contains almost parallel, almost evenly spaced contours. If the surface had been exactly planar over the entire zoomed-in domain, the contours would be exactly parallel and exactly evenly spaced. We can approach such exact parallelness by zooming in more closely around the labeled point.\nFigure 25.4: Zooming in on the neighborhoods of A, B, and C in Figure 25.3 shows a simple, almost planar, local landscape. The bottom row shows the contours of the tangent plane near each of the neighborhoos in the top row.\nJust as the function \\(\\line(x) \\equiv a x + b\\) describes a straight line, the function \\(\\text{plane}(x, y) \\equiv a + b x + c y\\) describes a plane whose orientation is specified by the value of the parameters \\(b\\) and \\(c\\). (Parameter \\(a\\) is about the vertical location of the plane, not it is orientation.)\nIn the bottom row of Figure 25.4, the facets tangent to the original surface at A, B, and C are displayed. Comparing the top and bottom rows of Figure 25.4) you can see that each facet has the same orientation as the surface; the contours face in the same way.\nRemember that the point of constructing such facets is to generalize the idea of a derivative from a function of one input \\(f(x)\\) to functions of two or more inputs such as \\(g(x,y)\\). Just as the derivative \\(\\partial_x f(x_0)\\) reflects the slope of the line tangent to the graph of \\(f(x)\\) at \\(x=x_0\\), our plan for the “derivative” of \\(g(x_0,y_0)\\) is to represent the orientation of the facet tangent to the graph of \\(g(x,y)\\) at \\((x=x_0, y=y_0)\\). The question for us now is what information is needed to specify an orientation.\nOne clue comes from the formula for a function whose graph is a plane oriented in a particular direction:\n\\[\\text{plane}(x,y) \\equiv a + b x + cy\\]\nAn instructive experience is to pick up a rigid, flat object, for instance a smartphone or hardcover book. Hold the object level with pinched fingers at the mid-point of each of the short ends, as shown in Figure 25.5 (left).\nYou can tip the object in one direction by raising or lowering one hand. (middle picture) And you can tip the object in the other coordinate direction by rotating the object around the line joining the points grasped by the left and right hands. (right picture) By combining these two motions, you can orient the surface of the object in a wide range of directions.1\nThe purpose of this lesson is to show that two-numbers are sufficient to dictate the orientation of a plane. In terms of Figure 25.5 these are 1) the amount that one hand is raised relative to the other and 2) the angle of rotation around the hand-to-hand axis.\nSimilarly, in the formula for a plane, the orientation is set by two numbers, \\(b\\) and \\(c\\) in \\(\\text{plane}(x, y) \\equiv a + b x + c y\\).\nHow do we find the right \\(b\\) and \\(c\\) for the tangent facet to a function \\(g(x,y)\\) at a specific input \\((x_0, y_0)\\)? Taking slices of \\(g(x,y)\\) provides the answer. In particular, these two slices: \\[\\text{slice}_1(x) \\equiv g(x, y_0) = a + b\\, x + c\\, y_0 \\\\ \\text{slice}_2(y) \\equiv g(x_0, y) = a + b x_0 + c\\, y\\]\nLook carefully at the formulas for the slices. In \\(\\text{slice}_1(x)\\), the value of \\(y\\) is being held constant at \\(y=y_0\\). Similarly, in \\(\\text{slice}_2(y)\\) the value of \\(x\\) is held constant at \\(x=x_0\\).\nThe parameters \\(b\\) and \\(c\\) can be read out from the derivatives of the respective slices: \\(b\\) is equal to the derivative of the slice\\(_1\\) function with respect to \\(x\\) evaluated at \\(x=x_0\\), while \\(c\\) is the derivative of the slice\\(_2\\) function with respect to \\(y\\) evaluated at \\(y=y_0\\). Or, in the more compact mathematical notation:\n\\[b = \\partial_x \\text{slice}_1(x)\\left.\\strut\\right|_{x=x_0} \\ \\ \\text{and}\\ \\ c=\\partial_y \\text{slice}_2(y)\\left.\\strut\\right|_{y=y_0}\\] These derivatives of slice functions are called partial derivatives. The word “partial” refers to examining just one input at a time. In the above formulas, the \\({\\large |}_{x=x_0}\\) means to evaluate the derivative at \\(x=x_0\\) and \\({\\large |}_{y=y_0}\\) means something similar.\nYou don’t need to create the slices explicitly to calculate the partial derivatives. Simply differentiate \\(g(x, y)\\) with respect to \\(x\\) to get parameter \\(b\\) and differentiate \\(g(x, y)\\) with respect to \\(y\\) to get parameter \\(c\\). To demonstrate, we will make use of the sum rule: \\[\\partial_x g(x, y) = \\underbrace{\\partial_x a}_{=0} + \\underbrace{\\partial_x b x}_{=b} + \\underbrace{\\partial_x cy}_{=0} = b\\] Similarly, \\[\\partial_y g(x, y) = \\underbrace{\\partial_y a}_{=0} + \\underbrace{\\partial_y b x}_{=0} + \\underbrace{\\partial_y cy}_{=c} = c\\]",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Partial change and the gradient vector</span>"
    ]
  },
  {
    "objectID": "Differentiation/25-partial.html#calculus-on-two-inputs",
    "href": "Differentiation/25-partial.html#calculus-on-two-inputs",
    "title": "25  Partial change and the gradient vector",
    "section": "",
    "text": "R/mosaic: Orientation of a plane\n\n\n\nTo explore the roles of the parameters \\(b\\) and \\(c\\) in setting the orientation of the line, open an R/mosaic session. The R/mosaic code below generates a particular instance of \\(\\text{plane}(x,y)\\) and plots it in two ways: a contour plot and a surface plot. Change the numerical values of \\(b\\) and \\(c\\) and observe how the orientation of the planar surface changes in the graphs. You can also see that the value of \\(a\\) is irrelevant to the orientation of the plane, just as the intercept of a straight-line graph is irrelevant to the slope of that line.\n\nplane &lt;- makeFun(a + b*x + c*y ~ x + y, a = 1, b = -2.5, c = 1.6)\ncontour_plot(plane(x, y) ~ x + y, \n             bounds(x=c(-2, 2), y=c(-2, 2))) %&gt;%\n  gf_refine(coord_fixed())\n\n\n\n\n\n\n\n\n\ninteractive_plot(plane(x, y) ~ x + y, \n                 bounds(x=c(-2, 2), y=c(-2, 2)))\n\n\n## Loading required namespace: plotly\n\n\n\n\n\nAs always it can be difficult to extract quantitative information from a surface plot. For the example here, you can see that the high-point on the surface is when \\(x\\) is most negative and \\(y\\) is most positive. Compare that to the contour plot to verify that two modes are displaying the same surface.\n\n\nNote: The gf_refine(coord_fixed()) part of the contour-plot command makes numerical intervals on the horizontal and vertical axes have the same length.)\n\n\n\n\n\n\n\n\n\n\n\n(a) A level surface\n\n\n\n\n\n\n\n\n\n\n\n(b) Rotated along the axis running top to bottom\n\n\n\n\n\n\n\n\n\n\n\n(c) Rotated along the axis running left to right\n\n\n\n\n\n\n\nFigure 25.5: Combining two simple movements can tip a plane to all sorts of different orientations.\n\n\n\n\n\n\n\n\n\n\n\n\nGet in the habit of noticing the subscript on the differentiation symbol \\(\\partial\\). When taking, for instance, \\(\\partial_y f(x,y,z, \\ldots)\\), all inputs other than \\(y\\) are to be held constant. Some examples:\n\\[\\partial_y 3 x^2 = 0\\ \\ \\text{but}\\ \\ \\\n\\partial_x 3 x^2 = 6x\\\\\n\\ \\\\\n\\partial_y 2 x^2 y = 2x^2\\ \\ \\text{but}\\ \\ \\\n\\partial_x 2 x^2 y = 4 x y\n\\]",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Partial change and the gradient vector</span>"
    ]
  },
  {
    "objectID": "Differentiation/25-partial.html#all-other-things-being-equal",
    "href": "Differentiation/25-partial.html#all-other-things-being-equal",
    "title": "25  Partial change and the gradient vector",
    "section": "25.2 All other things being equal …",
    "text": "25.2 All other things being equal …\nRecall that the derivative of a function with one input, say, \\(\\partial_x f(x)\\) tells you, at each possible value of the input \\(x\\), how much the output will change proportional to a small change in the value of the input.\nNow that we are in the domain of multiple inputs, writing \\(h\\) to stand for “a small change” is not entirely adequate. Instead, we will write \\(dx\\) for a small change in the \\(x\\) input and \\(dy\\) for a small change in the \\(y\\) input.\nWith this notation, we write the first-order polynomial approximation to a function of a single input \\(x\\) as \\[f(x+dx) = f(x) + \\partial_x f(x) \\times dx\\] Applying this notation to functions of two inputs, we have: \\[g(x + \\color{magenta}{dx}, y) = g(x,y) + \\color{magenta}{\\partial_x} g(x,y) \\times \\color{magenta}{dx}\\] and \\[g(x, y+\\color{brown}{dy}) = g(x,y) + \\color{brown}{\\partial_y} g(x,y) \\times \\color{brown}{dy}\\]\nEach of these statements is about changing one input while holding the other input(s) constant. Or, as the more familiar expression goes, “The effect of changing one input all other things being equal or all other things held constant.2\nEverything we’ve said about differentiation rules applies not just to functions of one input, \\(f(x)\\), but to functions with two or more inputs, \\(g(x,y)\\), \\(h(x,y,z)\\) and so on.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Partial change and the gradient vector</span>"
    ]
  },
  {
    "objectID": "Differentiation/25-partial.html#gradient-vector",
    "href": "Differentiation/25-partial.html#gradient-vector",
    "title": "25  Partial change and the gradient vector",
    "section": "25.3 Gradient vector",
    "text": "25.3 Gradient vector\nFor functions of two inputs, there are two partial derivatives. For functions of three inputs, there are three partial derivatives. We can, of course, collect the partial derivatives into Cartesian coordinate form. This collection is called the gradient vector.\nJust as our notation for differences (\\(\\cal D\\)) and derivatives (\\(\\partial\\)) involves unusual typography on the letter “D,” the notation for the gradient involves such unusual typography although this time on \\(\\Delta\\), the Greek version of “D.” For the gradient symbol, turn \\(\\Delta\\) on its head: \\(\\nabla\\). That is, \\[\\nabla g(x,y) \\equiv \\left(\\stackrel\\strut\\strut\\partial_x g(x,y), \\ \\ \\partial_y g(x,y)\\right)\\]\nNote that \\(\\nabla g(x,y)\\) is a function of both \\(x\\) and \\(y\\), so in general the gradient vector differs from place to place in the function’s domain.\nThe graphics convention for drawing a gradient vector for a particular input, that is, \\(\\nabla g(x_0, y_0)\\), puts an arrow with its root at \\((x_0, y_0)\\), pointing in direction \\(\\nabla g(x_0, y_0)\\), as in Figure 25.6.\n\n\n\n\n\n\n\n\nFigure 25.6: The gradient vector \\(\\nabla g(x=1,y=2)\\). The vector points in the steepest uphill direction. Consequently, it is perpendicular to the contour passing through its root.\n\n\n\n\n\nA gradient field (see Figure 25.7) is the value of the gradient vector at each point in the function’s domain. Graphically, to prevent over-crowding, the vectors are drawn at discrete points. The lengths of the drawn vectors are set proportional to the numerical length of \\(\\nabla g(x, y)\\), so a short vector means the surface is relatively level, a long vector means the surface is relatively steep.\n\n\n\n\n\n\n\n\nFigure 25.7: A plot of the gradient field \\(\\nabla g(x,y)\\).",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Partial change and the gradient vector</span>"
    ]
  },
  {
    "objectID": "Differentiation/25-partial.html#total-derivative-optional",
    "href": "Differentiation/25-partial.html#total-derivative-optional",
    "title": "25  Partial change and the gradient vector",
    "section": "25.4 Total derivative (optional)",
    "text": "25.4 Total derivative (optional)\nThe name “partial derivative” suggests the existence of some kind of derivative that is not just a part, but the whole thing. The total derivative is such a whole and gratifyingly made up of its parts, that is, the partial derivatives.\nSuppose you are modeling the temperature of some volume of the atmosphere, given as \\(T(t, x, y, z)\\). This merely says that the temperature depends on both time and location, something that is familiar from everyday life.\nThe partial derivatives have an easy interpretation: \\(\\partial_t T()\\) tells how the temperature is changing over time at a given location, perhaps because of the evaporation or condensation of water vapor. \\(\\partial_x T()\\) tells how the temperature changes in the \\(x\\) direction, and so on.\nThe total derivative gives an overall picture of the changes in a parcel of air, which you can thnk of as a tiny balloon-like structure but without the balloon membrane. The temperature inside the “balloon” may change with time (e.g. condensation or evaporation of water), but as the ballon drifts along with the motion of the air (that is, the wind), the evolving location can change the temperature as well. Think of a balloon caught in an updraft: the temperature goes down as the balloon ascends.\nFor an imaginary observer located in the balloon, the temperature is changing with time. Part of this change is the instrinsic change measured by \\(\\partial_t T\\) but we need to add to that the changes induces by the evolving location of the balloon. The partial change in temperature due to a change in altitude is \\(\\partial_z T\\), but it is important to realize that the coordinates of the location are themselves functions of time: \\(x(t), y(t), z(t)\\). Seeing the function \\(T()\\) for the observer in the balloon as a function of \\(t\\), we have \\(T(t, x(t), y(t), z(t))\\). This is a function composition: \\(T()\\) composed with each of \\(x()\\), \\(y()\\), and \\(z()\\). Recall in the chain rule \\(\\partial_v f(g(v)) = \\partial_v f(g(v)) \\partial_v g(v)\\) that the derivative of the composed quantity is the product of two derivatives.\nLikewise, the total derivative of temperature with respect to the observer riding in the balloon will be add together the parts due to changes in time (holding position constant), x-coordinate (holding time and the other space coordinates constant), and the like. Signifying the total differentiation with a capital \\(D\\), we have \\[D\\, T(t) = \\partial_t T() + \\partial_x T() \\cdot\\partial_t x + \\partial_y T()\\cdot \\partial_t y + \\partial_z T() \\cdot\\partial_t z\\] Note that \\(\\partial_t x\\) is the velocity of the balloon in the x-direction, and similarly for the other coordinate directions. Writing these velocities as \\(v_x, v_y, v_z\\), the total derivative for temperature of a parcel of air embedded in a moving atmosphere is\n\\[D\\ T(t) = \\partial_t T + v_x\\, \\partial_x T + v_y\\, \\partial_y T + v_z\\, \\partial_z T\\] Formulations like this, which put the parts of change together into a whole, are often seen in the mathematics of fluid flow as applied in meteorology and oceanology.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Partial change and the gradient vector</span>"
    ]
  },
  {
    "objectID": "Differentiation/25-partial.html#sec-differential-skier",
    "href": "Differentiation/25-partial.html#sec-differential-skier",
    "title": "25  Partial change and the gradient vector",
    "section": "25.5 Differentials",
    "text": "25.5 Differentials\n\nA little bit of this, a little bit of that. — Stevie Wonder, “The Game of Love”\n\nWe have framed calculus in terms of functions: transformations that take one (or more!) quantities as input and return a quantity as output. This was not the original formulation. In this section, we will use the original style to demonstrate how you can sometimes skip the step of constructing a function before differentiating to answer a question of the sort: “If this quantity changes by a little bit, how much will another, related quantity change?”\nAs an example, consider the textbook-style problem of a water skier being pulled along the water by a rope pulled in from the top of a tower of height \\(H\\). The skier is distance \\(x\\) from the tower. As the rope is winched in at a constant rate, does the skier go faster or slower as she approaches the tower.\n\n\n\n\n\n\n\n\n\nIn the function style of approach, we can write the position function \\(x(t)\\) with input the length of the rope \\(L(t)\\). Using the diagram, you can see that \\[x(t) = \\sqrt{\\strut L(t)^2 - H^2}\\ .\\]\nDifferentiate both sides with respect to \\(t\\) to get the velocity of the skier: \\(\\partial_t x(t)\\) through the chain rule: \\[\\underbrace{\\partial_t x(t)}_{\\partial_t f(g(t))} = \\underbrace{\\frac{1}{2\\sqrt{\\strut L(t)^2 - H^2}}}_{\\left[ \\partial_t f \\right](g(t)) } \\times \\underbrace{\\left[2 \\partial_t L(t)\\right]}_{\\partial_t g(t)} = \\frac{\\partial_t L(t)}{\\strut\\sqrt{L(t)^2 - H^2}}\\]\nNow to reformulate the problem without defining a function.\nNewton referred to “flowing quantities” or “fluents” and to what today is universally called derivatives as “fluxions.” Newton did not have a notion of inputs and output.3\nAt about the same time as Newton’s inventions, very similar ideas were being given very different names by mathematicians on the European continent. There, an infinitely small change in a quantity was called a “differential” and the differential of \\(x\\) was denoted \\(dx\\).\nThe first calculus textbook was subtitled, Of the Calculus of Differentials, in other words, how to calculate differentials. (See Figure 25.8.) Section I of this 1696 text is entitled, “Where we give the rules of this calculation,” those rules being recognizably the same as presented in Chapter 23 of this book.\n\n\n\n\n\n\n\n\n\nFigure 25.8: From the start of the first calculus textbook, by le marquis de l’Hôpital, 1696.\n\n\n\n\nDefinition I of Section I states,\n\n“We call quantities variable* that grow or decrease continuously; and to the contrary constant quantities are those that remain the same while the others change. … The infinitely small amount by which a continuous quantity increases or decreases is called the differential.*”\n\nThe differential is not a derivative. The differential is an infinitely small change in a quantity and a derivative is a rate of change. The differential of a quantity \\(x\\) is written \\(dx\\) in the textbook.4\nThe point of Section I of de l’Hôpital’s textbook is to present the rules by which the differentials of complex quantities can be calculated. You will recognize the product rule in de l’Hôpital’s notation:\n\n\n\n\n\n\nThe differential of \\(x\\,y\\) is \\(y\\,dx + x\\,dy\\)\n\n\n\nThe Pythagorean theorem relates the various quantities this way:\n\\[L^2 = x^2 + H^2\\]\nThe differential of each side of the equation refers to “a little bit” of increase in the quantity on that side of the equation: \\[d(L^2) = d(x^2)\\ \\ \\ \\implies\\ \\ \\ 2 L\\, dL = 2 x\\, dx\\] where we’ve used one of the “rules” for calculating differentials. This gives us \\[dx = \\frac{L}{x} dL\\] Think of this as a recipe for calculating \\(dx\\). If you tell me \\(L\\), \\(x\\), and \\(dL\\) then you can calculate the value of \\(dx\\). For instance, suppose the tower is 52 feet tall and that there is \\(L=173\\) feet of tow-rope extending to the skier. The Pythagorean theorem tells us the skier is \\(x=165\\) feet from the base of the tower. The rope is, let us suppose, being pulled in at the top of the tower at \\(dL = 10\\) feet per second. How fast is \\(x\\) changing? \\[dx = \\frac{173\\ \\text{ft}}{165\\ \\text{ft}} \\times 10 \\text{ft s}^{-2} = 10.05\\ \\text{ft s}^{-1}\\]\nWe will return to “a little bit of this” when we explore how to add up little bits to get the whole in ?sec-accum-symbolic.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Partial change and the gradient vector</span>"
    ]
  },
  {
    "objectID": "Differentiation/25-partial.html#footnotes",
    "href": "Differentiation/25-partial.html#footnotes",
    "title": "25  Partial change and the gradient vector",
    "section": "",
    "text": "In describing the orientation of aircraft and ships, three parameters are used: pitch, roll, and yaw. For a geometrical plane (as opposed to an aircraft or ship, which have distinct front and back ends), yaw isn’t applicable.↩︎\nThe Latin phrase for this is ceteris paribus, often used in economics.↩︎\nThe meaning of “output” as “to produce” dates from more than 100 years after Newton’s death.↩︎\nA “warning” is given in the textbook that the symbol \\(d\\) will always be used to mark the differential of a variable quantity and that \\(d\\) will never be used to indicate a parameter.↩︎",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Partial change and the gradient vector</span>"
    ]
  },
  {
    "objectID": "Differentiation/27-taylor.html",
    "href": "Differentiation/27-taylor.html",
    "title": "27  Polynomials",
    "section": "",
    "text": "27.1 Basics of polynomials with one input\nA polynomial is a linear combination of a particular class of functions: power-law functions with non-negative, integer exponents: 1, 2, 3, …. The individual functions are called monomials, a word that echoes the construction of chemical polymers out of monomers; for instance, the material polyester is constructed by chaining together a basic chemical unit called an ester.\nIn one input, say \\(x\\), the monomials are \\(x^1, x^2, x^3\\), and so on. (There is also \\(x^0\\), but that is better thought of as the constant function.) An n-th order polynomial has monomials up to exponent \\(n\\). For example, the form of a third-order polynomial is \\[a_0 + a_1 x^1 + a_2 x^2 + a_3 x^3\\]\nThe domain of polynomials, like the power-law functions they are assembled from, is the real numbers, that is, the entire number line \\(-\\infty &lt; x &lt; \\infty\\). But for the purposes of understanding the shape of high-order polynomials, it is helpful to divide the domain into three parts: a wriggly domain at the center and two tail domains to the right and left of the center.\nFigure 27.1: A \\(n\\)th-order polynomial can have up to \\(n-1\\) critical points that it wriggles among. A 7-th order polynomial is shown here in which there are six local maxima or minima alternatingly.\nFigure 27.1 shows a 7th order polynomial—that is, the highest-order term is \\(x^7\\). In one of the tail domains the function value heads off to \\(\\infty\\), in the other to \\(-\\infty\\). This is a necessary feature of all odd-order polynomials: 1, 3, 5, 7, …\nIn contrast, for even-order polynomials (2, 4, 6, …) the function value in the two tail domains go in the same direction, either both to \\(\\infty\\) (Hands up!) or both to \\(-\\infty\\).\nIn the wriggly domain in Figure 27.1, there are six argmins or argmaxes.\nAn \\(n\\)th-order polynomial can have up to \\(n-1\\) extrema.\nNote that the local polynomial approximations in Chapter 26 are at most 2nd order and so there is at most 1 wriggle: a unique argmax. If the approximation does not include the quadratic terms (\\(x^2\\) or \\(y^2\\)) then there is no argmax for the function.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Polynomials</span>"
    ]
  },
  {
    "objectID": "Differentiation/27-taylor.html#multiple-inputs",
    "href": "Differentiation/27-taylor.html#multiple-inputs",
    "title": "27  Polynomials",
    "section": "27.2 Multiple inputs?",
    "text": "27.2 Multiple inputs?\nHigh-order polynomials are rarely used with multiple inputs. One reason is the proliferation of coefficients. For instance, here is the third-order polynomial in two inputs, \\(x\\), and \\(y\\). \\[\\underbrace{b_0 + b_x x + b_y y}_\\text{first-order terms} + \\underbrace{b_{xy} x y + b_{xx} x^2 + b_{yy} y^2}_\\text{second-order terms} + \\underbrace{b_{xxy} x^2 y + b_{xyy} x y^2 + b_{xxx} x^3 + b_{yyy} y^3}_\\text{third-order terms}\\]\nThis has 10 coefficients. With so many coefficients it is hard to ascribe meaning to any of them individually. And, insofar as some feature of the function does carry meaning in terms of the modeling situation, that meaning is spread out and hard to quantify.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Polynomials</span>"
    ]
  },
  {
    "objectID": "Differentiation/27-taylor.html#sec-high-order-approx",
    "href": "Differentiation/27-taylor.html#sec-high-order-approx",
    "title": "27  Polynomials",
    "section": "27.3 High-order approximations",
    "text": "27.3 High-order approximations\nThe potential attraction of high-order polynomials is that, with their wriggly interior, they can take on a large number of appearances. This chameleon-like behavior has historically made them the tool of choice for understanding the behavior of approximations. That theory has motivated the use of polynomials for modeling patterns in data, but, paradoxically, has shown that high-order polynomials should not be the tool of choice for modeling data.2\nPolynomial functions lend themselves well to calculations, since the output from a polynomial function can be calculated using just the basic arithmetic functions: addition, subtraction, multiplication, and division. To illustrate, consider this polynomial: \\[g(x) \\equiv x - \\frac{1}{6} x^3\\] Since the highest-order term is \\(x^3\\) this is a third-order polynomial. (As you will see, we picked these particular coefficients, 0, 1, 0, -1/6, for a reason.) With such simple coefficients the polynomial is easy to handle by mental arithmetic. For instance, for \\(g(x=1)\\) is \\(5/6\\). Similarly, \\(g(x=1/2) = 23/48\\) and \\(g(x=2) = 2/3\\). A person of today’s generation would use an electronic calculator for more complicated inputs, but the mathematicians of Newton’s time were accomplished human calculators. It would have been well within their capabilities to calculate, using paper and pencil, \\(g(\\pi/4) = 0.7046527\\).3\nOur example polynomial, \\(g(x) \\equiv x - \\frac{1}{6}x^3\\), graphed in color in Figure 27.2, does not look exactly like the sinusoid. If we increased the extent of the graphics domain, the disagreement would be even more striking, since the sinusoid’s output is always in \\(-1 \\leq \\sin(x) \\leq 1\\), while the polynomial’s tails are heading off to \\(\\infty\\) and \\(-\\infty\\). But, for a small interval around \\(x=0\\), exactly aligns with the sinusoid.\n\n\n\n\n\n\n\n\nFigure 27.2: The polynomial \\(g(x) \\equiv x -x^3 / 6\\) is remarkably similar to \\(\\sin(x)\\) near \\(x=0\\).\n\n\n\n\n\nIt is clear from the graph that the approximation is excellent near \\(x=0\\) and gets worse as \\(x\\) gets larger. The approximation is poor for \\(x \\approx \\pm 2\\). We know enough about polynomials to say that the approximation will not get better for larger \\(x\\); the sine function has a range of \\(-1\\) to \\(1\\), while the left and right tails of the polynomial are heading off to \\(\\infty\\) and \\(-\\infty\\) respectively.\nOne way to measure the quality of the approximation is the error \\({\\cal E}(x)\\) which gives, as a function of \\(x\\), the difference between the actual sinusoid and the approximation: \\[{\\cal E}(x) \\equiv |\\strut\\sin(x) - g(x)|\\] The absolute value used in defining the error reflects our interest in how far the approximation is from the actual function and not so much in whether the approximation is below or above the actual function. Figure 27.3 shows \\({\\cal E}(x)\\) as a function of \\(x\\). Since the error is the same on both sides of \\(x=0\\), only the positive \\(x\\) domain is shown.\n\n\n\n\n\n\n\n\nFigure 27.3: The error \\({\\cal E}(x)\\) of \\(x - x^3/6\\) as an approximation to \\(\\sin(x)\\). Top panel: linear scale. Bottom panel: on a log-log scale.\n\n\n\n\n\nFigure 27.3 shows that for \\(x &lt; 0.3\\), the error in the polynomial approximation to \\(\\sin(x)\\) is in the 5th decimal place. For instance, \\(\\sin(0.3) = 0.2955202\\) while \\(g(0.3) = 0.2955000\\).\nThat the graph of \\({\\cal E}(x)\\) is a straight-line on log-log scales diagnoses \\({\\cal E}(x)\\) as a power law. That is: \\({\\cal E}(x) = A x^p\\). As always for power-law functions, we can estimate the exponent \\(p\\) from the slope of the graph. It is easy to see that the slope is positive, so \\(p\\) must also be positive.\nThe inevitable consequence of \\({\\cal E}(x)\\) being a power-law function with positive \\(p\\) is that \\(\\lim_{x\\rightarrow 0} {\\cal E}(x) = 0\\). That is, the polynomial approximation \\(x - \\frac{1}{6}x^3\\) is exact as \\(x \\rightarrow 0\\).\nThroughout this book, we’ve been using straight-line approximations to functions around an input \\(x_0\\). \\[g(x) = f(x_0) + \\partial_x f(x_0) [x-x_0]\\] One way to look at \\(g(x)\\) is as a straight-line function. Another way is as a first-order polynomial. This raises the question of what a second-order polynomial approximation should be. Rather than the polynomial matching just the slope of \\(f(x)\\) at \\(x_0\\), we can arrange things so that the second-order polynomial will also match the curvature of the \\(f()\\). Since the curvature involves only the first and second derivatives of a function, the polynomial constructed to match both the first and the second derivative will necessarily match the slope and curvature of \\(f()\\). This can be accomplished by setting the polynomial coefficients appropriately.\nStart with a general, second-order polynomial centered around \\(x_0\\): \\[g(x) \\equiv a_0 + a_1 [x-x_0] + a_2 [x - x_0]^2\\] The first- and second-derivatives, evaluated at \\(x=x_0\\) are: \\[\\partial_x g(x)\\left.{\\Large\\strut}\\right|_{x=x_0} = a_1 + 2 a_2 [x  - x_0] \\left.{\\Large\\strut}\\right|_{x=x_0} = a_1\\] \\[\\partial_{xx} g(x)\\left.{\\Large\\strut}\\right|_{x=x_0} =  2 a_2\\] Notice the 2 in the above expression. When we want to write the coefficient \\(a_2\\) in terms of the second derivative of \\(g()\\), we will end up with\n\\[a_2 = \\frac{1}{2} \\partial_{xx} g(x)\\left.{\\Large\\strut}\\right|_{x=x_0}\\]\nTo make \\(g(x)\\) approximate \\(f(x)\\) at \\(x=x_0\\), we need merely set \\[a_1 = \\partial_x f(x)\\left.{\\Large\\strut}\\right|_{x=x_0}\\] and \\[a_2 = \\frac{1}{2} \\partial_{xx} f(x) \\left.{\\Large\\strut}\\right|_{x=x_0}\\] This logic can also be applied to higher-order polynomials. For instance, to match the third derivative of \\(f(x)\\) at \\(x_0\\), set \\[a_3 = \\frac{1}{6} \\partial_{xxx} f(x)  \\left.{\\Large\\strut}\\right|_{x=x_0}\\] Remarkably, each coefficient in the approximating polynomial involves only the corresponding order of derivative. \\(a_1\\) involves only \\(\\partial_x f(x)   \\left.{\\Large\\strut}\\right|_{x=x_0}\\); the \\(a_2\\) coefficient involves only \\(\\partial_{xx} f(x)     \\left.{\\Large\\strut}\\right|_{x=x_0}\\); the \\(a_3\\) coefficient involves only \\(\\partial_{xx} f(x)     \\left.{\\Large\\strut}\\right|_{x=x_0}\\), and so on.\nNow we can explain where the polynomial that started this section, \\(x - \\frac{1}{6} x^3\\) came from and why those coefficients make the polynmomial approximate the sinusoid near \\(x=0\\).\n\n\n\n\n\n\n\n\nOrder\n\\(\\sin(x)\\) derivative\n\\(x - \\frac{1}{6}x^3\\) derivative\n\n\n\n\n0\n\\(\\sin(x) \\left.{\\Large\\strut}\\right|_{x=0} = 0\\)\n\\(\\left( 1 - \\frac{1}{6}x^3\\right)\\left.{\\Large\\strut}\\right|_{x=0} = 0\\)\n\n\n1\n\\(\\cos(x) \\left.{\\Large\\strut}\\right|_{x=0} = 1\\)\n\\(\\left(1 - \\frac{3}{6} x^2\\right) \\left.{\\Large\\strut}\\right|_{x=0}= 1\\)\n\n\n2\n\\(-\\sin(x) \\left.{\\Large\\strut}\\right|_{x=0} = 0\\)\n\\(\\left(- \\frac{6}{6} x\\right) \\left.{\\Large\\strut}\\right|_{x=0} = 0\\)\n\n\n3\n\\(-\\cos(x) \\left.{\\Large\\strut}\\right|_{x=0} = -1\\)\n\\(- 1\\left.{\\Large\\strut}\\right|_{x=0} = -1\\)\n\n\n4\n\\(\\sin(x) \\left.{\\Large\\strut}\\right|_{x=0} = 0\\)\n\\(0\\left.{\\Large\\strut}\\right|_{x=0} = 0\\)\n\n\n\nThe first four derivatives of \\(x - \\frac{1}{6} x^3\\) exactly match, at \\(x=0\\), the first four derivatives of \\(\\sin(x)\\).\nThe polynomial constructed by matching successive derivatives of a function \\(f(x)\\) at some input \\(x_0\\) is called a Taylor polynomial.\n\n\n\n\n\n\nTip 27.1: Practice: a Taylor polynomial for \\(e^x\\).\n\n\n\nLet’s construct a 3rd-order Taylor polynomial approximation to \\(f(x) = e^x\\) around \\(x=0\\).\nWe know it will be a 3rd order polynomial: \\[g_{\\exp}(x) \\equiv a_0 + a_1 x + a_2 x^2 + a_3 x^3\\] The exponential function is particularly nice for examples because the function value and all its derivatives are identical: \\(e^x\\). So \\[f(x= 0) = 1\\]\n\\[ \\partial_x f(x=0) = 1\\] \\[\\partial_{xx} f(x=0) = 1\\] \\[\\partial_{xxx} f(x=0) = 1\\] and so on.\nThe function value and derivatives of \\(g_{\\exp}(x)\\) at \\(x=0\\) are: \\[g_{\\exp}(x=0) = a_0\\] \\[\\partial_{x}g_{\\exp}(x=0) = a_1\\] \\[\\partial_{xx}g_{\\exp}(x=0) = 2 a_2\\]\n\\[\\partial_{xxx}g_{\\exp}(x=0) = 2\\cdot3\\cdot a_3 = 6\\, a_3\\] Matching these to the exponential evaluated at \\(x=0\\), we get \\[a_0 = 1\\] \\[a_1 = 1\\] \\[a_2 = \\frac{1}{2}\\] \\[a_3 = \\frac{1}{2 \\cdot 3} = \\frac{1}{6}\\]\nResult: the 3rd-order Taylor polynomial approximation to the exponential at \\(x=0\\) is \\[g_{\\exp}(x) = 1 + x + \\frac{1}{2} x^2 +  \\frac{1}{2\\cdot 3} x^3 +\\frac{1}{2\\cdot 3\\cdot 4} x^4\\]\nFigure 27.4 shows the exponential function \\(e^x\\) and its 3th-order Taylor polynomial approximation near \\(x=0\\):\n\n\n\n\n\n\n\n\nFigure 27.4: The 3th-order Taylor polynomial approximation to \\(e^x\\) arount \\(x=0\\)\n\n\n\n\n\nThe polynomial is exact at \\(x=0\\). The error \\({\\cal E}(x)\\) grows with increasing distance from \\(x=0\\):\n\n\n\n\n\n\n\n\nFigure 27.5: The error from a 3rd-order Taylor polynomial approximation to \\(e^x\\) around \\(x=0\\) is a power-law function with exponent \\(4\\).\n\n\n\n\n\n\n\n\n\n\n\nFigure 27.6: The error from a 3rd-order Taylor polynomial approximation to \\(e^x\\) around \\(x=0\\) is a power-law function with exponent \\(4\\).\n\n\n\n\n\nThe plot of \\(\\log_{10} {\\cal E}(x)\\) versus \\(\\log_{10} | x |\\) in ?fig-taylor-exp-5 shows that the error grows from zero at \\(x=0\\) as a power-law function. Measuring the exponent of the power-law from the slope of the graph on log-log axes give \\({\\cal E}(x) = a |x-x_0|^5\\). This is typical of Taylor polynomials: for a polynomial of degree \\(n\\), the error will grow as a power-law with exponent \\(n+1\\). This means that the higher is \\(n\\), the faster \\(\\lim_{x\\rightarrow x_0}{\\cal E}(x) \\rightarrow 0\\). On the other hand, since \\({\\cal E}_x\\) is a power law function, as \\(x\\) gets further from \\(x_0\\) the error grows as \\(\\left(x-x_0\\right)^{n+1}\\).\n\n\n\n\n\n\n\n\nCalculus history—Polynomial models of other functions\n\n\n\nBrooke Taylor (1685-1731), a near contemporary of Newton, published his work on approximating polynomials in 1715. Wikipedia reports: “[T]he importance of [this] remained unrecognized until 1772, when Joseph-Louis Lagrange realized its usefulness and termed it ‘the main [theoretical] foundation of differential calculus’.”Source\n\n\n\n\n\n\n\n\nFigure 27.7: Brook Taylor\n\n\n\n\n\nDue to the importance of Taylor polynomials in the development of calculus, and their prominence in many calculus textbooks, many students assume their use extends to constructing models from data. They also assume that third- and higher-order monomials are a good basis for modeling data. Both these assumptions are wrong. Least squares is the proper foundation for working with data.\nTaylor’s work preceded by about a century the development of techniques for working with data. One of the pioneers in these new techniques was Carl Friedrich Gauss (1777-1855), after whom the gaussian function is named. Gauss’s techniques are the foundation of an incredibly important statistical method that is ubiquitous today: least squares. Least squares provides an entirely different way to find the coefficients on approximating polynomials (and an infinite variety of other function forms). The R/mosaic fitModel() function for polishing parameter estimates is based on least squares. In Block 5, we will explore least squares and the mathematics underlying the calculations of least-squares estimates of parameters.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Polynomials</span>"
    ]
  },
  {
    "objectID": "Differentiation/27-taylor.html#indeterminate-forms",
    "href": "Differentiation/27-taylor.html#indeterminate-forms",
    "title": "27  Polynomials",
    "section": "27.4 Indeterminate forms",
    "text": "27.4 Indeterminate forms\nLet’s return to an issue that has bedeviled calculus students since Newton’s time. The example we will use is the function \\[\\text{sinc}(x)  \\equiv \\frac{\\sin(x)}{x}\\]\nThe sinc() function (pronounced “sink”) is still important today, in part because of its role in converting discrete-time measurements (as in an mp3 recording of sound) into continuous signals.\nWhat is the value of \\(\\text{sinc}(0)\\)? One answer, favored by arithmetic teachers is that \\(\\text{sinc}(0)\\) is meaningless, because it involves division by zero.\nOn the other hand, \\(\\sin(0) = 0\\) as well, so the sinc function evaluated at zero involves 0/0. This quotient is called an indeterminant form. The logic is this: Suppose we assume that \\(0/0 = b\\) for some number \\(b\\). then \\(0 = 0 \\times b = 0\\). So any value of \\(b\\) would do; the value of \\(0/0\\) is “indeterminant.”\nStill another answer is suggested by plotting out sinc(\\(x\\)) near \\(x=0\\) and reading the value off the graph: sinc(0) = 1.\n\n\n\n\nslice_plot(sin(x) / x ~ x, domain(x=c(-10,10)), npts=500)\n\n\n\n\n\n\n\n\n\n\nFigure 27.8: To judge from this plot, \\(\\sin(0) / 0 = 1\\).\n\n\n\nThe graph of sinc() looks smooth and the shape makes sense. Even if we zoom in very close to \\(x=0\\), the graph continues to look smooth. We call such functions well behaved.\nCompare the well-behaved sinc() to a very closely related function (which does not seem to be so important in applied work): \\(\\frac{\\sin(x)}{x^3}\\).\nBoth \\(\\sin(x)/x\\) and \\(\\sin(x) / x^3\\), evaluated at \\(x=0\\) involve a divide by zero. Both are indeterminate forms 0/0 at \\(x=0\\). But the graph of \\(\\sin(x) / x^3\\) (see Figure 27.9) is not we will behaved. \\(\\sin(x) / x^3\\) does not have any particular value at \\(x=0\\); instead, it has an asymptote.\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nFigure 27.9: Zooming in around the division by zero. Top: The graph of \\(\\sin(x)/x\\) versus \\(x\\). Bottom: The graph of \\(\\sin(x)/x^2\\). The vertical scales on the two graphs are utterly different.\n\n\n\nSince both \\(\\sin(x)/x\\left.{\\Large\\strut}\\right|_{x=0}\\) and \\(\\sin(x)/x^3\\left. {\\Large\\strut}\\right|_{x=0}\\) involve a divide-by-zero, the answer to the utterly different behavior of the two functions is not to be found at zero. Instead, it is to be found near zero. For any non-zero value of \\(x\\), the arithmetic to evaluate the functions is straight-forward. Note that \\(\\sin(x) / x^3\\) starts its mis-behavior away from zero. The slope of \\(\\sin(x) / x^3\\) is very large near \\(x=0\\), while the slope of \\(\\sin(x) / x\\) smoothly approaches zero.\nSince we are interested in behavior near \\(x=0\\), a useful technique is to approximate the numerator and denominator of both functions by polynomial approximations.\n\n\\(\\sin(x) \\approx x - \\frac{1}{6} x^3\\) near \\(x=0\\)\n\\(x\\) is already a polynomial.\n\\(x^3\\) is already a polynomial.\n\nRemember, these approximations are exact as \\(x\\) goes to zero. So sufficiently close to zero,\n\\[\\frac{\\sin(x)}{x} = \\frac{x - \\frac{1}{6} x^3}{x} = 1 + \\frac{1}{6} x^2\\] Even at \\(x=0\\), there is nothing indeterminant about \\(1 + x^2/6\\); it is simply 1.\nCompare this to the polynomial approximation to \\(\\sin(x) / x^3\\): \\[\\frac{\\sin(x)}{x^3} = \\frac{x - \\frac{1}{6} x^3}{x^3} = \\frac{1}{x^2} - \\frac{1}{6}\\]\nEvaluating this at \\(x=0\\) involves division by zero. No wonder it is badly behaved.\nThe procedure for checking whether a function involving division by zero behaves well or poorly is described in the first-ever calculus textbook, published in 1697. The title (in English) is: The analysis into the infinitely small for the understanding of curved lines. In honor of the author, the Marquis de l’Hospital, the procedure is called l’Hopital’s rule.4\nConventionally, the relationship is written \\[\\lim_{x\\rightarrow x_0} \\frac{u(x)}{v(x)} = \\lim_{x\\rightarrow x_0} \\frac{\\partial_x u(x)}{\\partial_x v(x)}\\]\nLet’s try this out with our two example functions around \\(x=0\\):\n\\[\\lim_{x\\rightarrow 0} \\frac{\\sin(x)}{x} = \\frac{\\lim_{x\\rightarrow 0} \\cos(x)}{\\lim_{x \\rightarrow 0} 1} = \\frac{1}{1} = 1\\]\n\\[\\lim_{x\\rightarrow 0} \\frac{\\sin(x)}{x^3} = \\frac{\\lim_{x\\rightarrow 0} \\cos(x)}{\\lim_{x \\rightarrow 0} 3x^2} = \\frac{1}{0} \\ \\ \\text{indeterminate}!\\]",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Polynomials</span>"
    ]
  },
  {
    "objectID": "Differentiation/27-taylor.html#computing-with-indeterminate-forms",
    "href": "Differentiation/27-taylor.html#computing-with-indeterminate-forms",
    "title": "27  Polynomials",
    "section": "27.5 Computing with indeterminate forms",
    "text": "27.5 Computing with indeterminate forms\nIn the early days of electronic computers, division by zero would cause a fault in the computer, often signaled by stopping the calculation and printing an error message to some display. This was inconvenient, since programmers did not always forsee division-by-zero situations and avoid them.\nAs you’ve seen, modern computers have adopted a convention that simplifies programming considerably. Instead of stopping the calculation, the computer just carries on normally, but produces as a result one of two indeterminant forms: Inf and NaN.\nInf is the output for the simple case of dividing zero into a non-zero number, for instance:\n\n17/0\n## [1] Inf\n\nNaN, standing for “not a number,” is the output for more challenging cases: dividing zero into zero, or multiplying Inf by zero, or dividing Inf by Inf.\n\n0/0\n## [1] NaN\n0 * Inf\n## [1] NaN\nInf / Inf\n## [1] NaN\n\nThe brilliance of the idea is that any calculation that involves NaN will return a value of NaN. This might seem to get us nowhere. But most programs are built out of other programs, usually written by other people interested in other applications. You can use those programs (mostly) without worrying about the implications of a divide by zero. If it is important to respond in some particularly way, you can always check the result for being NaN in your own programs. (Much the same is true for Inf, although dividing a non-Inf number by Inf will return 0.)\nPlotting software will often treat NaN values as “don’t plot this.” that is why it is possible to make a sensible plot of \\(\\sin(x)/x\\) even when the plotting domain includes zero.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Polynomials</span>"
    ]
  },
  {
    "objectID": "Differentiation/27-taylor.html#footnotes",
    "href": "Differentiation/27-taylor.html#footnotes",
    "title": "27  Polynomials",
    "section": "",
    "text": "The fundamental theorem of algebra says that an order-n polynomial has n roots (including multiplicities).↩︎\nThe mathematical background needed for those better tools won’t be available to us until Block 5, when we explore linear algebra.↩︎\nUnfortunately for these human calculators, pencils weren’t invented until 1795. Prior to the introduction of this advanced, graphite-based computing technology, mathematicians had to use quill and ink.↩︎\nIn many French words, the sequence “os” has been replaced by a single, accented letter, \\(\\hat{o}\\).↩︎",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Polynomials</span>"
    ]
  },
  {
    "objectID": "Differentiation/Differentiation-projects.html",
    "href": "Differentiation/Differentiation-projects.html",
    "title": "28  Differentiation projects",
    "section": "",
    "text": "28.1 Labor vs capital\nThe Cobb-Douglas production function is a simple mathematical model of how labor \\(L\\) and capital \\(K\\) combine to produce a factory’s output \\(P\\). It is \\[P(L, K) \\equiv A K^\\alpha L^{1-\\alpha}\\ .\\]\nFor simplicity, imagine that capital and labor are both measured in dollars per year—the amount that the labor force is paid in a year and the amount that one could rent a factory for a year.\nWe’ll stick with numbers like \\(K = 10\\) and \\(L = 20\\) to keep things easy to read, but feel free to interpret them as “millions of dollars.”\nCongratuations! Based on your ability to use the Cobb-Douglas model, you’ve been promoted to manager of the factory. One of your jobs is to decide how to balance expenditures on capital and labor to raise productivity.\nOne basic question is what happens when you raise either capital or labor, holding the other one constant. Using ap- propriate partial derivatives evaluated at \\(K = 10\\), \\(L = 20\\), calculate:\nYour economist friend tells you to watch out for “diminishing marginal returns.” This means that, as you increase spending on either labor or capital, the rate of increase in production tends to diminish. You’ll still get increased production as you increase spending, but it won’t increase as fast at high levels of expense as at low levels.\nBut what happens to the value rate of labor when capital spending is increased? You can answer this by comparing the value rate of labor, \\(\\partial_L P\\) , at two different capital spending levels, say \\((K = 10,L = 20)\\) and \\((K = 11,L = 20)\\). Notice that even though you’re looking at the rate with respect to labor, you’re changing the expenditure on capital.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Differentiation projects</span>"
    ]
  },
  {
    "objectID": "Differentiation/Differentiation-projects.html#labor-vs-capital",
    "href": "Differentiation/Differentiation-projects.html#labor-vs-capital",
    "title": "28  Differentiation projects",
    "section": "",
    "text": "If production \\(P(L, K)\\) is also measured in dollars per year (say, the value of the factory output each year), what is the dimension of the constant \\(A\\)?\nAccording to the model, what happens to production if both \\(K\\) and \\(L\\) are increased by a factor constant factor \\(\\beta\\)? (Hint: Substitute in \\(K \\rightarrow \\beta K\\) and \\(L \\rightarrow \\beta L\\) and simplify.)\nConsider a particular factory with \\(A = 2.5\\) and exponent \\(\\alpha = 0.33\\). In a sandbox, implement the function \\(P (K, L)\\). Use your function to compute the production of the factory for \\(K = 10\\) and \\(L=20\\). Confirm that you get \\(P(K=10,L=20)= 39.78\\)\nA factory that rents for $10/yr and where the labor costs $20/yr is silly. Calculate the value \\(P (K, L)\\) when \\(K\\) is $10 million/yr and \\(L\\) is $20 million/yr.\n\n\n\n\n\nThe rate at which an increase in spending on labor will increase productivity.\nThe rate at which an increase in spending on capital will increase productivity.\nBased on the above, if you had to choose between spending on capital or labor, and your goal is to increase productivity as much as possible, which would you spend on, capital or labor?\n\n\n\nCompute the partial derivative of production with respect to labor at a higher level of labor, say \\(L = 21\\), but holding \\(K = 10\\). How does the value of the derivative at \\(L = 21\\) compare to that at \\(L = 20\\)? Is this consistent with the idea of “diminishing marginal returns” for labor?\nDo the same for the partial derivative of production with respect to capital, evaluated at \\(L = 20\\) and \\(K = 11\\). How does the value of the derivative at \\(K = 11\\) compare to that at \\(K = 10\\). Is this consistent with the idea of “diminishing marginal returns” for capital?\nUse an appropriate partial second derivative to find the rate of diminishing partial returns for labor at \\(L = 20\\) and \\(K = 10\\). Show that it is consistent with the difference you got in Part (d).\nUse an appropriate partial second derivative to find the rate of diminishing partial returns for capital at \\(L = 20\\) and \\(K = 10\\). Show that it is consistent with the difference you got in Part (3).\nYou might think of the rate of increase in production with respect to labor as the “value rate” of labor. Similarly, the rate of increase in production with respect to capital is the value rate of capital. Due to diminishing marginal returns, an increase in labor spending, holding capital constant, decreases the value rate of labor. Similarly, an increase in capital spending holding labor spending constant decreases the value rate of capital.\n\n\ni. Compare $\\partial_L P$ at slightly different values of $K$ , holding $L$ constant at 20. Does the value rate of labor increase or decrease with spending on capital?\nii. Similarly, compare $\\partial_K P$ at slightly different values of  $L$, holding $K$ constant at 20. Does the value rate of labor increase or decrease with spending on capital?\niii. Finally, construct and evaluate the mixed partial derivative, $\\partial_L \\partial_K P at $K = 10$, $L = 20$. Compare this to the results you got for the way $\\partial_K P$ changes with increasing $L$ and the way $\\partial_L P$ changes with increasing $K$.\n\n28.1.1 Walking\nIf you’re like many people, you find it harder to walk uphill than down, and find it takes more out of you to walk longer distances than shorter. Let’s build a model of this, using nothing more than your intuition and the method of low-order polynomial approximations.\nLet’s call the map distance walked \\(d\\). (“Map distance” is the horizontal change in position, disregarding vertical changes.) The steepness of the hill will be the “grade” \\(g\\), which is measured as the horizontal distance covered divided by the vertical climb. If you’re going downhill, the grade is negative.\nThe key ingredient in the model: we will measure the “difficulty” or “exertion” to walking as the energy consumed during the walk: \\(E(d, g)\\).\nSome assumptions about walking and energy consumed:\n\nIf you don’t walk, you consume zero energy walking.\nThe energy consumed should be proportional to the length of the walk. This is an assumption, and is probably valid, only for walks of short to medium distances, as opposed to forced marches over tens of miles.\n\nWe will start with the full 2nd-order polynomial in two inputs, and then seek to eliminate terms that aren’t needed.\n\\[E_{big}(d, g) \\equiv a_0 + a_d\\, d + a_g\\, g + a_{dg}\\, d\\, g + a_{dd}\\,d^2 + a_{gg}\\,g^2\\] According to assumption (1), when \\(E(d=0, g) = 0\\). Of course, if you are walking zero distance, it does not matter what the grade is; the energy consumed is still zero.\nConsequently, we know that all terms that don’t include a \\(d\\) should go away. This leaves us with\n\\[E_{medium}(d, g) \\equiv  a_d\\, d + a_{dg}\\, d\\, g + a_{dd}\\,d^2 = d \\left[\\strut a_d + a_{dg}\\, g + a_{dd}\\,d\\right]\\] Assumption (2) says that energy consumed is proportional to \\(d\\). The multiplier on \\(d\\) in \\(E_{medium}()\\) is \\(\\left[\\strut a_d + a_{dg}\\, g + a_{dd}\\,d\\right]\\) which is itself a function of \\(d\\). A proportional relationship implies a multiplier that does not depend on the quantity itself. This means that \\(a_{dd} = 0\\).\nThis leaves us with a very simple model: \\[E(d, g) \\equiv \\left[\\strut a_1 + a_2\\, g\\right]\\, d\\] where we have simplified the labeling on the coefficients since there are only two in the model.\nPerhaps assumption (2) is misplaced and that the energy consumed per unit distance in a walk increases with the length of the walk. If so, we would need to return to the question of \\(a_{dd}\\). This is typical of the modeling cycle. Trying to be economical with model terms highlights the question of which terms are so small they can be ignored.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Differentiation projects</span>"
    ]
  },
  {
    "objectID": "differentiation-part.html",
    "href": "differentiation-part.html",
    "title": "BLOCK II. Differentiation",
    "section": "",
    "text": "A mathematical function is a relationship between inputs and an output. An important and useful way to work with functions is to examine change in output as the inputs are changed by a small amount. The process of calculating this change-in-output per change-in-input—a rate of change—is called differentation. Often, the rate of change is itself a function. Such rate-of-change functions are given a special label: a derivative function.\nThis Block introduces the concept of a rate-of-change function, ways of computing them, and how the derivative of a function can be inferred from a graph of the function. We will explore the connection between the value of the rate-of-change function and the location of an input that optimizes the output of the original function. We will consider the idea of rate-of-change for a function that has multiple inputs.\nSometimes, your knowledge of a real-world system takes the form of knowing the behavior of the rate-of-change function. This can be an important guide to constructing mathematical models.",
    "crumbs": [
      "BLOCK II. Differentiation"
    ]
  },
  {
    "objectID": "Linear-combinations/B5-linear-combinations.html",
    "href": "Linear-combinations/B5-linear-combinations.html",
    "title": "30  Linear combinations of vectors",
    "section": "",
    "text": "30.1 Scaling vectors\nTo scale a vector means to change its length without altering its direction. For instance, scaling by a negative number flips the vector tip-for-tail. Figure 30.1 shows two vectors \\(\\vec{v}\\) and \\(\\vec{w}\\) together with several scaled versions.\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\nFigure 30.1: Vectors \\(\\vec{v}\\) and \\(\\vec{w}\\) and some scaled versions of them.\nArithmetically, scaling a vector is accomplished by multiplying each component of the vector by the scalar, e.g.\n\\[\\vec{u} = \\left[\\begin{array}{r}1.5\\\\-1\\end{array}\\right]\\ \\ \\ \\ 2\\vec{u} = \\left[\\begin{array}{r}3\\\\-2\\end{array}\\right]\\ \\ \\ \\\n-\\frac{1}{2}\\vec{u} = \\left[\\begin{array}{r}-0.75\\\\0.5\\end{array}\\right]\\ \\ \\ \\ \\]\nGeometrically, however, a vector corresponds to one step in a journey. For example, a vector scaled by 2.5 is a journey of two-and-a-half steps; scaling by -10 means traveling backward ten steps.\nThe subspace associated with a single vector is the set of all possible journeys that scaling a vector can accomplish. Visually, this corresponds to all the points on an infinitely long line defined by two points: the tip and the tail of the vector.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Linear combinations of vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/B5-linear-combinations.html#adding-vectors",
    "href": "Linear-combinations/B5-linear-combinations.html#adding-vectors",
    "title": "30  Linear combinations of vectors",
    "section": "30.2 Adding vectors",
    "text": "30.2 Adding vectors\nTo add two vectors, place them tip to tail (without changing the direction). The sum is the new vector running from the tail of the first one to the tip of the second. (Figure 30.2)\n\n\n\n\n\n\n\n\nFigure 30.2: Adding two vectors, yellow and green, by placing them tail to tip. The result is the vector going from the tail of yellow to the tip of green. The blue vector shows this result.\n\n\n\n\n\nAdding vectors in this way takes advantage of the rootlessness of a vector. So long as we keep the direction and length the same, we can move a vector to any convenient place. For adding vectors, the convenient arrangement is to place the tail of the second vector at the tip of the first. The result—the blue pencil in Figure 30.2—runs from the first (yellow) pencil’s tail to the second (green) pencil’s tip.\nArithmetically, vector addition is simply a matter of applying addition component-by-component. For instance, consider adding two vectors \\(\\vec{v}\\) and \\(\\vec{w}\\):\n\\[\\underbrace{\\left[\\begin{array}{r}1.5\\\\-1\\\\2\\\\6\\end{array}\\right]}_\\vec{v} + \\underbrace{\\left[\\begin{array}{r}2\\\\4\\\\-2\\\\-3.2\\end{array}\\right]}_\\vec{w} = \\underbrace{\\left[\\begin{array}{r}3.5\\\\3\\\\0\\\\2.8\\end{array}\\right]}_{\\vec{v} + \\vec{w}}\\]\nAdding vectors makes sense only when they inhabit the same embedding space. In other words, the vectors must have the same number of components.\nArithmetic subtraction of one vector from another is a simple componentwise operation. For example:\n\\[\\underbrace{\\left[\\begin{array}{r}1.5\\\\-1\\\\2\\\\6\\end{array}\\right]}_\\vec{v} {\\Large -} \\underbrace{\\left[\\begin{array}{r}2\\\\4\\\\-2\\\\-3.2\\end{array}\\right]}_\\vec{w} = \\underbrace{\\left[\\begin{array}{r}-0.5\\\\-5\\\\4\\\\9.2\\end{array}\\right]}_{\\vec{v} - \\vec{w}}\\ .\\]\nFrom a geometrical point of view, many people like to think of \\(\\vec{v} - \\vec{w}\\) in terms of placing the two vectors tail to tail as in Figure 30.3. Read the result as the vector running from the tip of \\(\\vec{v}\\) to the tip of \\(\\vec{w}\\). In Figure 30.3, the yellow vector is \\(\\vec{v}\\) and the blue vector is \\(\\vec{w}\\). The result of the subtraction is the green vector.\n\n\n\n\n\n\n\n\nFigure 30.3: Subtracting blue from yellow gives green.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Linear combinations of vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/B5-linear-combinations.html#linear-combinations",
    "href": "Linear-combinations/B5-linear-combinations.html#linear-combinations",
    "title": "30  Linear combinations of vectors",
    "section": "30.3 Linear combinations",
    "text": "30.3 Linear combinations\nThinking of a scaled vector as a “step” of a given length in a given direction leads us to conceive of a linear combination of vectors as step-by-step instructions for a journey. A central use for the formalism of vectors is to guide our thinking and our algorithms for figuring out how best to get from one “place” to another. We have used quotation marks around “place” because we are not necessarily referring to a physical destination. We will get to what else we might mean by “place” later in this Block.\nAs a fanciful example of getting to a “place,” consider a treasure hunt. Here are the instructions to get there:\n\n\nOn June 1, go to the flagpole before sunrise.\nAt 6:32, walk 213 paces away from the Sun.\nAt 12:19, walk 126 paces toward the Sun.\n\n\nThe Sun’s position varies over the day. Consequently, the direction of the Sun on June 1 at 6:32 is different than at 12:19. (Figure 30.4)\n\n\n\n\n\n\nFigure 30.4: For June 1: \\(\\color{magenta}{\\text{Sun's direction at 6:32}}\\) and $$. (Location: latitude 38.0091, /longitude -104.8871). Source: suncalc.org\n\n\n\nThe treasure-hunt directions are in the form of a linear combination of vectors. So far, we know the direction of each vector. Imagine that the length is one stride or pace. (Admittedly, not a scientific unit of length.) Scaling \\(\\color{magenta}{\\text{the magenta vector}}\\) by -213 and \\(\\color{blue}{\\text{the blue vector}}\\) by 126, then adding the two scaled vectors gives a vector that takes you from the flagpole to the treasure.\nA stickler for details might point out that the “direction of the sun” has an upward component. Common sense dictates that the walk is in the direction of the Sun as projected onto Earth’s surface. Chapter 31 deals with projections of vectors.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Linear combinations of vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/B5-linear-combinations.html#functions-as-vectors",
    "href": "Linear-combinations/B5-linear-combinations.html#functions-as-vectors",
    "title": "30  Linear combinations of vectors",
    "section": "30.4 Functions as vectors",
    "text": "30.4 Functions as vectors\nSince calculus is about functions, one naturally expects a chapter on vectors in a calculus textbook to make a connection between functions and vectors. Recall from Section 7.4 (entitled “Functions and data”) the idea of representing a function as a table of inputs and the corresponding outputs.\nHere is such a table with some of our pattern-book functions.\n\n\n\n\n\nt\none(t)\nidentity(t)\nexp(t)\nsin(t)\npnorm(t)\n\n\n\n\n0.0\n1\n0.0\n1.000000\n0.0000000\n0.5000000\n\n\n0.1\n1\n0.1\n1.105171\n0.0998334\n0.5398278\n\n\n0.2\n1\n0.2\n1.221403\n0.1986693\n0.5792597\n\n\n0.3\n1\n0.3\n1.349859\n0.2955202\n0.6179114\n\n\n0.4\n1\n0.4\n1.491825\n0.3894183\n0.6554217\n\n\n... 51 rows in total ...\n\n\n\n\n\n\n\n\n\n\n4.6\n1\n4.6\n99.48432\n-0.9936910\n0.9999979\n\n\n4.7\n1\n4.7\n109.94717\n-0.9999233\n0.9999987\n\n\n4.8\n1\n4.8\n121.51042\n-0.9961646\n0.9999992\n\n\n4.9\n1\n4.9\n134.28978\n-0.9824526\n0.9999995\n\n\n5.0\n1\n5.0\n148.41316\n-0.9589243\n0.9999997\n\n\n\n\n\nIn the tabular representation of the pattern-book functions, each function is a column of numbers—a vector.\nFunctions that we construct by linear combination are, in this vector format, just a linear combination of the vectors. For instance, the function \\(g(t) \\equiv 3 - 2 t\\) is \\(3\\cdot \\text{one}(t) - 2 \\cdot \\text{identity}(t)\\)\n\n\n\n\n\nt\none(t)\nidentity(t)\ng(t)\n\n\n\n\n0.0\n1\n0.0\n3.0\n\n\n0.1\n1\n0.1\n2.8\n\n\n0.2\n1\n0.2\n2.6\n\n\n0.3\n1\n0.3\n2.4\n\n\n0.4\n1\n0.4\n2.2\n\n\n... 51 rows in total ...\n\n\n\n\n\n\n\n\n4.6\n1\n4.6\n-6.2\n\n\n4.7\n1\n4.7\n-6.4\n\n\n4.8\n1\n4.8\n-6.6\n\n\n4.9\n1\n4.9\n-6.8\n\n\n5.0\n1\n5.0\n-7.0\n\n\n\n\n\nThe table above is a collection of four vectors: \\(\\vec{\\mathtt t}\\), \\(\\vec{\\mathtt{ one(t)}}\\), \\(\\vec{\\mathtt{identity(t)}}\\), and \\(\\vec{\\mathtt{g(t)}}\\). Each of those vectors has 51 components. In math-speak, we can say that the vectors are “embedded in a 51-dimensional space.”\nIn the table, the function output is tabulated only for select, discrete values of the input. Such discreteness is not a fundamental problem. Interpolation techniques such as that described in Section 7.4 enable evaluation of the function for a continuous input.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Linear combinations of vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/B5-linear-combinations.html#matrices-and-linear-combinations",
    "href": "Linear-combinations/B5-linear-combinations.html#matrices-and-linear-combinations",
    "title": "30  Linear combinations of vectors",
    "section": "30.5 Matrices and linear combinations",
    "text": "30.5 Matrices and linear combinations\nA collection of vectors, such as the one displayed in the previous table, is called a matrix. Each vector in a matrix must have the same number of components.\nAs mathematical notation, we will use bold-faced, capital letters to stand for matrices, for example, \\(\\mathit{M}\\). The symbol \\(\\rightleftharpoons\\) is a reminder that a matrix can contain multiple vectors, just as the symbol \\(\\rightharpoonup\\) in \\(\\vec{v}\\) reminds us that the name “\\(v\\)” refers to a vector. (It is typical in mathematical writing to use single letters to refer to vectors instead of the descriptive, multi-letter names used in data frames.)\nIn the conventions for data, we give a name to each data frame column so that we can refer to it individually. In the conventions used in vector mathematics, single letters refer to the individual vectors.\nAs a case in point, let’s look at a matrix \\(\\mathit{M}\\) containing the two vectors which we’ve previously called \\(\\vec{\\mathtt{one(t)}}\\) and \\(\\vec{\\mathtt{identity(t)}}\\): \\[\\mathit{M} \\equiv \\left[\\begin{array}{rr}1 & 0\\\\\n1 & 0.1\\\\\n1 & 0.2\\\\\n1 & 0.3\\\\\n\\vdots & \\vdots\\\\\n1 & 4.9\\\\\n1 & 5.0\\\\\n\\end{array}\\right]\\ .\\] The linear combination which we might previous have called \\(3\\cdot \\vec{\\mathtt{t}} - 2\\,\\vec{\\mathtt{identity(t)}}\\) can be thought of as\n\\[\\left[\\overbrace{\\begin{array}{r}\n1\\\\\n1 \\\\\n1 \\\\\n1 \\\\\n\\vdots &\\\\\n1 \\\\\n1\n\\end{array}}^{3 \\times}\n\\stackrel{\\begin{array}{r}\n\\\\\n\\\\\n\\\\\n\\\\\n\\\\\n\\\\\n\\\\\n\\\\\n\\end{array}}{\\Large + \\ }\n\\overbrace{\\begin{array}{r}\n0\\\\\n0.1 \\\\\n0.2 \\\\\n0.3 \\\\\n\\vdots\\\\\n4.9 \\\\\n5.0\n\\end{array}}^{-2 \\times}\\right] = \\left[\\begin{array}{r}\n\\\\ \\\\ 3\\\\\n2.8\\\\2.6\\\\2.4\\\\\\vdots\\\\-6.8\\\\-7.0\\\\ \\\\ \\\\\n\\end{array}\\right]\\ ,\\] but this is not conventional notation. Instead, we would write this more concisely as \\[\\stackrel{\\Large\\mathit{M}}{\\left[\\begin{array}{rr}1 & 0\\\\\n1 & 0.1\\\\\n1 & 0.2\\\\\n1 & 0.3\\\\\n\\vdots & \\vdots\\\\\n1 & 4.9\\\\\n1 & 5.0\\\\\n\\end{array}\\right]} \\\n\\stackrel{\\Large\\vec{w}}{\\left[\\begin{array}{r}2\\\\-3\\end{array}\\right]}\\]\nIn symbolic form, the linear combination of the columns of \\(\\mathit{M}\\) using respectively the scalars in \\(\\vec{w}\\) is simply \\(\\mathit{M} \\, \\vec{w}\\). The construction of such linear combinations is called matrix multiplication.\nNaturally, the operation only makes sense if there are as many components to \\(\\vec{w}\\) as columns in \\(\\mathit{M}\\).\n\n“Matrix multiplication” might better have been called “\\(\\mathit{M}\\) linearly combined by \\(\\vec{w}\\).” Nevertheless, “matrix multiplication” is the standard term for such linear combinations.\n\n\nIn R, make vectors with the rbind() command, short for “bind rows,” as in\n\nrbind(2, 5, -3)\n##      [,1]\n## [1,]    2\n## [2,]    5\n## [3,]   -3\n\nNote that the vector components appear as successive arguments to the rbind() function.\nCollect multiple vectors into a matrix with the cbind() command, short for “bind columns.” The arguments to cbind() will typically be vectors created by rbind(). For instance, the matrix \\[\\mathit{A} \\equiv \\left[\\vec{u}\\ \\ \\vec{v}\\right]\\ \\ \\text{where}\\ \\ \\vec{u} \\equiv \\left[\\begin{array}{r}2\\\\5\\\\-3\\end{array}\\right]\\ \\ \\text{and}\\ \\ \\vec{v} \\equiv \\left[\\begin{array}{r}1\\\\-4\\\\0\\end{array}\\right]\\] can be constructed in R with these commands.\n\nu &lt;- rbind(2, 5, -3)\nv &lt;- rbind(1, -4, 0)\nA &lt;- cbind(u, v)\nA\n##      [,1] [,2]\n## [1,]    2    1\n## [2,]    5   -4\n## [3,]   -3    0\n\nTo compute the linear combination \\(3 \\vec{u} + 1 \\vec{v}\\), that is, \\(\\mathit{A} \\cdot \\left[\\begin{array}{r}3\\\\1\\end{array}\\right]\\) you use the matrix multiplication operator %*%. For instance, the following defines a vector \\[\\vec{x} \\equiv \\left[\\begin{array}{r}3\\\\1\\end{array}\\right]\\] to do the job in a way that is easy to read:\n\nx &lt;- rbind(3, 1)\nA %*% x\n##      [,1]\n## [1,]    7\n## [2,]   11\n## [3,]   -9\n\n\n\nIt is a mistake to use * instead of %*% for matrix multiplication. Remember that * is for componentwise multiplication, which is different from matrix multiplication. Componentwise multiplication with vectors and matrices will usually give an error message as with:\n\nA * x\n## Error in A * x: non-conformable arrays\n\nThe phrase “non-conformable arrays” is R-speak for “I do not know how to do componentwise multiplication with two incompatibly shaped objects.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Linear combinations of vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/B5-linear-combinations.html#sub-spaces",
    "href": "Linear-combinations/B5-linear-combinations.html#sub-spaces",
    "title": "30  Linear combinations of vectors",
    "section": "30.6 Sub-spaces",
    "text": "30.6 Sub-spaces\nPreviously, we have said that a vector with \\(n\\) components is “embedded” in an \\(n\\)-dimensional space. Think of an embedding space as a kind of club with restricted membership. For instance, a vector with two elements is properly a member of the 2-dimensional club, but a vector with more or fewer than two elements cannot have a place in the two-dimensional club. Similarly, there are clubs for 3-component vectors, 4-component vectors, and so on.\nThe clubhouse itself is a kind of space, the space in which any and all of the vectors that are eligible for membership can be embedded.\nNow imagine the clubhouse arranged into meeting rooms. Each meeting room is just part of the clubhouse space. Which part? That depends on a set of vectors who sponsor the meeting. For instance, in the ten-dimensional clubhouse, a few members, let’s say \\(\\color{blue}{\\vec{u}}\\) and \\(\\color{magenta}{\\vec{v}}\\) decide to sponsor a meeting. That meeting room, part of the whole clubhouse space, is called a subspace.\nA subspace has its own rules for admission. Vectors belong to the subspace only if they are a linear combination of the sponsoring members. The sponsoring members define the subspace, but the subspace itself consists of an infinity of vectors: all possible vectors that amount to a linear combination of the sponsors.\nAs an example, consider the clubhouse that is open to any and all vectors with three components. The diagram in Figure 30.5 shows the clubhouse with just two members present, \\(\\color{blue}{\\vec{u}}\\) and \\(\\color{magenta}{\\vec{v}}\\).\nAny vector can individually sponsor a subspace. In Figure 30.5 the subspace sponsored by \\(\\color{blue}{\\vec{u}}\\) is the extended line through \\(\\color{blue}{\\vec{u}}\\), that is, all the possible scaled versions of \\(\\color{blue}{\\vec{u}}\\). Similarly, the subspace sponsored by \\(\\color{magenta}{\\vec{v}}\\) is the extended line through \\(\\color{magenta}{\\vec{v}}\\). Each of these subspaces is one-dimensional.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 30.5: Two vectors \\(\\color{blue}{\\vec{u}}\\) and \\(\\color{magenta}{\\vec{v}}\\) embedded in 3-dimensional space. The subspace spanned by an individual vector is shown as a line.\n\n\n\n\n\nMultiple vectors can sponsor a subspace. The subspace sponsored by both \\(\\color{blue}{\\vec{u}}\\) and \\(\\color{magenta}{\\vec{v}}\\) contains all the vectors that can be constructed as linear combinations of \\(\\color{blue}{\\vec{u}}\\) and \\(\\color{magenta}{\\vec{v}}\\); the gray subspace in Figure 30.6.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 30.6: Two vectors \\(\\vec{u}\\) and \\(\\vec{w}\\). The subspace spanned by two vectors is a plane, shown here as a gray surface.\n\n\n\n\n\nOn the other hand, the subspace sponsored by \\(\\color{magenta}{\\vec{v}}\\) and \\(\\color{blue}{\\vec{u}}\\) is not the entire clubhouse. \\(\\color{magenta}{\\vec{v}}\\) and \\(\\color{blue}{\\vec{u}}\\) lie in a common plane, but not all the vectors in the 3-dimensional clubhouse lied in that plane. In fact, if you rotate Figure 30.6 to “look down the barrel” of either \\(\\color{magenta}{\\vec{v}}\\) or \\(\\color{blue}{\\vec{u}}\\), the plane will entirely disappear from view. A subspace is an infinitesimal slice of the embedding space.\n“Sponsored a subspace” is metaphorical. In technical language, we speak of the subspace spanned by a set of vectors in the same embedding space. Usually, we refer to a “set of vectors” as a matrix. For instance, letting \\[\\mathit{M} \\equiv \\left[{\\Large \\strut}\\color{blue}{\\vec{u}}\\ \\ \\color{magenta}{\\vec{v}}\\right]\\ ,\\] the gray plane in Figure 30.6 is the subspace spanned by \\(\\mathit{M}\\) or, more concisely, \\(span(\\mathit{M})\\).\nFor a more concrete, everyday representation of the subspace spanned by two vectors, a worthwhile experiment is to pick up two pencils pointing in different directions. Place the eraser ends together, pinched between thumb and forefinger. Point the whole rigid assembly in any desired direction; the angle between them will remain the same.\nPlace a card on top of the pencils, slipping it between pressed fingers to hold it tightly in place. The card is another kind of geometrical object: a planar surface. The orientation of two vectors together determines the orientation of the surface. This simple fact will be significant later on.\nOne can replace the pencils with line segments drawn on the card underneath each pencil. Now, the angle is readily measurable in two dimensions. The angle between two vectors in three dimensions is the same as the angle drawn on the two-dimension surface that rests on the vectors.\nNotice that one can also lay a card along a single vector. What is different here is that the card can be rolled around the pencil while still staying in contact; there are many different orientations for such a card even while the vector stays fixed. So a single fixed vector does not uniquely determine the orientation of the planar surface in which the vector can reside. It takes two vectors to determine a unique planar surface.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Linear combinations of vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/B5-linear-combinations.html#exercises",
    "href": "Linear-combinations/B5-linear-combinations.html#exercises",
    "title": "30  Linear combinations of vectors",
    "section": "30.7 Exercises",
    "text": "30.7 Exercises\nProblem with NA NA",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Linear combinations of vectors</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MOSAIC Calculus",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#welcome-to-calculus",
    "href": "index.html#welcome-to-calculus",
    "title": "MOSAIC Calculus",
    "section": "Welcome to calculus",
    "text": "Welcome to calculus\nCalculus is the set of concepts and techniques that form the mathematical basis for dealing with motion, growth, decay, and oscillation. The phenomena can be as simple as a ball arcing ballistically through the air or as complex as turbulent airflow over a wing generating lift. Calculus is used in biology and business, chemistry, physics and engineering. It is the foundation for weather prediction and understanding climate change. It is the basis for the algorithms for heart rate and blood oxygen measurement by wristwatches. It is a key part of the language of science. The electron orbitals of chemistry, the stresses of bones and beams, and the business cycle of recession and rebound are all understood primarily through calculus.\nCalculus has been central to science from the very beginnings. It is no coincidence that the scientific method was introduced and the language of calculus was invented by the same small group of people during the historical period known as the Enlightenment in the late 17th century. Learning calculus has always been a badge of honor and an entry ticket to professions. Millions of students’ career ambitions have been enhanced by passing a calculus course or thwarted by lack of access to one.\nIn the 1880s, a hit musical featured “the very model of a modern major general.” One of his claims for modernity: “I’m very good at integral and differential calculus.”\n\n\n\n\n\n\nVideo 1: A Gilbert and Sullivan opera pays homage to Calculus.\n\n\n\nWhat was modern in 1880 is not modern anymore. Yet, amazingly, calculus today is every bit as central to science and technology as it ever. Indeed, calculus remains central to fields that were not even imagined in 1880, such as AI, logistics, economics, and data science. One reason is that science, engineering, and society have now fully adopted the computer for almost all aspects of work, study, and life. The collection and use of data is growing dramatically. Machine learning has become the way human decision makers interact with such data.\nThink about what it means to become “computerized.” To take an everyday example, consider video. Over the span of a human life, we moved from a system which involved people going to theaters to watch the shadows recorded on cellulose film to the distribution over the airwaves by low-resolution television, to the introduction of high-def broadcast video, to on demand streaming from huge libraries of movies. Just about anyone can record, edit, and distribute their own video. The range of topics (including calculus) on which you can access a video tutorial or demonstration is incredibly vast. All of this recent progress is owed to computers.\nThe “stuff” on which computers operate, transform, and transmit is always mathematical representations stored as bits. The creation of mathematical representations of objects and events in the real world is essential to every task of any sort that any computer performs. Calculus is a key component of inventing and using such representations.\nYou may be scratching your head. If calculus is so important, why is it that many of your friends who took calculus came away wondering what it is for? What’s so important about “slopes” and “areas” and how come your high-school teacher might have had trouble telling you what calculus is for?\nThe disconnect between the enthusiasm expressed in the preceding paragraphs and the lived experience of students is very real. There are two major reasons for that disconnect, both of which we tackle head-on in this book.\nFirst, teachers of mathematics have a deep respect for tradition. Such respect has its merits, but the result is that almost all calculus is taught using methods that were appropriate for the era of paper and pencil—not for the computer era. As you will see, in this book we express the concepts of calculus in a way that carries directly over to the uses of calculus on computers and in working out answers to real-world problems.\nSecond, the uses of calculus are enabled not by the topics of Calc I and Calc II alone, but the courses for which Calc I/II are preliminary: linear algebra and dynamics. Only a small fraction of students who start in Calc I ever reach the parts of calculus that are the most useful. Fortunately, there is a large amount of bloat and rote in the standard textbook topics of Calc I/II. This can be removed to make room for the more important topics.\nThe computer language used in this book is R. This is a mainstream language in high demand by employers in many fields. The small amount of R that you need to learn for this book will open doors to much greater possibilities. We have augmented R with functions designed to simplify access to calculus-related computations. These functions are provided by the {mosaicCalc} R package and and build on other widely used, open-source software for R. We will often refer to the software as a whole as “R/mosaic.”\nFor convenience, we placed “Active R Chunks” within the text. Often, as in Active R chunk 1, these will be presented with working R/mosaic commands already included. Just press “Run Code” to have the commands evaluated.\n\n\n\nActive R chunk 1\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nSometimes, you will be asked to make modifications to the commands in a chunk so that you can see for yourself how the output changes.\n\n\n\n\n\n\nAccessing software\n\n\n\nThis book, as well as the exercises in the MOSAIC Calculus Workbook, provide immediate access to R/mosaic software via interactive elements such as Active R chunk 1 that run in your browser.\nThere are also versions of R that are provided as stand-alone applications or web services. The most popular of these, RStudio, includes rich facilities for managing large projects and constructing complex documents. (This book is written using RStudio and the quarto document system.)\nFor most students learning Calculus with MOSAIC Calculus, the run-in-your-browser version of R will be sufficient. (The same applies to statistics students working with Lessons in Statistical Thinking.) Instructors may prefer to work within the RStudio document system. Some instructors even introduce all their students to RStudio for writing reports, etc. Wherever you run the software—in-the-browser or using RStudio—it’s all the same language and works the same way.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#instructors-preface",
    "href": "index.html#instructors-preface",
    "title": "MOSAIC Calculus",
    "section": "Instructor’s preface",
    "text": "Instructor’s preface\nThe “MOSAIC” in the book’s title refers to a movement in undergraduate mathematics to integrate Modeling, Statistics and data science, Computing, and Calculus. Skill in all these areas is needed for successful work in the technical and scientific world. Traditionally, teaching does not honor the strong links among these areas and ignores the advantages of teaching them in a unified way. Indeed, modeling, though often mentioned in calculus textbook blurbs, is hardly taught at all. Introductory statistics courses, even those with a formal pre-requisite of calculus, do not draw on calculus concepts beyond the mention of “area under a curve.” Few and far between are introductory computing courses that reinforce calculus and statistics topics, or calculus courses that develop and build on computing and data skills.\nThe isolation of mathematical calculus from disciplines that ought to be considered allies leads to a devastating gap in the education of students. The half-life of a student in a calculus sequence is, roughly speaking, one course. (The decay in participation starts in algebra and trigonometry.) The result is that only a small fraction of students see relevant mathematics of multiple variables and hardly any encounter the powerful concepts of spaces, vectors, and matrix factorizations traditionally reserved for a junior-year linear algebra course. Contemporary statistical modeling techniques, including machine learning, draw heavily on a small core of linear-algebra topics.\nThe design of the mosaic serves the goal of providing for all students access to a broad, directly useful education in the mathematical sciences. One component of access is keeping the program small enough to fit in with a plausible student schedule. Our working definition of “small enough” is one-quarter of the first two years of university. Fitting within this envelope, students ought to achieve basic competence in computing, statistics, and calculus.\nCalculus provides opportunities to prepare students well for both modern statistics and computing. The most direct ties to statistics are modeling, functions of multiple variables, and concepts of linear spaces. The relevant links of calculus to computing are extensive and go both ways. To give but one example: It’s impractical for students to construct contour plots without a computer. With a computer at hand, students can learn about mathematical ideas such as gradients and iterative optimization. For instance, constrained optimization is hardly a topic of standard introductory calculus and Lagrange multipliers are mysterious even to many professors. The essential concepts become much clearer when they can be presented using graphical tools of contours and gradient fields.\nA book that attempts to build on the connections among historically distinct disciplines must also resolve inconsistencies in nomenclature and notation. We draw the instructor’s attention to some of these and the policies adopted in the book:\n\nVariable. In mathematics “variable” is used generically, sometimes meaning “unknown” and other times referring to a known quantity. In computing “variable” is used colloquially to mean “the name of an object,” and in statistics a “variable” refers to data: a column of a data frame or, more generally, a specific attribute of the units of observation that form the rows of the data frame. (See 7.1 Data frames for definitions of “data frame” and “unit of observation.”) We reserve “variable” to be used in the statistical sense. Consequently, a “function of several variables” becomes a “function with several inputs.”\nOutput. Evaluating a function, either a mathematically or on a computer, produces an output. Functions take inputs and produce outputs. Typical names for inputs are, following tradition, the last few letters of the alphabet: \\(t, u, v, w, x, y, z\\).\nFunction names. Functions always have a name. \\(f()\\), \\(g()\\), \\(h()\\) are the pronouns for discussing functions in general, but in specific applications functions often have more descriptive names, e.g. population() or elev() or risk(). The empty parentheses are a reminder that the thing being named is a function and not an input or parameter. Since \\(y\\) is used as an input name, we never use it for the name of a function or to identify the output of a function. So, \\(y=mx+b\\) is not an esteemed phrase in this book. Instead, when we want to define a straight-line function we write \\(g(x) \\equiv a x + b\\) or some other parameterization, for instance \\(g(x) \\equiv a (x - x_0)\\).\nSpecial inputs. Often, a problem or application context requires the identification of some special values for inputs to a function, for instance, argmaxes or zero crossings or starting time. These are often constructed by using an output name (often \\(t\\) through \\(z\\)) with a subscript or a non-numerical superscript as in \\(y^\\star\\).\nOutput (part 2). When we mean something like “the output of the function \\(f()\\) at its argmax,” we write \\(f(x_\\star)\\), or something similar. When we mean, “the output of function \\(f()\\) at some as yet unspecified input named \\(x\\),” we write \\(f(x)\\).\nFormulas. An expression like \\(ax + b\\) is a formula. One of the most common ways to define a function is by using a formula. But creating a function from a formula requires some special syntax, as demonstrated earlier with \\(g(x) \\equiv a x + b\\). The names used within the parentheses on the left side of \\(\\equiv\\) are the input names. Other symbols in the formula are called parameters.\nTilde expression. We use the R language in this book. Those familiar with R know that there is a special kind of expression called a “formula,” for instance a*x + b ~ x. One of the main uses for R formulas is to represent a mathematical formula when creating a function. “Formulas representing formulas” can lead to confusion. We address this by violating the technical vocabulary of R and calling an expression like a*x + b ~ x a “tilde expression.” This name properly draws attention to the ~ (“tilde”) character that is an essential component of R-language “formulas.” A typical use for a tilde expression is to create a computer version of a function. The computer version of \\(g(x) \\equiv a x + b\\) is g &lt;- makeFun(a*x + b ~ x). In this use, the ~ x part of the tilde expression identifies the input name, just as does the \\(x\\) in \\(g(x) \\equiv ...\\). You’ll also use tilde expressions for graphics and operations such as differentiation and anti-differentiation.\n\nThose familiar with R may be tempted to use the native function-building syntax, which looks like\n\ng &lt;- function(x, a, b) {\n  a*x + b\n}\n\nWe strongly encourage you to use makeFun() instead. One reason is to reinforce the use of tilde expressions which are needed to identify the “with-respect-to” input in differentiation and anti-differentiation, as well as the frame of a graph. Another reason has to do with rules of scoping in computer languages, which have no obvious analog in mathematical notation. We prefer to leave scoping to a computer science class, rather than making it a pre-requisite for calculus. makeFun() sidesteps the scoping difficulties.\n\nDifferential notation. Historically, Leibniz’s lovely ratio notation, for instance, \\[\\frac{dy}{dx}\\ ,\\] helped generations of students learn differential calculus and see the connections to integral calculus. It is, however, wordy, which is why other notations—\\(f'\\) or \\(\\dot{x}\\) or \\(f^{(1)}\\) so often appear. But Leibniz could hardly have anticipated a future in which writing is done mainly with keyboards and linear sequences of characters. There is no mainstream computer language in which df/dx or f' or \\dot{x} or f^{(1)} are valid names. To simplify the use of the computer, we use \\(\\partial_x y\\) notation for differentiation. This can be easily morphed into a legal computer name: dx_y. We use \\(\\partial\\) instead of the Latin \\(d\\), partly to mark differentiation as something special and partly because we will use notation like \\(\\partial_{xt} g\\) when dealing with functions of multiple variables, or \\(\\partial_{xx} f\\) for second derivatives.\n\nThe book is designed to support six to ten credit hours of calculus study. The algebra pre-requisites are kept to a minimum. Trigonometric identities are never used. In the starting “Preliminaries” part of the book nine “pattern-book” algebraic functions are introduced that form the basis for modeling work. “Preliminaries” also introduces computing notation, particularly that used for graphing functions.\n“Modeling,” Block I, introduces non-calculus topics that are essential to the rest of the book. It is worth spending considerable class time on Block I, regardless of the previous experience of students. Block II, “Differentiation,” is self-explanatory to a calculus instructor. Absence of extensive drill on symbolic differentiation of obscure functions or the use of Taylor polynomials or l’Hopital’s rule to provide even more drill is entirely intentional. Our goal in MOSAIC Calculus is to ensure that students understand what a derivative is and—importantly—what it is for. Symbolic differentiation is limited to the “pattern-book” functions and other functions constructed as compositions, products, and linear combinations of the pattern-book function.\nBlock III, “Vectors and linear combinations,” does not depend on previously covering differentiation. The sequence Preliminaries-Modeling-Vectors could make a suitable 3- or 4-credit course for students entering data science. Block III might have been reasonably titled “Linear algebra.” But the universal emphasis on determinants and inverses of square matrices in a conventional linear algebra course is not a suitable introduction for working with data, and we did not want to suggest that all the conventional topics of linear algebra are included.\nBlock IV, “Accumulation,” builds on Block II, where differentiation is treated less as an algebraic process than as a relationship between functions. Our focus is on occasions when anti-differentiation is a useful modeling tool for extracting certain forms of information from a function. Instructors are advised to minimize or wholly avoid the area-under-a-curve metaphor. Like cigarettes, that metaphor is addictive and creates dis-ease with the more important roles for accumulation in contexts like dynamics. The final chapter of Block IV is about symbolic integration of functions constructed from from the pattern-book functions. Including it is a concession to administrative practices at universities where topics like “integration by parts” are included in hard-to-change course-catalog copy. Those techniques are not used elsewhere in the book.\nBlock V, “Dynamics,” introduces systems, that is, wholes made of multiple connected parts. Although the context used is differential equations, techniques for finding solutions are not central. More important are the phenomena (e.g. oscillation), the opportunities for modeling and showing how simple mathematical models can provide insight to otherwise seemingly complex natural and social systems, extension of the linear-algebra material from Block III to eigenvalues and eigenvectors, and a glimpse at the surprisingly close connection between exponentials and sinusoids.\nThe last block, “Manifestations,” is arranged along different lines than the previous chapters. The point is to show how calculus operations show themselves in a wide variety of contexts. We are not making up opportunities for more drill in symbolic differentiation and anti-differentiation. In the “Probability” chapter, to give an example, what’s central is the relationships between functions. One of these relationships is standard in calculus books, that between what’s called the CDF and the PDF. Less basic and not at all standard, the relationship between prior, likelihood, and posterior functions. It is not required to cover all the chapters in “Manifestations.” They do not much depend one on the other. Choose the ones that are best suited to the directions in which your students are heading. And the topics need not be delayed to the end of the course. For instance, the constrained optimization topic in “Optimization” can be handled with the material up to Block IV.\nMany highly expert calculus instructors have taught with these materials. Some of their experiences may be relevant to instructors who haven’t yet used MOSAIC Calculus. First, the experts are surprised by how many esteemed, traditional calculus topics are given short shrift or even omitted altogether, and even more surprised that their exclusion does not diminish the course. Second, many experts find that the calculus in this book is not calculus as they have been trained to think about it, but that nonetheless “it works.” Third, instructors who try to avoid spending class time on the computational elements of the book find that their students echo the avoidance. As these instructors go through their first year, they discover that there are only a handful of computational patterns (e.g. tilde expressions, domains) and that students would have avoided many headaches by facing them head-on from the start of the course. Some advice: Don’t think that you have to learn R before you can master using R to teach this book. The mosaic software that powers this book is a better place to start than with the many more general introductions and tutorials on R computing available in printed and video form.\nMost universally, even the expert instructors find they are unfamiliar with tranches of the material. Leading examples are dimensions of measurement, splines, mechanics (e.g. torque), the uses of orthogonalization, and, broadly, dynamics. Teaching unfamiliar material is admittedly stressful, but highly beneficial to yourself and your students. Dimensions and units, in particular, are a great guide to thinking; take every opportunity to ask your class what are the dimensions of the inputs to and outputs from a function and whether an operation makes sense in terms of dimensions. Consistently, even tradition-minded experts find that thinking about physical dimension gives them unexpected insight into the tasks and methods of calculus.\n\nDaniel Kaplan, Saint Paul, Minnesota, October 2024",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledements",
    "href": "index.html#acknowledements",
    "title": "MOSAIC Calculus",
    "section": "Acknowledements",
    "text": "Acknowledements\nThis project was initiated by the Mathematical Sciences department at the US Air Force Academy. They recognized that a traditional calculus introduction is ill-suited to the needs of STEM in the 21st century.\nCritical support was given by the ARDI Foundation which awarded the Holland H. Coors Chair in Education Technology to one of the project members, Daniel Kaplan. This made possible a year-long residency at USAFA during which time he was able to work unhindered on this project.\nMacalester College, where Kaplan is DeWitt Wallace Professor of Mathematics, Statistics, and Computer science, was the site where the overall framework and many of the materials for a STEM-oriented calculus were developed. Particularly important in the germination were David Bressoud and Jan Serie, respectively chairs of the Macalester math and biology departments, as well as Prof. Thomas Halverson and Prof. Karen Saxe, who volunteered to team teach with Kaplan the first prototype course. Early grant support from the Howard Hughes Medical Foundation and the Keck Foundation provided the resources to carry the prototype course to a point of development where it became the entryway to calculus for Macalester students.\nProfs. Randall Pruim (Calvin University) and Nicholas Horton (Amherst College) were essential collaborators in developing software to support calculus in R. They and Kaplan formed the core team of Project MOSAIC, which was supported by the US National Science Foundation (NSF DUE-0920350).\nJoel Kilty and Alex McAllister at Centre College admired the Macalester course and devoted much work and ingenuity to write a textbook, Mathematical Modeling and Applied Calculus (Oxford Univ. Press), implementing their own version. Their textbook enabled us to reduce the use of sketchy notes in the first offering of this course at USAFA.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "linear-combinations-part.html",
    "href": "linear-combinations-part.html",
    "title": "BLOCK III. Vectors and linear combinations",
    "section": "",
    "text": "Often, quantities are made out of multiple components. For example, the location of an object in space has \\(x\\), \\(y\\), and \\(z\\) components. An important mathematical strategy for working with multiple components relates to the ideas of a vector and combinations of vectors as well as a set of vectors involved in a combination. (The set of vectors is called a matrix.)\nVectors appear naturally in physics: position, velocity, acceleration. They are also a principal building block of algorithms for machine learning, data science, and statistical modeling.\nThis Block introduces the basics of vectors and operations on vectors. There is a broad mathematical subject called “linear algebra” of which vectors and matrices are a part. Here, we focus on a compact set of ideas that are of particular importance in modeling and statistics.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations"
    ]
  },
  {
    "objectID": "accumulation-part.html",
    "href": "accumulation-part.html",
    "title": "BLOCK IV. Accumulation",
    "section": "",
    "text": "We have already studied the relationship between functions and their derivatives. In this Block, we will examine techniques that exploit that relationship to re-construct a function from knowledge of its derivative by a process called anti-differentiation.\nJust as derivatives tell about rates of change, anti-derivatives tell about the accumulation of change.",
    "crumbs": [
      "BLOCK IV. Accumulation"
    ]
  },
  {
    "objectID": "manifestations-part.html",
    "href": "manifestations-part.html",
    "title": "BLOCK VI. Manifestations",
    "section": "",
    "text": "The ideas of calculus are used throughout science and technology. Previous blocks have introduced the mathematical ideas themselves, often illustrated with examples of real-world systems. In this Block, we will explore some of the ways calculus concepts are manifested in different fields and for different uses. Of course, each reader may find some fields more interesting than others. Think of this Block as a sampler containing a handful of the myriad ways that calculus concepts are used.",
    "crumbs": [
      "BLOCK VI. Manifestations"
    ]
  },
  {
    "objectID": "Preliminaries/05-pattern-book-functions.html",
    "href": "Preliminaries/05-pattern-book-functions.html",
    "title": "5  Pattern-book functions",
    "section": "",
    "text": "5.1 Exponential and logarithm functions\nIt’s a common English phrase to say that something is “growing exponentially.” For instance, at the outbreak of an epidemic there are few cases and slow growth in the number of cases. But as the number of cases increases, the growth gets faster.\nThere is another way to think about exponential growth that makes it easier to comprehend. Again, imagine an epidemic in its eary stages. Suppose there are 20 cases. Time goes by and eventually the number of cases reaches 40, double the initial value. Suppose it takes one week for this doubling. That is, the doubling time is one week.\nNow a second week goes by. If the growth were proportional to the input, after the second week the case number would be 60. For exponential growth, by the end of the second week the output will double. So, there will be 80 cases after two weeks.\nAt the end of the third week the output of the exponential will double again, to 160 cases. Fourth week: 320 cases. Fifth week: 640 cases. To summarize, Table 5.2 shows the number of cases as a function of weeks elapsed.\nNote: You might speculate about where the epidemic will go in the long run. Most things that grow exponentially do so only for a short time. After that, other things come into play that limit or reverse the growth. So, over long periods of time the pattern can be better described as gaussian or sigmoidal.\nI suspect that most people are comfortable with the idea of exponential growth. But many people, even highly-educated professionals, react with fear to the word “logarithmic.” [This fear might originate in the way logarithms are typically taught in high-school is heavily algebraic and taken out of any meaningful contemporary context. Logarithms were invented around 1600 to facilitate by-hand mathematical operations like multiplication and exponentiation. Nowadays, we have computers to perform such calculations.] As we’ll see in Chapter 14, logarithms are an important tool for understanding quantities that can range from very, very small to very, very large. Table 5.2 gives an example: the left column is a logarithm of the right column.\nThe exponential and logarithmic functions are intimate companions. You can see the relationship by taking the graph of the logarithm, and rotating it 90 degrees, then flipping left for right as in Figure 5.11. (Note in Figure 5.11 that the graph is shown as if it were printed on a transparency which we are looking at from the back.)",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Pattern-book functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/05-pattern-book-functions.html#exponential-and-logarithm-functions",
    "href": "Preliminaries/05-pattern-book-functions.html#exponential-and-logarithm-functions",
    "title": "5  Pattern-book functions",
    "section": "",
    "text": "The exponential function has important applications throughout science, technology, and the economy. For large negative inputs, the value is very close to zero in much the same way as for the Gaussian or sigmoid functions. But the output increases faster and faster as the input gets bigger. Note that the output of the exponential function is never negative for any input.\n\n\n\n\n\n\n\n\n\nFigure 5.9: The output of the exponential function grows faster and faster as the input increases.\n\n\n\n\n\n\n\n\n\n\n\nTable 5.2: The number of cases as it increases over time for an epidemic that grows exponentially with a doubling time of one week..\n\n\n\n\n\nTime (weeks)\nNumber of cases\n\n\n\n\n0 (initial time)\n20\n\n\n1\n40\n\n\n2\n80\n\n\n3\n160\n\n\n4\n320\n\n\n5\n640\n\n\n6\n1280\n\n\n…\n…\n\n\n10\n20,480\n\n\n…\n…\n\n\n15 weeks\n655,360\n\n\n\n\n\n\n\n\n\n\nThe logarithmic function is defined only for positive inputs. As the input increase from just above zero, the output t constantly grows but at a slower and slower rate. It never levels out.\n\n\n\n\n\n\n\n\n\nFigure 5.10: The domain of the logarithm is the positive numbers.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.11: A flipped-over version of Figure 5.10. The exponential and the logarithmic functions are twins, related by reversing the roles of the input and output. The flipped over logarithm function is the same as the exponential function.\n\n\n\n\n\n\n\n\n\nWhy is it called the “logarithm?”\n\n\n\nThe name “logarithm” is anything but descriptive. The name was coined by the inventor, John Napier (1550-1617), to emphasize the original purpose of his invention: to simplify the work of multiplication and exponentiation. The name comes from the Greek words logos, meaning “reasoning” or “reckoning,” and arithmos, meaning “number.” A catchy marketing term for the new invention, at least for those who speak Greek!\nAlthough invented for the practical work of numerical calculation, the logarithm function has become central to mathematical theory as well as modern disciplines such as thermodynamics and information theory. The logarithm is key to the measurement of information and magnitude. As you know, there are units of information used particularly to describe the information storage capacity of computers: bits, bytes, megabytes, gigabytes, and so on. Very much in the way that there are different units for length (cm, meter, kilometer, inch, mile, …), there are different units for information and magnitude. For almost everything that is measured, we speak of the “units” of measurement. For logarithms, instead of “units,” by tradition another word is used: the base of the logarithm. The most common units outside of theoretical mathematics are base-2 (“bit”) and base-10 (“decade”). But the unit that is most convenient in mathematical notation is “base e,” where \\(e = 2.71828182845905...\\). This is genuinely a good choice for the units of the logarithm, but that is hardly obvious to anyone encountering it for the first time. To make the choice more palatable, it is marketed as the “base of the natural logarithm.” In this book, we will be using this natural logarithm as our official pattern-book logarithm.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Pattern-book functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/05-pattern-book-functions.html#sec-power-law-family",
    "href": "Preliminaries/05-pattern-book-functions.html#sec-power-law-family",
    "title": "5  Pattern-book functions",
    "section": "5.2 The power-law family (optional)",
    "text": "5.2 The power-law family (optional)\nFour of the pattern-book functions—\\(1\\), \\(1/x\\), \\(x\\), \\(x^2\\)— belong to an infinite family called the power-law functions. Some other examples of power-law functions are \\(x^3, x^4, \\ldots\\) as well as \\(x^{1/2}\\) (also written \\(\\sqrt{x}\\)), \\(x^{1.36}\\), and so on. Some of these also have special (albeit less frequently used) names, but all of the power-law functions can be written as \\(x^p\\), where \\(x\\) is the input and \\(p\\) is a number.\nYou have been using power-law functions from early in your math and science education. Some examples:1\n\n\n\nTable 5.3: Examples of power-law relationships\n\n\n\n\n\n\n\n\n\n\nSetting\nFunction formula\nexponent\n\n\n\n\nCircumference of a circle\n\\(C(r) = 2 \\pi r\\)\n1\n\n\nArea of a circle\n\\(A(r) = \\pi r^2\\)\n2\n\n\nVolume of a sphere\n\\(V(r) = \\frac{4}{3} \\pi r^3\\)\n3\n\n\nDistance traveled by a falling object\n\\(d(t) = \\frac{1}{2} g t^2\\)\n2\n\n\nGas pressure versus volume\n\\(P(V) = \\frac{n R T}{V}\\)\n\\(-1\\)\n\n\n… perhaps less familiar …\n\n\n\n\nDistance traveled by a diffusing gas\n\\(X(t) = D \\sqrt{\n\\strut t}\\)\n\\(1/2\\)\n\n\nAnimal lifespan (in the wild) versus body mass\n\\(L(M) = a M^{0.25}\\)\n0.25\n\n\nBlood flow versus body mass\n\\(F(M) = b M^{0.75}\\)\n0.75\n\n\n\n\n\n\nOne reason why power-law functions are so important in science has to do with the logic of physical quantities such as length, mass, time, area, volume, force, power, and so on. We introduced this topic in Chapter 1 and will return to it in more detail in Chapter 15.\nWithin the power-law family, it is helpful to know and be able to distinguish between several groups:\n\nThe monomials. These are power-law functions such as \\(m_0(x) \\equiv x^0\\), \\(m_1(x) \\equiv x^1\\), \\(m_2(x) \\equiv x^2\\), \\(\\ldots\\), \\(m_p(x) \\equiv x^p\\), \\(\\ldots\\), where \\(p\\) is a whole number (i.e., a non-negative integer). Of course, \\(m_0()\\) is the same as the constant function, since \\(x^0 = 1\\). Likewise, \\(m_1(x)\\) is the same as the identity function since \\(x^1 = x\\). As for the rest, they have just two general shapes: both arms up for even powers of \\(p\\) (like in \\(x^2\\), a parabola); one arm up and the other down for odd powers of \\(p\\) (like in \\(x^3\\), a cubic). Indeed, you can see in Figure 5.12 that \\(x^4\\) has a similar shape to \\(x^2\\) and that \\(x^5\\) is similar in shape to \\(x^3\\). For this reason, high-order monomials are rarely needed in practice.\n\n\n\n\n\n\n\n\n\n\n\\(x^0\\), that is, 1\n\n\n\n\n\n\n\n\n\n\\(x^1\\), that is, \\(x\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(x^2\\)\n\n\n\n\n\n\n\n\n\n\\(x^3\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(x^4\\)\n\n\n\n\n\n\n\n\n\n\\(x^5\\)\n\n\n\n\n\n\n\nFigure 5.12: The first six monomial functions: \\(x^0\\), \\(x^1\\), \\(x^2\\), \\(x^3\\), \\(x^4\\), and \\(x^5\\). In each plot the dashed \\(\\color{magenta}{\\text{magenta}}\\) line marks zero output.\n\n\n\n\nThe negative powers. These are power-law functions where \\(p&lt;0\\), such as \\(f(x) \\equiv x^{-1}\\), \\(g(x) \\equiv x^{-2}\\), \\(h(x) \\equiv x^{-1.5}\\). For negative powers, the size of the output is inversely proportional to the size of the input. In other words, when the input is large (not close to zero) the output is small, and when the input is small (close to zero), the output is very large. This behavior happens because a negative exponent like \\(x^{-2}\\) can be rewritten as \\(\\frac{1}{x^2}\\); the input is inverted and becomes the denominator, hence the term “inversely proportional”.\n\n\n\n\n\n\n\n\n\n\n\\(x^{-1}\\), that is, \\(1/x\\)\n\n\n\n\n\n\n\n\n\n\\(x^{-2}\\), that is, \\(1/x^2\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(x^{-3}\\), that is, \\(1/x^3\\)\n\n\n\n\n\n\n\n\n\n\\(x^{-4}\\), that is, \\(1/x^4\\)\n\n\n\n\n\n\n\nFigure 5.13: Graphs of power-law functions with negative integer exponents. The arrows point to the output being very large when \\(x\\) is near zero.\n\n\n\n\nThe non-integer powers, e.g. \\(f(x) = \\sqrt{x}\\), \\(g(x) = x^\\pi\\), and so on. When \\(p\\) is either a fraction or an irrational number (like \\(\\pi\\)), the real-valued power-law function \\(x^p\\) can only take non-negative numbers as input. In other words, the domain of \\(x^p\\) is \\(0\\) to \\(\\infty\\) when \\(p\\) is not an integer. You have likely already encountered this domain restriction when using the power law with \\(p=\\frac{1}{2}\\) since \\(f(x)\\equiv x^{1/2}=\\sqrt{x}\\), and the square root of a negative number is not a real number. You may have heard about the imaginary numbers that allow you to take the square root of a negative number, but for the moment, you only need to understand that when working with real-valued power-law functions with non-integer exponents, the input must be non-negative. (The story is a bit more complicated since, algebraically, rational exponents like \\(1/3\\) or \\(1/5\\) with an odd-valued denominator can be applied to negative numbers. Computer arithmetic, however, does not recognize these exceptions.)\n\n\n\n\n\n\n\n\n\n\n\\(x^{1/2}\\)\n\n\n\n\n\n\n\n\n\n\\(x^\\pi\\)\n\n\n\n\n\n\n\nFigure 5.14: The domain of power-law functions with non-integer power is \\(0 \\leq x &lt; \\infty\\).\n\n\n\n\n\n\n\n\n\nComputing power-law functions\n\n\n\nWhen a function like \\(\\sqrt[3]{x}\\) is written as \\(x^{1/3}\\) make sure to include the exponent in grouping parentheses: x^(1/3). Similarly, later in the book you will encounter power-law functions where the exponent is written as a formula. Particularly common will be power-law functions written \\(x^{n-1}\\) or \\(x^{n+1}\\). In translating this to computer notation, make sure to put the formula within grouping parentheses, for instance x^(n-1) or x^(n+1).",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Pattern-book functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/05-pattern-book-functions.html#note-computing-power-law-functions",
    "href": "Preliminaries/05-pattern-book-functions.html#note-computing-power-law-functions",
    "title": "5  Pattern-book functions",
    "section": "5.3 Note: Computing power-law functions",
    "text": "5.3 Note: Computing power-law functions\nWhen a function like \\(\\sqrt[3]{x}\\) is written as \\(x^{1/3}\\) make sure to include the exponent in grouping parentheses: x^(1/3). Similarly, later in the book you will encounter power-law functions where the exponent is written as a formula. Particularly common will be power-law functions written \\(x^{n-1}\\) or \\(x^{n+1}\\). In translating this to computer notation, make sure to put the formula within grouping parentheses, for instance x^(n-1) or x^(n+1).",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Pattern-book functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/05-pattern-book-functions.html#domains-of-pattern-book-functions",
    "href": "Preliminaries/05-pattern-book-functions.html#domains-of-pattern-book-functions",
    "title": "5  Pattern-book functions",
    "section": "5.3 Domains of pattern-book functions",
    "text": "5.3 Domains of pattern-book functions\nEach of our basic modeling functions, with two exceptions, has a domain that is the entire number line \\(-\\infty &lt; x &lt; \\infty\\). No matter how big or small is the value of the input, the function has an output. Such functions are particularly nice since we never have to worry about the input going out of bounds.\nThe two exceptions are:\n\nthe logarithm function, which is defined only for \\(0 &lt; x\\).\nsome of the power-law functions: \\(x^p\\).\n\nWhen \\(p\\) is negative, the output of the function is undefined when \\(x=0\\). You can see why with a simple example: \\(g(x) \\equiv x^{-2}\\). Most students had it drilled into them that “division by zero is illegal,” and \\(g(0) = \\frac{1}{0} \\frac{1}{0}\\), a double law breaker.\nWhen \\(p\\) is not an integer, that is \\(p \\neq 1, 2, 3, \\cdots\\) the domain of the power-law function does not include negative inputs. To see why, consider the function \\(h(x) \\equiv x^{1/3}\\).\n\n\n\n\n\n\n\n\nExponentials are not power-laws\n\n\n\nIt is essential that you recognize that the exponential function is utterly different from the functions from the power-law family.\nAn exponential function, for instance, \\(e^x\\) or \\(2^x\\) or \\(10^x\\) has a constant quantity raised to a power set by the input to the function.\nA power-law function works the reverse way: the input is raised to a constant quantity, as in \\(x^2\\) or \\(x^10\\).\nA mnemonic phrase for exponentials functions is\n\nExponential functions have \\(x\\) in the exponent.\n\nOf course, the exponential function can have inputs with names other than \\(x\\), for instance, \\(f(y) \\equiv 2^y\\), but the name “x” makes for a nice alliteration in the mnemonic.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Pattern-book functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/05-pattern-book-functions.html#footnotes",
    "href": "Preliminaries/05-pattern-book-functions.html#footnotes",
    "title": "5  Pattern-book functions",
    "section": "",
    "text": "The animal lifespan relationship is true when comparing species. Individual-to-individual variation within a species does not follow this pattern.↩︎",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Pattern-book functions</span>"
    ]
  },
  {
    "objectID": "Accumulation/33-intro.html",
    "href": "Accumulation/33-intro.html",
    "title": "34  Change & accumulation",
    "section": "",
    "text": "34.1 Accumulation\nImagine a simple setting: water flowing out of a tap into a basin or tank. The amount of water in the basin will be measured in a unit of volume, say liters. Measurement of the flow \\(f(t)\\) of water from the tap into the tank has different units, say liters per second. If volume \\(V(t)\\) is the volume of water in the tank as a function of time, \\(f(t)\\) at any instant is \\(f(t) = \\partial_t V(t)\\).\nThere is a relationship between the two functions \\(f(t)\\) and \\(V(t)\\). With derivatives, we can give a good description of that relationship: \\[f(t) = \\partial_t V(t)\\] This description will be informative if we have measured the volume of water in the basin as a function of time and want to deduce the rate of flow from the tap. Now suppose we have measured the flow \\(f(t)\\) and want to figure out the volume. The volume at any instant is the past flow accumulated to that instant. As a matter of notation, we write this view of the relationship as \\[V(t) = \\int f(t) dt,\\] which you can read as “volume is the accumulated flow.”\nOther examples of accumulation and change:",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Change & accumulation</span>"
    ]
  },
  {
    "objectID": "Accumulation/33-intro.html#accumulation",
    "href": "Accumulation/33-intro.html#accumulation",
    "title": "34  Change & accumulation",
    "section": "",
    "text": "velocity is the rate of change of position with respect to time. Likewise, position is the accumulation of velocity over time.\nforce is the rate of energy with respect to position. Likewise energy is the accumulation of force as position changes.\ndeficit is the rate of change of debt with respect to time. Likewise, debt is the accumulation of deficit over time.",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Change & accumulation</span>"
    ]
  },
  {
    "objectID": "Accumulation/33-intro.html#notation-for-anti-differentiation",
    "href": "Accumulation/33-intro.html#notation-for-anti-differentiation",
    "title": "34  Change & accumulation",
    "section": "34.2 Notation for anti-differentiation",
    "text": "34.2 Notation for anti-differentiation\nFor differentiation we are using the notation \\(\\partial_x\\) as in \\(\\partial_x f(x)\\). Remember that the subscript on \\(\\partial\\) names the with-respect-to input. There are three pieces of information this notation:\n\nThe \\(\\color{magenta}{\\partial}\\) symbol which identifies the operation as partial differentiation.\nThe name of the with-respect-to input \\(\\partial_{\\color{magenta}{x}}\\) written as a subscript to \\(\\partial\\).\nThe function to be differentiated, \\(\\partial_x \\color{magenta}{f(x)}\\).\n\nFor anti-differentiation, our notation must also specify the three pieces of information. It might be tempting to use the same notation as differentiation but replace the \\(\\partial\\) symbol with something else, perhaps \\(\\eth\\) or \\(\\spadesuit\\) or \\(\\forall\\), giving us something like \\(\\spadesuit_x f(x)\\).\nConvention has something different in store. The notation for anti-differentiation is \\[\\large \\int f(x) dx\\] 1. The \\(\\color{magenta}{\\int}\\) is the marker for anti-differentiation. 2. The name of the with-respect-to input is contained in the “dx” at the end of the notation: \\(\\int f(x) d\\color{magenta}{x}\\) 3. The function being anti-differentiated is in the middle \\(\\int \\color{magenta}{f(x)} dx\\).\nFor those starting out with anti-differentiation, the conventional notation can be confusing, especially the \\(dx\\) part. It is easy confuse \\(d\\) for a constant and \\(x\\) for part of the function being anti-differentiated.\nThink of the \\(\\int\\) and the \\(dx\\) as brackets around the function. You need both brackets for correct notation, the \\(\\int\\) and the \\(dx\\) together telling you what operation to perform.\nRemember that just as \\(\\partial_x f(x)\\) is a function, so is \\(\\int f(x) dx\\).",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Change & accumulation</span>"
    ]
  },
  {
    "objectID": "Accumulation/33-intro.html#rmosaic-notation",
    "href": "Accumulation/33-intro.html#rmosaic-notation",
    "title": "34  Change & accumulation",
    "section": "34.3 R/mosaic notation",
    "text": "34.3 R/mosaic notation\nRecall that the notation for differentiation in R/mosaic is D(f(x) ~ x). The R/mosaic notation for anti-differentiation is very similar:\nD(f(x) ~ x)\nThis has the same three pieces of information as \\(\\partial_x f(x)\\)\n\nD() signifies differentiation whereas antiD() signifies anti-differentiation.\n~ x identifies the with-respect-to input.\nf(x) ~ is the function on which the operation is to be performed.\n\nRemember that just as D(f(x) ~ x) creates a new function out of f(x) ~ x, so does antiD(f(x) ~ x).",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Change & accumulation</span>"
    ]
  },
  {
    "objectID": "Accumulation/33-intro.html#dimension-and-anti-differentiation",
    "href": "Accumulation/33-intro.html#dimension-and-anti-differentiation",
    "title": "34  Change & accumulation",
    "section": "34.4 Dimension and anti-differentiation",
    "text": "34.4 Dimension and anti-differentiation\nThis entire block will be about anti-differentiation, its properties and its uses. You already know that anti-differentiation (as the name suggests) is the inverse of differentiation. There is one consequence of this that is helpful to keep in mind as we move on to other chapters. This being calculus, the functions that we construct and operate upon have inputs that are quantities and outputs that are also quantities. Every quantity has a dimension, as discussed in Chapter 16. When you are working with any quantity, you should be sure that you know its dimension and its units.\nThe dimension of the input to a function does not by any means have to be the same as the dimension of the output. For instance, we have been using many functions where the input has dimension time and the output is position (dimension L) or velocity (dimension L/T) or acceleration (dimension L/T\\(^2\\)).\nImagine working with some function \\(f(y)\\) that is relevant to some modeling project of interest to you. Returning to the bracket notation that we used in Chapter 16, the dimension of the input quantity will be [\\(y\\)]. The dimension of the output quantity is [\\(f(y)\\)]. (Remember from 16 that [\\(y\\)] means “the dimension of quantity \\(y\\)” and that [\\(f(y)\\)] means “the dimension of the output from \\(f(y)\\).”)\nThe function \\(\\partial_y f(y)\\) has the same input dimension \\([y]\\) but the output will be \\([f(y)] / [y]\\). For example, suppose \\(f(y)\\) is the mass of fuel in a rocket as a function of time \\(y\\). The output of \\(f(y)\\) has dimension M. The input dimension \\([y]\\) is T.\nThe output of the function \\(\\partial_y f(y)\\) has dimension \\([f(y)] / [y]\\), which in this case will be M / T. (Less abstractly, if the fuel mass is given in kg, and time is measured in seconds, then \\(\\partial_y f(y)\\) will have units of kg-per-second.)\nHow about the dimension of the anti-derivative \\(F(y) = \\int f(y) dy\\)? Since \\(F(y)\\) is the anti-derivative of \\(f(y)\\) (with respect to \\(y\\)), we know that \\(\\partial_y F(y) = f(y)\\). Taking the dimension of both sides \\[[\\partial_y F(y)] = \\frac{[F(y)]}{[y]} = \\frac{[F(y)]}{\\text{T}} = [f(y)] = \\text{M}\\] Consequently, \\([F(y)] = \\text{M}\\).\nTo summarize:\n\nThe dimension of derivative \\(\\partial_y f(y)\\) will be \\([f(y)] / [y]\\).\nThe dimension of the anti-derivative \\(\\int f(y) dy\\) will be \\([f(y)]\\times [y]\\).\n\nOr, more concisely:\n\nDifferentiation is like division, anti-differentiation is like multiplication.\n\nPaying attention to the dimensions (and units!) of input and output can be a boon to the calculus student. Often students have some function \\(f(y)\\) and they are wondering which of the several calculus operations they are supposed to do: differentiation, anti-differentiation, finding a maximum, finding an argmax or a zero. Start by figuring out the dimension of the quantity you want. From that, you can often figure out which operation is appropriate.\nTo illustrate, imagine that you have constructed \\(f(y)\\) for your task and you know, say, \\[[f(y)] = \\text{M       and} \\  \\ \\ \\ \\ [y] = \\text{T}\\ .\\] Look things up in the following table:\n\n\n\nDimension of result\nCalculus operation\n\n\n\n\nM / T\ndifferentiate\n\n\nM T\nanti-differentiate\n\n\nM\nfind max or min\n\n\nT\nfind argmax/argmin or a function zero\n\n\nM T\\(^2\\)\nanti-differentiate twice in succession\n\n\nM / T\\(^2\\)\ndifferentiate twice in succession\n\n\n\nFor example, suppose the output of the accelerometer on your rocket has dimension L / T\\(^2\\). You are trying to figure out from the accelerometer reading what is your altitude. Altitude has dimension L. Look up in the table to see that you want to anti-differentiate acceleration twice in succession.",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Change & accumulation</span>"
    ]
  },
  {
    "objectID": "Accumulation/33-intro.html#sec-preliminary-terrors",
    "href": "Accumulation/33-intro.html#sec-preliminary-terrors",
    "title": "34  Change & accumulation",
    "section": "34.5 From Calculus Made Easy",
    "text": "34.5 From Calculus Made Easy\nCalculus Made Easy, by Silvanus P. Thompson, is a classic, concise, and elegant textbook from 1910. It takes a common-sense approach, sometimes lampooning the traditional approach to teaching calculus.\n\nSome calculus-tricks are quite easy. Some are enormously difficult. The fools who write the textbooks of advanced mathematics—and they are mostly clever fools—seldom take the trouble to show you how easy the easy calculations are. On the contrary, they seem to desire to impress you with their tremendous cleverness by going about it in the most difficult way. — From the preface\n\nThompson’s first chapter starts with the notation of accumulation, which he calls “the preliminary terror.”\n\nThe preliminary terror … can be abolished once for all by simply stating what is the meaning—in common-sense terms—of the two principal symbols that are used in calculating.\nThese dreadful symbols are:\n\n\\(\\Large\\  d\\) which merely means “a little bit of.”\n\nThus \\(dx\\) means a little bit of \\(x\\); or \\(du\\) means a little bit of \\(u\\). Ordinary mathematicians think it more polite to say “an element of,” instead of “a little bit of.” Just as you please. But you will find that these little bits (or elements) may be considered to be indefinitely small.\n\n\\(\\ \\ \\large\\int\\) which is merely a long \\(S\\), and may be called (if you like) “the sum of.”\n\nThus \\(\\ \\int dx\\) means the sum of all the little bits of \\(x\\); or \\(\\ \\int dt\\) means the sum of all the little bits of \\(t\\). Ordinary mathematicians call this symbol “the integral of.” Now any fool can see that if \\(x\\) is considered as made up of a lot of little bits, each of which is called \\(dx\\), if you add them all up together you get the sum of all the \\(dx\\)’s, (which is the same thing as the whole of \\(x\\)). The word “integral” simply means “the whole.” If you think of the duration of time for one hour, you may (if you like) think of it as cut up into \\(3600\\) little bits called seconds. The whole of the \\(3600\\) little bits added up together make one hour.\nWhen you see an expression that begins with this terrifying symbol, you will henceforth know that it is put there merely to give you instructions that you are now to perform the operation (if you can) of totaling up all the little bits that are indicated by the symbols that follow.\n\n\nThe next chapter shows what it means to “total up all the little bits” of a function.",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Change & accumulation</span>"
    ]
  },
  {
    "objectID": "Accumulation/35-integration.html",
    "href": "Accumulation/35-integration.html",
    "title": "36  Integration",
    "section": "",
    "text": "36.1 Net change\nPerhaps it goes without saying, but once you have the CAPITAL LETTER function, e.g. \\(F(t)\\), you can evaluate that function at any input that falls into the domain of \\(F(t)\\). If you have a graph of \\(F(t)\\) versus \\(t\\), just position your finger on the horizontal axis at input \\(t_1\\), then trace up to the function graph, then horizontally to the vertical axis where you can read off the value \\(F(t_1)\\). If you have \\(F()\\) in the form of a computer function, just apply \\(F()\\) to the input \\(t_1\\).\nIn this regard, \\(F(t)\\) is like any other function.\nHowever, in using and interpreting the \\(F(t)\\) that we constructed by anti-differentiating \\(f(t)\\), we have to keep in mind the limitations of the anti-differentiation process. In particular, any function \\(f(t)\\) does not have a unique anti-derivative function. If we have one anti-derivative, we can always construct another by adding some constant: \\(F(t) + C\\) is also an anti-derivative of \\(f(t)\\).\nBut we have a special purpose in mind when calculating \\(F(t_1)\\). We want to figure out from \\(F(t)\\) how much of the quantity \\(f(t)\\) has accumulated up to time \\(t_1\\). For example, if \\(f(t)\\) is the rate of increase in fuel (that is, the negative of fuel consumption), we want \\(F(t_1)\\) to be the amount of fuel in our tank at time \\(t_1\\). That cannot happen. All we can say is that \\(F(t_1)\\) is the amount of fuel in the tank at \\(t_1\\) give or take some unknown constant C.\nInstead, the correct use of \\(F(t)\\) is to say how much the quantity has changed over some interval of time, \\(t_0 \\leq t \\leq t_1\\). This “change in the quantity” is called the net change in \\(F()\\). To calculate the net change in \\(F()\\) from \\(t_0\\) to \\(t_1\\) we apply \\(F()\\) to both \\(t_0\\) and \\(t_1\\), then subtract:\n\\[\\text{Net change in}\\ F(t) \\ \\text{from}\\ t_0 \\ \\text{to}\\ t_1 :\\\\= F(t_1) - F(t_0)\\]",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Integration</span>"
    ]
  },
  {
    "objectID": "Accumulation/35-integration.html#net-change",
    "href": "Accumulation/35-integration.html#net-change",
    "title": "36  Integration",
    "section": "",
    "text": "Suppose you have already constructed the rate-of-change function for momentum \\(m()\\) and implemented it as an R function m(). For instance, \\(m(t)\\) might be the amount of force at any instant \\(t\\) of a car, and \\({\\mathbf M}(t)\\) is the force accumulated over time, better known as momentum. We will assume that the input to m() is in seconds, and the output is in kg-meters-per-second-squared, which has the correct dimension for force.\nYou want to find the amount of force accumulated between time \\(t=2\\) and \\(t=5\\) seconds.\n\n# You've previously constructed m(t)\nM &lt;- antiD(m(t) ~ t)\nM(5) - M(2)\n## [1] -1.392131\n\nTo make use of this quantity, you will need to know its dimension and units. For this example, where the dimension [\\(m(t)\\)] is M L T\\(^{-2}\\), and [\\(t\\)] = T, the dimension [\\({\\mathbf M}(t)\\)] will be M L T\\(^{-1}\\). In other words, if the output of \\(m(t)\\) is kg-meters-per-second-squared, then the output of \\(V(t)\\) must be kg- meters-per-second.",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Integration</span>"
    ]
  },
  {
    "objectID": "Accumulation/35-integration.html#the-definite-integral",
    "href": "Accumulation/35-integration.html#the-definite-integral",
    "title": "36  Integration",
    "section": "36.2 The “definite” integral",
    "text": "36.2 The “definite” integral\nWe have described the process of calculating a net change from the lower-case function \\(f(t)\\) in terms of two steps:\n\nConstruct \\(F(t) = \\int f(t) dt\\).\nEvaluate \\(F(t)\\) at two inputs, e.g. \\(F(t_2) - F(t_1)\\), giving a net change, which we will write as \\({\\cal F}(t_1, t_2) = F(t_2) - F(t_1)\\).\n\nAs a matter of notation, the process of going from \\(f(t)\\) to the net change is written as one statement. \\[{\\cal F}(t_1, t_2) = F(t_2) - F(t_1) = \\int_{t_1}^{t_2} f(t) dt\\]\nThe punctuation \\[\\int_{t_1}^{t_2} \\_\\_\\_\\_ dt\\] captures in one construction both the anti-differentiation step (\\(\\int\\_\\_dt\\)) and the evaluation of the anti-derivative at the two bound \\(t_2\\) and \\(t_1\\).\nSeveral names are used to describe the overall process. It is important to become familiar with these.\n\n\\(\\int_a^b f(t) dt\\) is called a definite integral of \\(f(t)\\).\n\\(a\\) and \\(b\\) are called, respectively, the lower bound of integration and the upper bound of integration, although given the way we draw graphs it might be better to call them the “left” and “right” bounds, rather than lower and upper.\nThe pair \\(a, b\\) is called the bounds of integration.\n\nAs always, it pays to know what kind of thing is \\({\\cal F}(t_1, t_2)\\). Assuming that \\(t_1\\) and \\(t_2\\) are fixed quantities, say \\(t_1 = 2\\) seconds and \\(t_2 = 5\\) seconds, then \\({\\cal F}(t_1, t_2)\\) is itself a quantity. The dimension of that quantity is [\\(F(t)\\)] which in turn is [\\(f(t)\\)]\\(\\cdot\\)[\\(t\\)]. So if \\(f(t)\\) is fuel consumption in liters per second, then \\(F(t)\\) will have units of liters, and \\({\\cal F}(t_1, t_2)\\) will also have units of liters.\nRemember also an important distinction:\n\n\\(F(t) = \\int f(t) dt\\) is a function whose output is a quantity.\n\\(F(t_2) - F(t_1) = \\int_{t_1}^{t_2} f(t) dt\\) is a quantity, not a function.\n\nOf course, \\(f(t)\\) is a function whose output is a quantity. In general, the two functions \\(F(t)\\) and \\(f(t)\\) produce outputs that are different kinds of quantities. For instance, the output of \\(F(t)\\) is liters of fuel while the output of \\(f(t)\\) is liters per second: fuel consumption. Similarly, the output of \\(S(t)\\) is dollars, while the output of \\(s(t)\\) is dollars per day.\nThe use of the term definite integral suggests that there might be something called an indefinite integral, and indeed there is. “Indefinite integral” is just a synonym for “anti-derivative.” In this book we favor the use of anti-derivative because it is too easy to leave off the “indefinite” and confuse an indefinite integral with a definite integral. Also, “anti-derivative” makes it completely clear what is the relationship to “derivative.”\nSince 1700, it is common for calculus courses to be organized into two divisions:\n\nDifferential calculus, which is the study of derivatives and their uses.\nIntegral calculus, which is the study of anti-derivatives and their uses.\n\nMathematical notation having been developed for experts rather than for students, very small typographical changes are often used to signal very large changes in meaning. When it comes to anti-differentiation, there are two poles of fixed meaning and then small changes which modify the meaning. The poles are:\n\nAnti-derivative: \\(\\int f(t) dt\\), which is a function whose output is a quantity.\nDefinite integral \\(\\int_a^b f(t) dt\\), which is a quantity, plain and simple.\n\nBut you will also see some intermediate forms:\n\n\\(\\int_a^t f(t) dt\\), which is a function with input \\(t\\).\n\\(\\int_a^x f(t) dt\\), which is the same function as in (a) but with the input name \\(x\\) being used.\n\\(\\int_t^b f(t) dt\\), which is a function with input \\(t\\).\nLess commonly, \\(\\int_x^t f(t) dt\\) which is a function with two inputs, \\(x\\) and \\(t\\). The same is true of \\(\\int_x^y f(t) dt\\) and similar variations.",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Integration</span>"
    ]
  },
  {
    "objectID": "Accumulation/35-integration.html#initial-value-of-the-quantity",
    "href": "Accumulation/35-integration.html#initial-value-of-the-quantity",
    "title": "36  Integration",
    "section": "36.3 Initial value of the quantity",
    "text": "36.3 Initial value of the quantity\nRecall that we are interested in a real quantity \\({\\mathbf F}(t)\\), but we only know \\(f(t)\\) and from that can calculate an anti-derivative \\(F(t)\\). The relationship between them is \\[{\\mathbf F}(t) = F(t) + C\\] where \\(C\\) is some fixed quantity that we cannot determine directly from \\(f(t)\\).\nStill, even if we cannot determine \\(C\\), there is one way we can use \\(F(t)\\) to make definite statements about \\({\\mathbf F}(t)\\). Consider the net change from \\(t_1\\) to \\(t_2\\) in the real quantity \\({\\mathbf F}\\). This is \\[{\\mathbf F}(t_2) - {\\mathbf F}(t_1) =  \\left[F(t_2) + C\\right] - \\left[F(t_1) + C\\right] = F(t_2) - F(t_1)\\] In other words, just knowing \\(F(t)\\), we can make completely accurate statements about net changes in the value of \\({\\mathbf F}(t)\\).\nLet’s develop our understanding of this unknown constant \\(C\\), which is called the constant of integration. To do so, watch the movie in Figure 36.1 showing the process of constructing the anti-derivative \\[F(t) = \\int_2^t f(t) dt\\ .\\]\n\n\n\n\n\n\n\n\nFigure 36.1: Constructing the anti-derivative \\(F(t)\\) by reading the slope from \\(f(t)\\) and using that slope to extend the picture of \\(F()\\)\n\n\n\n\n\n\nFocus first on the top graph. The function we are integrating, \\(f(t)\\), is known before we carry out the integration, so it is shown in the top graph.\n\n\\(f(t)\\) is the rate of increase in \\(F(t)\\) (or \\({\\mathbf F}(t)\\) for that matter). From the graph, you can read using the vertical axis the value of \\(f(t)\\) for any input \\(t\\). But since \\(f(t)\\) is a rate of increase, we can also depict \\(f(t)\\) as a slope. That slope is being drawn as a \\(\\color{magenta}{\\text{magenta}}\\) arrow. Notice that when \\(f(t)\\) is positive, the arrow slopes upward and when \\(f(t)\\) is negative, the arrow slopes downward. The steepness of the arrow is the value of \\(f(t)\\), so for inputs where the value of \\(f(t)\\) is far from zero the arrow is steeper than for values of \\(f(t)\\) that are near zero.\n\nNow look at both graphs, but concentrate just on the arrows in the two graphs. They are always the same: carbon copies of one another.\nFinally the bottom graph. We are starting the integral at \\(t_1=2\\). Since nothing has yet been accumulated, the value \\(F(t_1 = 2) = 0\\). From (1) and (2), you know the arrow shows the slope of \\(F(t)\\). So as \\(F(t&gt;2)\\) is being constructed the arrow guides the way. When the slope arrow is positive, \\(F(t)\\) is growing. When the slope arrow is negative, \\(F(t)\\) is going down.\n\nIn tallying up the accumulation of \\(f(t)\\), we started at time \\(t=2\\) and with \\(F(t=2) = 0\\). This makes sense, since nothing can be accumulated over the mere instant of time from \\(t=2\\) to \\(t=2\\). On the other hand, it was our choice to start at \\(t=2\\). We might have started at another value of \\(t\\) such as \\(t=0\\) or \\(t=-5\\) or \\(t=-\\infty\\). If so, then the accumulation of \\(f(t)\\) up to \\(t=2\\) would likely have been something other than zero.\nBut what if we knew an actual value for \\({\\mathbf F}(2)\\). This is often the case. For instance, before taking a trip you might have filled up the fuel tank. The accumulation of fuel consumption only tells you how much fuel has been used since the start of the trip. But if you know the starting amount of fuel, by adding that to the accumulation you will know instant by instant how much fuel is in the tank. In other words, \\[{\\mathbf F}(t) = {\\mathbf F}(2) + \\int_2^t f(t) dt\\ .\\] This is why, when we write an anti-derivative, we should always include mention of some constant \\(C\\)—the so-called constant of integration—to remind us that there is a difference between the \\(F(t)\\) we get from anti-differentiation and the \\({\\mathbf F}(t)\\) of the function we are trying to reconstruct. That is, \\[{\\mathbf F}(t) = F(t) + C = \\int f(t) dt + C\\ .\\] We only need to know \\({\\mathbf F}(t)\\) at one point in time, say \\(t=0\\), to be able to figure out the value of \\(C\\): \\[C = {\\mathbf F}(0) - F(0)\\ .\\]\nAnother way to state the relationship between the anti-derivative and \\({\\mathbf F}(t)\\) is by using the anti-derivative to accumulate \\(f(t)\\) from some starting point \\(t_0\\) to time \\(t\\). That is: \\[{\\mathbf F}(t) \\ =\\  {\\mathbf F}(t_0) + \\int_{t_0}^t f(t)\\, dt\\  = \\\n{\\mathbf F}(t_0) + \\left({\\large\\strut}F(t) - F(t_0)\\right)\\]\n\n\n\n\n\n\nCalculus history—Galileo in Pisa\n\n\n\nAn oft-told legend has Galileo at the top of the Tower of Pisa around 1590. The legend illustrates Galileo’s finding that a light object (e.g. a marble) and a heavy object (e.g. a ball) will fall at the same speed. Galileo published his mathematical findings in 1638 in Discorsi e Dimostrazioni Matematiche, intorno a due nuove scienze. (English: Discourses and Mathematical Demonstrations Relating to Two New Sciences)\nIn 1687, Newton published his world-changingPhilosophiae Naturalis Principia Mathematica. (English: Mathematical Principles of Natural Philosophy)\nLet’s imagine the ghost of Galileo returned to Pisa in 1690 after reading Newton’s Principia Mathematica. In this new legend, Galileo holds a ball still in his hand, releases it, and figures out the position of the ball as a function of time.\nAlthough Newton famously demonstrated that gravitational attraction is a function of the distance between to objects, he also knew that at a fixed distance—the surface of the Earth—gravitational acceleration was constant. So Galileo was vindicated by Newton. But, although gravitational acceleration is constant from top to bottom of the Tower of Pisa, Galileo’s ball was part of a more complex system: a hand holding the ball still until release. Acceleration of the ball versus time is therefore approximately a Heaviside function:\n\\(\\text{accel}(t) \\equiv \\left\\{\\begin{array}{rl}0 & \\text{for}\\ t \\leq 3\\\\\n{-9.8}  & \\text{otherwise}\\end{array}\\right.\\)\n\naccel &lt;- makeFun(ifelse(t &lt;= 3, 0, -9.8) ~ t)\n\nAcceleration is the derivative of velocity. We can construct a function \\(V(t)\\) as the anti-derivative of acceleration, but the real-world velocity function will be \\[{\\mathbf V}(t) = {\\mathbf V}(0) + \\int_0^t \\text{accel}(t) dt\\]\n\nV_from_antiD &lt;- antiD(accel(t) ~ t)\nV &lt;- makeFun(V0 + (V_from_antiD(t) - V_from_antiD(0)) ~ t, V0 = 0)\n\nIn the computer expression, the parameter V0 stands for \\({\\mathbf V}(0)\\). We’ve set it equal to zero since, at time \\(t=0\\), Galileo was holding the ball still.\nVelocity is the derivative of position, but the real-world velocity function will be the accumulation of velocity from some starting time to time \\(t\\), plus the position at that starting time: \\[x(t) \\equiv x(0) + \\int_0^t V(t) dt\\] We can calculate \\(\\int V(t) dt\\) easily enough with antiD(), but the function \\(x(t)\\) involves evaluating that anti-derivative at times 0 and \\(t\\):\n\nx_from_antiD &lt;- antiD(V(t) ~ t)\nx &lt;- makeFun(x0 + (x_from_antiD(t) - x_from_antiD(0)) ~ t, x0 = 53)\n\nWe’ve set the parameter x0 to be 53 meters, the height above the ground of the top balcony on which Galileo was standing for the experiment.\n\n\n\n\n\n\n\n\nFigure 36.2: The acceleration, velocity, and position of the ball as a function of time in Galileo’s Tower of Pisa experiment. The ball is released at time \\(t_0\\).\n\n\n\n\n\n\n\nIn the (fictional) account of the 1690 experiment, we had Galileo release the ball at time \\(t=0\\). That is a common device in mathematical derivations, but in a physical sense it is entirely arbitrary. Galileo might have let go of the ball at any other time, say, \\(t=3\\) or \\(t=14:32:05\\).\nA remarkable feature of integrals is that it does not matter what we use as the lower bound of integration, so long as we set the initial value to correspond to that bound.\n\n\n\n\n\n\nWhy \\(\\int f(x) dx\\) instead of \\(\\int f(t) dt\\)?\n\n\n\nFor a while you were writing integrals like this: \\(\\int_a^b f(t) dt\\). Then you replaced \\(b\\) with the input name \\(t\\) to get \\(\\int_a^t f(t) dt\\). But then you switched everything up by writing \\(\\int_a^t f(x) dx\\). Is that the same as \\(\\int_a^t f(t) dt\\)? If so, why do you get to replace the \\(t\\) with \\(x\\) in some places but not in others?\nRecall from Chapter 2 that the names used for inputs to a function definition don’t matter so long as they are used consistently on the left and right sides of \\(\\equiv\\). For instance, all these are the same function:\n\n\\(f(x) \\equiv m x + b\\)\n\\(g(t) \\equiv m t + b\\)\n\\(h(\\text{wind}) \\equiv m \\text{wind} + b\\)\n\nNow think about the integral \\(\\int_a^b f(t) dt\\): \\[\\int_a^b f(t) dt = F(b) - F(a)\\ .\\]\nOn the left-hand side, the input name \\(t\\) is prominent, appearing in two places: \\(f(\\color{magenta}{t}) d\\color{magenta}{t}\\). But \\(t\\) is nowhere on the right-hand side. We could have equally well written this as \\(\\int_a^b f(x) dx\\) or \\(\\int_a^b f(\\text{wind}) d\\text{wind}\\). The name we use for the input to \\(f()\\) does not matter so long as it is consistent with the name used in the \\(d\\_\\_\\) part of the notation. Often, the name placed in the blanks in \\(\\int f(\\_\\_) d\\_\\_\\) is called a dummy variable.\nWriting \\(\\int_a^t f(t) dt\\) is perfectly reasonable, but many authors dislike the multiple appearance of \\(t\\). So they write something like \\(\\int_a^t f(x) dx\\) instead.",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Integration</span>"
    ]
  },
  {
    "objectID": "Accumulation/35-integration.html#integrals-from-bottom-to-top",
    "href": "Accumulation/35-integration.html#integrals-from-bottom-to-top",
    "title": "36  Integration",
    "section": "36.4 Integrals from bottom to top",
    "text": "36.4 Integrals from bottom to top\nThe bounds of integration appear in different arrangements. None of these are difficult to derive from the basic forms:\n\nThe relationship between an integral and its corresponding anti-derivative function: \\[\\int_a^b f(x) dx = F(b) - F(a)\\] This relationship has a fancy-sounding name: the second fundamental theorem of calculus.\nThe accumulation from an initial-value \\[{\\mathbf F}(b)\\  =\\  {\\mathbf F}(a) + \\int_a^b f(x) dx\\  = \\ {\\mathbf F}(a) + F(b) - F(a)\\] For many modeling situations, \\(a\\) and \\(b\\) are fixed quantities, so \\(F(a)\\) and \\(F(b)\\) are also quantities; the output of the anti-derivative function at inputs \\(a\\) and \\(b\\). But either the lower-bound or the upper-bound can be input names, as in \\[\\int_0^t f(x) dx = F(t) - F(0)\\]\n\nNote that \\(F(t)\\) is not a quantity but a function of \\(t\\).\nOn occasion, you will see forms like \\(\\int_t^0 f(x)dx\\). You can think of this in either of two ways:\n\nThe accumulation from a time \\(t\\) less than 0 up until 0.\nThe reverse accumulation from 0 until time \\(t\\).\n\nReverse accumulation can be a tricky concept because it violates everyday intuition. Suppose you were harvesting a row of ripe strawberries. You start at the beginning of the row—position zero. Then you move down the row, picking strawberries and placing them in your basket. When you have reached position \\(B\\) your basket holds the accumulation \\(\\int_0^B s(x)\\, dx\\), where \\(s(x)\\) is the lineal density of strawberries—units: berries per meter of row.\nBut suppose you go the other way, starting with an empty basket at position \\(B\\) and working your way back to position 0. Common sense says your basket will fill to the same amount as in the forward direction, and indeed this is the case. But integrals work differently. The integral \\(\\int_B^0 s(x) dx\\) will be the negative of \\(\\int_0^B s(x) dx\\). You can see this from the relationship between the integral and the anti-derivative: \\[\\int_B^0 s(x) dx \\ = \\ S(0) - S(B) \\ =\\ -\\left[{\\large\\strut}S(B) - S(0)\\right]\\ = \\ -\\int_0^B s(x) dx\\]\nThis is not to say that there is such a thing as a negative strawberry. Rather, it means that harvesting strawberries is similar to an integral in some ways (accumulation) but not in other ways. In farming, harvesting from 0 to \\(B\\) is much the same as harvesting from \\(B\\) to 0, but integrals don’t work this way.\nAnother property of integrals is that the interval between bounds of integration can be broken into pieces. For instance:\n\\[\\int_a^c f(x) dx \\ = \\ \\int_a^b f(x) dx + \\int_b^c f(x) dx\\] You can confirm this by noting that \\[\\int_a^b f(x) dx + \\int_b^c f(x) dx \\ = \\ \\left[{\\large\\strut}F(b) - F(a)\\right] + \\left[{\\large\\strut}F(c) - F(b)\\right] = F(c) - F(a) \\ = \\ \\int_a^c f(x) dx\\ .\\]\nFinally, consider this function of \\(t\\): \\[\\partial_t \\int_a^t f(x) dx\\ .\\] First, how do we know it is a function of \\(t\\)? \\(\\int_a^t f(x) dx\\) is a definite integral and has the value \\[\\int_a^t f(x) dx = F(t) - F(a)\\  .\\] Following our convention, \\(a\\) is a parameter and stands for a specific numerical value, so \\(F(a)\\) is the output of \\(F()\\) for a specific input. But according to convention \\(t\\) is the name of an input. So \\(F(t)\\) is a function whose output depends on \\(t\\). Differentiating the function \\(F(t)\\), as with every other function, produces a new function.\nSecond, there is a shortcut for calculating \\(\\partial_t \\int_a^t f(x) dx\\): \\[\\partial_t \\int_a^t f(x) dx\\ =\\ \\partial_t \\left[{\\large\\strut}F(t) - F(a)\\right]\\ .\\] Since \\(F(a)\\) is a quantity and not a function, \\(\\partial_t F(a) = 0\\). That simplies things. Even better, we know that the derivative of \\(F(t)\\) is simply \\(f(t)\\): that is just the nature of the derivative/anti-derivative relationship between \\(f(t)\\) and \\(F(t)\\). Put together, we have: \\[\\partial_t \\int_a^t f(x) dx\\ =\\ f(t)\\ .\\]\nThis complicated-looking identity has a fancy name: the first fundamental theorem of calculus.\n\n\n\n\n\n\nMath out of the World: Backtracking the stars\n\n\n\nIn the 1920s, astronomers and cosmologists questioned the idea that the large-scale universe is static and unchanging. This traditional belief was undermined both by theory (e.g. General Relativity) and observations. The most famous of these were collected and published by Edwin Hubble, starting in 1929 and continuing over the next decade as improved techniques and larger telescopes became available. In recent years, with the availability of the space telescope named in honor of Hubble data has expanded in availability and quality. Figure 36.3 shows a version of Hubble’s 1929 graph based on contemporary data.\n\n\n\n\n\n\n\n\nFigure 36.3: The relationship between velocity and distance of stars, using contemporary data in the same format at Edwin Hubble’s 1929 publication.\n\n\n\n\n\nEach dot in Figure 36.3 is an exploding star called a supernova. The graph shows the relationship between the distance of the star from our galaxy and the outward velocity of that star. The velocities are large, \\(3 \\times 10^4 = 30,000\\) km/s is about one-tenth the speed of light. Similarly, the distances are big; 600 Mpc is the same as 2 billion light years or \\(1.8 \\times 10^{22} \\text{km}\\). The slope of the line in Figure 36.3 is \\(\\frac{3.75 \\times 10^4\\, \\text{km/s}}{1.8 \\times 10^{22}\\, \\text{km}} = 2.1 \\times 10^{-18}\\, \\text{s}^{-1}\\). For ease of reading, we will call this slope \\(\\alpha\\) and therefore the velocity of a start distance \\(D\\) from Earth is \\[v(D) \\equiv \\alpha D\\ .\\]\nEarlier in the history of the universe each star was a different distance from Earth. We will call this function \\(D(t)\\), distance as a function of time in the universe.\nThe distance travelled by each star from time \\(t\\) (billions of years ago) to the present is \\[\\int_t^\\text{now} v(t) dt  = D_\\text{now} - D(t)\\] which can be re-arranged to give \\[D(t) = D_\\text{now} - \\int_t^\\text{now} v(t) dt .\\] Assuming that \\(v(t)\\) for each star has remained constant at \\(\\alpha D_\\text{now}\\), the distance travelled by each star since time \\(t\\) depends on its current distance like this: \\[\\int_t^\\text{now} v(t) dt = \\int_t^\\text{now} \\left[ \\alpha D_\\text{now}\\right]\\, dt = \\alpha D_\\text{now}\\left[\\text{now} - t\\right]\\] Thus, the position of each star at time \\(t\\) is \\[D(t) = D_\\text{now} - \\alpha D_\\text{now}\\left[\\text{now} - t\\right] = D(t)\\] or, \\[D(t) = D_\\text{now}\\left({\\large\\strut} 1-\\alpha \\left[\\text{now} - t\\right]\\right)\\]\nAccording to this model, there was a common time \\(t_0\\) when when all the stars were at the same place: \\(D(t_0) = 0\\). This happened when \\[\\text{now} - t_0 = \\frac{1}{\\alpha} = \\frac{1}{2.1 \\times 10^{-18}\\, \\text{s}^{-1}} = 4.8 \\times 10^{17} \\text{s}\\ .\\] It seems fair to call such a time, when all the stars where at the same place at the same time, as the origin of the universe. If so, \\(\\text{now} - t_0\\) corresponds to the age of the universe and our estimate of that age is \\(4.8\\times 10^{17}\\text{s}\\). Conventionally, this age is reported in years. To get that, we multiply by the flavor of one that turns seconds into years: \\[\\frac{60\\, \\text{seconds}}{1\\, \\text{minute}} \\cdot \\frac{60\\, \\text{minutes}}{1\\, \\text{hour}} \\cdot \\frac{24\\, \\text{hours}}{1\\, \\text{day}} \\cdot \\frac{365\\, \\text{days}}{1\\, \\text{year}} = 31,500,000 \\frac{\\text{s}}{\\text{year}}\\] The grand (but hypothetical) meeting of the stars therefore occurred \\(4.8 \\times 10^{17} \\text{s} / 3.15 \\times 10^{7} \\text{s/year} = 15,000,000,000\\) years ago. Pretty crowded to have all the mass in the universe in one place at the same time. No wonder they call it the Big Bang!",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Integration</span>"
    ]
  },
  {
    "objectID": "Accumulation/35-integration.html#footnotes",
    "href": "Accumulation/35-integration.html#footnotes",
    "title": "36  Integration",
    "section": "",
    "text": "Momentum is velocity times mass. Newton’s Second Law of Motion stipulates that force equals the rate of change of momentum.↩︎",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Integration</span>"
    ]
  },
  {
    "objectID": "Preliminaries/02-notation.html",
    "href": "Preliminaries/02-notation.html",
    "title": "2  Notation",
    "section": "",
    "text": "2.1 Functions, inputs, parameters\nOur style of notation will be to give functions and their inputs explicit names. The basic principle is that a function name is a sequence of letters followed by an empty pair of parentheses, for instance, sin() or ln(). The parentheses provide clear indication that this is a function name.\nAs you saw in Chapter 1, our notation for defining a function includes the names of the parameters. In our mathematical notation, both the name of the function and the names of the inputs are shown on the left-hand side of the \\(\\equiv\\) symbol. For instance, \\[g(u, z) \\equiv u\\,\\cos(z)\\] defines a function named \\(g()\\) that takes two inputs. The input names are listed in the parentheses following the function name.\nThe right-hand side of a function definition is a formula. The formula specifies how each of the inputs will get used in a computation of the function output. When a function has more than one input, the input names serve to indicate where each input goes in the formula defining the calculation. For instance: \\[h(x, y) \\equiv x^2 e^y\\ .\\] \\(h()\\) is a completely different function than, say, \\(f(x, y) \\equiv y^2 e^x\\).\nA sensible person will define a function because they are planning to use it later on, perhaps multiple times. “Using” a function might mean including it in the formula in the definition of another function. But there is also a more specific sense of “using” to which we need to give a precise name. To apply a function means providing specific input quantities so that the output of the function can be calculated. An equivalent phrase is evaluate a function on an input(s). Consider, for instance, a simple function \\[z(t) \\equiv t^2\\ .\\] To apply the function \\(z()\\) to the input quantity 3, any of the following notation styles can be used: \\[z(3)\\ \\ \\ \\text{or}\\ \\ \\ \\ z(t=3) \\ \\ \\ \\text{or}\\ \\ \\ \\ z(t)\\left.\\Large\\strut\\right|_{t=3}\\ .\\] Remember that \\(z(3)\\) or its equivalents are not themselves functions. They are the quantity that results from applying a the function to a specific input quantity, namely \\(3\\) in this example.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Notation</span>"
    ]
  },
  {
    "objectID": "Preliminaries/02-notation.html#functions-inputs-parameters",
    "href": "Preliminaries/02-notation.html#functions-inputs-parameters",
    "title": "2  Notation",
    "section": "",
    "text": "Tip\n\n\n\nIn defining a function, say, \\(z(t) \\equiv t^2\\), one uses specific names for the arguments and writes the function formula using these same names. The application of a function to an input is different. When a function is being applied, the argument can be a numeral or any name that contains the value to serve as input. For instance, any of \\(z(b)\\), \\(z(\\text{age})\\), or \\(z(\\text{orbit\\_duration})\\) can be correct ways to apply z()$.\n\n\n\n2.1.1 Input names\nTo reduce human cognitive load in parsing function definitions, we will often select input names from a small set. This way, the name itself is a clue to what role that name is playing. Here are the most commonly used input names.\n\n\\(x\\) or \\(y\\) or \\(z\\).\n\\(t\\). This name is typically used when the input is meant to represent a time or duration.\nLess frequently, \\(u\\), \\(v\\), \\(w\\) when the other arguments are already in use.\n\nOf course, it’s often appropriate to use other names for arguments. In modeling, to make clearer the relationship of functions and the real-world setting, it is a good idea to use more descriptive names, like \\(T\\) for “temperature” or \\(V\\) for volume, or even \\(\\text{altitude}\\) (which helpfully describes itself).\n\n\n\n\n\n\nDefinition: “argument”\n\n\n\nIn everyday speech, an “argument” is a discussion between people with differing views. But in mathematics and computing, argument means something else entirely: it is a synonym for “input to a function.”\n\n\nOften, the functions we define will have formulas that include quantities other than the inputs. For instance, we might define: \\[h(t) \\equiv A \\sin(t) + B\\ .\\] This definition explicitly identifies \\(t\\) as the name of the function input. The quantities named \\(A\\) and \\(B\\) that appear in the formula are not listed as inputs on the left side of \\(\\equiv\\) but they are nonetheless essential for evaluating the function \\(h()\\).\nThere is a case to be made for identifying as inputs to the function all quantities needed for evaluating the function. In this style, the function would be defined as \\(h(t,A,B) \\equiv A \\sin(t) + B\\).\nIn writing mathematical notation for the human reader, there is a tradition of distinguishing between quantities that will differ from one evaluation to another and quantities that will be the same each time the function is evaluated. These latter quantities are called parameters.\nIn reading a definition such as \\[h(t) \\equiv A \\sin(t) + B\\ ,\\] the named quantities that are not listed inside the parentheses on the left-hand side of the definition—\\(A\\) and \\(B\\) in this example—will be the parameters. By writing a name in the parameter style, we are signaling that these quantities will not be changing when we apply the function. That leaves unstated what are the values of the parameters, a source of confusion for many newcomers to calculus.\n\n\n\n\n\n\nInput or parameter?\n\n\n\nThere is no absolute rule for identifying a named quantity used in a function’s formula as a parameter rather than as an input. It is a matter of style and the conventions of the field in which you’re working. When we get to the computer notation for defining functions, you will see that we simplify things by considering all named quantities used in a function formula as inputs.\n\n\n\nApplication area 2.1 —The period of a pendulum swing.\n\n\n\n\n\n\n\nApplication area 2.1 Period of swing\n\n\n\nA pendulum is a device that swings back and forth from a fixed pivot. The period of a pendulum is the time it takes to go through one complete cycle of motion—one “back” and one “forth.” It happens that it is simple to compute the period of a pendulum, \\[\\text{period}(L) \\equiv \\sqrt{\\strut L/g\\ }\\] where \\(L\\) is the length of the pendulum, \\(g\\) is the “acceleration due to gravity.”\n\nWe could have written the function as \\(\\text{period}(L, g) \\equiv \\sqrt{\\strut L/g\\ }\\), treating both quantities \\(L\\) and \\(g\\) as inputs. We wrote instead \\(\\text{period}(L)\\) to signify something to the human reader: that we are anticipating the user of \\(\\text{period}()\\) to be calculating the periods of various pendula, with different \\(L\\), but all in about the same location. That location will presumably be near the surface of the Earth, where \\(g \\approx 9.8\\) m/s2. In other words, the definition of \\(\\text{period}(L)\\) treats the acceleration due to gravity as a parameter rather than an input.\nOf course, you might be the kind of person who puts pendula in elevators or on Mars. If so, you would need to use a different value for \\(g\\) than \\(9.8\\) m/s2.\nYou will see much more use of parameters in Block 11 when we use parameters to “fit” functions to data.\n\n\n\n\n2.1.2 Parameter names\nTo make it easy to recognize parameters, we will use names like \\(a\\), \\(b\\), \\(c\\), \\(\\ldots\\), or their upper-case cousins \\(A\\), \\(B\\), \\(\\ldots\\). For instance, here is a definition of a function called a “cubic polynomial”: \\[h(x)\\equiv a + b x + c x^2 + d x^3\\ .\\]\nBut there will be occasions where we need to compare two or more functions and run out of appropriate names from the start of the alphabet. A way to keep things organized is to use subscripts on the letters, for instance comparing \\[g(x) \\equiv a_0 + a_1 x^2 + a_2 x^2 + a_3 x^3 + a_4 x^4\\] to \\[f(x) \\equiv b_0 + b_1 x^2 + b_2 x^2\\ .\\]\n\n\n\n\n\n\nTip\n\n\n\nPronounce names \\(a_0\\) or \\(b_3\\) as “a-sub-zero” and “b-sub-three” respectively.\n\n\n\n\n\n\n\n\nCalculus history: Letters from A to Z and \\(\\alpha\\) to \\(\\omega\\).\n\n\n\nThe tradition of using letters from the start of the alphabet as parameter names dates from the time of Isaac Newton.\nMathematicians and scientists often reach further back in history and use Greek letters as parameter names: \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\), \\(\\delta\\), … In this book, we minimize the use of Greek, but mastering the Greek alphabet is an important form of socialization when becoming a scientist.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Notation</span>"
    ]
  },
  {
    "objectID": "Preliminaries/02-notation.html#special-inputs",
    "href": "Preliminaries/02-notation.html#special-inputs",
    "title": "2  Notation",
    "section": "2.2 Special inputs",
    "text": "2.2 Special inputs\nWe will create functions as models of a real-world situation. Once created, we generally have to extract information from the function that informs the real-world choice, decision, or understanding that we need to make or develop.\nThere are many forms that the extracted information will take, depending on circumstance. With surprising frequency, two types of information turn out to be useful:\n\nThe set of inputs that produces a maximum or minimum output.\nInputs that produce a specific output.\n\nWe will call these special inputs and will study the techniques for determining them later in the book. Here, we focus on the notation we will use so that you can spot when a special input is being used.\nAs we’ve stated before, the names of inputs will tend to be letters from the back of the alphabet: \\(t\\), \\(u\\), \\(v\\), \\(x\\), \\(y\\), \\(z\\). Each such name refers to the entire set of possible values, that is, a space in the sense of Chapter 1.\nWhen we want to refer to a specific input value that describes a particular feature of a function, we will use the standard input names with a superscript—for instance, \\(x^\\star\\)—or a subscript like \\(y_1\\) or \\(u_0\\).",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Notation</span>"
    ]
  },
  {
    "objectID": "Preliminaries/02-notation.html#footnotes",
    "href": "Preliminaries/02-notation.html#footnotes",
    "title": "2  Notation",
    "section": "",
    "text": "Original word: “ramify”↩︎",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Notation</span>"
    ]
  },
  {
    "objectID": "Preliminaries/03-computing.html",
    "href": "Preliminaries/03-computing.html",
    "title": "3  Computing with R",
    "section": "",
    "text": "3.1 Commands and evaluation\nComputers need to distinguish between declarative and imperative statements. A declarative statement, like \\(g(z) \\equiv z \\cos(z)\\) defines and describes a relationship. An imperative statement is a direction to do some action. For instance, “The store is on the next block,” is declarative. “Bring some crackers from the store,” is imperative. Evaluating a function calls for an imperative statement.\nThe names and format of such instructions—e.g. make a mathematical function from a formula, draw a graph of a function, plot data—are given in the same name/parentheses notation we use in math. For example, makeFun() constructs a function from a formula, slice_plot() graphs a function, gf_point() makes one style of data graphic. These R entities saying “do this” are also called “functions.”\nWhen referring to such R “do this” functions, we will refer to the stuff that goes in between the parentheses as “arguments.” The word “input” would also be fine. The point of using “input” for math functions and “argument” for R “do-this” functions is merely to help you identify when we are talking about mathematics and when we are talking about computing.\nWith computers, writing an expression in computer notation goes hand-in-hand with evaluating the notation. In the mode we will use in this book, you enter your commands in an editor block that we call an “active R chunk.” An example is Active R chunk 3.1, where the editor has been pre-populated with a simple arithmetic command: 2 + 3.\nTo evaluate the command, press the “Run Code” button. You can edit the command in the usual way, placing the cursor in the editor block and typing.\nIn Active R chunk 3.1 (as it initially appears) there is a 1 at the far left. This is a line number, not part of a command. Active R chunks can contain multiple lines and even multiple commands.\nThe R language is set up to format the printing of returned values with an index, which is helpful when the value of the expressions is a large set of numbers. In the case here, with just a single number in the result of evaluating the expression, the index is simply stating the obvious.\nStrictly speaking, we can say that running the code sqrt(17) is invoking the function sqrt() on the input value 17. It’s very common in the world of computing to use the word “calling” rather than “invoking.” So, in Try it! 3.2 you have “called sqrt().\nAn important form of R expression is storage, a declarative statement. Storage uses a symbolic name and the &lt;- token. Active R chunk 3.2 gives a simple example using the name b.\nWe call &lt;- the “storage arrow.” Think of it as stowing the left-hand value in a box labelled with the name on the right-hand side. When storing a value, R is designed not to display the value as happened in the imperative statements in Try it 3.1 and [-try-sqrt-17].\nWithin a document, for instance a chapter of this book or from the MOSAIC Calculus Workbook all the active R chunks have access to the work of the other chunks in the document. Understandably, you need to run the code in a chunk in order for other chunks to access it.\nWhenever you refresh or re-open a document, the active R chunks will all in a not-yet-run state. You have to run the code individually for each chunk you want to use or which are used by other chunks. By and large, we write each R chunks to avoid the need to run earlier chunks … but not always!",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Computing with R</span>"
    ]
  },
  {
    "objectID": "Preliminaries/03-computing.html#commands-and-evaluation",
    "href": "Preliminaries/03-computing.html#commands-and-evaluation",
    "title": "3  Computing with R",
    "section": "",
    "text": "\\(\\equiv\\) and \\(=\\) are declarative\n\n\n\nNoting the distinction between declarative and imperative helps to better understand mathematical notation. In a mathematical statement like \\(h(x) \\equiv 3 x + 2\\), the \\(\\equiv\\) indicates that the statement is declarative. Similarly, the equal sign is used for declarative statements, as with \\[\\pi = 3.14159\\ldots\\] On the other hand, applying a function to a value, as in \\(h(3)\\), is an imperative statement.\n\n\n\n\n\n\n\n\nActive R chunk 3.1\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\n\n\nTry it! 3.1: Simple editing\n\n\n\nModify the contents of Active R chunk 3.1 to subtract 22 from 108. Confirm that running the code produces an output of 86.\n\n\n\n\n\n\n\n\n\nTry it! 3.2: “Calling” a function\n\n\n\nLeaving the command from Try it! 3.1 in place, add a second line to Active R chunk 3.1 to calculate \\(\\sqrt{17}\\). (In R, the square-root function is named sqrt().) When you run the code, you will see two outputs: one for the first line and the other for the second line.\n\n\n\n\n\n\n\nActive R chunk 3.2\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\n\nTry it! 3.3: Accessing stored values by name\n\n\n\nThe purpose of storage is to provide access to a value later on. In order to retrieve the value stored under a name, simply give the name as a command, as in Active R chunk 3.3.\n\n\n\nActive R chunk 3.3\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nSome students may have gotten a response from Active R chunk 3.3 in the form of an “error message” such as Error: object ‘b’ not found. Even experts get such error messages; it’s easy to make a mistake when forming a computer command. It’s important for newbies to understand that error messages are intended to help you fix things rather than to frighten you. Perhaps being gentler would help here—Sorry, but the object ‘b’ wasn’t found.—but “errors” are part of computing culture.\nThe message “object ‘b’ not found” is an indication that nothing has yet been stored under the name b. This would be the case if you had neglected to run the code in Active R chunk 3.2. If that’s your situation, go back to Active R chunk 3.2 and run the code, then try Active R chunk 3.3 again.\n\n\n\n\n\n\n\n\n\n\nUsing R as a stand-alone app\n\n\n\nProfessionals are used to R coming in the form of a stand-alone app. The most popular such app is RStudio, available as free, open-source software produced by a public benefit corporation, Posit PBC.\nThere are many tutorials on installing and using RStudio. People do this especially when they need to document and preserve their work or integrate computing into documents. (This book is an example of such a document.) MOSAIC Calculus is written so that you do not need to use anything but the active R chunks. Still, you might prefer to use the many helpful features that RStudio adds to work with the R language. (Advantages of the active R chunks: the student doesn’t need to install any software. It’s nice as well to be able to refer to computations by pointing to a specific active R chunk like Active R chunk 3.1.)\nIf you do use RStudio, note that in addition to the app, you need to install several packages from the R ecosystem. You do this from within R. The following R command will do the job:\n\ninstall.packages(\"mosaicCalc\")\n\nThen, each time you open RStudio, you need to tell R that you are planning to use the {mosaicCalc} package. Do this as follows:\n\nlibrary(mosaicCalc)",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Computing with R</span>"
    ]
  },
  {
    "objectID": "Preliminaries/03-computing.html#installing-mosaic",
    "href": "Preliminaries/03-computing.html#installing-mosaic",
    "title": "3  Computing with R",
    "section": "3.2 Installing mosaic",
    "text": "3.2 Installing mosaic\nThis book uses the R language extended by a series of packages known collectively as {mosaic}. To use the commands in the next sections, you will need to install these package. This is a one-time operation; you do it once for each computer after you have set up R and RStudio. These commands will do the job. Run them into your R console.\n\ninstall.packages(\"remotes\")\nremotes::install_github(\"ProjectMOSAIC/mosaicCalc\")\n\nYou do not need to reinstall the packages every time you start up a new R console. However, you must tell each R console that you open to refer to the packages. Do this with the following command, which will typically be the first thing you type when you open a console.\n\nlibrary(mosaicCalc)",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Computing with R</span>"
    ]
  },
  {
    "objectID": "Preliminaries/03-computing.html#sec-makefun",
    "href": "Preliminaries/03-computing.html#sec-makefun",
    "title": "3  Computing with R",
    "section": "3.2 Defining mathematical functions in R/mosaic",
    "text": "3.2 Defining mathematical functions in R/mosaic\nAs you progress through this book you will meet a dozen or so functions from R/mosaic. For instance, Chapter 4 introduces functions for drawing graphs. Chapter 7 shows how to plot data.\nIn this section, we introduce the R/mosaic function for creating mathematical functions.\nAs you know, our mathematical notation for defining a function looks like this: \\[h(t) \\equiv 1.5\\, t^2 - 2\\ .\\]\nThe R/mosaic equivalent to this is\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nLaying out the two notation forms side by side lets us label the elements they share:\n\nFor the human reading the mathematical notation, you know that the statement defines a function because you have been taught. Likewise, the computer needs to be told what to do with the provided information. That is the point of makeFun(). There are other R/mosaic commands that could take the same information and do something else with it, for example create a graph of the function or (for those who have had some calculus) create the derivative or the anti-derivative of the function.\nIn its simplest use, makeFun() takes only one argument. That argument must be in a form called a tilde expression. This name comes from the character tilde (~) in the middle. On the right-hand side of the tilde goes the name of the input: ~ t. On the left-hand side is the R expression for the formula to be used, written in terms of the input name and whatever parameters are used. For instance, Active R chunk 3.4 defines a parameterized version of \\(h()\\).\n\n\n\nActive R chunk 3.4: Arguments are always separated by commas.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn Active R chunk 3.4, three arguments are being provided:\n\nThe tilde expression a*t^2 - b ~ t\nA default value for parameter a.\nA default value for parameter b.\n\n\n\n\n\n\n\n\n\n\nTry it! 3.4: Evaluating h() at an input.\n\n\n\nIn Active R chunk 3.5, write each of the following commands.\n\nEvaluate h() for the input \\(t = 1\\).\nEvaluate h() for the input \\(t = 2\\).\nRepeat (i) and (ii), but override the default value of b, setting it instead to be \\(b=100\\).\n\nNaturally, you need to run the code in Active R chunk 3.4 before you can evaluate h().\n\n\n\nActive R chunk 3.5\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Computing with R</span>"
    ]
  },
  {
    "objectID": "Preliminaries/03-computing.html#names-and-assignment",
    "href": "Preliminaries/03-computing.html#names-and-assignment",
    "title": "3  Computing with R",
    "section": "3.3 Names and assignment",
    "text": "3.3 Names and assignment\nThe command\n\nh &lt;- makeFun(1.5*t^2 - 2 ~ t)\n\ngives the name h to the function created by makeFun(). Good choice of names makes your commands much easier for the human reader.\n\n\n\n\n\n\nValid and invalid names in R\n\n\n\nThe R language puts some restrictions on the names that are allowed. Keep these in mind as you create R names in your future work:\n\nA name is the only thing allowed on the left side of the storage arrow &lt;-. (For experts … there are additional allowed forms for the left-hand side. We will not need them in this book.)\nA name must begin with a letter of the alphabet, e.g. able, Baker, and so on.\nNumerals can be used after the initial letter, as in final4 or g20. You can also use the period . and underscore _ as in third_place. No other characters can be used in names: no minus sign, no @ sign, no / or +, no quotation marks, and so on.\n\nFor instance, while third_place is a perfectly legitimate name in R, the following are not: 3rd_place, third-place. But it is OK to have names like place_3rd or place3, etc., which start with a letter.\nR also distinguishes between letter case. For example, Henry is a different name than henry, even though they look the same to a human reader.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Computing with R</span>"
    ]
  },
  {
    "objectID": "Preliminaries/03-computing.html#formulas-in-r",
    "href": "Preliminaries/03-computing.html#formulas-in-r",
    "title": "3  Computing with R",
    "section": "3.4 Formulas in R",
    "text": "3.4 Formulas in R\nThe constraint of the keyboard means that computer formulas are written in a slightly different way than the traditional mathematical notation. This is most evident when writing multiplication and exponentiation. Multiplication must always be indicated with the * symbol, for instance \\(3 \\pi\\) is written 3*pi. For exponentiation, instead of using superscripts like \\(2^3\\) you use the “caret” character, as in 2^3. The best way to learn to implement mathematical formulas in a computer language is to read examples and practice writing them. Several examples are given in Table 3.1.\n\n\n\nTable 3.1: Arithmetic operations used frequently in this book. Here, the inputs are all numbers. But in the rest of the book we will often use names as the arguments.\n\n\n\n\n\nTraditional notation\nR notation\n\n\n\n\n\\(3 + 2\\)\n3 + 2\n\n\n\\(3 \\div 2\\)\n3 / 2\n\n\n\\(6 \\times 4\\)\n6 * 4\n\n\n\\(\\sqrt{\\strut4}\\)\nsqrt(4)\n\n\n\\(\\ln 5\\)\nlog(5)\n\n\n\\(2 \\pi\\)\n2 * pi\n\n\n\\(\\frac{1}{2} 17\\)\n(1 / 2) * 17\n\n\n\\(17 - 5 \\div 2\\)\n17 - 5 / 2\n\n\n\\(\\frac{17 - 5}{\\strut 2}\\)\n(17 - 5) / 2\n\n\n\\(3^2\\)\n3^2\n\n\n\\(e^{-2}\\)\nexp(-2)",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Computing with R</span>"
    ]
  },
  {
    "objectID": "Preliminaries/03-computing.html#footnotes",
    "href": "Preliminaries/03-computing.html#footnotes",
    "title": "3  Computing with R",
    "section": "",
    "text": "Note for R experts: Strictly speaking, the thing to the left of &lt;- must be an “assignable,” which includes names with indices (e.g. Engines$hp or Engines$hp[3:5] and other forms). We will not need indexing in MOSAIC Calculus↩︎",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Computing with R</span>"
    ]
  },
  {
    "objectID": "Preliminaries/07-data-functions-graphics.html",
    "href": "Preliminaries/07-data-functions-graphics.html",
    "title": "7  Data, functions, graphics",
    "section": "",
    "text": "7.1 Data frames\nThe organization of data as a table (as in Figure 7.1) is almost intuitive. However, data science today draws on much more sophisticated structures that are amenable to computer processing.\nData science is strikingly modern. Relational databases, the prime organization of data used in science, commerce, medicine, and government was invented in the 1960s. All data scientists have to master working with relational databases, but we will use only one component of them, the data frame.\nLet’s consider what Graunt’s 1665 data might look like in a modern data frame. Remember that the data in Figure 7.1 covers only one week (Aug 15-22, 1665) in only one place (London). A more comprehensive data frame might include data from other weeks and other places:\nAs you can see, the data frame is organized into columns and rows. Each column is called a variable and contains entries that are all the same kind of thing. For example, the location variable has city names. The deaths variable has numbers.\nEach row of the table corresponds to a unique kind of thing called a unit of observation. It’s not essential that you understand exactly what this means. It suffices to say that the unit of observation is a “condition of death during a time interval in a place.” Various everyday words are used for a single row: instance, case, specimen (my favorite), tupple, or just plane row. In a data frame, all the rows must be the same kind of unit of observation.\nThe modern conception of data makes a clear distinction between data and the construction of summaries of that data for human consumption. Such summaries might be graphical, or in the form of model functions, or even in the form of a set of tables, such as seen in the Bill of Mortality. Learning how to generate such summaries is an essential task in statistics and data science. The automatic construction of model functions (without much human intervention) is a field called machine learning, one kind of “artificial intelligence.”\nA data scientist would know how to process (or, “wrangle”) such data, for instance to use the begins and stops variables to calculate the duration of the interval covered. She would also be able to “join” the data table to others that contain information such as the population of the city or the mean temperature during the interval.\nTechnology allows us to store very massive data frames along with allied data. For example, a modern “bill of mortality” might have as a unit of observation the death of an individual person, including date, age, sex, occupation, and so on. Graunt’s bill of mortality encompasses 5319 deaths. Given that the population of the world in the 1660s was about 550 million, a globally comprehensive data frame on deaths covering only one year would have about 20 million rows. (Even today, there is no such globally comprehensive data, and in many countries births and deaths are not uniformly recorded.)\nA modern data wrangler would have no problem with 20 million rows, and would easily be able to pull out the data Graunt needed for his Aug. 15-22, 1665 report, summarizing it by the different causes of death and even breaking it down by age group. Such virtuosity is not needed for our purposes.\nThe basics that you need for our work with data are:",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data, functions, graphics</span>"
    ]
  },
  {
    "objectID": "Preliminaries/07-data-functions-graphics.html#sec-data-frames",
    "href": "Preliminaries/07-data-functions-graphics.html#sec-data-frames",
    "title": "7  Data, functions, graphics",
    "section": "",
    "text": "Table 7.1: A data frame organization of some data from Figure 7.1. Additional imagined “data” (in italics) has been added to illustrate why so many columns are needed, rather than Graunt’s two-column layout.\n\n\n\n\n\ncondition\ndeaths\nbegins\nstops\nlocation\n\n\n\n\nkingsevil\n10\n1665-08-15\n1665-08-22\nLondon\n\n\nlethargy\n1\n1665-08-15\n1665-08-22\nLondon\n\n\npalsie\n2\n1665-08-15\n1665-08-22\nLondon\n\n\nplague\n3880\n1665-08-15\n1665-08-22\nLondon\n\n\nspotted feaver\n190\n1665-07-12\n1665-07-19\nParis\n\n\nconsumption\n219\n1665-07-12\n1665-07-19\nParis\n\n\ncancer\n5\n1665-07-12\n1665-07-19\nParis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData frames are accessed by a file name.\nIndividual columns are accessed by a variable name.\nWe will use tilde expressions to identify one variable as the “response” variable and other variables as “explanatory variables.” The response corresponds to the output of a function, the explanatory variables are the inputs. See ?enr-inputs-response.\nWe will use software to construct functions that capture important patterns in the data, but that is a topic for later chapters.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data, functions, graphics</span>"
    ]
  },
  {
    "objectID": "Preliminaries/07-data-functions-graphics.html#accessing-data-tables",
    "href": "Preliminaries/07-data-functions-graphics.html#accessing-data-tables",
    "title": "7  Data, functions, graphics",
    "section": "7.2 Accessing data tables",
    "text": "7.2 Accessing data tables\nFor our work, you can access the data frames we need directly by name in R. For instance, the Engines data frame (Table 7.2) records the characteristics of several internal combustion engines of various sizes:\n\n\n\nTable 7.2: Various attributes of internal combustion engines, from the very small to the very large. Engines has 39 rows; only 8 are seen here.\n\n\n\n\n\nEngine\nmass\nBHP\nRPM\nbore\nstroke\n\n\n\n\nWebra Speed 20\n0.25\n0.78\n22000\n16.5\n16\n\n\nEnya 60-4C\n0.61\n0.84\n11800\n24.0\n22\n\n\nHonda 450\n34.00\n43.00\n8500\n70.0\n58\n\n\nJacobs R-775\n229.00\n225.00\n2000\n133.0\n127\n\n\nDaimler-Benz 609\n1400.00\n2450.00\n2800\n165.0\n180\n\n\nDaimler-Benz 613\n1960.00\n3120.00\n2700\n162.0\n180\n\n\nNordberg\n5260.00\n3000.00\n400\n356.0\n407\n\n\nCooper-Bessemer V-250\n13500.00\n7250.00\n330\n457.0\n508\n\n\n\n\n\n\nThe fundamental questions to ask first about any data frame are:\n\nWhat constitutes a row?\nWhat are the variables and what do they stand for?\n\nThe answers to these questions, for the data frames we will be using, are available via R documentation. To bring up the documentation for Engines, for instance, give the command:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWhen working with data, it is common to forget for a moment what are the variables, how they are spelled, and what sort of values each variable takes on. Two useful commands for reminding yourself are (illustrated here with Engines):\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn RStudio, the command View(Engines) is useful for showing a complete table of data in printed format. This may be useful for our work in this book, but is only viable for data frames of moderate size.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data, functions, graphics</span>"
    ]
  },
  {
    "objectID": "Preliminaries/07-data-functions-graphics.html#plotting-data",
    "href": "Preliminaries/07-data-functions-graphics.html#plotting-data",
    "title": "7  Data, functions, graphics",
    "section": "7.3 Plotting data",
    "text": "7.3 Plotting data\nWe will use just one graphical format for displaying data: the point plot. In a point plot, also known as a “scatterplot,” two variables are displayed, one on each graphical axis. Each case is presented as a dot, whose horizontal and vertical coordinates are the values of the variables for that case. For instance:\n\n\n\nActive R chunk 7.1: Running the code will create a point plot showing the relationship between engine stroke and bore. Each individual point is one row of the data frame\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nLater in MOSAIC Calculus, we will discuss ways to construct functions that are a good match to data using the pattern-book functions. Here, our concern is graphing such functions on top of a point plot. So, without explanation (until later chapters), we will construct a power-law function, called, stroke(bore), that might be a good match to the data. The we will add a second layer to the point-plot graphic: a slice-plot of the function we’ve constructed.\n\n\n\nActive R chunk 7.2: Code to make a graphic composed of two layers: 1) a point plot; 2) a slice plot of a power-law function named stroke() fitted to the data.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nThe second layer is made with an ordinary slice_plot() command. To place it on top of the point plot we connect the two commands with a bit of punctuation called a “pipe”: |&gt;.\n\n\n\n\n\n\nTip 7.1: Pipe punctuation\n\n\n\nThe pipe punctuation can never go at the start of a line. Usually, we will use the pipe at the very end of a line; think of the pipe as connecting one line to the next.\n\n\nslice_plot() is a bit clever when it is used after a previous graphics command. Usually, you need to specify the interval of the domain over which you want to display the function, as with …\n\nslice_plot(stroke_model(bore) ~ bore, domain(bore=0:1000))\n\n\n\n\n\n\n\n\nYou can do that also when slice_plot() is the second layer in a graphics command. But slice_plot() can also infer the interval of the domain from previous layers as in …\n\n1gf_point(stroke ~ bore, data = Engines) |&gt;\n2  slice_plot(stroke_model(bore) ~ bore)\n\n\n1\n\nThe graphics domain is set by the data.\n\n2\n\nslice_plot() inherits the domain from the layer created in (1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip 7.2: Response/Explanatory, Output/Inputs\n\n\n\nIn previous chapters, you have seen tilde expressions in use for two purposes:\n\nConstructing a function from a formula. e.g. g &lt;- makeFun(a*x + b ~ x)\nDirecting a slice plot, e.g. slice_plot(stroke(bore) ~ bore)\n\nThis chapter expands the use of tilde expressions to two new tasks:\n\nPlotting data, e.g. gf_point(stroke ~ bore)\nFitting a function to data, e.g. fitModel(stroke ~ A*bore^b, data=Engines)\n\nThere is an important pattern shared by all these tasks. In the tilde expression, the output is on the left-hand side of ~ while the inputs are on the right-hand side. That is:\n      function output          input(s)\nFor historical reasons, mathematics and data science/statistics use different terms. In data science, the terms “response” and “explanatory” are used instead of “input” and “output.”\n      response variable          explanatory variable(s)\nIn the tilde expression for fitModel() used above, the response variable is stroke while the explanatory variable is named in the RHS of the expression. As it happens, in fitModel() the RHS needs to do double duty: (1) name the explanatory variables (2) specify the formula for the function produced by fitModel().",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data, functions, graphics</span>"
    ]
  },
  {
    "objectID": "Preliminaries/07-data-functions-graphics.html#sec-functions-as-data",
    "href": "Preliminaries/07-data-functions-graphics.html#sec-functions-as-data",
    "title": "7  Data, functions, graphics",
    "section": "7.4 Functions as data",
    "text": "7.4 Functions as data\nIt is helpful to think of functions, generally, as a sort of data storage and retrieval device that uses the input value to locate the corresponding output and return that output to the user. Any device capable of this, such as a table or graph with a human interpreter, is a suitable way of implementing a function.\nTo reinforce this idea, we ask you to imagine a long corridor with a sequence of offices, each identified by a room number. The input to the function is the room number. To evaluate the function for that input, you knock on the appropriate door and, in response, you will receive a piece of paper with a number to take away with you. That number is the output of the function.\nThis will sound at first too simple to be true, but … In a mathematical function each office gives out the same number every time someone knocks on the door. Obviously, being a worker in such an office is highly tedious and requires no special skill. Every time someone knocks on the worker’s door, he or she writes down the same number on a piece of paper and hands it to the person knocking. What that person will do with the number is of absolutely no concern to the office worker.\n\n\n\n\n\n\nFigure 7.2: Part of the first table of logarithms, published by Henry Briggs in 1624.\n\n\n\nThe reader familiar with floors and corridors and office doors may note that the addresses are discrete. That is, office 321 has offices 320 and 322 as neighbors. But Calculus is mainly about functions with a continuous domain.\nFortunately, it is easy to create a continuous function out of a discrete table by adding on a small, standard calculation called “interpolation.” The simplest form, called “linear interpolation,” works like this: for an input of, say, 321.487… the messenger goes to both office 321 and 322 and collects their respective outputs. Let’s imagine that they are -14.3 and 12.5 respectively. All that is needed is a small calculation, which in this case will look like \\[-14.3 \\times (1 - 0.487...)   + 12.5 \\times 0.487...\\]\nChapter 49 introduces a modern, sophisticated form of interpolation that makes smooth functions.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data, functions, graphics</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/B5-Vectors.html",
    "href": "Linear-combinations/B5-Vectors.html",
    "title": "29  Vectors",
    "section": "",
    "text": "29.1 Length & direction\nA vector is a mathematical idea deeply rooted in everyday physical experience. Geometrically, a vector is simply an object consisting only of length and direction.\nA pencil is a physical metaphor for a vector, but a pencil has other non-vector qualities such as diameter, color, and an eraser. And, being a physical object, a pencil has a position in space.\nFigure 29.1: Three pencils, but just two vectors. The yellow and blue pencils have the same length and direction, so they are the same vector. A pencil has a position, but vectors do not. The green pencil shares the same direction but has a different length, so it differs from the blue/yellow vector.\nA line segment has an orientation but no forward or backward direction. In contrast, a vector has a unique direction: like an arrow, one end is the tip and the other the tail. In the pencil metaphor, the writing end is the tip; the eraser is the tail.\nFigure 29.2: Two different vectors. They have the same length and are parallel but point in opposite directions.\nVectors are always embedded in a vector space. Our physical stand-ins for vectors, the pencils, were photographed on a tabletop: a two-dimensional space. Naturally, pencils are embedded in everyday three-dimensional space. (The tabletop is a kind of two-dimensional subspace of three-dimensional space.)\nVectors embedded in three-dimensional space are central to physics and engineering. Quantities such as force, acceleration, and velocity are not simple numerical quantities but vectors with magnitude (that is, length) and direction. For instance, the statement, “The plane’s velocity is 450 miles per hour to the north-north-west,” is perfectly intelligible to most people, describing magnitude and direction. Note that the plane’s velocity vector does not specify the plane’s location; vectors have only the two qualities of magnitude and direction.\nThe gradients that we studied with partial differentiation (Chapter 24) are vectors. A gradient’s direction points directly uphill; its magnitude tells how steep the hill is.\nVectors often represent a change in position, that is, a step or displacement in the sense of “step to the left” or “step forward.” As we will see, constructing instructions for reaching a target is a standard mathematical task. Such instructions have a form like, “take three and a half steps along the green vector, then turn and take two steps backward along the yellow vector.” An individual vector describes a step of a specific length in a particular direction.\nVectors are a practical tool to keep track of relative motion. For instance, consider the problem of finding an aircraft heading and speed to intercept another plane that is also moving. The US Navy training movie from the 1950s shows how to perform such calculations with paper and pencil.\nNowadays, the computer performs such calculations. On the computer, vectors are represented not by pencils (!) but by columns of numbers. For instance, two numbers will do for a vector embedded in two-dimensional space and three for a vector embedded in three-dimensional space. From these numbers, simple arithmetic can produce the vector magnitude and direction.\nRepresenting a vector as a set of numbers requires the imposition of a framework: a coordinate system. In Figure 29.3, the vector (shown by the green pencil) lies in a two-dimensional coordinate system. The two coordinates assigned to the vector are the difference between the tip and the tail along each coordinate direction. In the figure, there are 20 units horizontally and 16 units vertically, so the vector is \\((20, 16)\\).\nFigure 29.3: Representing a vector as a set of numbers requires reference to a coordinate system, shown here as graph paper.\nBy convention, when we write a vector as a set of coordinate numbers, we write the numbers in a column. For instance, the vector in Figure 29.3, which we will call \\(\\vec{green}\\), is written numerically as:\n\\[\\vec{green} \\equiv \\left[\\begin{array}{c}20\\\\16\\end{array}\\right]\\] In more advanced linear algebra, the distinction between a column vector (like \\(\\vec{green}\\)) and a row vector (like \\(\\left[20 \\ 16\\right]\\)) is important. For our purposes in this block, we need only column vectors.\nIn physics and engineering, vectors describe positions, velocities, acceleration, forces, momentum, and other functions of time or space. In mathematical notation, a vector-valued function can be written \\(\\vec{v}(t)\\). It is common to perform calculus operations such differentiation, writing it as \\(\\partial_t \\vec{v}(t)\\). It is sometimes easier to grasp a vector-valued function by writing it as a column of scalar-valued functions: \\[\\vec{v}(t) = \\left[\\begin{array}{c}v_x(t)\\\\v_y(t)\\\\v_z(t)\\end{array}\\right]\\] where the \\(x\\), \\(y\\), and \\(z\\) refer to the axes of the coordinate system.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/B5-Vectors.html#length-direction",
    "href": "Linear-combinations/B5-Vectors.html#length-direction",
    "title": "29  Vectors",
    "section": "",
    "text": "Construct column vectors with the rbind() function, as in\n\ngreen &lt;- rbind(20, 16)\n\nCommas separate the arguments—the coordinate numbers—in the same way as any other R function.\nLater in this block, we will use data frames to define vectors. We will introduce the R syntax for that when we need it.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/B5-Vectors.html#the-nth-dimension",
    "href": "Linear-combinations/B5-Vectors.html#the-nth-dimension",
    "title": "29  Vectors",
    "section": "29.2 The nth dimension",
    "text": "29.2 The nth dimension\nIn many applications, especially those involving data, vectors have more than three components. Indeed, you will soon be working with vectors with hundreds of components. Services like Google search rely on vector calculations with millions of vectors, each having millions of components.\nLiving as we do in a palpably three-dimensional space and with senses and brains evolved for use in three dimensions, it is hard and maybe even impossible to grasp high-dimensional spaces.\nA lovely 1884 book, Flatland features the inhabitants of a two-dimensional world. The central character in the story, named Square, receives a visitor, Sphere. Sphere is from the three-dimensional world which embeds Flatland. Only with difficulty can Square assemble a conception of the totality of Sphere from the appearing, growing, and vanishing of Sphere’s intersection with the flat world. (Among other things, Flatland is a parody of humanity’s rigid thinking: Square’s attempt to convince Sphere that his three-dimensional world might be embedded in a four-dimensional one leads to rejection and disgrace. Sphere thinks he knows everything.)\n\n\n\n\n\n\n\n\n\nTo use high-dimensional vectors, represent them as a column of numbers.\n\\[\\left[\\begin{array}{r}6.4\\\\3.0\\\\-2.5\\\\17.3\\end{array}\\right]\\ \\ \\ \\left[\\begin{array}{r}-14.2\\\\-6.9\\\\18.0\\\\1.5\\\\-0.3\\end{array}\\right]\\ \\ \\ \\left[\\begin{array}{r}5.3\\\\-9.6\\\\84.1\\\\5.7\\\\-11.3\\\\4.8\\end{array}\\right]\\ \\ \\ \\cdots\\ \\ \\ \\left.\\left[\\begin{array}{r}7.2\\\\-4.4\\\\0.6\\\\-4.1\\\\4.7\\\\\\vdots\\ \\ \\\\-7.3\\\\8.3\\end{array}\\right]\\right\\} n\\]\nSensible people may consider it mathematical ostentation to promote simple columns of numbers into vectors in high-dimensional space. But doing so lets us draw the analogy between data and familiar geometrical concepts: lengths, angles, alignment, etc. Operations that are mysterious as a long sequence of arithmetic steps become concrete when seen as geometry.\nThere is nothing science-fiction-like about so-called “high-dimensional” spaces; they don’t correspond to a physical place. Nevertheless, many-component vectors often appear in advanced physics. Famously, the Theory of Relativity involves 4-dimensional space-time. The vector representing the state of an ordinary particle contains the position and velocity, \\((x, y, z, v_x, v_y, v_z)\\), as well as angular velocity: nine dimensions. In statistics, engineering, and statistical mechanics, the term degrees of freedom is the preferred alternative to “dimension.” Another example: computer-controlled machine tools have 5 degrees of freedom (or more). There is a cutting tool with an \\(x, y, z\\) position and orientation. (If ever you start to freak out about the idea of a 10-dimensional space, close your eyes and remember that this is only shorthand for the set of arrays with ten elements.)",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/B5-Vectors.html#geometry-arithmetic",
    "href": "Linear-combinations/B5-Vectors.html#geometry-arithmetic",
    "title": "29  Vectors",
    "section": "29.3 Geometry & arithmetic",
    "text": "29.3 Geometry & arithmetic\nThree mathematical tasks are essential to working with vectors:\n\nMeasure the length of a vector.\nMeasure the angle between two vectors.\nCreate a new vector by scaling a vector. Scaling makes the new vector longer or shorter and may reverse the orientation.\n\nWe have simple geometrical tools for undertaking these tasks: a ruler measures length, and a protractor measures angles. Along with pen and paper, these tools let us draw new vectors of any specified length.\nThe geometrical perspective is helpful for many purposes, but often we need to work with vectors using computers. For this, we use the numerical representation of vectors.\nThis section introduces the arithmetic of vectors. With this arithmetic in hand, we can carry out the above three tasks (and more!) on vectors that consist of a column of numbers. And while we can’t import a ruler, protractor, or paper into high-dimensional space, arithmetic is easy to do, regardless of dimension.\nTo scale a vector \\(\\vec{w}\\) means more or less to change the vector’s length. A good mental image for scaling sees the vector as a step or displacement in the direction of \\(\\vec{w}\\). Scaling means to go on a simple walk, taking one step after the other in the same direction as the \\(\\vec{w}\\). We write a scaled vector by placing a number in front of the name of the vector. \\(3 \\vec{w}\\) is a short walk of three steps; \\(117 \\vec{w}\\) is a considerably longer walk; \\(-5 \\vec{w}\\) means to take five steps backward. You can also take fraction steps: \\(0.5 \\vec{w}\\) is half a step, \\(19.3 \\vec{w}\\) means to take 19 steps followed by a 30% step. Scaling a vector by \\(-1\\) means flipping the vector tip-for-tail; this does not change the length, just the orientation.\nArithmetically, scaling a vector is accomplished simply by multiplying each of the vector’s components by the same number. Two illustrate, consider vectors \\(\\vec{v}\\) and \\(\\vec{w}\\), each with \\(n\\) components. (We use \\(\\vdots\\) to indicate components we haven’t bothered to write out.)\n\\[\\vec{v} \\equiv \\left[\\begin{array}{r}6\\\\2\\\\-4\\\\\\vdots\\\\1\\\\8\\end{array}\\right]\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\vec{w} \\equiv \\left[\\begin{array}{r}-3\\\\1\\\\-5\\\\\\vdots\\\\2\\\\5\\end{array}\\right]\\] To scale a vector by 3 is accomplished by multiplying each component by 3\n\\[3\\, \\vec{v} = 3\\left[\\begin{array}{r}6\\\\2\\\\-4\\\\\\vdots\\\\1\\\\8\\end{array}\\right] = \\left[\\begin{array}{r}18\\\\6\\\\-12\\\\\\vdots\\\\3\\\\24\\end{array}\\right]\\] Vector scaling is perfectly ordinary multiplication applied component by component, that is, *** componentwise ***.\nScaling involves a number (the “scalar”) and a single vector. Other sorts of multiplication involve two or more vectors.\nThe dot product is one sort of multiplication of one vector with another. The dot product between \\(\\vec{v}\\) and \\(\\vec{w}\\) is written \\[\\vec{v} \\bullet \\vec{w}\\].\nThe arithmetic of the dot product involves two steps:\n\nMultiply the two vectors componentwise. For instance: \\[\\underset{\\Large \\vec{v}}{\\left[\\begin{array}{r}6\\\\2\\\\-4\\\\\\vdots\\\\1\\\\8\\end{array}\\right]}\\  \\underset{\\Large \\vec{w}}{\\left[\\begin{array}{r}-3\\\\1\\\\-5\\\\\\vdots\\\\2\\\\5\\end{array}\\right]} = \\left[\\begin{array}{r}-18\\\\2\\\\20\\\\\\vdots\\\\2\\\\40 \\end{array}\\right]\\]\nSum the elements in the componentwise product. For the component-wise product of \\(\\vec{v}\\) and \\(\\vec{w}\\), this will be \\(-18 + 2 + 20 + \\cdots +2 + 40\\). The resulting sum is an ordinary scalar quantity; a dot product takes two vectors as inputs and produces a scalar as an output.\n\n\nR/mosaic provides a beginner-friendly function for computing a dot product. To mimic the use of the dot, as in \\(\\vec{v} \\bullet \\vec{w}\\), the function will be invoked using infix notation. You have a lot of infix notation experience, even if you have never heard the term. Some examples:\n3 + 2       7 / 4      6 - 2      9 * 3     2 ^ 4\nInfix notation is distinct from the functional notation that you are also familiar with, for instance sin(2) or makeFun(x^2 ~ x).\nYou can, if you like, invoke the +, -, *, /, and ^ operations using functional notation. Nobody does this because the commands are so ugly:\n\n\n`+`(3, 2)\n\n\n## [1] 5\n\n\n\n`/`(7, 4)\n\n\n## [1] 1.75\n\n\n\n`-`(6, 2)\n\n\n## [1] 4\n\n\n\n`*`(9, 3)\n\n\n## [1] 27\n\n\n\n`^`(2, 4)\n\n\n## [1] 16\n\n\n\n \nThe R language makes it possible to define new infix operators, but there is a catch. The new operators must always have a name that begins and ends with the % symbol, for example, %&gt;% or %*% or %dot%.\nHere is an example of using %dot% to calculate the dot product of two vectors embedded in five-dimensional space:\n\na &lt;- rbind(1, 2, 3, 5, 8, 13)\nb &lt;- rbind(1, 4, 2, 3, 2, -1)\na %dot% b\n## [1] 33\n\nThe vectors combined with %dot% must both have the same number of elements. Otherwise, an error message will result, as here:\n\nrbind(2, 1) %dot% rbind(3, 4, 5)\n## Error in rbind(2, 1) %dot% rbind(3, 4, 5): Vector &lt;u&gt; must have the same number of elements as vector &lt;b&gt;.\n\n\n\nWe have not yet shown you the use of the dot product in applications. At this point, remember that a dot product is not ordinary multiplication but a two-stage operation of pairwise multiplication followed by summation.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/B5-Vectors.html#sec-vector-length",
    "href": "Linear-combinations/B5-Vectors.html#sec-vector-length",
    "title": "29  Vectors",
    "section": "29.4 Vector lengths",
    "text": "29.4 Vector lengths\nThe arithmetic used to calculate the length of a vector is based on the Pythagorean theorem. For a vector \\(\\vec{u} = \\left[\\begin{array}{c}4\\\\3\\end{array}\\right]\\) the vector is the hypotenuse of a right triangle with legs of length 4 and 3 respectively. Therefore, \\[\\|\\vec{u}\\| = \\sqrt{4^2 + 3^2} = 5\\ .\\] For vectors with more than two components, follow the same pattern: sum the squares of the components, then take the square root.\nCompute the length of a vector \\(\\vec{u}\\) using the dot product: \\[\\|\\vec{u}\\| = \\sqrt{\\strut\\vec{u} \\bullet \\vec{u}}\\ .\\] Although length has an obvious physical interpretation, in many areas of science, including statistics and quantum physics, the square length is a more fundamental quantity. The square length of \\(\\vec{u}\\) is simply \\(\\|\\vec{u}\\|^2 = \\vec{u}\\bullet \\vec{u}\\).\n\nConsider the two vectors \\[\\vec{u} \\equiv \\left(\\begin{array}{c}3\\\\4\\end{array}\\right) \\  \\  \\ \\mbox{and}  \\ \\ \\ \\vec{w} \\equiv \\left(\\begin{array}{c}1\\\\1\\\\1\\\\1\\end{array}\\right)\n\\]\nThe length of \\(\\vec{u}\\) is \\(|| \\vec{u} || = \\sqrt{\\strut 3^2 + 4^2} = \\sqrt{\\strut 25} = 5\\).\nThe length of \\(\\vec{w}\\) is \\(|| \\vec{w} || = \\sqrt{\\strut 1^2 + 1^2 + 1^2 + 1^2} = \\sqrt{\\strut 4} = 2\\).\n\n\nApplication area 29.1 —Linear geometry and statistics\n\n\n\n\n\n\n\nApplication area 29.1 Statistical modeling\n\n\n\nIn statistics, the many applications of linear algebra often involve a simple constant vector, which we will write \\(\\vec{1}\\). It is simply a column vector of 1s, \\[\\vec{1} \\equiv \\left[\\begin{array}{c}1\\\\1\\\\1\\\\\\vdots\\\\1\\\\1\\\\ \\end{array}\\right]\\ .\\] Common statistical calculations can be expressed compactly in vector notation. For example, if \\(\\vec{x}\\) is an \\(n\\)-dimensional vector, then the mean of the components of \\(\\vec{x}\\), which is often written \\(\\bar{x}\\), is \\[\\bar{x} \\equiv \\frac{1}{n}\\  \\vec{x} \\bullet \\vec{1}\\ .\\] The symbol \\(\\bar{}\\) is pronounced “bar”, and \\(\\bar{x}\\) is pronounced “x-bar.”.\nAnother commonly used statistic is the variance of the components of a vector \\(\\vec{x}\\). Calculating the variance is more complicated than the mean: \\[\\text{var}(x) \\equiv \\frac{1}{n-1}\\  (\\vec{x} - \\bar{x}) \\bullet (\\vec{x} - \\bar{x})\\ .\\] The quantity \\(\\vec{x} - \\bar{x}\\) is an example of scalar subtraction, which is done on a component-wise basis. For instance, with \\[\\vec{x} = \\left[\\begin{array}{r}1\\\\2\\\\3\\\\4\\\\\\end{array}\\right]\\] then \\(\\bar{x} = 2.5\\). This being the case, \\[\\vec{x} - \\bar{x} = \\left[\\begin{array}{c}-1.5\\\\-0.5\\\\\\ 0.5\\\\\\ 1.5\\\\\\end{array}\\right]\\ ,\\] with the variance of \\(\\vec{x}\\) being \\[\\frac{1}{4-1} \\left[\\begin{array}{r}-1.5\\\\-0.5\\\\0.5\\\\1.5\\\\\\end{array}\\right] \\bullet \\left[\\begin{array}{r}-1.5\\\\-0.5\\\\0.5\\\\1.5\\\\\\end{array}\\right] = \\frac{5}{3}\\ .\\]",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/B5-Vectors.html#sec-angles-dot-product",
    "href": "Linear-combinations/B5-Vectors.html#sec-angles-dot-product",
    "title": "29  Vectors",
    "section": "29.5 Angles",
    "text": "29.5 Angles\nAny two vectors of the same dimension have an angle between them. Vectors have only two properties: length and direction. To find the angle between two vectors, pick up one vector and relocate its “tail” to meet the tail of the other vector.\nMeasure the angle between two vectors the short way round: between 0 and 180 degrees. Any larger angle, say 260 degrees, will be identified with its circular complement: 100 degrees is the complement of a 260-degree angle.\nIn 2- and 3-dimensional spaces, we can measure the angle between two vectors using a protractor: arrange the two vectors tail to tail, align the baseline of the protractor with one of the vectors and read off the angle marked by the second vector.\nIt is also possible to measure the angle using arithmetic. Suppose we have vectors \\(\\vec{v}\\) and \\(\\vec{w}\\) embedded in the same dimensional space. That is, \\(\\vec{v}\\) and \\(\\vec{w}\\) have the same number of components:\n\\[\\vec{v} = \\left[\\begin{array}{c}v_1\\\\v_2\\\\\\vdots\\\\v_n\\\\\\end{array}\\right]\\ \\ \\ \\text{and}\\ \\ \\ \\vec{w} = \\left[\\begin{array}{c}w_1\\\\w_2\\\\\\vdots\\\\w_n\\\\\\end{array}\\right]\\ ,\\]\nUsing the dot-product and length notation, we can write the formula for the cosine of the angle between two vectors as \\[\\cos(\\theta) \\equiv \\frac{\\vec{v}\\cdot\\vec{w}}{\\|\\vec{v}\\|\\ \\|\\vec{w}\\|}\\ .\\]\n\nRemember that the dot-product-based formula above gives the cosine of the angle between the two vectors. It turns out that in many applications, cosine is what’s needed. If you insist on knowing the angle \\(\\theta\\) rather than \\(\\cos(\\theta)\\), the trigonometric function \\(\\arccos()\\) will do the job. For instance, if \\(\\theta\\) is such that \\(\\cos(\\theta) = 0.6\\), compute the angle in degrees with\n\nacos(0.6)*180/pi\n## [1] 53.1301\n\nThe trigonometric functions in R (and in most other languages) do calculations with angles in units of radians. Multiplication by 180/pi converts radians to degrees. Figure 29.4 shows a graph of converting \\(\\cos(\\theta)\\) to \\(\\theta\\) in degrees.\n\n\n\n\n\n\n\n\nFigure 29.4: The \\(\\arccos()\\) function (acos() in R) converts \\(\\cos(\\theta)\\) to \\(\\theta\\).\n\n\n\n\n\n\n\nApplication area 29.2 —Another example of how geometry and statistics have much in common.\n\n\n\n\n\n\n\nApplication area 29.2 “Correlation” is an angle\n\n\n\nWhat does the angle \\(\\theta\\) between two vectors tell us?\nThe angle quantifies the alignment of the vectors. An angle of 0 tells us the vectors point in the same direction, and an angle of 180 degrees means that the vectors point in exactly opposing directions. Either of these—0 or 180 degrees—indicates that the two vectors are perfectly aligned. Such alignment means that appropriate scalar multiplication can make the two vectors equal.\nAngles such as 5 or 175 degrees indicate that the two vectors are mostly aligned but imperfectly. When the angle is 90 degrees—a right angle—the two vectors are perpendicular.\nThe vector alignment has an important meaning in terms of data. Suppose the two vectors are two columns in a data frame: two different variables. In statistics, the correlation coefficient, denoted \\(r\\), is a simple way to describe the relationship between two variables. A non-zero correlation indicates a connection between two variables. For instance, among children, height and age are correlated. Since height increases along with age (for children), the two variables are said to be positively correlated. The largest possible correlation is \\(r=1\\).\nA negative correlation means that one variable decreases as the other increases. Temperature and elevation are negatively correlated; temperature goes down as elevation goes up. The most negative possible correlation is \\(r=-1\\).\nA zero correlation indicates no simple (linear) relationship between the two variables. Zero correlation occurs when the variables are orthogonal, a term described in Section 29.6.\nSeeing columns in a data frame as vectors, the correlation coefficient \\(r\\) is exactly the cosine of the angle between the vectors. However, when Francis Galton invented the correlation coefficient in the 1880s, he did not describe it in these terms. Instead, he used arithmetic directly, producing formulas with which many generations of statistics students have struggled. Those students might have done better in statistics if \\(r\\) had been called alignment and measured in degrees.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/B5-Vectors.html#sec-orthogonality",
    "href": "Linear-combinations/B5-Vectors.html#sec-orthogonality",
    "title": "29  Vectors",
    "section": "29.6 Orthogonality",
    "text": "29.6 Orthogonality\nTwo vectors are said to be orthogonal when the angle between them is 90 degrees. In everyday speech, we call a 90-degree angle a “right angle.” The word “orthogonal” is a literal translation of “right angle.” (The syllable “gon” indicates an angle, as in the five-angled pentagon or six-angled hexagon. “Ortho” means “right” or “correct,” as in “orthodox” (right beliefs) or “orthodontics” (right teeth) or “orthopedic” (right feet).)\nTwo vectors are at right angles—we prefer “orthogonal” since “right” has many meanings not related to angles—when the dot product between them is zero.\n\n\n \n\nFind a vector that is orthogonal to \\(\\left[\\strut\\begin{array}{r}1\\\\2\\end{array}\\right]\\).\nThe arithmetic trick is to reverse the order of the components and put a minus sign in front of one of them, so \\(\\left[\\strut\\begin{array}{r}-2\\\\1\\end{array}\\right]\\).\nWe can confirm the orthogonality by calculating the dot product: \\(\\left[\\begin{array}{c}-2\\\\\\ 1\\end{array}\\right] \\cdot \\left[\\strut\\begin{array}{r}1\\\\2\\end{array}\\right] = -2\\times1 + 1 \\times 2 = 0\\).\nIn R, write this as\n\nu &lt;- rbind( 1, 2)\nv &lt;- rbind(-2, 1)\nu %dot% v\n## [1] 0\n\n\n\n\n \n\nFind a vector orthogonal to \\(\\left[\\strut\\begin{array}{r}1\\\\2\\\\3\\end{array}\\right]\\).\nWe have a little more scope here. A simple approach is to insert a zero component in the new vector and then use the two-dimensional trick to fill in the remaining components.\nFor instance, starting with \\(\\left[\\strut\\begin{array}{r}0\\\\\\_\\\\ \\_\\end{array}\\right]\\) the only non-zero components of the dot product will involve the 2 and 3 of the original vector. So \\(\\left[\\strut\\begin{array}{r}0\\\\ -3\\\\ 2\\end{array}\\right]\\) is orthogonal. Or, if we start with \\(\\left[\\strut\\begin{array}{r}\\_\\\\0\\\\\\_\\end{array}\\right]\\) we would construct \\(\\left[\\strut\\begin{array}{r}-3\\\\ 0\\\\ 1\\end{array}\\right]\\).",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "Accumulation/36-functions.html",
    "href": "Accumulation/36-functions.html",
    "title": "37  Functions as vectors",
    "section": "",
    "text": "37.1 Dot product for functions\nGiven two functions, \\(f(t)\\) and \\(g(t)\\) defined over some domain \\(D\\), we will compute the dot product of the functions as a sum of the product of the two functions, that is: \\[f(t) \\bullet g(t) \\equiv \\int_{D} f(t)\\,g(t)\\,dt\\ .\\] ::: {.example data-latex=““} Suppose that our two functions are \\(\\text{one}(t) \\equiv 1\\) and \\(\\text{identity}(t) \\equiv t\\) on the domain \\(0 \\leq t \\leq 1\\). Find the length of each function and the included angle between them.\nThe left panel of Figure 37.1 shows the functions \\(f(t) \\equiv t^2\\) and \\(\\color{magenta}{\\widehat{f(t)} \\equiv 1/3}\\) on the domain. The center panel shows the residual function, that is \\(f(t) - \\widehat{f(t)}\\). The right panel gives the square of the length of the residual function, which is \\(\\int_{-1}^1 \\left[f(t) - \\widehat{f(t)}\\right]^{1/2}\\, dt\\) as indicated by the area shaded in \\(\\color{blue}{\\text{blue}}\\).\nFigure 37.1: Projecting \\(f(t) \\equiv t^2\\) onto \\(g(t) \\equiv \\text{one}(t)\\).",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Functions as vectors</span>"
    ]
  },
  {
    "objectID": "Accumulation/36-functions.html#dot-product-for-functions",
    "href": "Accumulation/36-functions.html#dot-product-for-functions",
    "title": "37  Functions as vectors",
    "section": "",
    "text": "Length: \\(\\|\\text{one}(t)\\| = \\left[\\int_0^1 1 \\cdot 1\\,dt\\right]^{1/2} = \\left[\\ \\strut t\\left.{\\large\\strut}\\right|_0^1\\ \\right]^{1/2} = 1\\)\nLength: \\(\\|\\text{identity}(t)\\| = \\left[\\int_0^1 t \\cdot t\\,dt\\right]^{1/2} = \\left[\\ \\strut \\frac{1}{2}t^2\\left.{\\large\\strut}\\right|_0^1\\ \\right]^{1/2} = \\frac{1}{\\sqrt{2}}\\)\nIncluded angle: \\[\\cos(\\theta) = \\frac{\\text{one}(t) \\bullet \\text{identity}(t)}{\\|\\strut\\text{one}(t)\\| \\, \\|\\text{identity}(t)\\|}  =\n\\sqrt{2}\\ \\int_0^1 t\\, dt = \\sqrt{\\strut 2} \\left.{\\Large\\strut}\\frac{1}{2} t^2\\right|_0^1 = \\sqrt{\\frac{1}{2}}\\] Since \\(\\cos(\\theta) = \\sqrt{1/2}\\), the angle \\(\\theta\\) is 45 degrees. :::\n\n\nProject \\(f(t) \\equiv t^2\\) onto \\(g(t) = \\text{one}(t)\\) over the domain \\(-1 \\leq t \\leq 1\\).\nThe projection of \\(f(t)\\) onto \\(g(t)\\) will be \\[\\widehat{f(t)} = \\frac{f(t) \\bullet g(t)}{g(t) \\bullet g(t)}\\ g(t)\\]\n\n\\(f(t) \\bullet g(t) \\equiv \\int_{-1}^{1} t^2 dt = \\frac{1}{3} \\left.{\\Large \\strut}t^3\\right|_{-1}^{1} = \\frac{2}{3}\\)\n\\(g(t) \\bullet g(t) \\equiv \\int_{-1}^1 \\ dt = 2\\)\n\nThus, \\[\\widehat{f(t)} = \\frac{1}{3} \\text{one(t)} = \\frac{1}{3}\\ .\\]\n\n\n\n\nApplication area 37.1 — Seeing sounds as sinusoids.\n\n\n\n\n\n\n\nApplication area 37.1 Sinusoids and sounds\n\n\n\nThe table links to audio files recorded by a human speaker voicing various vowels. Play the sounds to convince yourself that they really are the vowels listed. (It may help to use the controls to slow down the playback.)\nVowel | Player\n------|-------\n\"o\" as in \"stone\" | &lt;audio controls&gt;&lt;source src = \"https://linguistics.ucla.edu/people/hayes/103/Charts/VChart/o.wav\" type = \"audio/wav\"&gt;&lt;/audio&gt;\n\"e\" as in \"eel\" | &lt;audio controls&gt;&lt;source src = \"https://linguistics.ucla.edu/people/hayes/103/Charts/VChart/y.wav\" type = \"audio/wav\"&gt;&lt;/audio&gt;\nAs you may know, the physical stimuli involved in sound are rapid oscillations in air pressure. Our standard model for oscillations is the sinusoid function, which is parameterized by its period and its amplitude. The period of a sound oscillation is short: between 0.3 and 10 milliseconds. The amplitude is small. To get a sense for how small, consider the change in air pressure when you take an elevator up 10 stories in a building. The pressure amplitude of sound at a conversational level of loudness corresponds to taking that elevator upward by 1 to 10 mm.\nThe shapes of the “e” (as in “eel”) and “o” (as in “stone”) sound waves—in short, the waveforms—are drawn in ?fig-sound-waves.\n\n\n\n\n\nThe waveforms of two vowel sounds. Only about five hundredths of a second is shown.\n\n\n\n\nThe function resembles none of our small set of pattern-book functions. It is more complicated, more detailed, more irregular than any of the basic modeling functions featured in this book.\nFor many tasks it is helpful to have a modeling approach that is well suited to such detailed and irregular functions. For example, we might want to identify the speaker from a recording, or to play the recording slower or faster without changing the essence of the sound, or to tweak the function to have additional properties such as being exactly on tune while maintaining its individuality as a sound.\nA remarkable aspect of the waveforms in ?fig-sound-waves is their periodicity. The 0.05 sec graphics domain shown includes roughly seven repetitions of a basic waveform. That is, each cycle lasts about \\(\\frac{0.05 \\text{s}}{7} \\approx 7 \\text{ms}\\). what distinguishes the “e” waveform from the “o” waveform is the shape of the waveform that is being repeated. The individual cycle of the “o” has three peaks of diminishing amplitude. The “e” cycle has two main peaks, high then low. It also has a very fast wiggle superimposed on the two peaks.\nAn important strategy for modeling such complicated oscillations is to decompose (synonym: analyze) them into a linear combination of simpler parts.",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Functions as vectors</span>"
    ]
  },
  {
    "objectID": "Accumulation/36-functions.html#sinusoids-as-vectors",
    "href": "Accumulation/36-functions.html#sinusoids-as-vectors",
    "title": "37  Functions as vectors",
    "section": "37.2 Sinusoids as vectors",
    "text": "37.2 Sinusoids as vectors\nThe sinusoid is our fundamental model of periodic phenomena. To get started with using sinusoids as vectors, we will start with a simple setting: a single sinusoid of a specified frequency.\nFigure 37.2 shows three sinusoids all with the same frequency, but shifted somewhat in time:\n\n\n\n\n\n\n\n\nFigure 37.2: Three sinusoids with a frequency of \\(\\omega=3\\) cycles per second.\n\n\n\n\n\nSince we have a dot product for functions, we can treat each of the three sinusoids as a vector. For instance, consider the length of waveforms A and B and the included angle between them.\n\n\n## vector lengths \nlengthA &lt;- Integrate(waveA(t) * waveA(t) ~ t, bounds(t=0:1)) |&gt; sqrt() \n## Loading required namespace: cubature\nlengthA\n## [1] 0.7071068\nlengthB &lt;- Integrate(waveB(t) * waveB(t) ~ t, bounds(t=0:1)) |&gt; sqrt()\nlengthB\n## [1] 0.7071068\nlengthC &lt;- Integrate(waveC(t) * waveC(t) ~ t, bounds(t=0:1)) |&gt; sqrt()\nlengthC\n## [1] 0.7071068\n## dot products\ndotAB   &lt;- Integrate(waveA(t) * waveB(t) ~ t, bounds(t=0:1)) \ndotAB\n## [1] -3.984443e-18\ndotAC   &lt;- Integrate(waveA(t) * waveC(t) ~ t, bounds(t=0:1))\ndotAC\n## [1] -0.1545085\ndotBC   &lt;- Integrate(waveB(t) * waveC(t) ~ t, bounds(t=0:1))\ndotBC\n## [1] -0.4755283\n\n\nThe cosine of the included angle \\(\\theta\\) between functions A and B is calculated using the the dot product formula: \\[\\cos(\\theta) = \\frac{A\\bullet B}{\\|A\\|\\, \\|B\\|}\\] or, computationally\n\ndotAB / (lengthA * lengthB)\n## [1] -7.968886e-18\n\nSince \\(\\cos(\\theta) = 0\\), wave A and B are orthogonal. Admittedly, there is no right angle to be perceived from the graph, but the mathematics of angles gives this result.\nThe graphical presentation of orthogonality between waveforms A and B is easier to appreciate if we plot out the dot product itself: the integral of waveform A times waveform B. Figure 37.3 shows this integral using colors, blue for positive and orange for negative. The integral is zero, since the positive (blue) areas exactly equal the negative (orange) areas.\n\n\n\n\n\n\n\n\nFigure 37.3: The dot product between waveforms A and B, graphically.\n\n\n\n\n\nIn contrast, waveform A is not orthogonal to waveform C, and similarly for waveform B. ?fig-AC-BC shows this graphically: the positive and negative areas in the two integrals do not cancel out to zero.\n\n\n\n\n\nThe dot products between waveforms A and C (top panel) and between B and C (bottom panel).\n\n\n\n\nWe can project waveform C onto the 2-dimensional subspace spanned by A and B. Since waveforms A and B are orthogonal, This can be done simply by projecting C onto each of A and B one at a time. Here’s a calculation of the scalar multipliers for A and for B and the model vector (that is, the component of C in the A-B subspace):\n\nA_coef &lt;- dotAC / lengthA^2\nB_coef &lt;- dotBC / lengthB^2\nmod_vec &lt;- makeFun(A_coef*waveA(t) + B_coef*waveB(t) ~ t)\n# length of mod_vec\nIntegrate(mod_vec(t)*mod_vec(t) ~ t, bounds(t=0:1)) |&gt; sqrt()\n## [1] 0.7071068\n\nYou can see that the length of the model vector is the same as the length of the vector being projected. This means that waveform C lies exactly in the subspace spanned by waveforms A and B.\nA time-shifted sinusoid of frequency \\(\\omega\\) can always be written as a linear combination of \\(\\sin(2\\pi\\omega t)\\) and \\(\\cos(2\\pi\\omega t)\\). The coefficients of the linear combination tell us both the amplitude of the time-shifted sinusoid and the time shift.\n\nConsider the function \\(g(t) \\equiv 17.3 \\sin(2*pi*5*(t-0.02)\\) on the domain \\(0 \\leq t \\leq 1\\) seconds. The amplitude is 17.3. The time shift is 0.02 seconds. Let’s confirm this using the coefficients on the linear combination of sine and cosine of the same frequency.\n\ng &lt;- makeFun(17.3 * sin(2*pi*5*(t-0.02)) ~ t)\nsin5 &lt;- makeFun(sin(2*pi*5*t) ~ t)\ncos5 &lt;- makeFun(cos(2*pi*5*t) ~ t)\nA_coef &lt;- Integrate(g(t) * sin5(t) ~ t, bounds(t=0:1)) /\n  Integrate(sin5(t) * sin5(t) ~ t, bounds(t=0:1))\nA_coef\n## [1] 13.99599\nB_coef &lt;- Integrate(g(t)*cos5(t) ~ t, bounds(t=0:1)) /\n  Integrate(cos5(t) * cos5(t) ~ t, bounds(t=0:1))\nB_coef\n## [1] -10.16868\n\nThe amplitude of \\(g(t)\\) is the Pythagorean sum of the two coefficients:\n\nsqrt(A_coef^2 + B_coef^2)\n## [1] 17.3\n\nThe time delay involves the ratio of the two coefficients:\n\natan2(B_coef, A_coef) / (2*pi*5) \n## [1] -0.02\n\nFor our purposes here, we will need only the Pythagorean sum and will ignore the time delay.\n\n?fig-cello-seg (top) shows the waveform of a note played on a cello. The note lasts about 1 second. The bottom panel zooms in on the waveform, showing 82 ms (that is, 0.082 s).\n\n\n\n\n\nWaveform recorded from a cello.\n\n\n\n\nThe whole note starts with a sharp “attack,” followed by a long period called a “sustain,” and ending with a “decay.” Within the sustain and decay, the waveform is remarkably repetitive, seen best in the bottom panel of the figure.\nIf you count carefully in the bottom panel, you will see that the waveform completes 9 cycles in the 0.082 s graphical domain. This means that the period is 0.082 / 9 = 0.0091 s. The frequency \\(\\omega\\) is the reciprocal of this: 1/0.0091 = 109.76 Hz. That is, the cello is vibrating about 110 times per second.\nIn modeling the cello waveform as a linear combination of sinusoids, the frequencies we use ought to respect the period of the cello vibration. Figure 37.4 shows the original waveform as well as the projection of the waveform onto a sinusoid with a frequency of 109.76 Hz. The figure also shows the residual from the projection, which is simply the original waveform minus the projected version.\n\n\n\n\n\n\n\n\nFigure 37.4: Top: The cello waveform and its projection onto a sinusoid with frequency \\(\\omega = 109.76\\) Hz. Bottom: The residual from the projection.\n\n\n\n\n\nThe sinusoid with \\(\\omega = 109.76\\) is not the only one that will repeat every 0.0091 s. So will a sinusoid with frequency \\(2\\omega = 219.52\\), one with frequency \\(3\\omega = 329.28\\) and so on. These multiples of \\(\\omega\\) are called the harmonics of that frequency. In Figure 37.5 (top) the cello waveform is projected onto \\(\\omega\\) and its first harmonic \\(2\\omega\\). In the middle panel, the projection is made onto \\(\\omega\\) and its first three harmonics. In the bottom panel, the projection is onto \\(\\omega\\) and its first eight harmonics.\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs the number of harmonics increases, the approximation gets better and better.\nUntil now, all the plots of the cello waveform have been made in what’s called the time domain. That is, the horizontal axis of the plots has been time, as seems natural for a function of time.\nThe decomposition into sinusoids offers another way of describing the cello waveform: the frequency domain. In the frequency domain, we report the amplitude and phase of the projection onto each frequency, plotting that versus frequency. Figure 37.6 shows the waveform in the frequency domain.\n\n\n\n\n\n\n\n\nFigure 37.6: The frequency domain description of the cello waveform.\n\n\n\n\n\nFrom the amplitude graph in Figure 37.6, you can see that only a handful of frequencies account for almost all of the signal. Thus, the frequency domain representation is in many ways much more simple and compact than the time domain representation.\nThe frequency domain description is an important tool in many fields. As you will see in Block 6, models of many kinds of systems, from the vibrations of buildings during an earthquake, aircraft wings in response to turbulence, and the bounce of a car moving over a rutted road have a very simple form when stated in the frequency domain. Each sinusoid in the input (earthquake shaking, air turbulence, rutted road) gets translated into the same frequency sinusoid in the output (building movement, wing bending, car bound): just the amplitude and phase of the sinusoid is altered.\nThe construction of the frequency domain description from the waveform is called a Fourier Transform, one of the most important techiques in science.\n\nApplication area 37.2 — Molecules as tuning forks!\n\n\n\n\n\n\n\nApplication area 37.2 Molecular spectroscopy\n\n\n\nAn important tool in chemistry is molecular vibrational spectroscopy in which a sample of the material is illuminated by an infrared beam of light. The frequency of infrared light ranges from about \\(300 \\times 10^7\\) Hz to \\(400 \\times 10^{10}\\) Hz, about 30 million to 40 billion times faster than the cello frequency.\nInfrared light is well suited to trigger vibrations in the various bonds of a molecule. By measuring the light absorbed at each frequency, a frequency domain picture can be drawn of the molecules in the sample. This picture can be compared to a library of known molecules to identify the makeup of the sample.\nThe analogous procedure for stringed musical instruments such as the cello or violin would be to rap on the instrument and record the hum of the vibrations induced. The Fourier transform of these vibrations effectively paint a picture of the tonal qualities of the instrument.\n\n\n\nFrom Taylor to Lagrange\nIn Chapter 26 we met a method introduced by Brook Taylor (1685–1731) to construct a polynomial of order-\\(n\\) that approximates any smooth function \\(f(x)\\) close enough to some center \\(x_0\\). The method made use of the ability to differentiate \\(f(x)\\) at \\(x_0\\) and produced the general formula: \\[f(x) \\approx f(x_0) + \\frac{f'(x_0)}{1} \\left[x-x_0\\right] + \\frac{f''(x_0)}{2!} \\left[x-x_0\\right]^2 + \\frac{f'''(x_0)}{3!} \\left[x-x_0\\right]^3 + \\cdots + \\frac{f^{(n)}(x_0)}{n!} \\left[x-x_0\\right]^n\\] where \\(f'(x_0) \\equiv \\partial_x f(x)\\left.{\\Large\\strut}\\right|_{x=x_0}\\) and so on.\nUsing polynomials as approximating functions has been an important theme in mathematics history. Brook Taylor was neither the first nor the last to take on the problem.\nIn 1795, Joseph-Louis Lagrange (1736 – 1813) published another method for constructing an approximating polynomial of order \\(n\\). Whereas the Taylor polynomial builds the polynomial that exactly matches the first \\(n\\) derivatives at the center point \\(x_0\\), the Lagrange polynomial has a different objective: to match exactly the values of the target function \\(f(x)\\) at a set of knots (input values) \\(x_0\\), \\(x_1\\), \\(x_2\\), \\(\\ldots, x_n\\). ?fig-lagrange-sine shows the situation with the knots shown as orange dots.\n\n## Warning: Removed 38 rows containing missing values (`geom_line()`).\n## Warning: Removed 8 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\nFigure 37.7: The Lagrange polynomial of order \\(n\\) is arranged to pass exactly through \\(n+1\\) points on the graph of a function \\(f(x)\\).\n\n\n\n\n\nThe Lagrange polynomial is constructed of a linear combinations of functions, one for each of the knots. In the example of Figure @ref(fig:lagrange-sine), there are 6 knots, hence six functions being combined. For knot 2, for instance, has coordinates \\(\\left(\\strut x_2, f(x_2)\\right)\\) and the corresponding function is:\n\\[p_2(x) = \\frac{(x-x_1)}{(x_2 -x_1)}\\left[\\strut\\cdot\\right]\\frac{(x-x_3)(x-x_4)(x-x_5)(x-x_6)}{(x_2 -x_3)(x_2 -x_4)(x_2 -x_5)(x_2 -x_6)}\\] The gap indicated by \\(\\left[\\strut\\cdot\\right]\\) marks where a term being excluded. For \\(p_2(x)\\) that excluded term is \\(\\frac{(x-x_2)}{(x_2 - x_2)}\\). The various functions \\(p_1(x)\\), \\(p_2(x)\\), \\(p_3(x)\\) and so on each leave out an analogous term.\nThree important facts to notice about these ingenious polynomial functions:\n\nThey all have the same polynomial order. For \\(k\\) knots, the order is \\(k-1\\).\nEvaluated at \\(x_i\\), the value of \\(p_i(x_i) = 1\\). For instance, \\(p_2(x_2) = 1\\).\nEvaluated at \\(x_j\\), where \\(j\\neq i\\), the value of \\(p_j(x_i) = 0\\). For example, \\(p_2(x_3) = 0\\).\n\nThe overall polynomial will be the linear combination \\[p(x) = y_1\\, p_1(x) +\ny_2\\, p_2(x) + \\cdots + y_k\\, p_k(x)\\ .\\] Can you see why?",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Functions as vectors</span>"
    ]
  },
  {
    "objectID": "Accumulation/36-functions.html#time-and-tide",
    "href": "Accumulation/36-functions.html#time-and-tide",
    "title": "37  Functions as vectors",
    "section": "37.3 Time and tide",
    "text": "37.3 Time and tide\nPeriodicities from https://tidesandcurrents.noaa.gov/harcon.html?id=8451552&type=\n\nhour &lt;- with(RI_tide, hour)\nb    &lt;- with(RI_tide, level)\nsin1 &lt;- sin(2*pi*hour/12.41)\ncos1 &lt;- cos(2*pi*hour/12.41)\nsin2 &lt;- sin(2*pi*hour/23.94)\ncos2 &lt;- cos(2*pi*hour/23.94)\nsin3 &lt;- sin(2*pi*hour/12)\ncos3 &lt;- cos(2*pi*hour/12)\nsin4 &lt;- sin(2*pi*hour/12.66)\ncos4 &lt;- cos(2*pi*hour/12.66)\nA &lt;- cbind(1, sin1, cos1, sin2, cos2, sin3, cos3, sin4, cos4)\nmod1 &lt;- b %onto% cbind(1, sin1, cos1)\nmod2 &lt;- b %onto% cbind(1, sin1, cos1, sin2, cos2)\nx &lt;- qr.solve(A, b)\nmod3 &lt;- A %*% x\ngf_point(level ~ hour, data = RI_tide)  %&gt;%\n  gf_line(mod3 ~ hour, color=\"magenta\")\n\n\n\n\n\n\n\n\nAnchorage, AK\nComponents: - M2 12.42 hours - S2 12 hours - N2 12.658 hours - K1 23.935 hours\n\nhour &lt;- with(Anchorage_tide, hour)\nb    &lt;- with(Anchorage_tide, level)\nsin1 &lt;- sin(2*pi*hour/12.42)\ncos1 &lt;- cos(2*pi*hour/12.42)\nsin2 &lt;- sin(2*pi*hour/23.935)\ncos2 &lt;- cos(2*pi*hour/23.935)\nsin3 &lt;- sin(2*pi*hour/12)\ncos3 &lt;- cos(2*pi*hour/12)\nsin4 &lt;- sin(2*pi*hour/12.658)\ncos4 &lt;- cos(2*pi*hour/12.658)\nA &lt;- cbind(1, sin1, cos1, sin2, cos2, sin3, cos3, sin4, cos4)\nmod1 &lt;- b %onto% cbind(1, sin1, cos1)\nmod2 &lt;- b %onto% cbind(1, sin1, cos1, sin2, cos2)\nx &lt;- qr.solve(A, b)\nmod3 &lt;- A %*% x\nresid &lt;- b - mod3\ngf_line(level ~ hour, data = Anchorage_tide)  %&gt;%\n  gf_line(mod3 ~ hour, color=\"magenta\") %&gt;%\n  gf_lims(x = c(0,1000))\n## Warning: Removed 75114 rows containing missing values (`geom_line()`).\n## Removed 75114 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\ngf_line(resid ~ hour)",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Functions as vectors</span>"
    ]
  },
  {
    "objectID": "Accumulation/36-functions.html#fourier-transform",
    "href": "Accumulation/36-functions.html#fourier-transform",
    "title": "37  Functions as vectors",
    "section": "37.4 Fourier transform",
    "text": "37.4 Fourier transform\n\n# Fill in the missing data\nRaw &lt;- Anchorage_tide |&gt; select(hour, level) %&gt;%\n  mutate(hour = round(hour, 1))\nEven &lt;- tibble(hour=seq(min(Raw$hour), max(Raw$hour), by=0.1))\nBoth &lt;- Even |&gt; full_join(Raw)\n## Joining with `by = join_by(hour)`\n# fill in the missing data\nFix1 &lt;- Both |&gt; \n  mutate(level = ifelse(is.na(level), lag(level), level)) %&gt;%\n  mutate(level = ifelse(is.na(level), lag(level), level)) %&gt;%\n  mutate(level = ifelse(is.na(level), lag(level), level)) %&gt;%\n  mutate(level = ifelse(is.na(level), lag(level), level)) %&gt;%\n  mutate(level = ifelse(is.na(level), lag(level), level)) %&gt;%\n# Fill in a constant value for the missing days\n  mutate(level = ifelse(is.na(level), 4.867, level))\nFFT &lt;- abs(fft(Fix1$level))\n# bin 974 is 1 per day\nFFT2 &lt;- \n  tibble(freq=2*(1:10000)/974, amp=FFT[2:10001]) %&gt;%\n  mutate(period = 24/freq) %&gt;%\n  mutate(amp = ifelse(amp &lt; .3e4, 0, amp)) %&gt;%\n  mutate(speed = 360/period)\ngf_line(amp ~ period, size=.1,data = FFT2 |&gt; filter(period&lt;100)) |&gt; gf_lims(x=c(10, 14))\n## Warning: Removed 9550 rows containing missing values (`geom_line()`).",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Functions as vectors</span>"
    ]
  },
  {
    "objectID": "Accumulation/37-euler.html",
    "href": "Accumulation/37-euler.html",
    "title": "38  Integrals step-by-step",
    "section": "",
    "text": "38.1 Euler method\nThe starting point for this method is the definition of the derivative of \\(F(t)\\). Reaching back to Chapter 8,\n\\[\\partial_t F(t) \\equiv \\lim_{h\\rightarrow 0} \\frac{F(t+h) - F(t)}{h}\\] To translate this into a numerical method for computing \\(F(t)\\), let’s write things a little differently.\nWith these changes, we have \\[f(t_0) = \\frac{F(t_0+dt) - F(t_0)}{dt}\\ .\\] The one quantity in this relationship that we do not yet know is \\(F(t_0 + dt)\\). So re-arrange the equation so that we can calculate the unknown in terms of the known. \\[F(t_0 + dt) = F(t_0) + f(t_0)\\, dt\\ .\\]",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Integrals step-by-step</span>"
    ]
  },
  {
    "objectID": "Accumulation/37-euler.html#euler-method",
    "href": "Accumulation/37-euler.html#euler-method",
    "title": "38  Integrals step-by-step",
    "section": "",
    "text": "First, since the problem setting is that we don’t (yet) know \\(F(t)\\), let’s refer to things we do know. In particular, we know \\(f(t) = \\partial_t F(t)\\).\nAgain, recognizing that we don’t yet know \\(F(t)\\), let’s re-write the expression using something that we do know: \\(F(t_0)\\). Stated more precisely, \\(F(t_0)\\) is something we get to make up to suit our convenience. (A common choice is \\(F(t_0)=0\\).)\nLet’s replace the symbol \\(h\\) with the symbol \\(dt\\). Both of them mean “a little bit of” and \\(dt\\) makes explicit that we mean “a little bit of \\(t\\).”\nwe will substitute the limit \\(\\lim_{h\\rightarrow 0}\\) with an understanding that \\(dt\\) will be something “small.” How small? we will deal with that question when we have to tools to answer it.\n\n\n\nLet’s consider finding the anti-derivative of \\(\\dnorm()\\), that is, \\(\\int_0^t \\dnorm(x) dx\\). In one sense, you already know the answer, since \\(\\partial_x \\pnorm(x) = \\dnorm(x)\\). In reality, however, we know \\(\\pnorm()\\) only because it has been numerically constructed by integrating \\(\\dnorm()\\). The \\(\\pnorm()\\) function is so important that the numerically constructed answer has been memorized by software.",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Integrals step-by-step</span>"
    ]
  },
  {
    "objectID": "Accumulation/37-euler.html#area",
    "href": "Accumulation/37-euler.html#area",
    "title": "38  Integrals step-by-step",
    "section": "38.2 Area",
    "text": "38.2 Area\nThe quantity \\[\\Large \\color{magenta}{f(t_0)}\\, \\color{orange}{dt}\\] gives rise to a visualization that has been learned by generations of calculus students. The visualization is so compelling and powerful that many students (and teachers, and textbook authors) mistake the visualization for integration and anti-differentiation themselves.\nWe will start the visualization with a simple graph of \\(f(t)\\), which is called the integrand in the integral \\(\\int_a^b f(t) dt\\). Figure 38.1 shows the graph of \\(f(t)\\). A specific point \\(t_0\\) has been marked on the horizontal axis. Next to it is another mark at \\(t_0 + dt\\). Of course, the distance between these marks is \\(\\color{orange}{dt}\\).\n\n\n\n\n\n\n\n\nFigure 38.1: Illustrating the interpretation of \\(f(t_0) dt\\) as an “area”.\n\n\n\n\n\nPer the usual graphical convention, a position along the vertical axis corresponds to a possible output of \\(f(t)\\). The output for \\(t=t_0\\) is \\(\\color{magenta}{f(t_0)}\\). That same quantity corresponds to the length of the vertical orange segment connecting \\((t_0, 0)\\) to \\((t_0, f(t_0))\\).\nThe \\(\\color{orange}{dt}\\) line segment and the \\(\\color{magenta}{f(t_0)}\\) segment constitute two sides of a rectangle, shown as a shaded zone. The “area” of that rectangle is the product \\(\\color{magenta}{f(t_0)}\\ \\color{orange}{dt}\\).\nIn this sort of visualization, an integral is the accumulation of many of these \\(f(t) dt\\) rectangles. For instance, Figure 38.2 the visualization of the integral \\[\\int_{0}^3 f(t) dt\\ .\\]\n\n## Warning in is.na(x): is.na() applied to non-(list or vector) of type\n## 'expression'\n\n## Warning in is.na(x): is.na() applied to non-(list or vector) of type\n## 'expression'\n\n## Warning in is.na(x): is.na() applied to non-(list or vector) of type\n## 'expression'\n\n\n\n\n\n\n\nFigure 38.2: Visualizing the integral \\(\\int_0^3 f(t) dt\\) as the total “area” of several \\(f(t) dt\\) bars. The width of each of the bars is \\(dt\\). The height depends on the value of the function \\(f(t)\\) at the bar. For illustration, two of the bars are marked with vertical and horizontal line segments.\n\n\n\n\n\nAs always in calculus, we imagine \\(dt\\) as a “small” quantity. In ?fig-bars-0-3B you can see that the function output changes substantially over the sub-domain spanned by a single rectangle. Using smaller and smaller \\(dt\\), as in ?fig-bars-0-3-small brings the visualization closer and closer to the actual meaning of an anti-derivative.\n\n\n\n\n\nVisualizing the integral \\(\\int_0^3 f(t) dt\\) as the total “area” of several \\(f(t) dt\\) bars. The width of each of the bars is \\(dt\\). The height depends on the value of the function \\(f(t)\\) at the bar. For illustration, two of the bars are marked with vertical and horizontal line segments.\n\n\n\n\n\n\n\n\n\n\nWhy do you keep putting “area” in quotes?\n\n\n\nWhen \\(f(t_i) &lt; 0\\), then \\(f(t_i) dt\\) will be negative. There is no such thing as a negative area, but in constructing an integral the \\(f(t_i)dt\\), being negative, diminishes the accumulated area.\n\n\n\n\n\n\n\n\nFigure 38.3: The \\(\\int_{-2}^3 g(t) dt\\) covers subdomains where \\(g(t) &gt; 0\\) and where \\(g(t) &lt; 0\\). In those latter subdomains, the “area” is negative, and shown in light orange here.\n\n\n\n\n\nAnother problem is that area is a physical quantity, with dimension L\\(^2\\). The quantity produced by integration will have physical dimension \\([f(t)][t]\\), the product of the dimension of the with-respect-to quantity and the output of the function.\n“Area” is an effective metaphor for visualizing integration, but the goal of integration is not to calculate an area but, typically, some other kind of quantity.",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Integrals step-by-step</span>"
    ]
  },
  {
    "objectID": "Accumulation/37-euler.html#the-euler-step",
    "href": "Accumulation/37-euler.html#the-euler-step",
    "title": "38  Integrals step-by-step",
    "section": "38.3 The Euler Step",
    "text": "38.3 The Euler Step\nThe previous section a visualization of an integral in terms of an area on a graph. As you know, a definite integral \\(\\int_a^b f(t) dt\\) can also be computed by constructing the anti-derivative \\(F(t) \\equiv \\int f(t) dt\\) and evaluating it at the upper and lower bounds of integration: \\(F(b) - F(a)\\). In this section, we will look at the numerical process of constructing an anti-derivative function, which uses many of the same concepts as those involved in finding an integral by combining areas of rectangles.\nA definite integral produces a quantity, not a function. The anti-derivative function constructed by using quantities like \\(f(t) dt\\) will be a series of quantities rather than a formula. In particular, it will have the form of a data table, something like this:\n\n\n\n\n\n\\(t\\)\n\\(F(t)\\)\n\n\n\n\n-2\n10.62\n\n\n-1.5\n6.47\n\n\n-1\n3.51\n\n\n-0.5\n2.02\n\n\n0\n2.4\n\n\n0.5\n3.18\n\n\n1.0\n5.14\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\nTo start, we will need to create a series of \\(t\\) values. We will do this by specifying a starting value for \\(t\\) and then creating successive values by adding a numerical increment \\(dt\\) to the entries one after the other until we reach a terminating value. For instance, in the above table, the starting value for \\(t\\) is \\(-2\\), the numerical increment is \\(dt=0.5\\), and the terminating value is \\(1\\).\nIn previous chapters of this book we have worked with data tables, but always the data table was given to us, we did not have to construct it.1 Now we need to construct the data frame with the \\(t\\) column containing appropriate values. Computer languages provide many ways to accomplish this task. We will use a simple R/mosaic function Picket(), which constructs a data table like the one shown above. You provide two arguments: the domain for \\(t\\), that is, the desired upper and lower bounds of integration; the interval size \\(dt\\) (which is called h in the argument list). For instance, to construct the \\(t\\) column of the table shown above, you can use Picket() this way:\n\nPts &lt;- Picket(bounds(t = -2:1), h=0.5)\nPts\n## # A tibble: 6 × 3\n##       t preweight weight\n##   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n## 1  -2           1    0.5\n## 2  -1.5         1    0.5\n## 3  -1           1    0.5\n## 4  -0.5         1    0.5\n## 5   0           1    0.5\n## 6   0.5         1    0.5\n\nAs you can see, the data table produced by Picket() has the \\(t\\) column, as well as a second column named weight. We haven’t explained weight yet, but you can see that it is the same value we specified as h.\nThe name Picket() is motivated by the shape of a picket fence. The pickets are evenly spaced, which keeps things simple but is not a requirement.\nNote that the picket does not say anything at all about the function \\(f(t)\\) being anti-differentiated. The picket can be applied to any function although precision might require a smaller \\(dt\\) for functions that have a lot going on in a small domain.\nThe next step in using the picket to perform anti-differentiation is to apply the function \\(f()\\) to the pickets. That is, we will add a new column, perhaps called vals to the data table.\nAdding a new column is a common task when dealing with data. We will do this with a new function, mutate(), whose specific function is adding new columns (or modifying old ones). Here’s the command to apply \\(f()\\) to t and call the new column vals:\n\n# Find the height of the pickets\nPts &lt;- Pts %&gt;%\n  mutate(vals = f(t))\n\nWith this modification, the data table looks like:\n\n## # A tibble: 6 × 4\n##       t preweight weight  vals\n##   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1  -2           1    0.5 -9.12\n## 2  -1.5         1    0.5 -7.27\n## 3  -1           1    0.5 -4.50\n## 4  -0.5         1    0.5 -1.46\n## 5   0           1    0.5  1.28\n## 6   0.5         1    0.5  3.31\n\nNow that we know the value of the function at each of the pickets, the next step is to multiply the value by the spacing between pickets. That spacing, which we set with the argument h = 0.5 in our original call to Picket() is in the column called weight. we will call the result of the multiplication step. Note that the following R command incorporates the previous calculation of vals; we are looking to build up a single command that will do all the work.\n\n# Multiply the height by the picket spacing\nPts &lt;- Pts %&gt;%\n  mutate(vals = f(t),\n         step = vals * weight)\n\n\nPts\n## # A tibble: 6 × 5\n##       t preweight weight  vals   step\n##   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n## 1  -2           1    0.5 -9.12 -4.56 \n## 2  -1.5         1    0.5 -7.27 -3.63 \n## 3  -1           1    0.5 -4.50 -2.25 \n## 4  -0.5         1    0.5 -1.46 -0.732\n## 5   0           1    0.5  1.28  0.639\n## 6   0.5         1    0.5  3.31  1.66\n\nWe used the name step to identify the product of the height and spacing of the pickets to help you think about the overall calculation as accumulating a series of steps. Each step provides a little more information about the anti-derivative that we will now calculate. In terms of the area metaphor for integration, each step is the area of one vertical bar of the sort presented in the previous section.\nWe will call these Euler steps, a term that will be especially appropriate when, in Block 6, we use integration to calculate the trajectories of systems—such as a ball in flight—that change in time.\nThe final step in constructing the anti-derivative is to add up the steps. This is simple addition. But we will arrange the addition one step at a time. That is, for the second row, the result will be the sum of the first two steps. For the third row, the result will be the sum of the first three steps. And so on. The name for this sort of accumulation of the previous steps is called a cumulative sum. Another name for a cumulative sum is a “running sum”: the sum-so-far as we move down the column of steps. Cumulative sums are computed in R by using cumsum(). Here, we are calling the result of the cumulative sum F to emphasize that it is the result of anti-differentiating \\(f()\\). But keep in mind that the anti-derivative is not just the F column, but the table with both t and F columns. That is, the table has a column for the input as well as the output. That is what it takes to be a function.\n\n# Doing everything in one command\nPts &lt;- \n  Picket(bounds(t = -2:1), h=0.5) %&gt;%\n  mutate(vals = f(t),\n         step = vals * weight,\n         F = cumsum(step))\n\n\n## # A tibble: 6 × 6\n##       t preweight weight  vals   step      F\n##   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n## 1  -2           1    0.5 -9.12 -4.56   -4.56\n## 2  -1.5         1    0.5 -7.27 -3.63   -8.19\n## 3  -1           1    0.5 -4.50 -2.25  -10.4 \n## 4  -0.5         1    0.5 -1.46 -0.732 -11.2 \n## 5   0           1    0.5  1.28  0.639 -10.5 \n## 6   0.5         1    0.5  3.31  1.66   -8.88\n\nWe can summarize the steps in this Euler approach to numerical integration graphically:\n\n\n\n\n{#fig-euler-integration1, fig-align=‘center’ width=90%}\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 38.4: Steps in a numerical construction of an anti-derivative. (1) Create a set of picket locations over the domain of interest. The locations are spread horizontally by amount dt, so each picket will be dt units wide. (2) evaluate the original function at the picket points to give picket heights. (3) Multiply the picket height by the picket width to create an “area”. (4) Starting at zero for the left-most picket, add in successive picket areas to construct the points on the anti-derivative function (green). Note that the vertical axis in (4) has a different dimension and units than in steps (1)-(3). In (4) the vertical scale is in the units of the anti-derivative function output.\n\n\n\n\n\nFigure 38.5 shows a dynamic version of the process of constructing an anti-derivative by Euler steps. The integrand \\(f(t)\\) is shown in the top panel, the anti-derivative \\(F(t)\\) is shown being built up in the bottom panel. The \\(\\color{magenta}{\\text{magenta}}\\) bar in the top plot is the current Euler step. That step is added to the previously accumulated steps to construct \\(F(t)\\).\n\nPROVIDE LINK TO MOVIE IN PDF version.\n\n\n\n\n\n\n\n\n\nFigure 38.5: A dynamic view of building \\(F(t)\\) from \\(f(t)\\) by accumulating Euler steps.\n\n\n\n\n\n\nApplication area 38.1 — What does “daily” tell you?\n\n\n\n\n\n\n\nApplication area 38.1 Russian COVID-19 cases\n\n\n\nThe following graphic from a well-respected news magazine, The Economist, shows the reported number of cases and deaths from Covid-19 during a two-year period in Russia. (“Sputnik” is the name given to a Russian-developed vaccine, named after the first man-made satellite in Earth orbit, launched by the Soviet Union on Oct. 4, 1957 and precipitating a Cold-War crisis of confidence in in the US.)\n\n\n\n\n\n\nFigure 38.6\n\n\n\nThe figure caption gives information about the units of the quantities being graphed. Notice the word “daily,” which tells us, for example, that in mid-2021 there were about 10,000 new cases of Covid-19 each day and correspondingly about 350 daily deaths.\nHow many total cases and total deaths are reported in the graphic?\nThere are, of course, two distinct ways to present such data which can be easily confused by the casual reader. One important way to present data is as cumulative cases and deaths as a function of date. We will call these \\(C(t)\\) and \\(D(t)\\). Another prefectly legitimate presentation is of the rate of change \\(\\partial_t C(t)\\) and \\(\\partial_t D(t)\\) which, following our informal capital/lower-case-letter convention, we could write \\(c(t)\\) and \\(d(t)\\). Since there is no such thing as a “negative” case or death, we know that \\(C(t)\\) and \\(D(t)\\) are monotonic functions, never decreasing. So the graphs cannot possibly be of \\(C(t)\\) and \\(D(t)\\), since the graphs are far from monotonic. Consequently, the displayed graphs are \\(c(t)\\) and \\(d(t)\\), as confirmed by the word “daily” in the caption.\nTo find \\(C(t)\\) and \\(D(t)\\) requires integrating \\(c(t)\\) and \\(d(t)\\). The value of \\(C(t)\\) and \\(D(t)\\) at the right-most extreme of the graph can be found by calculating the “area” under the \\(c(t)\\) and \\(d(t)\\) curves. But care needs to be taken in reading the horizontal axis. Although the axes are labelled with the year, the tick marks are spaced by one month. (Notice “month” does not appear in the caption.) The far right end of the graph is in early July 2021. The far left end, when the graph moves away from zero cases and deaths, is early April 2020.\nYou can do a reasonable job estimating the “area” by extending the tick marks on the horizontal axis and counting the resulting rectangles that fall under the curve.\n\n\n\n\n\n\nFigure 38.7: Dividing the domain into regions of width \\(dt = 1\\) month.”\n\n\n\nFor the graph of cases, the “area” of each rectangle is \\(\\frac{5000\\, \\text{cases}}{\\text{ day}}\\cdot \\text{1 month}\\). This has the right dimension, “cases,” but the units are screwy. So replace 1 month with 30.5 days (or thereabouts) to get an “area” of each rectangle of 172,500 cases. Similarly, the “area” of the rectangles on the right graph is 3050 deaths.",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Integrals step-by-step</span>"
    ]
  },
  {
    "objectID": "Accumulation/37-euler.html#better-numerics-optional",
    "href": "Accumulation/37-euler.html#better-numerics-optional",
    "title": "38  Integrals step-by-step",
    "section": "38.4 Better numerics (optional)",
    "text": "38.4 Better numerics (optional)\nExcept as a textbook exercise, you will likely never have to compute a numerical anti-derivative from scratch as we did in the previous section. This is a good thing. To understand why, you have to know one of the important features of modern technical work. That feature is: We never work alone in technical matters. There is always a set of people whose previous work we are building on, even if we never know the names of those people. This is because technology is complicated and it is evidently beyond the reach of any human to master all the salient aspects of each piece of technology being incorporated into the work we consider our own.\nOf course this is true for computers, since no individual can build a useful computer from first principles. It is also true for software. One detail in particular is relevant to us here. Computer arithmetic of the sort used in the previous section—particularly addition—is prone to error when adding up lots and lots of small bits. This means that it is not always sensible to choose very small \\(dt\\) to get a very accurate approximation to the anti-derivative.\nFortunately, there are specialists in numerical mathematics who work on ways to improve the accuracy of calculations for mid-sized \\(dt\\). Their work has been incorporated into the results of antiD() and Integrate() and so the details are, for us, unimportant. But they are only unimportant because they have been taken care of.\nTo illustrate how considerably more accuracy can be gained in calculating an anti-derivative, consider that the rectangular bars drawn in the previous sections are intended to approximate the “area” under the function. With this in mind, we can replace the rectangular bars with more suitable shapes that stay closer to the function over the finite extend of each \\(dt\\) domain. The rectangular bars model the function as piecewise constant. A better job can be done with piecewise linear approximations or piecewise quadratic approximations. Often, such refinements can be implemented merely by changing the weight column in the picket data frame used to start off the process.\nOne widely used method, called Gauss-Legendre quadrature can calculate a large segment of an integral accurately (under conditions that are common in practice) with just five evaluations of the integrand \\(f(t)\\).\n\n\n\n\nTable 38.1: Picket locations and weights For the integral \\(\\int_a^b f(t) dt\\) where \\(c = \\frac{a+b}{2}\\) and \\(w = (b-a)/2\\).\n\n\n\n\n\nlocation\nweight\n\n\n\n\n\\(c - 0.90618 w\\)\n\\(0.236927 \\times w\\)\n\n\n\\(c - 0.53847 w\\)\n\\(0.478629 \\times w\\)\n\n\n\\(c\\)\n\\(0.568889 \\times w\\)\n\n\n\\(c + 0.53847 w\\)\n\\(0.478629 \\times w\\)\n\n\n\\(c + 0.90618 w\\)\n\\(0.236927 \\times w\\)\n\n\n\n\n\nThe locations and weights may seem like a wizard parody of mathematics, but those precise values are founded in an advanced formulation of polynomials rooted in the theory of linear combinations to which you will be introduced in Block 5. Needless to say, you can hardly be expected to have any idea where they come from. That is why it is useful to build on the work of experts in specialized areas. It is particularly helpful when such expertise is incorporated into software that faithfully and reliably implements the methods. The lesson to take to heart: Use professional software systems that have been extensively vetted.",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Integrals step-by-step</span>"
    ]
  },
  {
    "objectID": "Accumulation/37-euler.html#footnotes",
    "href": "Accumulation/37-euler.html#footnotes",
    "title": "38  Integrals step-by-step",
    "section": "",
    "text": "The root of the word “data” is the Latin for “given”.↩︎",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Integrals step-by-step</span>"
    ]
  },
  {
    "objectID": "Dynamics/B6-diff-eq.html",
    "href": "Dynamics/B6-diff-eq.html",
    "title": "40  Differential equations",
    "section": "",
    "text": "40.1 Dynamical systems\nIn this Block, we take on what an important application of derivatives: the representation of dynamical systems.\n“Dynamical systems” (but not under that name) were developed initially in the 1600s to relate planetary motion to the force of gravity. Nowadays, they are used to describe all sorts of physical systems from oscillations in electrical circuits to the ecology of interacting species to the spread of contagious disease.\nAs examples of dynamical systems, consider a ball thrown thrown through the air or a rocket being launched to deploy a satellite. At each instant of time, a ball has a position—a point in \\(x,y,z\\) space—and a velocity \\(\\partial_t x\\), \\(\\partial_t y\\), and \\(\\partial_t z\\). These six quantities, and perhaps others like spin, constitute the instantaneous state of the ball. Rockets have additional components of state, for example the mass of the fuel remaining.\nThe “dynamical” in “dynamical systems” refers to the change in state. For the ball, the state changes under the influence of mechanisms such as gravity and air resistance. The mathematical representation of a dynamical system codifies how the state changes as a function of the instantaneous state. For example, if the instantaneous state is a single quantity called \\(x\\), the instantaneous change in state is the derivative of that quantity: \\(\\partial_t x\\).\nTo say that \\(x\\) changes in time is to say that \\(x\\) is a function of time: \\(x(t)\\). When we write \\(x\\), we mean \\(x()\\) evaluated at an instant. When we write \\(\\partial_t x\\), we mean “the derivative of \\(x(t)\\) with respect to time” evaluated at the same instant as for \\(x\\).\nThe dynamical system describing the motion of \\(x\\) is written in the form of a differential equation, like this:\n\\[\\partial_t x = f(x)\\ .\\] Notice that the function \\(f()\\) is directly a function of \\(x\\), not \\(t\\). This is very different from the situation we studied in Block 3, where we might have written \\(\\partial_t y = \\cos\\left(\\frac{2\\pi}{P} t\\right)\\) and assigned you the challenge of finding the function \\(y(t)\\) by anti-differentiation. (The answer to the anti-differentiation problem, of course, is \\(y(t) = \\frac{P}{2\\pi}\\sin\\left(\\frac{2\\pi}{P} t\\right) + C\\).)\nDynamical systems with multiple state quantities are written mathematically as sets of differential equations, for instance: \\[\\partial_t y = g(y, z)\\\\\n\\partial_t z = h(y, z)\\] We typically use the word system rather than “set,” so a dynamical system is represented by a system of differential equations.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Differential equations</span>"
    ]
  },
  {
    "objectID": "Dynamics/B6-diff-eq.html#dynamical-systems",
    "href": "Dynamics/B6-diff-eq.html#dynamical-systems",
    "title": "40  Differential equations",
    "section": "",
    "text": "It is essential that you train yourself to distinguish two very different statements\n\nanti-differentiation problems like \\(\\partial_{\\color{blue}{t}} y = g(\\color{blue}{t})\\), which has \\(t\\) as both the with-respect-to variable and as the argument to the function \\(g()\\).\n\nand\n\ndynamical systems like \\[\\partial_{\\color{blue}{t}} \\color{magenta}{y} = g(\\color{magenta}{y})\\ .\\]\n\nThis is one place where Leibniz’s notation for derivatives can be useful: \\[\\underbrace{\\frac{d\\color{magenta}{y}}{d\\color{blue}{t}} = g(\\color{blue}{t})}_{\\text{as in antidifferentiation}}\\ \\ \\ \\text{versus}\\ \\ \\ \\underbrace{\\frac{d\\color{magenta}{y}}{d\\color{blue}{t}} = g(\\color{magenta}{y})}_{\\text{dynamical system}}\\]\n\n\n\n\\(\\ \\)\nLet’s illustrate the idea of a dynamical system with a children’s game: “Chutes and Ladders”. Since hardly any children have studied calculus, the game isn’t presented as differential equations, but as a simple board and the rules for the movement along the board.\n\n\n\n\n\n\n\n\nFigure 40.1: The game of Chutes and Ladders\n\n\n\n\n\nA player’s state in this game is shown by the position of a token, but we will define the state to be the number of the square that the player’s token is on. In Chutes and Ladders the state is one of the integers from 1 to 100. In contrast, the dynamical systems that we will study with calculus have a state that is a point on the number line, or in the coordinate plane, or higher-dimensional space. Our calculus dynamical system describe the change of state using derivatives with respect to time, whereas in chutes and ladders the the state jumps from one value to the next value.\nThe game board displays not only the set of possible states but also the rule for changing state jumping from one state to another.\nIn the real game, players roll a die to determine how many steps to take to the next state. But we will play a simpler game: Just move one step forward on each turn, except … from place to place there are ladders that connect two squares. When the state reaches a square holding the foot of a ladder, the state is swept up to the higher-numbered square at the top of the ladder. Similarly, there are chutes. These work much like the ladders but carry the state from a higher-numbered square to a lower-numbered square.\nThe small drawings on the board are not part of the action of the game. Rather, they represent the idea that good deeds lead the player to progress, while wrong-doing produces regression. Thus, the productive gardener in square 1 is rewarded by being moved upward to the harvest in square 38. In square 64 a brat is pulling on his sister’s braids. This misdeed results in punishment: he is moved back to square 60.\nOur dice-free version of Chutes and Ladders is an example of a discrete-time, discrete-state dynamical system. Since there is no randomness involved, the movement of the state is deterministic. (With dice, the movement would be stochastic.)\nThe differential equations of a dynamical system correspond to a continuous-time, continuous-space system. This continuity is the reason we use derivatives to describe the motion of the state. The movement in the systems we will explore is also deterministic. (In ?sec-forcing we will encounter briefly some instances of stochastic systems.)",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Differential equations</span>"
    ]
  },
  {
    "objectID": "Dynamics/B6-diff-eq.html#state",
    "href": "Dynamics/B6-diff-eq.html#state",
    "title": "40  Differential equations",
    "section": "40.2 State",
    "text": "40.2 State\nThe mathematical language of differential equations and dynamical systems is able to describe a stunning range of systems, for example:\n\nphysics\n\nswing of a pendulum\nbobbing of a mass hanging from a spring.\na rocket shooting up from the launch pad\n\ncommerce\n\ninvestment growth\ngrowth in equity in a house as a mortgage is paid up. (“Equity” is the amount of the value of the house that belongs to you.)\n\nbiology\n\ngrowth of animal populations, including predator and prey.\nspread of an infectious disease\ngrowth of an organism or a crop.\n\n\nAll these systems involve a state that describes the configuration of the system at a given instant in time. For the growth of a crop, the state would be, say, the amount of biomass per unit area. For the spread of infectious disease, the state would be the fraction of people who are infectious and the fraction who are susceptible to infection. “State” in this sense is used in the sense of “the state of affairs,” or “his mental state,” or “the state of their finances.”\nSince we are interested in how the state changes over time, sometimes we refer to it as the dynamical state.\nOne of the things you learn when you study a field such as physics or epidemiology or engineering is what constitutes a useful description of the dynamical state for different situations. In the crop and infectious disease examples above, the state mentioned is a strong simplification of reality: a model. Often, the modeling cycle leads the modeler to include more components to the state. For instance, some models of crop growth include the density of crop-eating insects. For infectious disease, a model might include the fraction of people who are incubating the disease but not yet contagious.\nConsider the relatively simple physical system of a pendulum, swinging back and forth under the influence of gravity. In physics, you learn the essential dynamical elements of the pendulum system: the current angle the pendulum makes to the vertical, and the rate at which that angle changes. There are also fixed elements of the system, for instance the length of the pendulum’s rod and the local gravitational acceleration. Although such fixed characteristics may be important in describing the system, they are not elements of the dynamical state. Instead, they might appear as parameters in the functions on the right-hand side of the differential equations.\nTo be complete, the dynamical state of a system has to include all those changing aspects of the system that allow you to calculate from the state at this instant what the state will be at the next instant. For example, the angle of the pendulum at an instant tells you a lot about what the angle will be at the next instant, but not everything. You also need to know which way the pendulum is swinging and how fast.\nFiguring out what constitutes the dynamical state requires knowledge of the mechanics of the system, e.g. the action of gravity, the constraint imposed by the pivot of the pendulum. You get that knowledge by studying the relevant field: electrical engineering, economics, epidemiology, etc. You also learn what aspects of the system are fixed or change slowly enough that they can be considered fixed. (Sometimes you find out that something your intuition tells you is important to the dynamics is, in fact, not. An example is the mass of the pendulum.)",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Differential equations</span>"
    ]
  },
  {
    "objectID": "Dynamics/B6-diff-eq.html#state-space",
    "href": "Dynamics/B6-diff-eq.html#state-space",
    "title": "40  Differential equations",
    "section": "40.3 State space",
    "text": "40.3 State space\nThe state of a dynamical system tells you the configuration of the system at any instant in time. It is appropriate to think about the instantaneous state as a single point in a state space, a coordinate system with an axis for each component of state. As the system configuration changes with time—say, the pendulum loses velocity as it swings to the left—the instantaneous state moves along a path in the state space. Such a path is called a trajectory of the dynamical system.\nIn this book, we will work almost exclusively with systems that have a one- or two-dimensional state. Consequently, the state space will be either the number line or the coordinate plane. The methods you learn will be broadly applicable to systems with higher-dimensional state.\nFor the deterministic dynamical systems we will be working with, a basic principle is that a trajectory can never cross itself. This can be demonstrated by contradiction. Suppose a trajectory did cross itself. This would mean that the motion from the crossing point couple possibly go in either of two directions; the state might follow one branch of the cross or the other. Such a system would not be deterministic. Determinism implies that from each point in state space the flow goes only in one direction.\nThe dimension of the state space is the same as the number of components of the state; one axis of state space for every component of the state. has important implications for the type of motion that can exist.\n\nIf the state space is one-dimensional, the state as a function of time must be monotonic. Otherwise, the trajectory would cross itself, which is not permitted.\nA state space that is two- or higher-dimensional can support motion that oscillates back and forth. Such a trajectory does not cross itself, instead it goes round and round in a spiral or a closed loop.\n\nFor many decades, it was assumed that all dynamical systems produce either monotonic behavior or spiral or loop behavior. In the 1960s, scientists working on a highly simplified model of the atmosphere discovered numerically that there is a third type of behavior, the irregular and practically unpredictable behavior called chaos. To display chaos, the state space of the system must have at least three elements.\n\n\nThat calculus is the language of change can be seen in the words used in this section. For instance, instantaneous, continuous, and monotonic are all words introduced in Block 1 of this book.\n\nApplication area 40.1 —Describing mathematically the dynamics of an epidemic.\n\n\n\n\n\n\n\nApplication area 40.1 The state of COVID\n\n\n\nWhat does it take to describe the dynamical state of an epidemic?\nNews reports of the COVID pandemic usually focus on the number of new cases each day and the fraction of the population that has been vaccinated. But this is not adequate, even for a simple description of the dynamics.\nFrom a history of new-case counts over time (e.g. ?fig-NYT-covid-history) you can see that the number of new cases waxes and wanes. Knowing that the number of cases today is, say, 100 thousand does not tell you what the number of cases will be in two weeks: 100 thousand is encountered both on the way up and on the way down.\n\n\n![COVID-19 new-case counts in the US over the first two years of the pandemic. Source: [New York Times]]](www/NYT-covid-report.png){#fig-NYT-covid-history fig-align=‘center’ width=90%}",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Differential equations</span>"
    ]
  },
  {
    "objectID": "Dynamics/B6-diff-eq.html#dynamics",
    "href": "Dynamics/B6-diff-eq.html#dynamics",
    "title": "40  Differential equations",
    "section": "40.4 Dynamics",
    "text": "40.4 Dynamics\nThe dynamics of a system is a description of how the individual components of the state change as a function of the entire set of components of the state.\nAt any instant in time, the state is a set of quantities. We will use \\(x\\), \\(y\\), and \\(z\\) for the purpose of illustration, although most of our work in this introduction will be with systems that have just one or two state variables.\nThe differential equations describing the \\(x, y, z\\) system have a particular form:\n\\[\\partial_t x(t) = f(x(t), y(t), z(t))\\ \\, \\\\\n\\partial_t y(t) = g(x(t), y(t), z(t))\\ \\, \\\\\n\\partial_t z(t) = h(x(t), y(t), z(t))\\ .\\]\nThe way these equations are written is practically impossible to read: the expression \\((t)\\) is repeated 12 times! It takes concentration to look beyond the \\((t)\\) to see the overall structure of the equations. to avoid this problem of not seeing the forest for the \\((t)\\)s, the convention is to omit the \\((t)\\): \\[\\partial_t x = f(x, y, z)\\\\\n\\partial_t y = g(x, y, z)\\\\\n\\partial_t z = h(x, y, z)\\] This leaves it to the reader to remember that \\(x\\) is really \\(x(t)\\) and so on.\nThis more concise way of writing the differential equations makes it easier to describe how to interpret the equations. Formally, \\(\\partial_t x\\) is a function, the derivative of the function \\(x(t)\\) with respect to time. But try to put this highly literal interpretation on a back burner. Think of the expression \\(\\partial_t x =\\) as meaning, “the way the \\(x\\)-component of state changes in time is described by ….” We need three differential equations because there are three components of state in the \\(x,y,z\\) system, and we need to describe for each component the way that component changes.\nOn the right side of each equation is a function that takes the state quantities as inputs. Each individual equation can be interpreted as completing the elliptical sentence (that is, ending in “…”) in the previous paragraph, so that the whole equation reads like, “The way the \\(x\\)-component of state changes at any instant in time is specified by the function \\(h()\\) evaluated at the instantaneous state.” These functions are called dynamical functions since they give the rules for the dynamics.\nRemember that \\(x\\), \\(y\\), and \\(z\\) are state variables, so they are all functions of time. At any instant in time, the values \\(x\\), \\(y\\), \\(z\\) have a specific value. Thus, at any instant in time, evaluating the functions \\(f(x, y, z)\\), \\(g(x, y, z)\\), and \\(h(x, y, z)\\) at the current state produces a specific, scalar value. If we wanted to make this perfectly explicit, we could write \\(g_x(x(t), y(t), z(t))\\), which makes it clear that the output of \\(g_x()\\) is a function of time.\n\nMathematically, a dynamical system consists of two things:\n\nThe state variables, which is a set of quantities that vary in time.\nThe dynamics, which is the set of dynamical functions, one function for each of the state variables.\n\n\nA simple example is the dynamics of retirement-account interest. In a retirement account, you put aside money—this is called “contributing”—each month. The value \\(V(t)\\) of the account accumulates over time, both due to new monthly deposits and to the interest \\(r\\) earned on the current account value. If you are setting aside \\(M\\) dollars per month, the dynamics are: \\[\\partial_t V = r V + M\\ .\\] The left-hand side of this equation is boilerplate for “the way the \\(V\\) component of state changes is described by the dynamical function \\(rV + M\\).” This is a function of \\(V\\) with parameters \\(r\\) and \\(M\\). In this example, there is just the one state variable \\(V\\), so the dynamical function has only one argument: \\(V\\).\nRemember that the dynamical function is something that the modeler constructs from her knowledge of the system. To model the dynamics of a pendulum requires some knowledge of physics. Without getting involved with the physics, we note that the oscillatory nature of pendulum movement means that there must be at least two state variables. A physicist learns that a good way to describe the motion uses these two quantities: the angle \\(\\theta(t)\\) of the pendulum rod with respect to the vertical and the angular velocity \\(v(t)\\) telling how the velocity changes with time. Since there are two state variables, there must be two dynamical functions. For a pendulum, one of the functions, the one for \\(\\partial_t v\\) comes from applying Newton’s Second Law: \\(F = m a\\). (Remember that \\(\\partial_t v\\) is an acceleration.) So one of the differential equations is \\[\\partial_t v  =  f(\\theta, v) \\equiv - \\sin(\\theta)\\]\nThe other equation comes from the definition that the derivative of the position \\(\\theta\\) is the velocity. \\[\\partial_t \\theta  =  g(\\theta, v) \\equiv  v\\\\\n\\]\n\n\n\n\n\n\nWhy?\n\n\n\nWhy define a state variable \\(v\\) when it is, by definition, the same as \\(\\partial_t \\theta(t)\\)?\nEven though the dynamical equation \\(\\partial_t \\theta(t) = v\\) is a calculus tautology, we need always to be explicit about what are the two quantities in the dynamical state. The \\(\\partial_t \\theta\\) differential equation comes for free from basic calculus concepts. The second equation is about the physics, that is, the relationship between forces and acceleration.\nThere is a style of writing dynamics equations that discards such tautologies. For example, the pendulum dynamics are often written \\[\\partial_{tt} \\theta(t) = - \\sin(\\theta)\\ .\\] This sort of equation, containing a second-order derivative, is called a second-order differential equation. In contrast, the two equations, one for \\(\\partial_t \\theta\\) and one for \\(\\partial_t v\\) are called first-order differential equations because each involves a first-order derivative. We will return to this second-order style in Chapter 47 since it is often encountered in physics and engineering. For now, we are avoiding the second-order style because it obscures the fact that there are two state variables: \\(\\theta(t)\\) and \\(v(t)\\).\n\n\n\n\\(\\ \\)\nConsider the population of two interacting species, say rabbits and foxes. As you know, the relationship between rabbits and foxes is rather unhappy from the rabbits’ point of view even if it is fulfilling for the foxes.\nMany people assume that such populations are more or less fixed: that the rabbits are in a steady balance with the foxes. In fact, as any gardener can tell you, some years there are lots of rabbits and others not: an oscillation. Just from this fact, we know that the dynamical state must have at least two components.\nIn a simple, but informative, model, the two components of the dynamical state are \\(r(t)\\) and \\(f(t)\\), the population of rabbits and foxes respectively. In the absence of foxes, the dynamics of rabbits are exponential growth; each successive generation is larger than the previous one. This can be described by a dynamical equation \\(\\partial_t r(t) = \\alpha r(t)\\), where \\(\\alpha\\) is a fixed quantity that describes rabbit fecundity.\nSimilarly, in the absence of food (rabbits are fox food), the foxes will starve or emigrate, so the dynamical equation for foxes is very similar \\(\\partial_t f(t) = - \\gamma f(t)\\), where \\(\\gamma\\) is a fixed quantity that indicates the rate at which foxes die or emigrate.\nOf course, in real ecosystems there are many other quantities that change and that are relevant. For instance, foxes eat not only rabbits, but birds and frogs and earthworms and berries. And the diet of rabbits eat weeds and grass (which is generally in plentiful supply), but also the gardener’s flowers and carrots (and other vegetables). Growth in the rabbit population leads to decrease in available flowers and vegetables, which in turn leads to slower growth (or even population decline) for rabbits.\nIn the spirit of illustrating dynamics, we will leave out these important complexities and imagine that the state consists of just two numbers: how many rabbits there are and how many foxes. The dynamics therefore involve two equations, one for \\(\\partial_t r\\) and one for \\(\\partial_t f\\). For the rabbit/fox model, we will allow the rabbit population change (\\(\\partial_t r\\)) to be affected by fox prediation and similarly let the fox population change (\\(\\partial_t f\\)) reflect the consumption of rabbits as food, writing: \\[\\partial_t r = \\ \\ \\ \\ \\ \\alpha\\, r - \\overbrace{\\beta\\, f r}^{\\text{fox predation}}\\\\\n\\partial_t f = \\underbrace{\\delta\\, r f}_{\\text{rabbits as food}} - \\gamma\\, f\\]\nThe quantities \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\), and \\(\\delta\\) are parameters quantify the biology of the system: the reproduction rate of rabbits, the need of foxes for food (rabbits) to reproduce, the hunting success of foxes, and the death or emigration of foxes in response to a shortage of food.\nHow are you supposed to know that \\(r\\) and \\(f\\) are state variables while quantities like \\(\\beta\\) and \\(\\gamma\\) are parameters? Because there is a differential equation involving \\(\\partial_t r\\) and \\(\\partial_t f\\), while no differential equation has been given describing \\(\\partial_t \\beta\\) or \\(\\partial_t \\alpha\\).\nComing up with this description of dynamics requires knowing something about rabbits and foxes. The particular forms used, for instance the interaction term \\(r f\\), come from modeling experience. The interaction term is well named because it is about the literal, biological interaction of foxes and rabbits.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Differential equations</span>"
    ]
  },
  {
    "objectID": "Dynamics/B6-diff-eq.html#state-space-and-flow-field",
    "href": "Dynamics/B6-diff-eq.html#state-space-and-flow-field",
    "title": "40  Differential equations",
    "section": "40.5 State space and flow field",
    "text": "40.5 State space and flow field\nFor the purpose of developing intuition it is helpful to represent the instantaneous state as a point in a graphical frame and the dynamics as a field of vectors showing how, for each possible state, the state changes. For instance, in the Rabbit-Fox dynamics, the state is the pair \\((r, f)\\) and the state space is the coordinate plane spanned by \\(r\\) and \\(f\\).\nThe present state of the system might be any point in the state space. But once we know the present state, the dynamical functions evaluated at the present state tell us how the state changes over a small increment in time. The step over a small increment of time can be represented by a vector.\nLet’s illustrate with the Rabbit-Fox system, whose dynamical equations are given above. The dynamical functions take a position in state space as input. Each of the functions returns a scalar.\nTo make a plot, we need numerical values for all the parameters in those equations.\nThe vector field corresponding to the dynamics is called a flow, as if it were a pool of swirling water. Figure 40.2 shows the flow of the rabbit/fox system.\n\n\n\n\n\n\n\n\nFigure 40.2: The dynamics of the rabbit/fox system shown as a vector field over the state space. The parameters have been set, for the purpose of illustration, to \\(\\alpha = 2/3\\), \\(\\beta = 4/3\\), \\(\\gamma = 1\\), and \\(\\delta = 1\\).\n\n\n\n\n\nStaying with the analogy to a pool of swirling water or the currents in a river, you can place a lightweight marker such as a leaf at some point in the flow and follow its path over time. This path—position in state space as a function of time—is called the trajectory of the flow. There are many possible trajectories, depending on where you place the leaf.\nIn Chapter 33 we considered the path followed by a robot arm. In that chapter, we separated out the \\(x\\)- and \\(y\\)-components of the arm’s position over time, calling them functions \\(x(t)\\) and \\(y(t)\\). Analogously, the the decomposition of a trajectory from an initial condition in the flow—this would be \\(r(t)\\) and \\(f(t)\\) for the rabbit/fox system—gives us the solution to the differential equation.\nEach component of the solution is called a time series and is often plotted as a function of time, for instance \\(r(t)\\) versus \\(t\\).\nFrom the flow field, you can approximate the trajectory that will be followed from any initial condition. Starting from the initial condition, just follow the flow. You already have some practice following a flow from your study of the gradient ascent method of optimization described in Chapter 23. At the argmax, the gradient is nil. Thus, the gradient ascent method stops at the argmax. We will see an analogous behavior in dynamical systems: any place where the flow is nil is a potential resting point for the state, called a fixed point.\n\n\\(\\ \\)\nLet’s return to the pendulum and examine its flow field. We will modify the equations just a little bit to include air resistance in the model. Air resistance is a force, so we know it will appear in the \\(\\partial_t v_\\theta(t)\\) equation. A common model for air resistance has it proportional in size to the square of the velocity and with a direction that is the opposite of the velocity. In a differential equation, the model of air resistance can be written as \\(- \\alpha\\, L\\, \\text{sign}(v(t))\\ v(t)^2\\), where \\(\\text{sign}()\\) is a piecewise function that has the value \\(+1\\) when the argument is positive and \\(-1\\) when the argument is negative. \\(L\\) is the length of the pendulum. \\[\\partial_t \\theta = v\\\\\n\\partial_t v = - \\sin(\\theta) - \\alpha\\,L^2\\, \\text{sign}(v)\\ v^2\\] (Keep in mind as always that for dynamical systems a state variable like \\(\\theta\\) is also a function of time \\(\\theta(t)\\).) Whenever you have a state variable, you know that it is a function of time and so the explicit \\((t)\\) is often omitted for the sake of conciseness.\n?fig-pendulum-in-air shows the flow field of the pendulum. Also shown is a trajectory and the two time series corresponding to that trajectory.\n\n## Solution containing functions theta(t), v(t).\n\n\n\n\n\n\n\nFigure 40.3: The flow field of a pendulum with air resistance. From the initial condition (marked by \\(\\color{red}{\\text{x}}\\)), a trajectory is sketched out for \\(0 \\leq t \\leq 20\\). The individual components of that trajectory are graphed as time series.\n\n\n\n\n\n\n\n\n\n\n\nFigure 40.4: The flow field of a pendulum with air resistance. From the initial condition (marked by \\(\\color{red}{\\text{x}}\\)), a trajectory is sketched out for \\(0 \\leq t \\leq 20\\). The individual components of that trajectory are graphed as time series.\n\n\n\n\n\nThe pendulum was started out by lifting it to an angle of \\(45^\\circ\\) and giving it an initial upward velocity. The bob swings up for a bit before being reversed by gravity and swinging toward \\(\\theta = 0\\) and beyond. Due to air resistance, the amplitude of swinging decreases over time.\n\nThe flow of a dynamical system tells how different points in state space are connected. Because movement of the state is continuous in time and the state space itself is continuous, the connections cannot be stated in the form “this point goes to that point.” Instead, as has been the case all along in calculus, we describe the movement in terms of a “velocity” vector. Each dynamical function specifies one component of the “velocity” vector, taken together they tell the direction and speed of movement of the state at each instant in time.\nPerhaps it would be better to use the term state velocity instead of “velocity.” In physics and most aspects of everyday life, “velocity” refers to the rate of change of physical position of an object. Similarly, the state velocity tells the rate of change of the position of the state. It is a useful visualization technique to think of the state as an object skating around the state space in a manner directed by the dynamical functions. But the state space almost always includes components other than physical position. For instance, in the rabbit/fox model, the state says nothing about where individual rabbits and foxes are located in their environment; it is all about the density of animals in a region.\nIn physics, often the state space consists of position in physical state as well as the physical velocity in physical space. For instance, the state might consist of the three \\(x, y, z\\) components of physical position as well as the three \\(v_x, v_y, v_z\\) components of physical velocity. Altogether, that is a six-dimensional state space. The state velocity also has six components. Three of those components will be the “velocity of the velocity,” that is, the direction and speed with which the physical velocity is changing.\n\n\\(\\ \\)\nReturning to the Chutes and Ladders game used as an example near the start of this chapter …\nThe state in chutes and ladders is one of the hundred numbers 1, 2, \\(\\ldots\\), 100. This is a discrete state space. Therefore, we can describe the “flow” in a very concrete way: how each state is directly connected to another. Figure 40.5 shows these connections. There is no velocity involved because there is no infinitesimal movement of state. For instance, state 47 connects directly to state 26.\n\n\n\n\n\n\n\n\nFigure 40.5: The “flow” connecting the discrete states in the dice-free Chutes and Ladders game. Source: Maj. Austin Davis\n\n\n\n\n\nIn the no-dice game, the state follows the arrows. Looking carefully at Figure 40.5, you can see that each state has a forward connection to at most one state. This is the hallmark of determinism.\nIn the children’s game, the play is not deterministic because a die is used to indicate which state follows from each other state. A die has six faces with the six numbers 1 to 6. So, each state is connected to six other states in the forward direction. Which of the six is to be followed depends on the number that comes up on the die. Multiple forward connections means the dynamics are stochastic (random).\nStraightforward examination of the flow often tells you a lot about the big picture of the system. In dice-free Chutes and Ladders, The 100 states are divided into three isolated islands. State 1 is part of the island in the lower right corner of Figure 40.5. Follow the arrows starting from any place on that island and you will eventually reach state 84. And state 84 is part of a cycle \\(84 \\rightarrow 85 \\rightarrow \\cdots \\rightarrow 28 \\rightarrow 84 \\rightarrow \\cdots\\). Once you are on that cycle, you never get off. We will see such cycles in continuous-time dynamical systems as well.\n\n\n\n\n\n\n\nCalculus history—Numerical weather forecasting\n\n\n\nWeather forecasting by numerical process is a highly influential book, from 1922, by Lewis Fry Richardson. He envisioned a calculation for a weather forecast as a kind of function. The domain for the forecast is the latitude and longitude of a point on the globe, rather than the rectilinear organization of corridor.\nOne fantastic illustration of the idea shows a building constructed in the form of an inside-out globe. Source At each of many points on the globe, there is a business. (You can see this most easily in the foreground, which shows several boxes of workers.)\n\n\n\n\n\n\n\n\nFigure 40.6: An artist’s depiction of the organization of calculations for weather forecasting by Richardson’s system.\n\n\n\n\n\nIn each business there is a person who will report the current air pressure at that point on the globe, another person who reports the temperature, another reporting humidity, and so on. To compute the predicted weather for the next day, the business has a staff assigned to visit the neighboring businesses to find out the pressure, temperature, humidity, etc. Still other staffers take the collected output from the neighbors and carry out the arithmetic to translate those outputs into the forecast for tomorrow. For instance, knowing the pressure at neighboring points enables the direction of wind to be calculated, thus the humidity and temperature of air coming in to and out of the region the business handles. In today’s numerical weather prediction models, the globe is divided very finely by latitude, longitude, and altitude, and software handles both the storage of present conditions and the calculation from that of the future a few minutes later. Repeating the process using the forecast enables a prediction to be made for a few minutes after that, and so on.\nSome of the most important concepts in calculus relate to the process of collecting outputs from neighboring points and combining them: for instance finding the difference or the sum. To illustrate, here is the first set of equations from Richardson’s Weather forecasting … written in the notation of calculus:\n\n\n\n\n\n\n\n\n\nYou can hardly be expected at this point to understand the calculations described by these equations, which involve the physics of air flow, the coriolis force, etc. but it is worth pointing out some of the notation:\n\nThe equations are about the momentum of a column of air at a particular latitude (\\(\\phi\\)) and longitude.\n\\(M_E\\) and \\(M_N\\) are east-west and north-south components of that momentum.\n\\(\\partial M_E /\\partial t\\) is the rate at which the east-west momentum will change in the next small interval of time (\\(\\partial t\\)).\n\\(p_G\\) is the air pressure at ground level from that column of air.\n\\(\\partial p_G / \\partial n\\) is about the difference between air pressure in the column of air and the columns to the north and south.\n\nCalculus provides both the notation for describing the physics of climate and the means to translate this physics into arithmetic calculation.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Differential equations</span>"
    ]
  },
  {
    "objectID": "Dynamics/B6-flow-on-line.html",
    "href": "Dynamics/B6-flow-on-line.html",
    "title": "42  Flows on the line",
    "section": "",
    "text": "42.1 Dynamical function and flow\nIn the previous chapter, we saw how to draw a flow field in a two-dimensional state space, evaluating the dynamical functions and using the results to construct a vector. We cannot practically visualize both the flow and the shapes of the two dynamical functions in a single plot, which makes it harder to understand structures such as fixed points.\nHappily, with a one-dimensional state space, we can easily show both the flow vectors and the single dynamical function at once.\nFor ease of reference, we will name the dynamical function for the rest of this section \\(f(x)\\), so that the differential equation is \\[\\partial_t x = f(x)\\ .\\]\nThe flow itself appears as the example in Figure 42.1. The state space is the number line and the flow vectors are, as usual arrows that point from place to place in the state space.\nFigure 42.1: A one-dimensional state space shown with its flow vectors.\nBecause the state space can be drawn without using the vertical coordinate of the page, we can use that vertical coordinate to show something else: the dynamical function, as in Figure 42.2.\nFigure 42.2: A one-dimensional state space shown with its flow vectors.\nThe correspondence between the dynamical function and the flow field is easy to see in such a presentation. Where the output of the dynamical is large and positive (say, near \\(x=0\\)), the flow is in the positive direction and relatively fast, as shown by a long, right-pointing flow vector. When the output of the dynamical function is negative (around \\(x=3\\), for instance) the flow is in the negative direction: a left pointing arrow.\nNear a zero crossing of the dynamical function, the flow arrows are negligibly short: the state velocity is very slow. Indeed, at the zero crossings, the state velocity is exactly zero. Such zero crossings are called fixed points: since the state velocity is zero, the state never moves!\nWe can see the dynamics near fixed points more closely by zooming in, as in Figure 42.3 which shows two of the system’s fixed points.\nFigure 42.3: Zooming in on the flow for the system shown in @fig-phase-line-intro2.\nNotice in Figure 42.3 that the flow is slower the nearer the state is to the fixed point, but it is only exactly zero at the fixed point.\nA calculus technique you will be familiar with from previous Blocks is zooming in a region that we want to examine in detail.\nFigure 42.4: Zooming in closely on each of the fixed points seen in @fig-phase-line-intro3.\nThe short pieces of the dynamical function shown in Figure 42.4, are, like short pieces of any continuous function: almost exactly straight lines. For the left fixed point, the dynamical function is \\(f(x) \\approx -2.804 (x + 3.055)\\) while for the right it is \\(f(x) \\approx 5.065 (x + 1.586)\\). In Section @ref(symbolic-solutions-ODE) we found symbolically the solutions for dynamical functions in this form. For \\(x_0\\approx-3.055\\) the solution is \\[x(t) \\approx (x_0 + 3.055)e^{-2.804 t} - 3.055\\ ,\\] while for \\(x_0\\approx -1.586\\) the solution is \\[x(t) \\approx (x_0 +1.586)\\, e^{5.065 t} - 1.586\\ .\\] There is something fundamentally different about these two solutions. One of them is exponential decay toward the fixed point, while the other grows exponentially away from the fixed point. We call the dynamics near the fixed-point with exponential decay stable and the dynamics near fixed-point with exponential growth unstable.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Flows on the line</span>"
    ]
  },
  {
    "objectID": "Dynamics/B6-flow-on-line.html#dynamical-function-and-flow",
    "href": "Dynamics/B6-flow-on-line.html#dynamical-function-and-flow",
    "title": "42  Flows on the line",
    "section": "",
    "text": "Graphics such as Figure 42.2 let you see both the flow and the dynamical functions together in one place.\nHow about also showing trajectories? Unfortunately, the two-dimensional extent of a computer screen or a piece of paper make it hard to include still more information in an intelligible way. It would be nice to have a third dimension for the display.\nMajor Austin Davis developed such a display, using time as the third dimension. In the movie below, the state space is shown as a horizontal line, as before. The vertical axis shows the dynamical function as in Figure 42.2. The dynamical function is shown in another way: as the hue and intensity of color, which lets you focus on the activity in the state space. This activity is shown by the moving gray triangles. Each triangle is placed on the phase line to mark an initial condition, then moves right or left according to the dynamics.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Flows on the line</span>"
    ]
  },
  {
    "objectID": "Dynamics/B6-flow-on-line.html#generic-behavior",
    "href": "Dynamics/B6-flow-on-line.html#generic-behavior",
    "title": "42  Flows on the line",
    "section": "42.2 Generic behavior",
    "text": "42.2 Generic behavior\nSo long as two dynamical systems have similar fixed points with the same stability, their trajectories will be much the same. For example, our model dynamical function might be different in detail, as in Figure 42.5, and still produce the same behavior.\n\n\n\n\n\n\n\n\nFigure 42.5: The dynamical function shown in black is a distortion of \\(f(x)\\) from the previous plots. Yet the flow field is practically identical and leads to the same outcomes as \\(f(x)\\) for any initial condition.\n\n\n\n\n\nSo long as two flows have similar fixed points with the same stability, their trajectories will be much the same. Consequently, studying the fixed points without worrying about the details of the dynamics gives a huge amount of information about the system.\nFor example, Figure 42.6 shows a score of different time series following the solutions from a score of initial conditions. The long-term behaviors for all the time series is similar: they converge to one or another of the stable fixed points.\n\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\nFigure 42.6: Time series from the differential equation \\(\\partial_t x = f(x)\\) starting at many initial conditions. The locations of the three fixed points are marked with horizontal lines. All the solutions convert to one or the other of the two stable fixed points in the system, and depart from the unstable fixed point.\n\n\n\n\n\nIt is worth pointing out a consequence of the mathematics of continuous functions: if a system with a continuous dynamical function has a region of state space with two different fixed points, there must be an unstable fixed point in between them.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Flows on the line</span>"
    ]
  },
  {
    "objectID": "Dynamics/B6-flow-on-line.html#linearization",
    "href": "Dynamics/B6-flow-on-line.html#linearization",
    "title": "42  Flows on the line",
    "section": "42.3 Linearization",
    "text": "42.3 Linearization\nYou can see in Figure 42.6 that many of the solutions approach their final, equilibrium value in an exponential manner. This is particularly true for the solutions with initial conditions very near the stable fixed points. All these solutions are characterized quantitatively by the parameter \\(a\\) in the exponential solution \\(A e^{a t}\\). (Remember, \\(a &lt; 0\\) when there is exponential decay.)\nQuantitative knowledge of \\(a\\) is helpful to understand the time scale of the exponential approach to stable fixed points. We can find a numerical value for \\(a\\) for each fixed point by constructing a linear approximation to the dynamical function near each of the fixed points.\nThe procedure involves the same principles as introduced in Block 2 for constructing low-order polynomial approximations to functions, but here “low-order” means “first order.”\nThe analysis is done separately for each of the fixed points, so the first step is to find the fixed points, the values \\(x^\\star\\) such that \\(f(x^\\star) = 0\\).\nRecall from Block 2 the Taylor polynomial approximation to a function \\(f(x)\\) centered on a point \\(x^\\star\\): \\[f(x) \\approx f(x^\\star) + \\partial_x f(x^\\star) \\left[x - x^\\star\\right]\\] When \\(x^\\star\\) is a fixed point, \\(f(x^\\star) = 0\\) so the approximation is simply \\(f(x) \\approx \\partial_x f(x^\\star) \\left[x - x^\\star\\right]\\). Keep in mind that \\(\\partial_x f(x^\\star)\\) is the derivative function \\(\\partial_x f\\) evaluated at the input \\(x^\\star\\), so \\(\\partial_x f(x^star)\\) is simply a quantity, not a function. Indeed, \\(\\partial_x f(x^star)\\) is exactly the quantity \\(a\\) in the exponential solution \\(e^{a t}\\).\nThis process of constructing the linear approximation \\(f(x) \\approx a \\left[x - x^\\star\\right]\\) is called linearization.\n\nConsider the first-order differential equation \\[\\partial_t x = f(x) \\equiv r x (x - x/K)\\] where \\(r\\) and \\(K\\) are parameters that are greater than zero. Linearizing the nonlinear function \\(f(x)\\) lets us figure out how fast or slow is the exponential growth or decay of the solutions for initial conditions near the fixed points.\n\nThere are two fixed points, one at \\(x_1^\\star = 0\\) and the other at \\(x_2^\\star = K\\). What is the exponential parameter \\(a\\) for each of the fixed points.\nThe derivative (with respect to \\(x\\)) \\(\\partial_x f(x)\\) can be found with the product rule from Block 2. It is \\[\\partial_x f(x) = r\\, (1 - x/K) - r\\, x\\, (1/K)\\]\nEvaluating \\(\\partial_x f(x)\\) at the two fixed points \\(x_1^\\star = 0\\) and \\(x_2^\\star = K\\) gives\n\n\\[\\partial_x f(x_1^\\star) = r\\ \\ \\ \\text{and}\\ \\ \\ \\partial_x f(x_2^\\star) = -r\\] Solutions near \\(x_1^\\star\\) will grow exponentially as \\(e^{r t}\\), unstable since \\(0 &lt; r\\). Solutions near \\(x_2^\\star\\) will decay toward \\(x_2^\\star\\) in an exponential manner as \\(e^{-r t}\\).\n\n\nIt is critical to distinguish carefully between \\(x^\\star\\), which is the location of the fixed point being examined, and \\(x_0\\), which is the initial condition of the state, that is, \\(x(t=0)\\).\n\n\n\\(\\ \\)\nLet’s return to the model of saving for retirement in Chapter 40: \\[\\partial_t V = r\\, V + M\\ .\\] The state variable here is named \\(V\\). The dynamical function is \\[g(V) = r\\, V + M\\] where \\(r\\) is the interest rate (say, 3% per year which is \\(r=0.03\\) per year) and \\(M\\) is the monthly contribution. To keep the units consistent, we set the units of \\(t\\) to be years, of \\(r\\) to be 1/years, of \\(V\\) to be dollars and of \\(M\\) to be dollars-per-year. So a monthly contribution of \\(1000 would come to\\)M=12000$ dollars-per-year.\nFind the amount \\(V\\) that will result from 30 years of savings with an initial condition \\(V_0 = 0\\).\nStep i) Find the fixed point. This is a value \\(V^\\star\\) such that \\[r\\, V^\\star + M = 0\\ \\ \\ \\implies \\ \\ \\ V^\\star = -M/r\\ .\\] Step ii) Find the derivative of the dynamical function evaluated at the fixed point: Since \\(g(V)\\) happens to be a straight-line function, we know the derivative is a constant. So \\(b = \\partial_x g(V^\\star) = r\\).\nStep iii) Translate the state variable into \\(y = V - V^\\star\\). The dynamics in terms of \\(y\\) are \\(\\partial_t y = b y\\), which has an exponential solution \\(y = A e^{bt}\\).\nStep iv) \\(A\\) is the initial condition in terms of \\(y\\). This will be \\(y_0 = V_0 - V^\\star\\). Since we stated that \\(V_0 = 0\\) (no savings at the start), \\(y_0 = -V^\\star\\) and the solution is \\[y(t) = -V^\\star e^{bt} = \\frac{M}{r} e^{rt}\\ .\\]\nStep v) Translate the solution in step (iv) back into terms of \\(V(t)\\). Since \\(y(t) = V(t) - V^\\star\\), this will be \\(V(t) = y(t) + V^\\star\\) or, \\[V(t) = \\frac{M}{r} e^{r t} + V^\\star = \\frac{M}{r} \\left[ e^{r t} - 1\\right]\\ .\\] To get an idea of this retirement plan, that is, \\(r=3\\%\\) and \\(M=12000\\) dollars-per-year, let’s see how much you will have after 30 years and 40 years.\n\nV &lt;- makeFun((M/r)*(exp(r*t)-1) ~ t, r=0.03, M=12000)\nV(30)\n## [1] 583841.2\nV(40)\n## [1] 928046.8\n\nAfter 40 years of contributions, your retirement account will have almost one-million dollars.\nYou could have accomplished the same calculation using integrateODE(), like this:\n\nSoln &lt;- integrateODE(dV ~ r*V + M, V=0, M=12000, r=0.03,  \n                     domain(t=0:40))\n## Solution containing functions V(t).\nSoln$V(30)\n## [1] 583841.2\nSoln$V(40)\n## [1] 928046.8",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Flows on the line</span>"
    ]
  },
  {
    "objectID": "Dynamics/B6-flow-on-line.html#bifurcation",
    "href": "Dynamics/B6-flow-on-line.html#bifurcation",
    "title": "42  Flows on the line",
    "section": "42.4 Bifurcation",
    "text": "42.4 Bifurcation\nA broad, pressing, social concern goes under the name sustainability. Is it sustainable to burn fossil fuels at steady historical levels, let alone at the increasing rate seen since over the last century? Climate scientists answer resoundingly with a no. Is it sustainable to increase food production to the levels needed for developing economies to approach the sort of consumption seen in rich economies?\nDynamical systems are highly relevant to the questions surrounding sustainability. If the economy is near a stable fixed point, then it is sustainable; the trajectory will bring the state of the economy toward the fixed point. On the other hand, if the economy is near an unstable fixed point, we can expect exponential change.\nIf such exponential changes are not seen, does that mean we are not near an unstable fixed point? One of the terms used to mark the possibility that a stable system can quickly turn unstable is tipping point, defined as\n\nThe point at which a series of small changes or incidents becomes significant enough to cause a larger, more important change. Source: New Oxford American Dictionary\n\nThe mathematics of tipping points is not at all the same as exponential growth. Certainly, in exponential growth one sees a relatively slow rate of change increase to a large rate of change, a situation described by journalists as “sky-rocketing” or “explosive” or, literally, “exponential.” As you’ve seen, exponential growth is a phenomenon seen in linear dynamical systems; there is no special point at which the dynamics changes.\nThere is an area of mathematical theory called catastrophe theory. We will use a famous example to show how catastrophes or tipping points are modeled mathematically.\nThe example comes from a 1977 article in Nature, one of the world’s most prestigious scientific journals. The article, by Robert May, is entitled “Thresholds and breakpoints in ecosystems with a multiplicity of stable states.” The words “thresholds” and “breakpoints” have not been encountered yet in this book, but “multiplicity of stable states” should bring to mind the sort of dynamics seen in Figure 42.2.\nThe setting for the catastrophe is an otherwise bucolic scene, livestock grazing on a pasture. A pasture is a kind of factory for producing vegetable biomass; the grazing is the consumption of the biomass produced.\nAs a model for the production of biomass, denoted \\(v\\) for “vegetation,” we will use \\[\\partial_t v = r v \\left(1 - \\frac{v}{K}\\right)\\] which, as we’ve seen, has an unstable fixed point at \\(v_1^\\star=0\\) and a stable fixed point at \\(v_2^\\star=K\\). Physically, the fixed point \\(v_1^\\star\\) corresponds to a bare field, without any vegetation. It is unstable because any small disturbance in the form of a stray seed landing in the dirt can lead to germination and the rapid growth of vegetation as seeds from the germinated plant spread across the field. Once the field is covered in vegetation, the growth can be exponentially rapid at first but then runs into limited resources: there is only so much sunlight that falls on the field, and the growing vegetation will eventually consume the soil nutrients and water.\nThis biomass production model corresponds to a sustainable system. Once the biomass level is at \\(v_2^\\star\\) it will stay there.\nBut biomass production is not the only thing going on in the pasture. The grazing animals—let’s imagine they are cows—are consuming the grass. To start very simply, suppose that each cow consumes amount \\(C\\) of biomass each day. If there are \\(H\\) cows, the total consumption is \\(H C\\) per day. This modifies the dynamics to a slightly new form \\[\\partial_t v = r \\,v(1-\\frac{v}{K}) - HC\\]. The original, ungrazed dynamics are compared with the grazed dynamics in Figure 42.7.\n\n\n\n\n\n\n\n\nFigure 42.7: Comparing the pasture dynamics for different herd sizes.\n\n\n\n\n\nWith grazing, the net growth of biomass is reduced due to the removal of the consumed biomass by the cows’ consumption. For a moderate herd size, there is still a stable fixed point, but it is at a lower level of biomass than would be seen in the ungrazed field. But if the herd size is too large, the ecosystem completely collapses.\nThis is an example of a tipping point or catastrophe. For moderate herd sizes, there remains a stable fixed point. A farmer might be tempted to add another cow to the pasture, and that is sustainable: there is still a stable fixed point. Indeed, the movement of the stable fixed point might not even be noticed. But add even one cow too many and the fixed point entirely disappears. Still, herd management can fix the problem; take away the cow that tipped the pasture and the fixed point will return.\n\nApplication area 42.1 —Details of chewing\n\n\n\n\n\n\n\nApplication area 42.1 Cows eat grass\n\n\n\nIn Chapter 16 we illustrated the iterative process for building models: using the results of a model to suggest possible improvements in the model. Let’s look again at how cows eat grass.\nMissing from the pasture model is a simple idea of how cows eat. If there is very little biomass, the cows cannot continue to eat their fill. Will the reduction in consumption per cow preserve the stable fixed point?\nIn his Nature article, May modeled the consumption rate by a single cow with the functional form \\[C(v) \\equiv \\frac{\\beta v^2}{v_0 - v^2}\\] which is graphed for \\(\\beta=0.1\\) and \\(v_0 = 3\\) in Figure 42.8\n\nconsumption &lt;- makeFun((beta*v^2/(v0^2 + v^2))~ v, beta=0.1, v0=1)\nslice_plot(consumption(v) ~ v, bounds(v=0:10)) %&gt;%\n  gf_labs(y=\"Consumption (tons/day)\", x=\"v: available biomass (tons)\")\n\n\n\n\n\n\n\nFigure 42.8: Consumption of vegetation by a single cow as a function of the amount available in the pasture.\n\n\n\n\n\nYou can recognize this as a form of sigmoid. When the amount of vegetation is very large, a cow will eat her fill. That is the saturation of the sigmoid. For small \\(v\\), the cow needs to hunt around for vegetation tall enough to eat, reducing the consumption steeply.\nFigure 42.9 modifies the pasture dynamics to incorporate this sigmoid model of consumption.\n\n\n\n\n\n\n\n\nFigure 42.9: Pasture dynamics for a sigmoid consumption function.\n\n\n\n\n\nWe can start the story with 3 cows in the pasture. Vegetation growth is more than sufficient to provide for these cows. You can see this from the stable fixed point at about 9 tons of biomass, which is more than enough to reach saturation of the sigmoid consumption function.\nThe farmer decides to increase the herd to 5 cows. Nothing much happens. The stable fixed point is at about 8 tons of biomass, entirely adequate to keep the cows well fed in a sustainable manner.\nCan the pasture be sustainable with 7 cows? The stable fixed point remains, now at about 6.5 tons at biomass. The cows are still sustainably well fed.\nYou can see in the 7-cow dynamics a hint of what of what might go wrong. There is a new, unstable fixed point at about 3 tons of biomass. If the pasture ever happened transiently to fall below 3 tons—say due to a summer frost followed by a return to normal weather—the vegetation biomass will head toward the new stable fixed point at 0.5 tons of biomass. At this level, the cows are eating only about one-quarter their normal amount and we can fairly say that the ecosystem has collapsed. But until such a disaster happens, the farmer will see only a sustainable level of biomass with cows well fed.\nIt is only we, who have a mathematical model of the situation, who can anticipate the potential problems.\nSince things are fine with 7 cows in the field, the farmer lets an eighth cow join the herd. That is the tipping point. The happy herd fixed point at 6.5 tons of biomass has disappeared, and the ecosystem collapses, even without a weather disaster.\nRemoving the eighth cow from the pasture will not fix the situation. With seven or even six cows in the pasture, the system won’t be able to grow out of the stable fixed point with 0.5 tons of biomass. Reducing the herd size to five will remove that 0.5 ton fixed point, but the grass will grow back very slowly; the dynamics give positive growth, but very close to zero.\nAvoiding such catastrophes is a major motivation for mathematical modeling.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Flows on the line</span>"
    ]
  },
  {
    "objectID": "Dynamics/B6-modeling.html",
    "href": "Dynamics/B6-modeling.html",
    "title": "44  Modeling dynamics",
    "section": "",
    "text": "44.1 A single state quantity\nWe will start with situations that can be modeled with a single state variable. Throughout our examples, we will call that state variable \\(S\\), so the differential equation describing how \\(S\\) changes in time will always be \\[\\partial_t S = f(S)\\]. The modeler chooses the shape of \\(f()\\) depending on the situation being modeled.\nIn principle, there is an infinite number of shapes for \\(f(S)\\). But many modeling settings involve behavior that is simple. We will work with function shapes where the dynamical function \\(f(S)\\) is continuous and has one or two fixed points.\nFigure 44.1 shows four generic dynamical function shapes where these is one fixed point. The location of the fixed point is, of course, the \\(S_0\\) at which \\(f(S_0)=0\\), that is, the intersection of the function and the blue dashed line.\nFigure 44.1: Four generic functions shapes with one state variable having one fixed point.\nThe fundamental distinction is between models with stable and models with unstable fixed points. When the situation being modeled has a steady equilibrium, only the stable shapes of \\(f(S)\\) are relevant. A second distinction is between functions with a bounded shape and those with a linear shape. Use a linear shape when you are concerned only with the behavior near the fixed point. But when your model needs to account for behavior far from the fixed point, you need first to have a way to represent what is meant by “far from.” That is represented by the location of the shoulder of the curve relative to the fixed point.\nA setting for a linear stable model is the cooling or warming of an object to the ambient temperature. \\(S\\) stands for the temperature of the object and the fixed point is the ambient temperature. This model often goes under the name Newton’s Law of Cooling. The prestige of the model name distracts from the fact that this is a very simple model. But in the real world there are likely to be complications that make Newton’s Law imprecise, unlike, say, Newton’s Second Law of Motion which is exact (at non-quantum and non-relativistic scales). For instance, a cup of hot water will cool faster than a cup of cold water. In hot water, evaporation from the surface speeds up the warming. When ice is involved, the rate of temperature change slows when melting or freezing is encountered.\nAnother classic setting for a linear, stable model is radioactive decay. The rate at which the atoms in a mass of a radioactive isotope decay is a constant \\(\\alpha\\) that depends on the nuclear structure. A mass of \\(n\\) atoms will produce \\(\\alpha n\\) individual decay events in a given time interval. But, since each decay event reduces \\(n\\) by 1, the differential equation describing the number of radioactive atoms is \\[\\partial_t n = - \\alpha nt\\] which leads to exponential decay towards zero. Notice that only the \\(0 \\leq n\\) half of the state space is relevant, because the number of atoms cannot be negative.\nAn example of a setting where the bounded stable model applies is one where \\(S\\) stands for the amount of prey or food stock, and there is a constant population of predators who have a limited capacity for eating. Consider, for example, \\(S\\) being the availability of acorns. There might be a fixed population of, say, oaks that produce acorns at a given rate. When predation is low, the acorns accumulate. But when there are a lot of acorns, predators might focus on that bountiful food source. Still, they can only eat so many acorns per day before they are full.\nThe linear unstable model is often used to model population growth. The underlying idea, which might or might not correspond to reality, is that there is a set reproduction rate per member of the population. The differential equation is \\[\\partial_t{S} = a S + c\\] with \\(S\\) being the size of the population. The parameter \\(c\\) captures immigration (\\(0 &lt; c\\)) or emigration (\\(c &lt; 0\\)). To be more detailed, the model can be written as summing a birth process and a death process: \\[\\partial_t S = b S - d S + c\\] where \\(b\\) is the birth rate and \\(d\\) is the death rate. Of course, the detailed model collapses arithmetically to the \\(a S + c\\) model, with \\(a = b - d\\). Still, elaborations on the birth or death processes may include the influence of changing factors. We will return to this idea in a bit.\nTake care to use the parameters \\(a\\) and \\(c\\) correctly. The output of \\(f(S) = a S + c\\) must have dimension \\([S]/[t]\\), for instance organisms per hour might be appropriate for bacteria. Thus the individual terms \\(aS\\) and \\(c\\) must have dimenion \\([S]/[t]\\). Consequently, the parameter \\(a\\) has dimension \\(1/[t]\\) as in per hour. That might seem odd until you remember that \\(a\\) is about the creation of new organisms, \\([S]/t\\), per existing organism. Thus the dimension of \\(a\\) is \\([S] / [S][t]\\) which works out to be simply \\(1/[t]\\).\nFor bacteria, the linear unstable model may be realistic for short periods of time, or, more precisely, for as long as the population is small compared to the carrying capacity. (See ?sec-nonlinear-on-line.) In contrast, human and other animal populations often have an important age structure, which is just to say that neither a 6 nor a 60 year old person has the same reproduction rate as a 26 year old. Such an age structure calls for a dynamical state with multiple components.\nBut if the environment is steady—no food shortages, no disease, economy unchanging, etc.—it can be reasonable to describe even age-structured populations as a percentage growth per unit time, e.g. percent per year. Realize that such a description is not only about the biology of reproduction, but summarizes the whole system of aging, death, and reproduction. This summary description may no longer be relevant when the system as a whole changes. An example of this in the human population is seen in countries where the number of births per woman has fallen substantially—by half or more—over the time of a generation. Such falls typically accompany a growth in economic wealth and the realization that more resources (e.g. education) needs to be provided to each offspring.\nThe bounded unstable model is a way to incorporate factors that interfere with sustained exponential growth. Exponential growth requires that the growth rate \\(\\partial_t S\\) increase as \\(S\\) increases: a kind of positive feedback. In the bounded model, the growth rate becomes constant for large \\(S\\). A constant growth rate means that \\(S\\) will grow steadily, that is \\(\\partial_t S = c\\) which has a solution that grows linear in time, as distinct from the exponential solution that results from \\(\\partial_t S = a S\\).\nAn application of the bounded unstable model is seen in the description of micro-organism growth given by Jacques Monod (1910-1976), a Nobel Prize winning biochemist. His idea was that the organisms are reproducing in a kind of sea that has a limited concentration of an essential nutrient, but very large amounts of the nutrient spread out over space. Even though the nutrient is begin consumed by the organisms, more nutrient diffuses in from far away to keep the concentration steady. At large population sizes, the growth rate is nutrient concentration limited, hence constant.\nIn Chapter 14 we introduced the idea of a modeling cycle: taking an initial model, examining the consequence/predictions of that model, and then modifying the model to better correspond to observed reality or new mechanism.\nA case in point is the linear unstable model for population growth. The linear model is always appropriate near a fixed point. This is just a consequence of the calculus idea that any function can be approximated by a linear function over a small enough domain. In defining derivatives, the question was what constitutes “small enough.” So a linear dynamical function is a good starting point for dynamics near a fixed point. But, as we’ve seen, extending the linear model far from the fixed point leads to population explosion: exponential growth. This can be a valid idea for modeling a pathogen growing in a bowl of room-temperature chicken salad: the pathogen need not consume all the salad to become a threat, so in the domain of interest—human health—the linear model can do the job.\nBut we observe generally that exponential growth does not continue indefinitely. The demographer Thomas Malthus (1766-1834) famously propounded a “principle of population” which held that it is in the nature of populations to growth exponentially until linally limited by famine or disease. He wrote, “[G]igantic inevitable famine stalks in the rear, and with one mighty blow levels the population with the [lack of] food of the world.”\nMalthus’s model is exponential growth that runs into a wall of limited food. Malthus saw human reproduction as the engine of the immense poverty and suffering of the lower classes in early industrial Britain. This became the basis of an important political dispute, two poles of which are “there is no point helping the poor, because they create their own poverty,” and “the poverty is due to exploitation, not reproduction.”\nFor us in calculus, there is a middle road: Malthus’s mathematical model, the unstable linear model, is much too abrupt and narrow minded and can easily be made more realistic. Adding that realism removes the “one mighty blow” from the situation. Let’s add that realism now.\nRecall the earlier suggestion that the linear unstable model \\(\\partial_t S = a S + c\\) be broken into components: \\[\\partial_t S = \\underbrace{b S}_{\\text{births}}\n- \\underbrace{d S}_{\\text{deaths}} + \\underbrace{c}_{\\text{immigration}}\\ .\\]\nEven in Malthus’s time, there were calls to alleviate poverty by encouraging people to leave for less crowded lands: emigration. In the dynamics, emigration corresponds to a negative value for \\(c\\).\nMaking \\(c\\) negative does not do the job on its own. Note that whatever the value of \\(c\\), the dynamics are unstable. Emigration at a constant rate changes the location of the fixed point, but since the dynamics are unstable, growth will still be unbounded. Suppose, however, that government policy sets an emigration goal not as a constant number of people per year but as a constant fraction, \\(eS\\) of the population. Now the dynamics become \\[\\partial_t S = b S - d S - e S\\ .\\] These dynamics are stable or unstable depending on the value of \\(b - (d+e)\\). If that value is negative, the dynamics are stable, if positive, the dynamics are unstable. The situation resembles (mathematically) that of a nuclear power reactor: the control parameter \\(e\\) has to be carefully manipulated to set the population at a fixed level.\nBut there are other things that come into play. One of them is that the parameter setting the death rate, \\(d\\), need not be constant. From Malthus’s perspective, \\(d\\) would change in episodes set by disease and starvation. In Malthus’s era, pandemics were common and wiped out a major fraction of the population in “one mighty blow.” Similarly, starvation plays out on a smaller time scale than reproduction and seems to cut through the population.\nBut the death rate can also be a function of population \\(S\\). For instance, \\(d = d_0 + d_1 S\\) corresponds to a death rate that increases gradually with population size. (The parameters \\(d_0\\) and \\(d_1\\) are positive.)\nSimilarly, birth rate can depend on population, that is \\(b = b_0 - b_1 S\\). As the population gets larger, there is less food and less space, and these changes can reduce the reproduction rate. (The parameters \\(b_0\\) and \\(b_1\\) are positive.)\nThese models for \\(d\\) and \\(b\\) are simplistic. Why should they have a linear form? The answer is … calculus. Whatever are the functions \\(b(S)\\) and \\(d(S)\\), they must be approximately linear over a small domain.\nLet’s plug in the refined models for birth and death rates into the population models. We get: \\[\\partial_t S = (b_0 - b_1 S) S - (d_0 + d_1 S) S - e\\ .\\] A little algebraic simplification reduces this to:\n\\[\\partial_t S = (b_0 - d_0) S - (b_1 + d_1) S^2 - e\\] Whatever are the size of the quantities \\(b_0, b_1, d_0, d_1\\), so long as they are positive, the dynamical function \\(f(S)\\) will have two fixed points, one at small \\(S\\) and the other at large \\(S\\). For small \\(S\\), the fixed point is unstable. The population will grow away from this fixed point. The fixed point at large \\(S\\) will be stable, hence no population explosion.\nOne of the major flaws with the Malthusian viewpoint is that it treated all the dynamical functions as linear, whereas in reality the functions can have a quadratic shape. The classical differential equation for limited population growth, \\[\\partial_t S = a S (1-S/K)\\ ,\\] was introduced by Pierre-François Verhulst in 1838, just four years after Malthus’s death.\nAnother important flaw with Malthus’s model is that it failed to account for the transition from purely agricultural economies to economies that produced large amounts of other goods and services. It turns out as populations grow wealthier, their reproduction rates decrease. With wealth available in non-food terms—clothing, public health, education—reproduction rates can go down even without the “one mighty blow” of starvation and disease.\nThe next section examines both of these factors—the introduction of multiple state variables and the ability to “soften” the explosive unstable linear dynamics with nonlinear corrections—in making subtle models of the behavior of systems.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Modeling dynamics</span>"
    ]
  },
  {
    "objectID": "Dynamics/B6-modeling.html#a-single-state-quantity",
    "href": "Dynamics/B6-modeling.html#a-single-state-quantity",
    "title": "44  Modeling dynamics",
    "section": "",
    "text": "If \\(S\\) is observed to oscillate or to reverse the direction of change, then there must be at least one more state variable, that is, at least one more quantity that changes in time. A single first-order differential equation will not be able to model the situation.\nIf there are no fixed points, then the only possible dynamics are continuous increase in \\(S\\) or continuous decrease in \\(S\\). Often, the point of interest will be whether there is some other factor that changes increase to decrease or vice versa. Again, such situations should be modeled in the context of at least two state variables.\n\n\n\n\n\n\n\n\nRemember that in a differential equation \\(\\partial_t S = f(S)\\), the input to \\(f()\\) is has dimension \\([S]\\), while the output of \\(f()\\) has a different dimension: \\([S]/[t]\\). Thus, in the oak/acorn example above, the output of \\(f()\\) has a dimension corresponding to acorns per day, a rate of consumption. \\(S\\) is acorns, the output of \\(f(S)\\) is acorns per day.\n\n\n\n\n\\(\\ \\)\nUnder ideal reproductive conditions, some bacteria can split in two every 20 minutes. What does this tell us about \\(a\\)?\nIt is easy to get confused. For instance, a single bacterium that splits every 20 minutes will go from an initial population of \\(S=1\\) at time \\(t=0\\) to a population of \\(S=2\\) at \\(t=\\frac{1}{3}\\) hour to \\(S=4\\) at \\(t=\\frac{2}{3}\\) hour to \\(S=8\\) at \\(t=1\\) hour. This might make it seem that the reproduction rate is 8 per hour. But this calculation of the population size at hour 1 is redundant with the accumulation that will be accomplished in solving the differential equation.\nThe correct way to calculate \\(a\\) from the stated information that there is a split every 20 minutes works like this:\n\nThe dynamics are \\(\\partial_t S = a S\\). Therefore the solution is \\(S(t) = S_0 e^{a t}\\).\nAccording to the given information about splitting, when the initial condition is that \\(S(0) = 1\\), \\[S(1/3) = S_0 e^{a/3} = e^{a/3} = 2\\ .\\]\nWorking from \\(e^{a/3} = 2\\) gives the parameter value we see: \\(a = 3 \\ln(2)\\).\n\n\n\n\n\nApplication area 44.2 —Unstable atoms are everywhere!\n\n\n\n\n\n\n\nApplication area 44.2 Nuclear decay\n\n\n\nRadioactive decay of nuclei is a situation where the linear unstable model is realistic. Previously, we’ve pointed out that radioactive decay of an isotope is well modeled by the linear stable model. This leads the number of atoms of the radioactive isotope to decay exponentially.\nBut there is an important exception. Some radioactive isotopes are fissile, for instance uranium-233, uranium-235, plutonium-239, and plutonium-241. Fissile materials decay via two different mechanisms. One is each-atom-on-its-own decay that applies generally to radioactive isotopes. The other mechanism involves the decay of one atom triggering the decay of others.\nThe means of triggering involves a sub-atomic particle called a neutron. The decay of a fissile atom releases one or more neutrons, depending on the nuclear structure. If one of these neutrons has the correct energy, and if they collide in the appropriate way with a second fissile atom, that second atom will itself undergo decay, leading to the release of more neutrons. Such a process is called a chain reaction.\nTypically, a fissile chain reaction takes place inside an engineered device called a reactor. A simple differential equation of a chain reaction can be constructed using the number of neutrons in the reactor, and is \\[\\partial_t N = \\alpha N\\ .\\] As you’ve already seen, the solution to this is \\(N(t) = A e^{\\alpha t}\\): exponential growth. The value of \\(\\alpha\\) is determined by the design of the device. If the device is small, then neutrons can escape from the device before triggering a reaction, so alpha is small. If there are non-fissile neutron absorbers in the device, neutrons are taken out of play so, again, alpha is small and can even be negative.\nIn everyday terms, rapid exponential growth is called an explosion. Exponential growth cannot continue forever, and indeed a fissile bomb blows itself up, resulting in negative \\(\\alpha\\) as the fissile material is scattered.\nControl of a fission bomb requires a mechanism that changes \\(\\alpha\\) from negative (stable \\(N\\), so no explosion) to positive. This can be accomplished, for instance, by bringing together pieces of fissile material that by themselves has negative \\(\\alpha\\) into a critical mass where \\(\\alpha\\) positive.\nNon-explosive nuclear reactions, as in power plants, call for a kind of juggling act. The device cannot have negative \\(\\alpha\\) or the reaction would die out exponentially. It cannot have positive \\(\\alpha\\) or the reaction would become explosive.\nA power reactors is designed to keep \\(\\alpha\\) near zero. Near zero not at zero. The reactor design needs to allow \\(0 &lt; \\alpha\\) to start up the reaction, but trim this down to zero when the reactor is at the desired power. Typically, there is some combination of active and passive mechanisms to provide this capability. An active mechanism is the insertion of “control rods,” which absorb neutrons by reactor operators to make \\(\\alpha &lt; 0\\) when the power generation is higher than desired. Passive mechanisms can involve the transformation of reactor water into steam, which reduces the ability of neutrons to induce new fissile decays.\nReactors can be very complicated, however. The explosion of the Soviet Union’s Chernobyl reactor in 1986 was caused by a chain of events that led to control rods being withdrawn beyond the design intentions. Due to the construction of the rods, reinserting the rods to dampen the run-away reaction caused the reaction to accelerate. Avoiding such disasters requires a combination of safety oriented design and proper training of the operators. Neither of these were a feature of the 1980s Soviet environment, where secretiveness interfered with proper training and economic motivations to produce were so strong as to encourage widespread cheating.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Modeling dynamics</span>"
    ]
  },
  {
    "objectID": "Dynamics/B6-modeling.html#multiple-state-quantities",
    "href": "Dynamics/B6-modeling.html#multiple-state-quantities",
    "title": "44  Modeling dynamics",
    "section": "44.2 Multiple state quantities",
    "text": "44.2 Multiple state quantities\nThe previous section examined dynamics of a single state variable. Now we will consider the possibilities when there is a second state variables. It turns out that adding more state variables above two does not introduce many fundamentally new behaviors, so we will focus on dynamical systems with two variables. We will continue to use capital letters, like \\(S\\), to stand for the state variables. But with two (or more) state variables, we will need to give them different names so we can keep tract of what’s doing what to what. The parameters in the dynamical functions will, as has been our practice, be written as lower-case letters, such a, b, r, c, and so on.\nA starting observation is that for dynamics to be genuinely two-dimensional, the differential equations for the state variables need to be coupled to one another. For example, a dynamical system that nominally has two state variables \\(X\\) and \\(Y\\) is:\n\\[\\partial_t X = f(X)\\\\\n\\partial_t Y = g(Y)\\ .\\]\nThe state variables here are not coupled, since the change in each variable depends only on the value of that variable and not on the other.\nCoupled state variables have dynamics that look like this:\n\\[\\begin{eqnarray}\n\\partial_t X & = f(X, Y)\\\\\n\\partial_t Y & = g(X, Y)\\ .\\\\\n\\end{eqnarray}\\] The time evolution of each state variable depends on the other state variable.\nThe most mathematically simple form of coupled dynamics is this:\n\\[\\begin{eqnarray}\n\\partial_t R & = a B\\\\\n\\partial_t B & = c R\\ .\\\\\n\\end{eqnarray}\\]\nWhat type of real-world setting might such a simple model correspond to? Surprisingly, even this simple model has important things to say about complex phenomena such as love and warfare.\nWe will start with warfare, where the signs of the parameters are easy to determine. The model, called Lanchester’s Law, is \\[\\begin{eqnarray}\n\\partial_t R & = - b B\\\\\n\\partial_t B & = -r R\\\\\n\\end{eqnarray}\\] with both parameters \\(r\\) and \\(b\\) taken to be positive.\nThe state variables \\(R\\) and \\(B\\) stand for the size of the two armies in conflict: the Red army versus the Blue army. As the two armies meet in battle, the Blue army causes casualties in the Red army. These casualties reduce the size of the Red army. Similarly, the Red army causes casualties in the Blue army.\nThe model describes the rate of reduction in the armies as being proportional to the size of the opposing army. But the two armies can be different in their efficiency of causing casualties, reflected by possibly different values of the \\(r\\) and \\(b\\) parameters.\nThe dynamics are not exponential. Exponential decay of the army size would correspond to a model like \\(\\partial_t R = - r R\\), an army fighting itself. But the Lanchester model has \\(\\partial_t R = - b B\\).\nWe will defer for a moment finding the trajectories of the state variables in the course of battle. (Hint: in the model, one army wipes out the other.) Instead, we will focus on a surprisingly rich implication of of such simple dynamics.\nLanchester’s Law has a surprising consequence for measuring the overall strength of a force in a way that combines size (\\(R\\) and \\(B\\)) and effectiveness (\\(r\\) and \\(b\\)) and the implications that has for tactics.\nLanchester proposed that the quantity \\[Q(R, B) \\equiv rR^2 - b B^2\\] is a good way to characterize the dynamics. His reasoning was based on a fundamental idea from physics and chemistry: that quantities are conserved. In physics, examples of conserved quantities are energy, momentum, and angular momentum. In chemical reactions, the number of atoms of each species is conserved.\nIt is hard to say how Lanchester came up with the formula \\(rR^2 - b B^2\\): insight is hard to explain. But we can easily demonstrate that it is conserved, that is, the quantity does not change in time regardless of how the battle proceeds. We will do this by showing \\(\\partial_t Q(R, B) = 0\\).\n\\[\\partial_t Q(R, B) = \\partial_t \\left[\\strut rR^2 - b B^2\\right]\\] Applying the chain rule we find that \\[\\partial_t r R^2 = 2 r R\\, \\partial_t R\\ \\ \\ \\ \\text{and}\\ \\ \\ \\partial_t b B^2 = 2 b B\\, \\partial_t B\\ .\\] Substituting in \\(\\partial_t R = - b B\\) and \\(\\partial_t B = - r R\\) gives \\[\\partial_t Q(R, B) =  - 2 r b R B + 2 b r  B R = 0\\ .\\] The conserved quantity \\(Q(R, B)\\) describes an aspect of the battle that goes unchanged over the course of the battle. At any moment, it describes the match between the overall capability of the two armies. That the difference between the capabilities is conserved does not mean the individual capabilities are unchanged in battle. Those capabilities decrease as the army sizes, \\(R\\) and \\(B\\) are reduced. But at any instant in time, the capability of each army is proportional to the square of the army size. change The consequence, is that the capability of each army is, at all times,\nTo illustrate, consider a battle between two armies of archers. The B-army archers are more skilled: they can fire 12 arrows per minute. The R-army archers can fire at only half the rate—6 arrows per minute. But the R army is twice the size of the B army.\nAre the armies equally matched? It may at first glance seem so, since both armies can fire at the same rate. For instance, if there are 1000 archers in the R army and 500 in the B army, both armies start capable of firing 6000 arrows per minute. But Lanchester’s Law tells us that the R army is twice as capable as the B army: \\(6 \\times 1000^2\\) is twice as big as \\(12 \\times 500^2\\).\nTo see why the R army is superior, remember that each arrow can take out only one of the opposing archers. For each B arrow that is on target, the firing rate of R is reduced by 6 arrows per minute. But for each R arrow that hits, the firing rate of B is reduced by 12 arrows per minute. The initial casualty rate for the two armies is the same, but B sees a twice as large reduction in its firing rate.\n\n\n\n\n\n\nCalculus history—Fighting strength\n\n\n\nFrederick William Lanchester (1868-1946) was a British engineer, considered one of the greats of British automotive engineering. But that hardly does justice to him.\nWhile voyaging across the Atlantic to America, he became captivated by the gliding flight of herring gulls. This led to his development of his circulation theory of flight, a foundation of aerofoil theory. In 1906 he published Aerial Flight containing the first full theory of lift and drag. In Aerodonetics (1908) he developed his phugoid theory of aircraft stability, describing oscillations and stalls.\nIn 1914, Lanchester wrote a book-length series of journal articles that were published in 1916 as Aircraft in Warfare: the Dawn of the Fourth Arm. Imagine trying to theorize about a form of conflict that had never been seen!\n\nThe difficulty … is that to get the future into true perspective, it is necessary to be able to look forward along two parallel lines of development—i.e. to visualize the improvement of aircraft possible in the near future as a matter of engineering development, and simultaneously to form a live conception of what this improvement and evolution will open up in the potentialities of the machine as an instrument of war. (p.3)\n\n\n\nMathematician Steven Strogatz proposed in the 1990s that similar equations might be used to describe how love between two people varies over time. Strogatz’s equations are usually written with state variables R and J, standing for Romeo and Juliet. Positive values represent love, negative values are hate. And best to think of the model as a cartoon, but a cartoon that captures some of the behavior seen in reality.\nTo start, let’s consider a form of love that is really more like warfare:\n\\[\\partial_t R = -j J\\\\\n\\partial_t J = - r R \\ .\\] In this pathological relationship, the more Romeo loves Juliet, the faster Juliet’s love decreases toward hate. And vice versa.\nImagine that, somehow, these two perverse people started out in love: \\(R(t=0) &gt; 0\\) and \\(J(t=0) &gt; 0\\). As with Lanchester’s Law, both lovers fall increasingly out of love. Depending on the initial intensity of their love and whether or not \\(jJ^2\\) is bigger than \\(rR^2\\) (Lanchester’s conserved quantity), one of the parties will become indifferent, that is, a love level of 0 while the other’s love level is still positive. For the purpose of example, let’s suppose that Juliet is the first to reach indifference. But unlike the warring armies, the love quantity can become negative. As Juliet’s love becomes negative—remember, Romeo still has a positive level of love—then the true perversity of Romeo’s personality becomes apparent. Juliet’s increasing hostility causes Romeo’s love to grow. Without bound, because this is a linear dynamical function. The result is that Juliet’s hate increases even while Romeo’s love increases. But don’t blame Juliet. If Romeo had been the first to reach indifference, Juliet would suffer the unrequited love and Romeo would be villainously hateful.\nStrangely, if Romeo and Juliet started out mutually hating each other, their personalities would still lead to one having unbounded love for the other, who hates their partner without limit.\nA mathematically small change in the Romeo-Juliet dynamical system can lead to a profound change in the outcome. For instance, changing one sign, as in \\[\\begin{eqnarray}\n\\partial_t R & = r J\\\\\n\\partial_t J & = - j J\\ ,\\\\\n\\end{eqnarray}\\] produces, as we will see, a never-ending cycle of alternating love/hate.\nThe two dynamical functions in the previous examples, \\(-r R\\) and \\(-j J\\) are low-order polynomials. A sensible human being might suggest that the constant term in the polynomials should be added, but the cold, analytic mind of the mathematician would see that this would only change the location of the fixed point and not its stability. That suggests that the next more complicated model to consider involves includes both variables in the dynamical function. Like this:\n\\[\\begin{eqnarray}\n\\partial_t R & = a R + b J\\\\\n\\partial_t J & = c R + d J\\ ,\\\\\n\\end{eqnarray}\\]\nwhere the coefficients \\(a, b, c, d\\) might be either positive or negative depending on the personality of the lovers. Analysis of this model will have to await the introduction of new mathematical tools in Chapter 46.\nWhether Strogatz’s love model is realistic or not, it does illustrate a basic idea of model building: start with simple dynamical functions, check out their consequences, then modify the dynamical functions. We will do this now, with the modifications being purely mathematical along the lines of including different terms in low-order polynomial approximations and considering positive and negative coefficients. As you will see, the simple models correspond to a surprisingly wide range of behaviors.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Modeling dynamics</span>"
    ]
  },
  {
    "objectID": "Dynamics/B6-modeling.html#classical-phase-plane-models",
    "href": "Dynamics/B6-modeling.html#classical-phase-plane-models",
    "title": "44  Modeling dynamics",
    "section": "44.3 Classical phase-plane models",
    "text": "44.3 Classical phase-plane models\nWe have been using the term state space to refer to the set of possible values for the state variables. When there is only one state variable, the state space corresponds to the number line, or perhaps just the positive half of the number line. When there are two state variables, the state space corresponds to the coordinate plane; any point in the plane is a legitimate state for the system.\nHistorically, another term is used for the two-variable state space: the phase plane. This is just a matter of terminology, but it is so prevalent that you will occasionally see it mentioned. We don’t use it since dynamical systems can have a state space that is 1, 2, 3, or higher dimensional, but the phrase “phase plane” only works for 2-dimensional state spaces.\nIn this section, we will look at some famous models that involve two state variables. Out of respect for history, we will call these “classical phase plane” models, but this is entirely equivalent to saying “classical dynamical models with two state variables.”\nOur purpose in studying these classical models is two-fold: to show how simple models can make it easier to draw out the consequences of the mechanisms that we think are at work in real-world systems; and to show you how modifications to purely linear models can produce dynamics that are realistic even away from fixed points.\nTo start, let’s return to the Rabbit-Fox dynamics models. Classically this is called the predator-prey model. It is also called the “Lotka-Volterra” model in honor of it is inventors: American biophysicist Alfred Lotka (1880-1949) and Italian mathematical physicist Vito Volterra (1860-1940).\nThe two first-order differential equations in the Lotka-Volterra model are \\[\\begin{eqnarray}\n\\partial_t R & = \\alpha R - \\beta F R\\\\\n\\partial_t F & = \\delta F R - \\gamma F\\ ,\\\\\n\\end{eqnarray}\\] each of which contains a linear term (\\(\\alpha R\\) in the \\(R\\) equation, \\(\\gamma F\\) in the F equation). Each equation also contains an interaction term. Here, the mathematical/statistical name for the product of two quantities corresponds nicely with the physical reality that the terms describe what happens to rabbits when they interact with a fox, and similarly what happens to foxes. The parameters \\(\\alpha, \\beta, \\gamma\\), and \\(\\delta\\) can, mathematically, be either positive or negative, but they make sense in terms of rabbits and foxes only if all of them are positive. So the “interaction” is always negative for the rabbits and positive for the foxes.\nRewriting the model provides a bit of insight: \\[\\begin{eqnarray}\n\\partial_t R &  = \\underbrace{(\\alpha - \\beta F)}_{k_R}\\ R\\\\\n\\partial_t F & = \\ \\underbrace{(-\\gamma + \\delta R)}_{k_F}\\ F\\ . \\\\\n\\end{eqnarray}\\] Think about the \\(k_R\\) and \\(k_F\\) terms as the reproduction rates. If the fox population were constant, then the rabbit dynamics would be exponential growth or decay, depending on the sign of \\(k_R = \\alpha - \\beta F\\). Similarly, if the rabbit population were constant, the fox dynamics would be exponential decay or growth, depending on the sign of \\(k_F = -\\gamma + \\delta R\\).\nThe two equations are coupled so that the rabbit population alternating between growth and decay leads the fox population to so alternate, and vice versa.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Modeling dynamics</span>"
    ]
  },
  {
    "objectID": "Dynamics/B6-modeling.html#epidemic",
    "href": "Dynamics/B6-modeling.html#epidemic",
    "title": "44  Modeling dynamics",
    "section": "44.4 Epidemic",
    "text": "44.4 Epidemic\nIn a communicable disease, such as COVID-19, the infectious agent is transmitted from an infective person to another person who is susceptible. The time course of an epidemic can be modeled simply with two state variables. We will let \\(S(t)\\) be the number of susceptible people and \\(I(t)\\) the number of infective people at any time \\(t\\).\nThe dynamics of the \\(S\\) variable can be simple: \\(S\\) changes when an infective person meets (interacts with!) a susceptible person. That susceptible person, with some probability, becomes infective. So \\[\\partial_t S = -\\beta S I\\ .\\] The dynamics of the \\(I\\) variable are a just a little more complicated. First, every person who is converted from susceptible to infective becomes a new infective. This is the \\(\\beta S I\\) term in the following differential equation. Second, infectives gradually recover. This is often modeled as a simple \\(-\\alpha I\\) term.\n\\[\\partial_t I = \\beta S I - \\alpha I\\] This is famously called the SIR model, standing for the susceptible, infective, recovered chain of events.\nIn the model, recovery means “no longer able to infect.” Thus, a person who has been isolated is considered “recovered,” whether or not they display symptoms of the disease.\nWe usually think of recovery in terms of a time span, for instance taking a week to recover. But \\(\\alpha I\\) does not work this way. To see why, consider the situation starting on the day that there are no more infective people, that is, \\(S=0\\). The dynamics from this day forward are simplified: \\(\\partial_t I = =\\alpha I\\).\nOf course, the solution to this simplified differential equation is \\(I(t) = I_0 e^{-\\alpha t}\\); the size of the infective group gets smaller exponentially. Figure 44.2 compares what \\(I(t)\\) would look like if it takes, say, one week to recover and what \\(I(t)\\) looks like under the model’s \\(\\partial_t I = =\\alpha I\\) dynamics.\n\n\n\n\n\n\n\n\nFigure 44.2: Comparing \\(I(t)\\) for the “takes a week to recover” model and the \\(\\partial_t I = -\\alpha I\\) model.\n\n\n\n\n\nTo show the dynamics of the SIR model, we need to propose numerical values for \\(\\alpha\\) and \\(\\beta\\). This is not a trivial matter if the goal is to match the dynamics to a particular disease and size of population. For our purposes here, we will imagine that \\(S(0) = 0.999\\) and \\(I(0) = 0.001\\), which is to say we are looking at the proportion of the population that is susceptible or infective.\nFigure 44.3 shows the flow field for \\(\\beta = 1/2\\) and \\(\\alpha = 1/7\\).\n\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\nFigure 44.3: SIR flow field for \\(\\beta=1/2, \\alpha = 1/7\\), and the trajectory for initial conditions \\(S_0 = 0.999, I_0 = 0.001\\).",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Modeling dynamics</span>"
    ]
  },
  {
    "objectID": "Dynamics/B6-second-order.html",
    "href": "Dynamics/B6-second-order.html",
    "title": "47  Force-balance equations",
    "section": "",
    "text": "47.1 Ballistics\nA lot of the theory of second-order differential equations was developed in the setting of a ball being set off with an initial velocity from an initial position. Such a focus on the flight of balls might seem trivial. Fortunately, language allows us to construct a scientific-sounding word by adding the suffix “istic” to the root “ball.” This suffixing produces the word ballistics.\nThe importance of ballistics to Newton can be seen by a famous diagram he drew, shown in Figure 47.1. In the diagram, Newton traces the path of a ball shot horizontally from a cannon placed at the top of a mountain.\nFigure 47.1: Newton’s diagram showing ballistic motion under the force of gravity.\nSince the motion in Newton’s diagram has both vertical and horizontal components, we will need two second-order differential equations:\n\\[\\text{Horizontal}: \\ \\ \\partial_{tt} x = 0\\\\\n\\ \\ \\ \\text{Vertical}: \\ \\ \\ \\ \\ \\ \\partial_{tt} y = -g\\] The zero on the right-hand side of the equation of horizontal movement reflects that gravity does not act horizontally.\nWe found a solution for the vertical equation in the previous section, \\[y(t) = -\\frac{1}{2} g\\,t^2 + 0\\,t + y_0\\ .\\] The \\(0\\, t\\) component to the solution reflects that the vertical component of the ball, coming out of the cannon, is zero.\nThe solution for the horizontal component of motion can be found by anti-differentiating both sides of the equation of hortizontal motion: \\[\\int \\partial_{tt} x(t)\\, dt = \\partial_t x(t) = \\int 0\\, dt = v_0\\] where \\(v_0\\) is the initial horizontal velocity. A second stage of anti-differentiation gives \\(x(t)\\) itself: \\[\\int \\partial_t x(t) = \\int v_0 dt = v_0\\, t + x_0\\]\nRegrettably, symbolic anti-differentiation works only in simple cases. To support more realistic models of ballistics, let’s see how to translate the two second-order differential equations into sets of first-order equations. The state variables will be \\(x(t)\\) and \\(y(t)\\), but we also have to add another pair, \\(u(t)\\) and \\(v(t)\\) standing for the horizontal and vertical velocities respectively. The first-order equations will be: \\[\\partial_t x = u\\\\\n\\partial_t y = v\\\\\n\\partial_t u = 0\\\\\n\\partial_t v = -g\n\\] To illustrate, we will solve this set of four first-order equations numerically. We need to specify the initial values for \\(x_0\\), \\(y_0\\), \\(u_0\\) and \\(v_0\\). We will let the cannon be located at horizontal position \\(x_0 = 0\\) and vertical position \\(y_0 = 100\\) meters. The vertical velocity is, initially, zero, so \\(v_0 = 0\\). And suppose the cannon produces an initial horizontal velocity of \\(u_0 = 250\\) meters/sec. The constant \\(g\\) is known to be 9.8 meters/sec2.\nHere’s the trajectory:\ntraj &lt;- integrateODE(\n  dx ~ u, dy ~ v, du ~ 0, dv ~ -9.8, #dynamics\n  x=0, y=100, u = 250, v=0, #initial conditions\n  bounds(t=0:5)\n) \n# traj_plot(y(t) ~ x(t), traj)\n# traj_plot(v(t) ~ u(t), traj)\nFigure 47.2: Trajectory of the cannon ball shot with an initial horizontal velocity and no initial vertical velocity. The trajectory is plotted in slices of state space: position \\((x, y)\\) and velocity \\((u, v)\\). The time at which the ball reaches the points marked on the trajectory give the time.\nThe left panel in Figure 47.2 shows that the trajectory is a parabola. At about \\(t=4.4\\) secs the \\(y\\) position is zero. Since zero is the location of the ground, the part of the trajectory for \\(4.4 &lt; t\\) is invalid, since the ball has already hit the ground. The ball travels a little more than 1100 meters horizontally before hitting the ground.\nThe right panel might seem somewhat strange. You can see that the vertical component of velocity, \\(v(t)\\) starts out at zero and increases linearly with time, becoming more and more negative as gravity continuous to accelerate the ball downward. The vertical velocity, \\(u(t)\\), stays constant at \\(u(t) = 250\\) meters per second. This is because there is no horizontal force on the ball.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Force-balance equations</span>"
    ]
  },
  {
    "objectID": "Dynamics/B6-second-order.html#sec-ballistics",
    "href": "Dynamics/B6-second-order.html#sec-ballistics",
    "title": "47  Force-balance equations",
    "section": "",
    "text": "Calculus history—Computing trajectories\n\n\n\nThe world’s first programmable, electronic, general-purpose digital computer was started up in 1945 at the University of Pennsylvania, where it is still on display. The date and location have something to say about why the computer was built. 1945 is, of course, at the end of World War II. The computer was built to carry out some important war-time calculations. The place, Philadelpha, Pennsylvania, has to do with the location of the US Army’s center for developing and testing ordnance: the Aberdeen Proving Ground which is only 75 miles from the University of Pennsylvania.\nThe name given to the computer, ENIAC, has a science-fiction flavor but is in fact rooted in its purpose: the Electronic Numerical Integrator and Computer. ENIAC was constructed to calculate the trajectories of artillery shells. Knowing the trajectory is essential to being able to fire artillery accurately.\nThe ballistics of real world artillery shells is more complex than the simple model we constructed earlier. What’s missing from that model is air resistance, which is a function of the shell’s velocity and altitude. To illustrate, let’s add in a simple model of air resistance to the earlier ballistic model. In this model, the force of air resistence is a vector pointing in the opposite direction to overall velocity and proportional to velocity squared.\nThe velocity vector is simply \\(\\left[\\begin{array}{c}u\\\\v\\end{array}\\right]\\). The air resistence force will be \\[-\\alpha\\sqrt{\\strut u^2 + v^2} \\left[\\begin{array}{c}u\\\\v\\end{array}\\right]\\ .\\] Consequently, the horizontal component of the air-resistence vector is \\(-\\alpha\\, u \\sqrt{\\strut u^2 + v^2}\\) and the vertical component is \\(-\\alpha\\, v \\sqrt{\\strut u^2 + v^2}\\).\nRepresenting air resistence by the function \\(r(u, v)  \\equiv \\alpha \\sqrt{\\strut u^2 + v^2}\\), the dynamics are \\[\\begin{eqnarray}\n\\partial_t x & = u\\\\\n\\partial_t y & = v\\\\\n\\partial_t u & = -u\\ r(u,v)\\\\\n\\partial_t v &= -g - v\\ r(u,v)\\\\\n\\end{eqnarray}\\] integrateODE() carries out the calculation in R/mosaic.\n\nr &lt;- makeFun(alpha*sqrt(u^2 + v^2) ~ u & v, alpha=0.003)\ntraj2 &lt;- integrateODE(\n  dx ~ u, dy ~ v, du ~ -u*r(u,v), dv ~ -9.8 - v*r(u,v), #dynamics\n  x=0, y=100, u = 250, v=0, #initial conditions\n  bounds(t=0:6)\n)\n# traj_plot(y(t) ~ x(t), traj2)\n# traj_plot(v(t) ~ u(t), traj2)\n\n\n\n\n\n\n\n\n\nFigure 47.3: Adding air resistence to the model changes the trajectory. For reference, the trajectory without air resistence is plotted in \\(\\color{orange}{\\text{orange}}\\).\n\n\n\n\n\nAir resistance causes the cannon ball to travel a shorter horizontal distance before hitting the ground and to arrive with a much reduced velocity.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Force-balance equations</span>"
    ]
  },
  {
    "objectID": "Dynamics/B6-second-order.html#the-harmonic-oscillator",
    "href": "Dynamics/B6-second-order.html#the-harmonic-oscillator",
    "title": "47  Force-balance equations",
    "section": "47.2 The harmonic oscillator",
    "text": "47.2 The harmonic oscillator\nConsider the motion of a weight attached to a spring, as in ?fig-spring-mass. We will denote the vertical position of the mass by \\(y(t)\\). Such a spring-mass system has a fixed point where the spring is stretched just enough to cancel out gravity and the velocity is zero. We will measure \\(y\\) relative to this fixed point.\n::: {.cell .column-margin layout-align=“center” fig.cap = “A spring-mass system in motion. Source=’Svjo CC BY-SA via Wikimedia Commons”’} ::: {.cell-output-display}  ::: :::\nAccording to Hooke’s Law, a stretched or compressed spring exerts a force that is proportional to the amount of extension or compression. With our measuring \\(y\\) relative to the fixed point, the Hooke’s Law force will be \\[m\\, \\partial_{tt} y = - s\\, y\\ ,\\] where \\(m\\) is the amount of mass and \\(s\\) is the stiffness of the spring. This force-balance equation corresponds to the second-order differential equation \\[\\partial_{tt} y = - \\frac{s}{m} y\\ .\\]\nYou can see that the motion is oscillatory, which suggests that the solution to the differential equation will be of the form \\(y(t) = A \\sin(\\omega t)\\). Taking this as an ansatz leads to finding a value of \\(\\omega\\), which is called the angular frequency of the oscillation. (In terms of the period of oscillation \\(P\\), the angular frequency is \\(\\omega = 2 \\pi/P\\).)\nTo find \\(\\omega\\), plug in the ansatz to the differential equation:\n\\[\\partial_{tt} A \\sin(\\omega t) = - \\frac{s}{m}\\, A \\sin(\\omega t)\\] Differentiating \\(\\sin(\\omega t)\\) once let’s us re-write the left-hand side of the equation in terms of a first derivative\n\\[\\partial_{t} A \\omega\\, \\cos(\\omega t) = - \\frac{s}{m}\\, A \\sin(\\omega t)\\] Differentiating again gives \\[- \\omega^2 A\\sin(\\omega\\, t) = - \\frac{s}{m}\\, A\\sin(\\omega t)\\ .\\] Simplifying this by cancelling out the \\(A \\sin(\\omega t)\\) term gives \\(\\omega^2 = \\frac{s}{m}\\), where \\(\\omega\\) is the angular frequency of the oscillation.\n\nInstead of using \\(A \\sin(\\omega t)\\) as the ansatz we could have used \\(A \\sin(\\omega t) + B \\cos(\\omega t)\\). Working through this ansatz would produce the same result, that \\(\\omega^2 = \\frac{s}{m}\\). So the solution to the spring-mass system will be, in general, a linear combination of the sine and the cosine functions with angular frequency \\(\\omega\\).",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Force-balance equations</span>"
    ]
  },
  {
    "objectID": "Dynamics/B6-second-order.html#exponential-or-sinusoid",
    "href": "Dynamics/B6-second-order.html#exponential-or-sinusoid",
    "title": "47  Force-balance equations",
    "section": "47.3 Exponential or sinusoid?",
    "text": "47.3 Exponential or sinusoid?\nIn Chapter 46, we established that solutions to second-order linear differential equations have the form \\(m_1 e^{\\lambda_1 t} + m_2 e^{\\lambda_2 t}\\). Yet in the previous section, we saw one linear second-order differential equation, \\(\\partial_{tt} y = - \\omega^2 y\\) where the solution is a linear combination of a sine and a cosine function: \\(y(t) = A \\sin(\\omega t) + B \\cos(\\omega t)\\) with \\(\\omega = \\sqrt{\\frac{s}{m}}\\).\nHow is it possible for the solution to be both in the form of a linear combination of exponentials and a linear combination of sine and cosine? Sinusoids oscillate up and down and up and down, whereas exponentials are monotonic.\nTo find out what might be the relationship between an exponential and a sinusoid, let’s plug an exponential ansatz \\(y(t) = A e^{\\lambda t}\\) into the spring-mass system \\(\\partial_{tt} y = -\\omega^2 y\\).\n\\[\\partial_{tt} A e^{\\lambda t} = \\lambda^2 A e^{\\lambda t} = -\\omega^2 A e^{\\lambda t}\\ .\\] As before, we will cancel out the common term \\(A e^{\\lambda t}\\) to get a simple relationship: \\[\\lambda^2 = -\\omega^2\\ \\ \\ \\implies\\ \\ \\ \\lambda = \\pm \\sqrt{\\strut-1}\\  \\omega \\ .\\] Generally, the symbol \\(i\\) is used to stand for \\(\\sqrt{\\strut -1}\\), so our eigenvalues can be written \\(\\lambda = \\pm i \\omega\\). The solution to the spring-mass system, according to this analysis, is: \\[y(t) = m_1 e^{i\\omega t} + m_2 e^{-i \\omega t}\\]\nIn other words, \\(e^{i \\omega t}\\)—notice the \\(i\\) in the argument to the exponential—is a sinusoid with angular frequency \\(\\omega\\).",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Force-balance equations</span>"
    ]
  },
  {
    "objectID": "Dynamics/B6-second-order.html#exponentials-with-imaginary-inputs",
    "href": "Dynamics/B6-second-order.html#exponentials-with-imaginary-inputs",
    "title": "47  Force-balance equations",
    "section": "47.4 Exponentials with “imaginary” inputs",
    "text": "47.4 Exponentials with “imaginary” inputs\nThe “imaginary” in the section title is used in its mathematical sense. In interpreting the word “imaginary,” you should keep in mind a long history in mathematics of assigning insulting names to mathematical objects that, at the time they were first introduced. That is why some numbers are vilified as “negative,” and some as “irrational.” The insult is even more dire for numbers like \\(i\\), which are called the “imaginary” numbers. Regrettably, the word “imaginary” leads many people to shy away from them, just as many people avoid genres such as fantasy fiction. That imaginary numbers are introduced as kind of freakish—is there a numerical value for \\(\\sqrt{\\strut -1}\\)?—and rarely touched until advanced calculus, means that students are unused to them.\nYou will only get comfortable with “imaginary” numbers when you start to work with them extensively, as happens in physics and engineering courses. Our goal here is merely to increase your awareness of imaginary numbers and some of the ways they are used in the sciences. To that end, we offer three different approaches to understanding the function \\(e^{i\\omega t}\\).\n\nBasic, pragmatic understanding. This is the level of understanding that you must have to make sense of the rest of this chapter and ?sec-forcing. Here it is: \\[e^{i\\omega t}\\ \\text{is simply a shorthand for}\\ \\cos(\\omega t).\\] So whenever you see \\(e^{i \\omega t}\\), think of \\(\\cos(\\omega t)\\).\nAlgebraic understanding via Taylor Polynomials. (optional) This level of understanding can give you confidence that the basic, pragmatic understanding in (1) has honest roots. It also shows the way that (1) is not 100% on target (although good enough for a large fraction of mathematical work). But for many people, algebra is a rocky road to understanding.\n\nThe starting point for the algebraic understanding is the Taylor polynomial approximation for \\(e^{\\omega t}\\). Recall from Chapter 26 that \\[e^{\\omega t} = 1 + \\omega t + \\frac{1}{2!}\\omega^2 t^2 + \\frac{1}{3!}\\omega^3 t^3 + \\frac{1}{4!} \\omega^4 t^4 + \\frac{1}{5!} \\omega^5 t^5 + \\frac{1}{6!} \\omega^6 t^6 + \\cdots\\] You may also recall the Taylor polynomial expansion of sine and cosine: \\[ \\cos(\\omega t) = 1 - \\frac{1}{2!} \\omega^2 t^2 + \\frac{1}{4!}\\omega^4 t^4 - \\frac{1}{6!} \\omega^6 t^6 + \\cdots\\]\n\\[\\color{magenta}{\\sin(\\omega t) = \\omega t - \\frac{1}{3!}\\omega^3 t^3 + \\frac{1}{5!} \\omega^5 t^5 +  \\cdots}\\] You can see some association between \\(e^{wt}\\), \\(\\cos(\\omega t)\\), and \\(\\sin{\\omega t}\\) by looking at \\[\\cos(\\omega t) + \\color{magenta}{i \\sin(\\omega t)} = 1 + \\color{magenta}{i \\omega t} -\\frac{1}{2!} \\omega^2 t^2 - \\color{magenta}{i \\frac{1}{3!} \\omega^3 t^3} + \\frac{1}{4!}\\omega^4 t^4 + \\color{magenta}{i \\frac{1}{5!} \\omega^5 t^5} - \\frac{1}{6!}\\omega^6 t^6 + \\cdots\\] Now consider the Taylor polynomial for \\(e^{i\\omega t}\\). This will be the same as the Taylor polynomial for \\(e^{\\omega t}\\) but everywhere substituting \\(i \\omega\\) in place of the plain \\(\\omega\\). That is:\n\\[e^{i \\omega t} = 1 + \\color{magenta}{i\\omega t} + \\frac{1}{2!}i^2\\omega^2 t^2 + \\color{magenta}{\\frac{1}{3!}i^3\\omega^3 t^3} + \\frac{1}{4!} i^4\\omega^4 t^4 + \\color{magenta}{\\frac{1}{5!} i^5\\omega^5 t^5} + \\frac{1}{6!} i^6\\omega^6 t^6 + \\cdots\\] Since \\(i\\equiv \\sqrt{\\strut -1}\\), we have the following facts for the powers \\(i^n\\):\n\\[i^2 = -1\\ \\ \\ \\ \\ \\color{magenta}{i^3 = -i}\\ \\ \\ \\ \\ i^4 = 1\\ \\ \\ \\ \\ \\color{magenta}{i^5 = i}\\ \\ \\ \\ \\ i^6 = -1\\ \\ \\text{and so on}.\\] Substitute these facts about \\(i^n\\) into the Taylor polynomial for \\(e^{i\\omega t}\\):\n\\[e^{i \\omega t} = 1 + \\color{magenta}{i\\omega t} - \\frac{1}{2!}\\omega^2 t^2 - \\color{magenta}{i \\frac{1}{3!}\\omega^3 t^3} + \\frac{1}{4!} \\omega^4 t^4 + \\color{magenta}{i \\frac{1}{5!} \\omega^5 t^5} - \\frac{1}{6!} \\omega^6 t^6 + \\cdots\\] which exactly matches the Taylor polynomial for \\(\\cos{\\omega t} + \\color{magenta}{i \\sin(\\omega t)}\\).\n\nThe arithmetic of complex numbers. (optional) A complex number is a number like \\(2 - 3i\\) which consists of two parts: the real-part \\(2\\) and the imaginary part \\(-3\\). When you multiply one complex number by another you get a complex number (although either the real or imaginary parts might happen to be zero.) For example: \\[(2 + 3i)^2 = (2+3i)(2+3i) = \\underbrace{4}_{2\\times 2} + \\underbrace{ \\ 6 i\\ }_{2 (3i)} +   \\underbrace{\\ 6 i\\ }_{(3i)2}\\ \\  \\underbrace{- 9}_{(3i)(3i)}\\  = -5 +12 i.\\] R knows the rules for arithmetic on complex numbers. Here’s a demonstration of the oscillations that result from raising a complex number to successive powers.\n\n\nlambda &lt;- 0.65 + 0.76i\nlambda^2\n## [1] -0.1551+0.988i\nlambda^3\n## [1] -0.851695+0.524324i\nlambda^4\n## [1] -0.952088-0.3064776i\nlambda^5\n## [1] -0.3859342-0.9227973i\nlambda^6\n## [1] 0.4504687-0.8931283i\nlambda^7\n## [1] 0.9715821-0.2381771i\nlambda^8\n## [1] 0.812543+0.5835873i\nlambda^9\n## [1] 0.0846266+0.9968644i\nlambda^10\n## [1] -0.7026097+0.7122781i\n\nNotice that the real part of the result oscillates between negative and positive. The imaginary part also oscillates, but delayed a bit from the real part. Just like sine and cosine.\nWe can get a clearer picture by plotting \\(e^{i\\omega t}\\) over the domain \\(0 &lt; t &lt; 10\\). As an example, in ?fig-complex-exponential-plot we will set \\(\\omega = 2\\). We need to be a little careful, since our plotting functions are not arranged to display complex numbers. But there is an easy workaround: plot the “real” and “imaginary” parts separately. The R operators Re() and Im() do this work.\n\nf &lt;- makeFun(exp(1i * omega * t) ~ t, omega = 2)\nslice_plot(Re(f(t)) ~ t, \n           bounds(t=0:10), color = \"magenta\") %&gt;%\n  slice_plot(Im(f(t)) ~ t, color=\"brown\")\n\n\n\n\nThe real and imaginary parts of \\(e^{i \\omega t}\\) plotted as a function of \\(t\\).",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Force-balance equations</span>"
    ]
  },
  {
    "objectID": "Dynamics/B6-second-order.html#damping",
    "href": "Dynamics/B6-second-order.html#damping",
    "title": "47  Force-balance equations",
    "section": "47.5 Damping",
    "text": "47.5 Damping\nIt is common for there to be friction, called damping, in a spring mass system. To keep things very simple, we will consider that the friction is proportional to the velocity and, as in the cannonball example, in the direction opposite to velocity. That is: \\[\\partial_{tt} y = -r\\, \\partial_t y -b y\\ ,\\] where \\(b\\) would be the positive number \\(\\frac{s}{m}\\) and \\(r\\) is another positive number reflecting the magnitude of friction. (Think of \\(r\\) as standing for “resistance.”)\nAs always, this second-order differential equation can be written as a pair of first-order differential equations. One of the first-order differential equations will be \\[\\partial_t y = v\\ ,\\], which is just the definition of velocity \\(v\\). The other first-order equation will be \\[\\partial_t v = -r v  - b y\\ .\\] Both equations are linear.\nIn the previous chapter, we wrote such a pair of linear first-order differential equations in terms of a vector \\[\\vec{w(t)} = \\left[\\begin{array}{c}v(t)\\\\y(t)\\end{array}\\right]\\ .\\] In terms of the vector \\(\\vec{w(t)}\\) the dynamics can be written in vector/matrix form: \\[\\partial_t \\vec{w} = \\left[\\begin{array}{c}-r \\ \\ \\  -b\\ \\ \\\\1 \\ \\ \\ \\ \\ \\ \\ 0\\end{array}\\right]\\, \\vec{w}\\ .\\] This form suggests, at least to the avid reader of the previous chapter, that we look for a solution \\(y(t) = m_1\\, e^{\\lambda_1\\, t} + m_2\\, e^{\\lambda_2\\, t}\\) in terms of the eigenvectors and eigenvalues of the matrix \\(\\left[\\begin{array}{cc}r & b\\\\1 & 0\\end{array}\\right]\\).\nWe used the R function eigen() to compute the eigenvalues and eigenvectors of the matrix, given numerical values for \\(r\\) and \\(b\\). Let’s now try to find an algebraic formula for the eigenvalues. After all, it is the eigenvalues that determine the stability of the fixed point.\nAs an ansatz for the for the original second-order differential equation \\[\\partial_{tt} y = r\\, \\partial_t y + b y\\ ,\\] let’s use \\(y(t) = A e^{\\lambda t}\\), a simple exponential function. Plugging in the ansatz to the differential equation gives: \\[A \\lambda^2 e^{\\lambda t} = - r A \\lambda e^{\\lambda t} - b A e^{\\lambda t}\\ .\\] We can cancel out the common term \\(A e^{\\lambda t}\\) from all the terms in the equation, and bring all the terms to the left-hand side of the equation, leaving us with \\[\\lambda^2 + r \\lambda + b = 0\\ .\\] This is a quadratic polynomial in \\(\\lambda\\), so we can use the “quadratic formula” to find values for \\(\\lambda\\) that are consistent with the parameters \\(a\\) and \\(b\\). In applying the quadratic formula you have to remember that the standard statement is for the roots of \\(a x^2 + b x + c = 0\\) and make adjustment for the fact that our polynomial uses the parameter names differently: \\(\\lambda^2 + r \\lambda + b = 0\\).\n\\[\\lambda = \\frac{- r \\pm \\sqrt{\\strut r^2 - 4 b}}{2}\\ .\\] Recall that the parameter \\(r\\) describes the amount of friction or resistence in the system; it is a positive number. Similarly, the nature of springs is that \\(b\\) is a positive number. The relative values of \\(r\\) and \\(b\\) determine the motion of the system.\nSuppose the stiffness of the spring is much larger than the friction. Then \\(r^2 &lt; 4b\\). This being the case, the \\(\\sqrt{\\strut r^2 - 4 b}/2\\) will be an imaginary number. Altogether, the eigenvalues will be \\(\\lambda = -\\frac{r}{2} \\pm {i \\omega}\\). The solution will be \\[y = m_1 e^{\\lambda_1 t} + m_2 e^{\\lambda_2 t} \\\\\n= m_1 e^{-\\frac{r}{2}t + i \\omega t} + m_2 e^{\\frac{r}{2} - i\\omega t} \\\\\n= m_1 e^{-r t/2} e^{i\\omega t} + m_2 e^{-r t/2} e^{-i \\omega t} \\\\\n= e^{-r t/2}\\underbrace{\\left[m_1 e^{i \\omega t} + m2 e^{i\\omega t}\\right]}_{\\text{sinusoid}(\\omega t)}\\] Result: an exponentially decaying sinusoid.\nTo graph this function, we need to choose appropriate numerical values for \\(r\\) and \\(b\\). Let’s set \\(r=1\\). Since \\(r^2 &lt; 4b\\), we must have \\(\\frac{1}{4} &lt; b\\): we will choose \\(b = 6\\) which meets this criterion. Figure 47.4 shows the solution to the differential equation:\n\ntraj &lt;- integrateODE(dv~ -r*v - b*y, dy ~ v, \n                     v=10, y=0, r=1, b=6, \n                     bounds(t=0:20))\n## Solution containing functions v(t), y(t).\ntraj_plot(y(t) ~ t, traj)\n\n\n\n\n\n\n\nFigure 47.4: An exponentially decaying sinusoid arising from \\(r = 1\\) and \\(b = 6\\).\n\n\n\n\n\nThis is the situation with a swinging door. You shove it to swing open, after which it oscillates with a decreasing amplitude.\nIn contrast, suppose the spring is weak compared to the damping such that \\(4b &lt; r^2\\). Now \\(\\sqrt{\\strut r^2 - 4b}\\) is a positive number, not imaginary. What’s more, since \\(b\\) is positive, \\(\\sqrt{\\strut r^2 - 4 b} &lt; r\\). This means that both eigenvalues are negative. We will illustrate the situation with \\(r=1, b=0.2\\):\n\ntraj2 &lt;- integrateODE(dv~ -r*v - b*y, dy ~ v, \n                      v=10, y=0, r=1, b=0.1, \n                      bounds(t=0:20))\n## Solution containing functions v(t), y(t).\ntraj_plot(y(t) ~ t, traj2) %&gt;%\n  gf_lims(y = c(0, NA))\n\n\n\n\n\n\n\nFigure 47.5: A heavily damped spring-mass system with \\(r = 1\\) and \\(b = 0.1\\).\n\n\n\n\n\nThe situation in Figure 47.5 is the sort of behavior one expects when giving a shove to an exit door in theater or stadium. The shove causes the door to swing open, after which it slowly returns to the closed position. That gives plenty of time for the people following you to get to the door before it closes.\nFinally, consider the case where \\(r^2 - 4 b = 0\\), a balance between resistance and springiness. In this case, both eigenvalues are \\(\\lambda = -r/2\\).\n\ntraj3 &lt;- integrateODE(dv~ -r*v - b*y, dy ~ v, v=10, y=0, r=1, b=0.25, bounds(t=0:20))\n## Solution containing functions v(t), y(t).\ntraj_plot(y(t) ~ t, traj3) %&gt;%\n  gf_lims(y = c(0, NA))\n\n\n\n\n\n\n\nFigure 47.6: A critically damped oscillation with \\(r=1\\), \\(b=0.25\\).\n\n\n\n\n\nThis is a situation called critically damped. The door swings open, then closes as fast as it can without any oscillation.\n\n\\(\\ \\) Consider the second-order linear differential equation \\[\\partial_{tt}\\ y + 2\\, \\partial_t\\, y - 3\\, y = 0\\ .\\] Is this system stable?\nFor this system, \\(a=2\\) and \\(b = - 3\\), so the eigenvalues are \\[\\lambda = \\left(-2 \\pm \\sqrt{\\strut 4 + 12}\\right)/2 = 1 \\pm \\sqrt{\\strut 16}/2 = -1 \\pm 2\\] In other words, \\(\\lambda_1 = -3\\) and \\(\\lambda_2 = +1\\). This indicates that the system is a saddle: unstable in one direction and stable in the other.\nTo confirm our work, let’s use eigen() to find the eigenvalues of the matrix \\(\\left[\\begin{array}{cc}2 & 3\\\\1 & 0\\end{array}\\right]\\):\n\nM &lt;- cbind(rbind(-2,1), rbind(3,0))\neigen(M)\n## eigen() decomposition\n## $values\n## [1] -3  1\n## \n## $vectors\n##            [,1]       [,2]\n## [1,] -0.9486833 -0.7071068\n## [2,]  0.3162278 -0.7071068\n\nAlthough R is doing all the calculations for us, it is possible to write the directions of the eigenvectors only in terms of the eigenvectors: \\[\\vec{\\Lambda_1} = \\left[\\begin{array}{c}\\lambda_1\\\\1\\end{array}\\right]\\ \\ \\text{and}\\ \\ \\vec{\\Lambda_2} = \\left[\\begin{array}{c}\\lambda_2\\\\1\\end{array}\\right]\\]\nFor the system with \\(\\lambda_1 = 3\\) and \\(\\lambda_2 = -1\\), you can confirm that the eigenvectors calculated with this formula point in the same directions as the eigenvectors reported by eigen().\n\nLet’s return to the car-following control system introduced in Chapter 46. Recall that \\(x\\) was defined to be the distance between two cars and \\(x_0\\) the distance to be maintained. In terms of \\(y = x - x_0\\) the system was \\[\\partial_{tt} y = - b y\\ .\\] You can see that this system has no damping; \\(y(t)\\) will be a sinusoidal oscillation. The ride will be more pleasant, however, if the oscillations can be damped out. To accomplish this, we should add a new term to the second-order differential equation, a damping term to give \\[\\partial_{tt} y = -a\\, \\partial_t y- b\\, y\\ .\\] We should set the parameters \\(a\\) and \\(b\\) to make the real part of the eigenvalues negative. Only then will we have designed a workable control system.\n\nApplication area 47.1 —Following the car in front using derivatives.\n\n\n\n\n\n\n\nApplication area 47.1 Driving a car\n\n\n\nFor a human driver, following a car at a steady distance requires careful attention but in practice is not too difficult a task. Could it be the case that drivers have an intuitive understanding of the need for damping? Perhaps complex eigenvalues ought to be a standard topic in driving schools? That might be, but there is a more down-to-earth explanation of how humans handle the car-following task.\nThe quantity \\(\\partial_{tt} y\\) is the acceleration, and the control pedal that leads to positive acceleration is called the “accelerator.” But the pedal does not set acceleration. In reality, the pedal sets velocity as well as acceleration. A simple model is \\(\\text{pedal} = r \\partial_t y + s \\partial_{tt} y\\), where \\(y\\) is the velocity of the car and \\(r\\) and \\(s\\) are positive parameters.\nTo understand this model of the pedal, think what happens when you press the accelerator and hold it. The car accelerates, but only up to the point where a steady state velocity is reached. Or, consider what happens if you partially release the pedal. The car slows down until it reaches a new, slower, steady-state velocity.\nWith a human driver, the control system is not \\(\\partial_{tt} y = - b y\\). Instead, the control system is \\[\\text{pedal} - p_0 =  - b y\\ .\\] For steady-state driving at the desired velocity \\(\\partial_t y\\) we press the pedal by an amount \\(p_0\\). To perform the car-following task, we push down or lighten up on the pedal, depending on whether we are farther or closer to the car ahead than our desired distance.\nCombining the models for how \\(\\text{pedal}\\) is controlled and how \\(\\text{pedal}\\) relates to velocity and acceleration, we have \\[r \\partial_t y + s \\partial_{tt} y  - p_0 = -b y\\] or, re-arranging terms \\[ \\partial_{tt} y = \\underbrace{- \\frac{r}{s} \\partial_t y}_{\\text{damping}} - \\frac{b}{s} y + p_0\\ .\\] The nature of the gas pedal itself leads to a damping term in the dynamics, without our having to think about it consciously.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Force-balance equations</span>"
    ]
  },
  {
    "objectID": "Dynamics/B6-second-order.html#footnotes",
    "href": "Dynamics/B6-second-order.html#footnotes",
    "title": "47  Force-balance equations",
    "section": "",
    "text": "it is remarkable that the same \\(m\\) appears both in Newton’s Second Law and in the description of the force of gravity. There was no mathematical theory for this until Albert Einstein (1879-1955)↩︎\nAnother bit of physics which is still not included in the differential equation is that it will only hold until the object hits the ground, at which point the force of gravity will be counter-acted by the force of the ground on the object.↩︎",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Force-balance equations</span>"
    ]
  },
  {
    "objectID": "Manifestations/B4-splines.html",
    "href": "Manifestations/B4-splines.html",
    "title": "49  Data-driven functions",
    "section": "",
    "text": "49.1 Generating smooth motion\nAs a motivating example, consider the programming of robotic arms as in the video:\nSince this isn’t a robots course, we will simplify. The arm has a resting position. When a car frame comes into place, the arm moves so that its welding electrodes are at a specific, known place in space near the car body. Then it moves in sequence to other places where a weld is required, perhaps passing through waypoints to avoid obstacles.\nThe problem of converting the discrete list of weld and waypoints into a continuous signal for the actuator is an instance of a mathematical process called interpolation. In real robot arms, there are multiple joints that need to be controlled simultaneously. For our illustration, we will use a simple setup where the robot hand rolls along a set of rails in the y-direction and another x-rail running crosswise to the y direction.\nThe task for our example robot will be to visit the points shown in Figure 49.1 in order, taking 15 seconds to traverse the whole path.\n## Warning in (nudge_x != 0) || (nudge_y != 0): 'length(x) = 16 &gt; 1' in coercion\n## to 'logical(1)'\n\n\n\n\n\n\n\nFigure 49.1: The waypoints on a path the robot hand is supposed to follow. All the action is taking place in roughly 1x1 meter area.\nFigure 49.1 shows a continuous path in \\((x,y)\\) coordinates together with discrete labels indicating when each waypoint is to be reached. Note that the path is not a function \\(y(x)\\). Mathematical functions are required to be single valued, meaning that for each value of the input (in the function domain) there can be only one, unique output value. The path in Figure 49.1 often involves two or more different \\(y\\) values for a single \\(x\\) value. There is even a small domain of \\(x\\) near \\(x=900\\) where the path at any given \\(x\\) crosses six different \\(y\\)-values.\nEven so, functions can be a useful way of describing the \\((x,y)\\) path. The key is the plural: functions. For the path in Figure 49.1 we need two quantities varying with time in a coordinated way. One approach, familiar to navigators, is to specify direction of movement and velocity at each instant of time. Perhaps not as familiar, but more fundamental mathematically, is to specify \\(x\\) as a function of time and, separately, \\(y\\) as a function of time. Using this formalism, the trajectory of the robot arm will consist of two functions, \\(x(t)\\) and \\(y(t)\\). To build those functions, well start with the waypoints stored in the data frame Robot_stations.\nt\nx\ny\n\n\n\n\n1\n496\n191\n\n\n2\n1037\n138\n\n\n3\n1251\n191\n\n\n... 16 rows in total ...\n\n\n\n\n\n\n\n15\n928\n432\n\n\n16\n737\n240\nThe \\(x(t)\\) and \\(y(t)\\) functions in this table aren’t complete enough to operate the robot. We need to provide the \\(x,y\\)-location data in the form of two continuous functions of \\(t\\) so that the robot, at any time \\(t\\), can look up where it is supposed to be, what its velocity should be, and how that velocity should be changing in time (acceleration).\nOne strategy is to construct the functions as piecewise linear functions of \\(t\\), like this:\nFigure 49.2: Two functions \\(x(t)\\) and \\(y(t)\\) which describe the path shown in Figure 49.1.\nIt can be difficult at first glance to see the relationship between the \\(x(t)\\) and \\(y(t)\\) functions and the path shown in Figure 49.1. As an exercise, look specifically at the segment \\(9 \\leq t \\leq 10\\). In Figure 49.2, this is the segment connecting points 9 and 10. In the path view, you can see that on this segment \\(x\\) changes a lot while \\(y\\) changes only a little. Correspondingly, in the function view, \\(\\partial_t x(t)\\) is large in magnitude compared to \\(\\partial_t y(t)\\).\nEach functions in Figure 49.2 is an interpolating function. You’re entitled to think of the \\(x(t)\\) function as connecting with lines the sequence of \\(x\\) versus \\(t\\) coordinates from the table and similarly for \\(y(t)\\). Each of the two functions is continuous. But, based on your work in Blocks 1 through 3, you have a richer set of concepts for interpreting those two functions.\nFor instance, let’s look at \\(\\partial_t y(t)\\). Since \\(y\\) is a position along the cross rail, \\(\\partial_t y(t)\\) is the velocity in that direction. Figure 49.3 shows the velocity versus time for both the \\(x\\) and \\(y\\) components of the movement.\nFigure 49.3: Velocity versus time time along the path defined by \\(x(t)\\) and \\(y(t)\\) as shown in Figure Figure 49.2.\nThe speed of the robot arm maxes out at about 600 mm-per-second. You can get a sense for this by moving your finger two feet in 1 second: a normal human speed of movement.\nSince the original \\(x(t)\\) and \\(y(t)\\) functions are piecewise linear, it makes sense that the derivatives with respect to time are piecewise constant. But the robot hand is a physical thing; it has to have a velocity at every instant in time. It cannot instantaneously have an undefined velocity.\nThink about what it is that causes the change from one velocity step to another. There is a motor that is spinning and changing its rate of spin, perhaps using a pulley and a belt to move the robot hand to the right position at any instant of time. Changing the velocity requires a force to create an acceleration. We can differentiate the velocity to see what the acceleration must be to create the simple piecewise linear function shown in Figure 49.2.\nMathematically, the second derivatives \\(\\partial_{tt} x(t)\\) and \\(\\partial_{tt} y(t)\\) do not exist, because \\(\\partial_{t} x(t)\\) and \\(\\partial_{t} y(t)\\) are discontinuous. There is no physical amount of force that will change the velocity in an instant.\nAs an accommodation to the physical existence of the robot hand, we’ve softened the transition between consecutive velocity segments to allow it to take 0.2 seconds, ramping up from zero force 0.1 second before the hand reaches the station, to maximum force at the station, then back down to zero 0.1 second after the hand reaches the station. Consequently the actual motion is smoother and the maximum acceleration is about half that of gravity. Figure 49.5 shows the resulting trajectory which can be likened to that of a baseball player rounding a base.\nFigure 49.5: A smoothed x-trajectory near station 2. The position of the station is marked with a dot.\nA consequence of smoothing the trajectory is that the robot hand comes near, but does not touch the station. It misses by about 2 mm. For many human tasks that might be good enough, but for precision manufacturing a miss by 2 mm is about 1000 times more than allowed.\nIf you like working with practical problems, you might find a simple solution to the problem. For instance, we could have aimed the robot hand 2 mm further to the right than the actual station. In falling short by 2mm, the hand would miss the new target but cross right over the originally intended station.\nSolutions like this are sometimes called ad hoc, meaning that they are so specifically tailored to one situation that they do not generalize well to slightly different problems. The next section introduces a superior approach that is much more general.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Data-driven functions</span>"
    ]
  },
  {
    "objectID": "Manifestations/B4-splines.html#piecewise-but-smooth",
    "href": "Manifestations/B4-splines.html#piecewise-but-smooth",
    "title": "49  Data-driven functions",
    "section": "49.2 Piecewise but smooth",
    "text": "49.2 Piecewise but smooth\nThe approach we will take to smoothly connect the points on the path is based on ideas of derivatives and on the construction of low-order polynomials. In Block 2, we emphasized low-order polynomials up to the square term, and we will pick that up again here for demonstration purposes. For this example, we will construct only the \\(y(t)\\) function. Constructing \\(x(t)\\) would be done using the same procedure.\nOur task is to find a function \\(y(t)\\) to interpolate discrete points such as those shown in Figure 49.6. The discrete points are called knots1 in the language of interpolating functions. Each knot is a coordinate pair \\((t_i, y(t_i))\\) shown as an orange dot in Figure 49.6.\nThe piecewise linear interpolating function is easily constructed and is shown as a dotted curve. As we saw in the previous section, such a function has a discontinuous first derivative. We would like something smoother, with a continuous first derivative. A curve such as the one we seek is shown as the multi-colored function.\n\n\n\n\n\n\n\n\nFigure 49.6: Two interpolating functions of the four discrete points (orange). One is piecewise linear (dotted curve), the other is piecewise quadratic (multi-color curve).\n\n\n\n\n\nThe framework we will adopt for the smooth interpolating function is piecewise quadratic segments between adjacent knots. There are four knots, requiring three segments. We will call the segment \\(p_1(y)\\) connecting the first knot to the second, with \\(p_2(y)\\) connecting the second to the third knot and \\(p_3(y)\\) connecting the third to the fourth knot. Each of those segments will be a second-order polynomial. To keep things organized, we will use coefficient names systematically:\n\\[p_1(t) \\equiv a_1 + b_1 \\left[t - t_1\\right] + c_1 \\left[t - t_1\\right]^2\\\\\np_2(t) \\equiv a_2 + b_2 \\left[t - t_2\\right] + c_2 \\left[t - t_2\\right]^2\\\\\np_3(t) \\equiv a_3 + b_3 \\left[t - t_3\\right] + c_3 \\left[t - t_3\\right]^2\\\\\\] The four knots are \\[\\left[\\begin{array}{c}\\left(t_1, x_1\\right)\\\\\n\\left(t_2, y_2\\right)\\\\\n\\left(t_3, y_3\\right)\\\\\n\\left(t_4, y_4\\right)\\\\\n\\end{array}\\right]\\] which you can think of as two columns of a data frame, one with the \\(t\\)-coordinates of the knots and the other with the \\(y\\)-coordinates. For the knots in Figure 49.6 the data table is\n\n\n\n\n\nt\ny\n\n\n\n\n1\n0.0\n\n\n2\n2.0\n\n\n3\n0.5\n\n\n4\n1.7\n\n\n\n\n\n\n\nConstructing the interpolating function is a matter of making good choices for \\(a_1,\\) \\(a_2,\\) \\(a_3,\\) \\(b_1,\\) \\(b_2,\\) \\(b_3,\\) \\(c_1,\\) \\(c_2,\\) and \\(c_3\\).\nWe require these things of each of the interpolating polynomials:\n\nIt passes exactly through the two knots marking the segment’s endpoints. That is \\(p_1(t_1) = y_1\\) and \\(p_1(t_2) = y_2 = p_2(t_2)\\) and \\(p_2(t_3) = y_3 = p_3(t_3)\\) and, finally, \\(p_3(t_4) = y_4\\). Note that at the interior knots where two polynomials join, the left-hand polynomial and the right-hand polynomial should exactly match the function value and each other.\nThe derivative (with respect to \\(t\\)) should match where the segments join. That is, \\(\\partial_t p_1(t_1) = \\partial_t p_2(t_2)\\) and \\(\\partial_t p_2(t_3) = \\partial_t p_3(t_3)\\). Thus, the function we want to build will be \\(C^1\\), that is, have a continuous first derivative.\n\nHow to accomplish (1) and (2)?\nNotice first that because we wrote each of the polynomials in the style of Taylor polynomials, we can read the values of \\(a_1\\), \\(a_2\\), and \\(a_3\\) directly from the data table: \\[p_1(t_1) = a_1 = y_1\\\\p_2(t_2) = a_2 = y_2\\\\p_3(t_3) = a_3 = y_3\\\\\\]\nWe can find other coefficients from the requirement that the right side of each segment pass through the knot on that side. This gives:\n\\[p_1(t_2) = y_2 = a_1 + b_1 \\left[t_2-t_1\\right] + c_1\\left[t_2-t_1\\right]^2\\\\\np_2(t_c) = y_3 = a_2 + b_2 \\left[t_3-t_2\\right] + c_2\\left[t_3-t_2\\right]^2\\\\\np_3(t_c) = y_4 = a_3 + b_3 \\left[t_4-t_3\\right] + c_3\\left[t_4-t_3\\right]^2\\] (Notice that \\(t_2 - t_1\\) and the like are simply numbers that can be computed from the known knot points.)\nAnother two conditions are that the derivatives of the polynomials from either side of each interior knot point must match at the knot point. Finding the derivatives of the segments is a simple exercise:\n\\[\\partial_t p_1(t) = b_1 + 2 c_1 \\left[t - t_1\\right]\\\\\n\\partial_t p_1(t) = b_2 + 2 c_2 \\left[t - t_2\\right]\\\\\n\\partial_t p_1(t) = b_3 + 2 c_3 \\left[t - t_3\\right]\\] Matching these derivatives at the \\(t_2\\) and \\(t_3\\) knot points—the interior knots where two segments come together—gives two more equations: \\[\n\\partial_t p_1(t_2) = b_1 + 2 c_1 \\left[t_2 - t_1\\right] = b_2 = \\partial_t p_2(t_2)\\\\\n\\partial_t p_2(t_3) = b_2 + 2 c_2 \\left[t_3 - t_2\\right] = b_3 = \\partial_t p_3(t_3) \\] All together, we have five equations in six unknowns: \\(b_1, b_2, b_3\\) and \\(c_1, c_2, c_3\\).\nPlugging in the specific values \\(t_1\\) through \\(t_4\\), and \\(x_1\\) through \\(x_4\\) from the data table translates the equations for the polynomial values and derivatives gives this system of equations: \\[\nb_1 + c_1 = x_2 - x_1 = \\ \\ \\ \\ \\ \\ 2\\\\\nb_2 + c_2 = x_3 - x_2 = -1.5\\\\\nb_3 + c_3 = x_4 - x_3= \\ \\ 1.2\\\\\nb_1 + 2 c_1 - b_2 = 0\\\\\nb_2 + 2 c_2 - b_3 = 0\\]\nThis is not the place to go into the details of solving the five equations to find the six unknowns. (Block 5 introduces the mathematics of such things, which turns out to the same math used to find model parameters to “fit” data.) But there are some simple things to say about the task.\nFirst, you may recall being told in high-school mathematics that to find six unknowns you need six equations. We have only five equations to work with. But it is far from true that there is no solution for six unknowns with five equations. There are in fact an infinite number of solutions. (Again, Block 5 will show the mathematics behind this statement.) Essentially, all we need to do is make up a sixth equation to identify a particular one of the infinite number of solutions. It is nice if this made-up equation reflects something interpretable about the curve.\nWe will choose to have the sixth equation specify what the derivative of the interpolating function should be at the far right end of the graph. That right-most derivative value will be \\[\\partial_t p_3(t_4) = b_3 + 2 c_3 \\left[t_4 - t_3\\right]\\ .\\] We can set this value to anything we like. For instance, in Figure 49.6 the right-most derivative is set to zero; you can see this from the curve being flat at the right-most knot point.\n\n\n\n\n\n\n\n\nFigure 49.7: Four different \\(C^1\\) piecewise quadratic functions that interpolate the knot points. The functions have different values of the derivative at the right end of the domain.\n\n\n\n\n\nKeeping in mind the piecewise nature of the interpolating polynomial, it may seem surprising that changing the slope at \\(t_4\\) leads to a change in value of the function almost everywhere. Yet the stiffness of the parabolic segments means that conditions in one segment have an impact on adjacent segments. In turn, the segments adjacent to these also change, a change that percolates down to every segment in turn.\n\nQuadratic spline functions can be created with the R/mosaic qspliner() function. The second argument is a data frame giving the knot locations. The first argument is a tilde expression specifying the variables to use from the data frame.\n\nxfun &lt;- qspliner(x ~ t, data = Robot_stations)\nyfun &lt;- qspliner(y ~ t, data = Robot_stations)\n\n\n\n\n\n\n\n\n\nFigure 49.8: A quadratic spline interpolation of the \\(x\\)-coordinates of the robot-path knots. The data hardly speak for themselves, since the interpolationg function tends to alternate between concave up and concave down in adjacent segments between the knots.\n\n\n\n\n\n\nPutting together the \\(x(t)\\) and \\(y(t)\\) interpolating functions, each of which has that extremum between knot points, leads to an absurdly complicated path, as seen in Figure 49.9.\n\n## Warning in (nudge_x != 0) || (nudge_y != 0): 'length(x) = 16 &gt; 1' in coercion\n## to 'logical(1)'\n\n\n\n\n\n\n\nFigure 49.9: Connecting the robot-path knots with a piecewise quadratic polynomial, constructed to be \\(C^1\\). The path is pretty perhaps, but absurd.\n\n\n\n\n\nQuadratic splines are rarely used in practice. (A cubic spline provides helpful flexibility. See Section 49.3.) In Figure 49.8 you can see one of the reasons: the quadratic form is so stiff that the interpolating function tends to shift from concave up to concave down (or vice versa) at each knot point. This results in the interpolating function tending to have a local minimum or maximum between adjacent knots, even if the data themselves to not indicate such a structure.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Data-driven functions</span>"
    ]
  },
  {
    "objectID": "Manifestations/B4-splines.html#sec-cubic-splines",
    "href": "Manifestations/B4-splines.html#sec-cubic-splines",
    "title": "49  Data-driven functions",
    "section": "49.3 C2 smooth functions",
    "text": "49.3 C2 smooth functions\nIn the previous section, we arranged the functions \\(x(t)\\) and \\(y(t)\\) composed from the piecewise quadratic segments to be \\(C^1\\) smooth. (Recall that \\(C^1\\) smooth means that the derivatives \\(\\partial_t x(t)\\) and \\(\\partial_t y(t)\\) are continuous.) We established this continuity by make sure that each segment has a value of the derivative at its end-point know that matches the derivative of the adjacent segment.\nTo arrange \\(C^2\\) continuity requires that the segment include a new parameter. Most commonly, this is done by moving from quadratic segments to cubic segments. This can be done by an approach similar to that of the previous section but somewhat more elaborate. Such a \\(C^2\\) interpolating function is called a cubic spline. Cubic splines are very commonly encountered in applications requiring interpolation.\nWith the ability to match piecewise cubic polynomials to a set of knots, we can easily construct the smooth path to connect the knots in Figure 49.1. Figure 49.10 shows a \\(C^2\\) path connecting the knots. The path is constructed by plotting simultaneously the output of two functions, \\(x(t)\\) and \\(y(t)\\), with the input \\(t\\) on the domain \\(1 \\leq t \\leq 16\\).\n\nCubic spline functions can be created with the R/mosaic spliner() function. The second argument is a data frame giving the knot locations. The first argument is a tilde expression specifying the variables to use from the data frame.\n\nxfun &lt;- spliner(x ~ t, data = Robot_stations)\nyfun &lt;- spliner(y ~ t, data = Robot_stations)\n\n\n\n## Warning in (nudge_x != 0) || (nudge_y != 0): 'length(x) = 16 &gt; 1' in coercion\n## to 'logical(1)'\n\n\n\n\n\n\n\nFigure 49.10: Connecting the robot-path knots with a piecewise cubic polynomial, constructed to be \\(C^2\\). This is a much smoother path than produced by interpolation with quadratic polynomials.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.11: A cubic spline interpolation of the \\(x\\)-coordinates of the robot-path knots. The cubic spline respects the monotonicity of consecutive knot points.\n\n\n\n\n\n\nApplication area 49.1 —Continuous second derivatives keep motion smooth.\n\n\n\n\n\n\n\nApplication area 49.1 Avoiding infinite force\n\n\n\nWe saw that using a line-segment interpolation produces discontinuity in the derivative of the function. Mathematically, discontinuity in the velocity can be thought of as an infinite acceleration, requiring an infinite force. In the physical world, accelerations must be finite. Even if a force is large, there is often slack in connections between components and the components are not perfectly rigid.\nThe video shows motion of a robotic dog. At the start, the motors in the robot are being asked to make a straight-line transition between waypoints. The result is vibration and a tremor-like movement.\nThen the motors are given a smoothly interpolated signal. (Much of the video hows the programming involved. You can skip directly to time 13:02 to see the motion with smoothly interpolated waypoints.) The smooth interpolation produces a gentle and vibration-free movement.\n\n\n\nWHAT SHOULD GO HERE IN PDF?\n\nLink to entire video by James Bruton.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Data-driven functions</span>"
    ]
  },
  {
    "objectID": "Manifestations/B4-splines.html#bézier-splines",
    "href": "Manifestations/B4-splines.html#bézier-splines",
    "title": "49  Data-driven functions",
    "section": "49.4 Bézier splines",
    "text": "49.4 Bézier splines\nThe sort of interpolating functions described in the previous two sections were designed to be smooth at the \\(C^1\\) level (quadratic spline) or the \\(C^2\\) level (cubic spline). Such smoothness makes sense for, say, robotic motion where we want at all times to keep the force on each robot joint small.\nNot all path-related design problems require such smoothness. Indeed, in some settings, non-smoothness is called for. For instance, Figure 49.12 shows the outline of a familiar shape\n\n\n\n\n\n\n\n\n\nFigure 49.12: The outline of a letter in a computer font is often specified by a series of knot points (red dots). The path passes smoothly through some of the knot points, but has a discontinuous derivative at others.\n\n\n\n\nFor both quadratic and the more commonly used cubic splines, matching the derivatives on either side of a knot is essential to constructing the function. For Bézier splines, each segment is mathematically independent from every other segment. It is up to the human designer of the curve to determine whether the curves derivative should be continuous or discontinuous at the knot point between two segments.\nThe shape of a Bézier spline segment is established by four independent points. The first and last points determine the endpoints of the segment. Each endpoint is associated with a control point that sets the angle and “speed” with which the path leaves or enters the endpoint. You can interact with the graph in Figure 49.13 to develop an intuition.\n\n\n\n\n\n\n\nLink to the Bezier app for PDF version\n\n\n\n\n\n\n\n\n\nFigure 49.13: A single Bézier segment is defined by two endpoints and two control points. Drag the control points to see how the shape of the curve is defined by them.\n\n\n\n\n\nThe curve for a given segment is gratifyingly smooth. The real power of Bézier splines stems from how segments can be connected in various ways. Figure 49.14 shows two Bézier segments that have been initialized to have a smooth junction at endpoints 4 and 5. The smoothness is set by the corresponding control points (marked 3 and 6). So long as those four points (3, 4, 5, 6) are colinear and in order, the junction will be smooth. You can alter control points 2 and 7 in any way you like; the junction will remain smooth.\n\n\n\n\n\n\n\nWHAT TO PUT HERE FOR PDF version?\n\n\n\n\n\n\n\n\n\nFigure 49.14: Two Bézier segments can be arranged in to create a smooth or non-smooth junction between them.\n\n\n\n\n\nConsider the path followed by a Bézier curve as it leaves one of the endpoints. Figure 49.14 has been initialized so that the tangent to the curve is horizontal at the right endpoint and almost vertical at the left endpoint. The further the control point is from the endpoint, the longer the Bézier curve will stay close to the tangent line. Another way to think of this is that the position of a control point has little impact on the shape of the curve near the opposite endpoint. You can observe this on the canvas by, say, moving control point 2 and observing the relatively little change near endpoint 4.\n\n\n\n\n\n\n\nProvide link to bezier app\n\n\n\n\n\n\n\n\n\nFigure 49.15: A Bézier curve leaves each endpoint in a direction that is tangent to the line drawn between the endpoint and its control point.\n\n\n\n\n\nAlgebraically, each Bézier segement is a pair of cubic functions, \\(x(t)\\) for the x-coordinate and \\(y(t)\\) for the y-coordinate. The input \\(t\\) varies between 0 and 1 for each segment. The coordinate pair \\(\\left({\\large\\strut} x(0), y(0)\\right)\\) is one endpoint of the curve, while \\(\\left({\\large\\strut} x(1), y(1)\\right)\\). Each intermediate value of \\(t\\) corresponds to a point on the interior of the curve.\nThe \\(x(t)\\) and \\(y(t)\\) functions have the same form, the difference between the functions being only the values of the end values (\\(x_1\\) and \\(x_4\\) for the \\(x(t)\\) function, and similarly \\(y_1\\) and \\(y_4\\) for the \\(y(t)\\) function), as well as the control-point values (\\(x_2\\) and \\(x_3\\) for one function, \\(y_2\\) and \\(y_3\\) for the other.)\n\\[x(t) = (1-t)^3\\, x_1 + 3(1-t)^2 t\\, x_2 + 3(1-t) t^2\\, x_3 + t^3\\, x_4\\\\\\text{and}\\\\\ny(t) = (1-t)^3\\, y_1 + 3(1-t)^2 t\\, y_2 + 3(1-t) t^2\\, y_3 + t^3\\, y_4\\]\n\n\n\n\n\n\nCalculus history—Splines in wood and metal\n\n\n\nBefore the advent of digital design and manufacturing, smooth curves were described by clay or wooden models hand-crafted by skilled workers. Material was removed to conform to the models by machine tools directed by cams running over the models, by hand sanding and polishing, as shown in this video of propeller manufacture during World War II.\n\n\n\n\n\n\nVideo 49.1: Machining in the pre-digital world: airplane propellers\n\n\n\nSpline functions and digital actuators have largely replaced such analog models.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Data-driven functions</span>"
    ]
  },
  {
    "objectID": "Manifestations/B4-splines.html#footnotes",
    "href": "Manifestations/B4-splines.html#footnotes",
    "title": "49  Data-driven functions",
    "section": "",
    "text": "Called such possibly because the curves are tied together at each of the knots.↩︎",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Data-driven functions</span>"
    ]
  },
  {
    "objectID": "Manifestations/B4-optimization.html",
    "href": "Manifestations/B4-optimization.html",
    "title": "50  Optimization and constraint",
    "section": "",
    "text": "50.1 Gradient descent\nThe general approach we will take to the solving phase of optimization problems will be iterative as in Chapter 48. Start with an initial guess for an argmin and then construct a new function that can improve the guess. Applying this improvement function iteratively leads to better and better estimates of the true argmin.\nFor illustration purposes, we will use optimization problems where the objective function has two inputs. Such objective functions can be graphed on paper or a display screen and it is possible to see the action of the iterative improvement process directly. For optimization in problem with many inputs, the improvement can be monitored from the objective function output at each step.\nFigure 50.2 shows a mechanical system consisting of a mass suspended from a fixed mounting by three nonlinear springs.\nFigure 50.2: A mass suspended from three springs.\nThe mass is shown by a black circles. Springs are the zig-zag shapes. The bold bar is the fixed mounting, as if from a beam on the ceiling of a room. The system has an equilibrium configuration where the springs are stressed sufficiently to balance each other left to right and to balance the gravitational force downward on the mass.\nWe want to calculate the equilibrium position. The basic strategy is to model the potential energy of the system, which consists of:\nSince the configuration of the system is set by the coordinate \\((x_1, y_1)\\), the potential energy is a function \\(E(x_1, y_1)\\). For brevity, we will leave out the physics of the formulation of the potential-energy function; shown in Figure 50.3.\nFigure 50.3: The potential energy of the spring-mass system in Figure 50.2.\nThe potential energy function \\(E(x,y)\\) has a bowl-like shape. The bottom of the bowl—the argmin—is near \\((x=1.7, y=-1.3)\\). In terms of Figure 50.2, the equilibrium position is a bit upward and to the right of the position shown in the figure.\nWith a graph of the objective function like Figure 50.3, the solution phase is simple; a graph will do the job. But for more complicated objective functions, with more than 2 inputs, drawing a complete graph is not feasible. For example, in the spring-mass system shown in Figure 50.4, the potential energy function has six inputs: \\(x_1, y_1, x_2, y_2, x_3, y_3\\). In genuine applications of optimization, there are often many more inputs.\nFigure 50.4: A more complicated spring-mass system.\nIn a multi-input optimization problem, we don’t have a picture of the whole objective function. Instead, we are able merely to evaluate the objective function for a single given input at a time. Typically, we have a computer function that implements the objective function and we are free to evaluate it at whatever inputs we care to choose. It is as if, instead of having the whole graph available, the graph is covered with an opaque sheet with a loophole, as in Figure 50.5.\nFigure 50.5: A more realistic view of what we can know about a function.\nWe can see the function only in a small region of the domain and need to use the information provided there to determine which way to move to find the argmin.\nThe situation is analogous to standing on the side of a smooth hill in a dense fog and finding your way to the bottom. The way forward is to figure out which direction is uphill, which you can do directly from your sense of balance by orienting your stance in different ways. Then, if your goal is the top of the hill (argmax) start walking uphill. If you seek a low point (argmin), walk downhill.\nThe mathematical equivalent to sensing which direction is uphill is to calculate the gradient of the objective function. In Chapter 25 we used partial differentiation with respect to each of the input quantities to assemble the gradient vector, denoted \\(\\nabla f() = \\left({\\large \\strut} \\partial_x f(), \\ \\partial_y f()\\right)\\). In terms of Figure 50.5, where we are standing at about \\((x_i=0.8, y_i=-2.3)\\), we would evaluate the each of the partial derivatives in the gradient vector at \\((0.8, -2.3)\\).\nThe gradient points in the steepest direction uphill so, once you know the direction, take a step in that direction to head toward the argmax, or a step in the opposite direction if you seek the argmin. The process of following the gradient toward the top of the hill is called gradient ascent. Correspondingly, following the gradient downhill is gradient descent.\nFigure 50.6: The gradient provides information about the shape of the local function in a convenient form to guide the step to the next locale in your journey toward the argmin or argmax.\nFor humans, the length of a step is fixed by the length of our legs and the size of our feet. The mathematical step has no fixed size. Often, the modeler gains some appreciation for what constitutes a small step from the modeling process. Referring to Figure 50.4 for example you can see that a small increment in \\(x\\) is, say, \\(0.1\\), and similarly for \\(y\\). There is little point in taking an infinitesimal step—that gets you almost nowhere! Instead, be bold and take a finite step. Then, at your new location, calculate the gradient vector again. If it is practically the same as at your earlier position, you can wager on taking a larger step next time. If the new gradient direction is substantially different, you would be well advised to take smaller steps.\nFortunately, a variety of effective ideas for determining step size have been implemented in software and packaged up as algorithms. The modeler need only provide the objective function in a suitable forma starting guess for the inputs.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Optimization and constraint</span>"
    ]
  },
  {
    "objectID": "Manifestations/B4-optimization.html#gradient-descent",
    "href": "Manifestations/B4-optimization.html#gradient-descent",
    "title": "50  Optimization and constraint",
    "section": "",
    "text": "Spring-mass systems: an example context\nAs our example context for discussing the optimization process, we will consider how to use optimization to calculate the configuration of simple mechanical systems consisting of interconnected springs and masses. Such configuration problems are especially important today in understanding the structure and function of proteins, but we will stick to the simpler context of springs and masses.\n\n\n\n\n\n\nthe gravitational potential energy of the mass.\nthe energy stored in stretched or compressed springs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe R/mosaic function argM() is set up to find argmins and argmaxes using the familiar tilde-formula/domain style of arguments used throughout this book. For instance, the potential energy of the spring-mass system shown in Figure 50.2 is available as mosaicCalc::PE_fun1()\n\nargM(PE_fun1(x, y) ~ x & y, bounds(x=0:3, y=-3:0))\n## # A tibble: 1 × 3\n##       x     y .output.\n##   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n## 1  1.65 -1.21    -3.55\n\n\n\nApplication area 50.1 —Algorithms can implement formulas, but the reverse is not necessarily true.\n\n\n\n\n\n\n\nApplication area 50.1 Formulas and algorithms\n\n\n\nTextbook formulas in physics, chemistry, engineering, and economics often have a root in an optimization problem. Since a formula is the desired result, symbolic differentiation is used in the solution phase. This allows parameters to be represented with symbols rather than as specific numbers. Usually the objective functions involved are simple. And to make the objective functions simple enough for symbolic work, it is common to make approximations, for example by replacing functions like \\(\\sin(x)\\) with \\(x\\) and \\((1+p)^n\\) with \\(1+np\\). But simplifying the objective function should really be considered part of the solution phase rather than the modeling phase.\nNumerical techniques are the most widely used in practice. Optimization is an important operation in both science and management and much human ingenuity has gone into the development of effective algorithms. The modeler rarely if ever needs to reach beyond the software provided in technical computing environments such as R, MATLAB, Mathematica, or the many packages available for Python.\nIn data science and machine learning, often advanced solution-phase software is provided as web services and APIs (application programming interfaces). An example is the Google technology product TensorFlow used to find optimal parameters for functions in the machine technique called “deep learning.”\n\n\n\n\nThe potential energy function of the spring-mass system in Figure 50.4 is available as the R/mosaic function PE_fun2(). This potential energy function has six inputs: the \\(x\\) and \\(y\\) coordinates of each of the three masses. The code below shows how to use the R/mosaic argM() function to locate an argmin of the potential energy.\n\nargM(PE_fun2(\n  x1, y1, x2, y2, x3, y3) ~ x1 & y1 & x2 & y2 & x3 & y3, \n  bounds(x1 = 0:3, y1=-3:0, x2=0:3, y2=-3:0, x3=0:3, y3=-3:0))\n## # A tibble: 1 × 7\n##      x1    y1    x2    y2    x3    y3 .output.\n##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n## 1 0.800 -2.15  1.60 -3.05  2.40 -2.70    -8.86\n\nThe argM() function reports the final result, the end of the path followed in descending the gradient field. Figure 50.7 gives a movie of the path as it is being followed.\n\n\n\n\n\n\n\n\nFigure 50.7: The path to equilibrium for the 3-body spring-mass system shown in Figure 50.4. The top two frames show a 2-dimensional slice through the 6-dimensional gradient field. The bottom frame translates the current point on the path into a picture of the spring-mass locations.\n\n\n\n\n\nAt the start of the movie, the masses are (absurdly) misplaced and far from their equilibrium position. As system configuration moves downhill toward the argmin of the potential energy function, the masses sort themselves out.\nThe two gradient-field frames show a different two-dimensional slices of the potential energy function which has six inputs. Watch the gradient-fields carefully to see that the field itself is changing as time goes by. All six inputs are changing. At each point in time, we are plotting the gradient field as a function of the two inputs shown on the axes. These stay the same through the whole movie, but the other four inputs are changing as the system moves along the gradient descent path. The last frame shows the gradient field at the final position in six-dimensional space. You can see that the early parts of the path are not aligned with the end-of-path gradient fields, but they were aligned at the earlier time when each point in the path was passed.\nThe familiar tilde-expression format used by argM() and the other R/mosaic functions is suitably compact for function of one or two arguments, but for functions with many inputs it becomes ungainly. For objective functions with many inputs, a different programming style is more appropriate that packages up the multiple inputs into a single, vector input. Since this is not a programming book, we won’t go into the vector-input programming style, but be aware that in professional-level work, learning new tools for programming becomes essential.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Optimization and constraint</span>"
    ]
  },
  {
    "objectID": "Manifestations/B4-optimization.html#objectives-and-constraints",
    "href": "Manifestations/B4-optimization.html#objectives-and-constraints",
    "title": "50  Optimization and constraint",
    "section": "50.2 Objectives and Constraints",
    "text": "50.2 Objectives and Constraints\nMany real-world decision-making settings do not fit neatly into the framework of constructing an objective function and then finding the argmin (or argmax). A common situation is having multiple objectives. These objectives often compete and the output of the respective objective functions may not necessarily be directly comparable. For instance, in health care one objective is to save lives, while another is to minimize costs. But lives and money are not directly comparable.\nOften, the original problem statement does not include all of the objectives and the modeler needs to be perceptive to discern important objectives left out of the initial description of the problem. When such missing objectives become apparent, it is necessary to visit the modeling phase of the problem to insert the new objectives. By adopting the right approach to modeling, such situations can be readily handled and, even better, the modeling phase can bring new insight into the real-world problem.\nTo illustrate, let’s returning to the mathematically simplified problem of constructing an optimal cardboard box. Before, we stipulated that the raw cardboard stock has dimension 20 inches by 30 inches. Now we will generalize and work with a piece of cardboard that has edges of length \\(y\\) from which, as before, we will cut out square corners of length \\(x\\) on a side. (See ?fig-box-shape). Our objective is to make a box with the largest possible volume. (This will be an argmax problem.)\n\n\n\n\n\n\n\n\nFigure 50.8: The cardboard cut lines and the eventual shape of the folded box.\n\n\n\n\n\n\n\n\n\n\n\nFigure 50.9: The cardboard cut lines and the eventual shape of the folded box.\n\n\n\n\n\nThe area of the bottom of the box is \\((y - 2x)^2\\) and the box height is \\(x\\). The objective function is the volume of the box, area times height: \\[V(x, y) \\equiv x (y - 2x)^2\\ .\\] There are two inputs, \\(x\\) and \\(y\\), so a simple plot should suffice to find the argmax.\n\n\n\n\n\n\n\n\nFigure 50.10: The volume of the box (in cubic inches) constructed by cutting corners of size \\(x\\)-by\\(x\\) out of a \\(y\\)-by-\\(y\\) piece of cardboard.\n\n\n\n\n\nScanning Figure 50.10 reveals a couple of things that you might not have anticipated. First, the argmax is in the extreme lower-right corner of the graphics frame, not in the center as in previous examples. Second, the argmax in this corner, \\((y=0, x=10)\\) is logically inconsistent with the idea of a cardboard box.\nThe inconsistency stems from an inadmissible value for \\(x\\). For \\(2x &gt; y\\), the bottom of the box would have negative edge length. But because the objective function \\(V(x,y)\\) squares this negative quantity—in the \\((y - 2x)^2\\) term—the output of the objective function does not signal that anything is wrong. The source of the problem is not the objective function formula itself, but neglecting to consider carefully what is the proper practical domain for the function.\nTo make the calculation realistic, we should search for the argmax only in that region of the graphics frame where \\(y &gt; 2x\\). That restriction on the search domain is called a constraint. In this case, the constraint takes the form of an inequality \\(y &gt; 2x\\) so we call it an inequality constraint. (Later, we will work with equality constraints.)\n\n\n\n\n\n\n\n\nFigure 50.11: The inequality constraint that \\(y &gt; 2x\\) renders much of the graphics frame inadmissible as a possible solution. The inadmissible region is shaded in blue. The argmax must be sought in the unshaded region of the frame.\n\n\n\n\n\nWith the \\((x,y)\\)-domain restricted to the values that are physically realistic, we can see that the argmax is still on the edge of the frame, at \\(y=30\\) and \\(x\\approx 5\\), where the volume of the box will be about 1800 in3. This result should cause you pause, since there was nothing in the problem statement that limited \\(y\\) to be 30” or less. If we replotted with a larger domain for \\(y\\), we should see still larger boxes, without any limit.\nThe interpretation of the problem as originally posed is: With enough cardboard we can make a box of any size! Since the goal was to recommend the “best” size, this conclusion is not so useful. The weak conclusion stems from a fault in the problem statement. The statement omitted an important second objective function: use as little cardboard as possible.\nIf using as little cardboard as possible were our sole objective, the optimization problem has an easy-to-find solution: we would make a zero-volume box out of zero-area of cardboard. What we want, somehow, is to make as big a box as possible out of as little cardboard as possible: we have two objectives! In this case, the objectives are in conflict: making a bigger box (good) uses more cardboard (bad).\nCommon sense tells us to balance the two objectives, but how to represent this mathematically? Ideally—note that “ideally” is sometimes far from “realistically” or “productively”—we would know how much box-volume is worth to us and how much cardboard costs, and we could construct an objective function that incorporates both value and cost. For instance, if each cubic inch of volume is worth 1 cent, and each square inch of cardboard costs 3 cents, then the objective function will be the following (with output in cents):\n\\[\\text{Profit}(x,y) \\equiv 1\\, x (y-2x)^2 - 3 y^2\\]\n\n\n\n\n\n\n\n\nFigure 50.12: The “profit” (value minus cost) of the cardboad box (cents).\n\n\n\n\n\nEven with including the cardboard cost in the objective function, we will still want to make \\(y\\) as large as possible. Not much guidance there!\nBut let’s imagine a new factor coming into play. At the meeting where the box-design decisions are being made and where you are presenting your analysis in Figure 50.12, the graphic designer speaks up. “The trending shape for this year is cubic. We want the box, whatever it is size, to be a cube.”\nLuckily, you the modeler can quickly incorporate this into your analysis. To be a cube, the height \\(x\\) of the box has to be the same as the width and depth \\(y - 2x\\). So you can incorporate the designer’s wish into the model of the decision factors by adding a new constraint:\n\\[x = y - 2x \\ \\ \\ \\implies y-3x=0\\ \\ \\ \\ \\text{constraint: box must be cubic}\\] This is called an equality constraint. Figure 50.13 shows the equality constraint in green: to be a cube, \\(x\\) and \\(y\\) must be somewhere along the green line.\n\n\n\n\n\n\n\n\nFigure 50.13: The profit function shown in more detail along with the equality constraint (green) for the box to be cube-shaped.\n\n\n\n\n\nFollow the green path uphill. As \\(x\\) gets larger along the constraint, the output of the objective function grows, reaching a level of 1350 cents when \\((x=15, y=45)\\) at the right border of the graphics frame.\nIt is worth pointing out, for later use, that the be-a-cube constraint is almost parallel to the objective function contours.\n\nApplication area 50.2 —Using a budget optimally.\n\n\n\n\n\n\n\nApplication area 50.2 Budgets\n\n\n\nMany organizations use a budget mechanism to manage their affairs. The organization defines divisions or projects, and each of these is given a dollar budget to stay within. The individual division or project manager can arrange things more or less as she thinks best, so long as she stays within the budget. This is a kind of constraint: a budget constraint.\nSuppose you have been tasked to set up a new factory and given a budget of $5,500,000 to do so. You were given this task because you have a particular expertise in how best to set up the factory, but your design will of course depend on the relative prices of the different inputs to the production process.\nFor simplicity, let’s imagine that there are two main inputs: labor \\(L\\) and capital/equipment \\(K\\). It would be silly to spend all the budget on labor and none on capital; the workers would have no tools to work with. Similarly, capital without labor has no productive value. The best design for the factory will be a mix of labor and capital.\nSince the purpose of the factory is to make things for sale, a good objective function will be the sales value of the output produced by the factory. Economists have a favored form for production functions of this sort, a power-law called the Cobb-Douglas function. The essential insight behind the Cobb-Douglas function is that doubling both capital and labor (as if you built a second factory alongside the first) should double production. The Cobb-Douglas form for production as a function of capital and labor is \\[Q(L, K) = p b L^a K^{1-a}\\ .\\] You will use your expertise to set the values of the \\(a\\) and \\(b\\) parameters. The price \\(p\\) of each unit of output will be set by the market: Let’s assume for planning purposes that it is \\(p - \\$450\\) per unit. Suppose you have determined that \\(a=0.3\\) and \\(b=40\\) are appropriate. This production function is shown in Figure 50.14.\n\n\n\n\n\n\n\n\nFigure 50.14: A Cobb-Douglas production function for factory output with \\(p=100\\), \\(b=40\\) and \\(a=0.3\\). (Output units in millions of dollars).\n\n\n\n\n\nAs you can see from Figure 50.14, the more labor and the more capital you use, the higher the production. Notice that the production function itself does not have an argmax interior to the domain being plotted. It is one of those “more is better” situations.\nSuppose that labor costs $6000 per person-month. Capital, in units of production stations, costs $13,000 per unit. Your budget constraint reflects the total cost of capital and labor: \\(6000 \\cdot L + 15000 \\cdot K \\leq 5500000\\). This constraint is graphed in Figure 50.15.\n\n\n\n\n\n\n\n\nFigure 50.15: The production function for factory output with the budget constraint shown in green.\n\n\n\n\n\nAny mixture of labor and capital that falls outside the green zone stays within your budget. What’s the best mixture? The one that gives the largest production. You can read this off the graph, \\(L\\approx 650\\) person-months and \\(K\\approx 125\\) workstations which gives slightly more than $7 million dollars in production.\nThe argmax is right on the frontier of the constraint region. Put into more operational terms: You will want to spend your entire budget to maximize production. This is hardly a surprise to anyone who has to work within a budget. Knowing that you’re going to use the whole budget, you might as well have found the argmax by walking along the constraint frontier from left to right. As you start near \\(K=250\\) and \\(L=380\\), the path you walk goes uphill in terms of the production function. The path continues uphill until you reach the argmax. Near the argmax, the path is level. After the path crosses the argmax, it is leading downhill. At the argmax, the production function contours are parallel to the constraint boundary.\nYou might like to think of this in terms of bicycling along a path in hilly terrain. (The hill is shown in Figure 50.15 as the contours, the path is the boundary of the green area, running diagonally from top left to bottom right in the graph.) When you reach the local high point on the path, you may not be at the top of the hill. But you will be on a flat spot on the path, meaning that the path is parallel to the contour of the hill at that point.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Optimization and constraint</span>"
    ]
  },
  {
    "objectID": "Manifestations/B4-optimization.html#constraint-cost",
    "href": "Manifestations/B4-optimization.html#constraint-cost",
    "title": "50  Optimization and constraint",
    "section": "50.3 Constraint cost",
    "text": "50.3 Constraint cost\nOptimization techniques have an important role to play as aids to human decision making. Let’s see how the mathematical representation of a constraint as a function can facilitate the human decision-making process.\nIn the previous section, the box designer’s request that the box be cubic was translated into an equality constraint, \\(y-3x=0\\), shown as the green line in Figure 50.13. The skilled modeler can bring additional power to the analysis by translating that constraint, \\(y-3x=0\\) into a function, for example \\[\\text{Equation:}\\ \\  \\ y - 3x = 0\\ \\ \\longrightarrow\\ \\ \\ \\text{Function:}\\ \\ \\text{cube-box}(x, y) = y / 3x\\ .\\] Any \\((x^+, y^+)\\) that produces \\(\\text{cube-box}(x^+, y^+) = 1\\) is a pair that satisfies the constraint. In other words, the equality constraint amounts the 1-contour of the cube_box() function.\nTranslating the constraint into a function provides the opportunity to reframe the situation from the mandate, “the box must be a cube,” into a question, “How cubic-like is the box?” If the value of \\(\\text{cube-box}(x,y) &gt; 1\\), the box is flatter than a cube; something in the direction of a pizza box. If \\(\\text{cube-box}(x,y) &lt; 1\\) the box is taller than a cube, as if flowers were being shipped in it.}\nThe constraint-to-function translated situation is shown in Figure 50.16:\n\n\n\n\n\n\n\n\nFigure 50.16: Zooming in on the objective function Profit() and showing the function version of the constraint, cube_box() using \\(\\color{magenta}{\\text{magenta}}\\) contours, with the heavy green line being the contour at cube_box(x,y)=1.\n\n\n\n\n\nEarlier, we saw that if restricted to inputs on the contour \\(\\text{cube-box}(x,y) = 1\\), the optimal output value of Profit() is about $13.50. Now we have a broader picture. For instance, suppose we allow a “little” deviation in box shape from a cube, say, cube_box(x,y) = 1.05. If we allowed this, the value of the Profit() function could be increased from $13.50 to about $22.50 .\nWhether the $9 increase in value justifies the deviation from a cube by 5% is a matter of judgement. We don’t have an immediate way to translate the output of cube_box() into the same units as the output of profit(). The two different units are said to be incommensurate, meaning that they cannot be directly compared. Nonetheless, we now have a basis for a conversation. It might go like this:\nModeler to Designer: I realize that from your perspective, a cube is the optimal shape for the box.\nDesigner: Right. Cubes are in fashion this year. Last year it was the Golden Ratio.\nModeler: It might sound surprising, but we find that so long as you are close to the optimal, it does not much matter if you are exactly on it. How close to a perfect cube would be good enough?\nDesigner: What’s really important is that the box be perceived as a cube in our sales material. I think that most customers would think “cube” so long as the edge lengths are within about 15% of one another.\nModeler: That’s very helpful. Let’s see if I can translate that into the cube_box() function.\n[Modeler does some scribbling while mumbling to himself. “\\(y-2x\\) is the base width and depth of the box, and \\(x\\) is the height of the box. So if \\(y-2x = 1.15 x\\) then \\(y = 3.15 x\\). \\(\\text{cube-box}(x, 3.15 x) = 1.05\\).]\nModeler: [to Designer] The 15% deviation corresponds to an output of 1.05 from \\(\\text{cube-box}()\\).\nModeler: [To product manager] Making that change in shape increases profit per box from $13.50 to $22.50.\nProduct manager: Good job! How about a 30% deviation? That let’s us get up to about $33 in profit.\nDesigner: But it would make the box shaped like a brick! Bricks are so 1990s!\nModeler: It sounds like a 15% deviation would be about right.\nMaking the constraint negotiable by representing it with a function, broadens the scope of the discussion and points to new ways of improving the result.\n\nApplication area 50.3 — Finding an optimal mixture of capital and labor.\n\n\n\n\n\n\n\nApplication area 50.3 Factory production\n\n\n\nLet’s return to a previous example about determining optimal levels of labor and capital in a factory. In that example, the objective function was the money value of the product produced. There was also a budget constraint. Translating the budget constraint into a function, which we will call expenditure(K, L), we have \\(\\text{expenditure}(K, L) = 6000 L + 13000 K\\). Our budget amounts to enforcing \\(\\text{expenditure}(K, L) = \\$5,500,000\\).\nA manager presented with a budget knows that she should work within the constraints of that budget. Mathematically, however, it is easy to imagine the budget being changed, either relaxed or tightened. This mathematical possibility provides the means to extract new information that can be helpful in making decisions at a higher level, that is, above the rank of the manager. This can be helpful to higher management—the people who are responsible for seeing the bigger picture.\nFigure 50.17 show the production function plotted along with the expenditure function. The budget constraint corresponds to a single output level of the expenditure function, that is, a single contour of the expenditure function. Other contours correspond to different values for the budget constraints. Such a graph makes it easy to calculate the consequences for relaxing (or tightening) the constraint.\n\n\n\n\n\n\n\n\nFigure 50.17: The production function for factory output (blue, curved lines) and the labor/capital expenditure function (magenta, straight lines). Contour labels are in millions of dollars. Contour levels for expenditure (magenta: 3.1, 3.9, 4.6, etc.) were selected so that the magenta contours are nearly tangent to the blue factory-output contours. This makes it easy to see the K/L position for optimal factory output for each of the indicated expenditure levels.\n\n\n\n\n\nWith the budget fixed at $5.5 million—that is, on the $5.5-million contour of the expenditure function—the maximum production was $7.1 million.\nWhat happens if we pretend that the budget level was different? Doing so is a matter of looking at a different contour of the expenditure function. For example, if the budget had been smaller, say only $5 million, then production also goes down, to $6.5 million. On the other hand, if we had the means to increase the budget to $6 million, production would go up to $7.7 million.\nIn this example we see that an increase in budget of $500K produces an increase in production worth $600K. It might seem logical that it is worth raising the budget to harvest the extra production, but that is not necessarily the case. To see why, recall that we use constraints such as the budget constraint in this problem to represent a real-world situation where there are multiple objectives, not just the particular objective represented by the objective function. There is a budget in the first place because there are other, competing uses for the money. For instance, the money might be better spent in some other product line that is even more profitable. Or perhaps higher management, taking a long-term strategic view, would prefer to spend the funds on research and development.\nThe point of exploring theoretical changes in the budget is to provide information to the higher-level decision makers, the people who set the budget as opposed to the managers who have to work within the budget. The format that most non-technical people would find accessible is simple: a $500K increase in the budget will result in $600K greater production.\nIn mathematical presentations, this same information is often formatted differently, as a ratio called the Lagrange multiplier. The Lagrange multiplier in this example would be 600/500, that is, 1.2. There is no intrinsic advantage of the Lagrange multiplier format over the common sense format, but the Lagrange multiplier is the format used in many textbooks, so it is worthwhile to know the nomenclature. Some economists have a more evocative name for the ratio: the shadow price of the constraint. Thus, the theoretical exploration of relaxing the constraint provides a straightforward way to put a cost on the constraint. This gives a reasonable way to determine the value of something when there is no direct market for it.\nAn important example of a shadow price comes in the setting of life-saving interventions. For example, increasing spending on highway safety can save lives. If $7.5 billion in increased expenditures saves 1000 lives, the shadow price is $7.5 M per life. People who misinterpret the constraint-to-function methodology often think that it is about cravenly putting a money value on life. In reality, the method merely reveals the money value on life implicit in decisions such as budget allocations. Knowing that the shadow price is $7.5 M does not say what the value of life should be. But it provides a mechanism for comparing different uses for the money. For instance, if the shadow price for increased regulation of toxic industrial chemicals is $11.3 M per life, the relative shadow prices provide an indication that budget money might reasonably be shifted from chemical regulation to highway safety. Economists and epidemiologists who undertake such calculations reveal that the mixture of spending on different life-preserving interventions is far from optimal.\n\n\n\nGenerations of calculus students have been taught a method of mathematical optimization in the presence of constraints that involves positing a Lagrange multiplier, typically written as \\(\\lambda\\), and carrying out a series of differentiations followed by equation solving to find an argmax, which simultaneously provides a numerical value for \\(\\lambda\\). It is easier to understand the motivation behind this by considering the gradient of the objective function and the gradient of the constraint function. If the goal is, say, to maximize production and simultaneously minimize expenditures, we would want to walk up the production gradient and down the expenditure gradient.\nFigure 50.18 shows two gradient fields, one for the production function in the factory-design example and one for expenditure. (The negative of the expenditure gradient is shown, since the goal is to keep expenditures small.)\n\n\n\n\n\n\n\n\nFigure 50.18: The production and expenditure functions displayed as gradient fields. Expenditure is brown, production is magenta.\n\n\n\n\n\nAt each point in the graphics frame, the two gradient vectors form an angle. For example, near the point labeled (a) the angle is roughly 140 degrees, while near (b) the angle is 180 degrees.\nAny value of \\(K\\) and \\(L\\) where the angle is less than 180 degrees is sub-optimal or dominated by some other choice of \\(K\\) and \\(L\\). For instance, near label (a), you could improve both production and expenditures by moving to the southeast. When the angle is 180 degrees, the objective and constraint functions are in complete opposition to one another; any movement in favor of one comes at the cost in the other.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Optimization and constraint</span>"
    ]
  },
  {
    "objectID": "Manifestations/B4-optimization.html#note-other-optimization-algorithms",
    "href": "Manifestations/B4-optimization.html#note-other-optimization-algorithms",
    "title": "50  Optimization and constraint",
    "section": "50.4 Note: Other optimization algorithms",
    "text": "50.4 Note: Other optimization algorithms\nContemporary work often involves problems with tens, hundreds, thousands, or even millions of inputs. Even in such large problems, the mechanics of finding the corresponding gradient vector are straightforward. Searching through a high-dimensional space, however, is not generally a task that can be accomplished using calculus tools. Instead, starting in the 1940s, great creativity has been applied to develop algorithms with names like linear programming, quadratic programming, dynamic programming, etc. many of which are based on ideas from linear algebra such as the qr.solve() algorithm that you will meet in Block 5, or ideas from statistics and statistical physics that incorporate randomness as an essential component. An entire field, operations research, focuses on setting up and solving such problems. Building appropriate algorithms requires deep understanding of several areas of mathematics. But using the methods is mainly a matter of knowing how to set up the problem and communicate the objective function, constraints, etc. to a computer.\nPurely as an example, let’s examine the operation of an early algorithmic optimization method: Nelder-Mead, dating from the mid-1960s. (There are better, faster methods now, but they are harder to understand.)\nNelder-Mead is designed to search for maxima of objective functions with \\(n\\) inputs. The video shows an example with \\(n=2\\) in the domain of a contour plot of the objective function. Of course, you can simply scan the contour plot by eye to find the maxima and minima. The point here is to demonstrate the Nelder-Mead algorithm.\nStart by selecting \\(n+1\\) points on the domain that are not colinear. When \\(n=2\\), the \\(2+1\\) points are the vertices of a triangle. The set of points defines a simplex, which you can think of as a region of the domain that can be fenced off by connecting the vertices.\nEvaluate the objective function at the vertices of the simplex. One of the vertices will have the lowest score for the output of the objective. From that vertex, project a line through the midpoint of the fence segment defined by the other \\(n\\) vertices. In the video, this is drawn using dashes. Then try a handful of points along that line, indicated by the colored dots in the video. One of these will have a higher score for the objective function than the vertex used to define the line. Replace that vertex with the new, higher-scoring point. Now you have another simplex and can repeat the process. The actual algorithm has additional rules to handle special cases, but the gist of the algorithm is simple.\n\n\n\n\n\n\nVideo 50.1: A demonstration of successive steps in the Nelder-Mead optimization algorithm. Based on the objective function’s values at the three vertices in the previous triangle, a new triangle is formed by discarding the worst of the previous vertices and adding a new vertex in the direction of improvement. Source: Miles Chen, Department of Statistics, UCLA",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Optimization and constraint</span>"
    ]
  },
  {
    "objectID": "Manifestations/B4-probability.html",
    "href": "Manifestations/B4-probability.html",
    "title": "51  Probability and evidence",
    "section": "",
    "text": "51.1 Probability density\nA probability, as you may know, is a dimensionless number between zero and one (inclusive). In this chapter, you will be dealing with functions relating to probabilities. The input to these functions will usually be a quantity that can have dimension, for instance, miles driven by a car. For some of the functions we will see in this chapter, the output will be a probability. For other functions in this chapter, the output will be a probability density.\nProbability relates to the abstract notion of an event. An event is a process that produces an outcome. For instance:\nAn event with a discrete outcome—coin flip, medical screening test—can be modeled by assigning a probability number to each of the possible outcomes. To be a valid probability model, each of those assigned numbers should be greater than or equal to zero. In addition, the sum of the assigned numbers across all the possible outcomes should be 1.\nFor events with a continuous outcome, such as the dart toss where the outcome is distance from the center, the probability model takes the form of a function whose domain is the possible outcomes. For the model to be a valid probability model, we require that the function output should never be less than zero. There is another requirement as well: the integral of the function over the entire domain should be 1. For the dart-toss event, if we denote the distance from the bullseye as \\(r\\) and the assigned number for the probability model as \\(g(r)\\), the integral requirement amounts to \\[\\int_0^\\infty g(r) dr = 1\\ .\\]\nNote that the output \\(g(r)\\) is not a probability, it is a probability density. To see why, let’s use the fundamental theorem of calculus to break up the integral into three segments:\nThe total integral is \\[\\int_0^\\infty g(r) dr = 1\\ = \\int_0^a g(r) dr + \\int_a^b g(r) dr + \\int_b^\\infty g(r) dr.\\] The probability that the dart lands at a distance somewhere between \\(a\\) and \\(b\\) is \\[\\int_a^b g(r) dr\\ .\\] Since \\(r\\) is a distance, the dimension \\([r] =\\ \\)L. Suppose the units of \\(r\\) are centimeters. We need \\(\\int g(r) dr\\) to be a dimensionless number. Since the dimension of the integral is \\([r] \\cdot [g(r)] = [1]\\), it must be that \\([g(r)] = [1/r] = \\text{L}^{-1}\\). Thus, \\(g(r)\\) is not a probability simply because it is not dimensionless. Instead, in the dart example, it is a “probability-per-centimeter.” This kind of quantity—probability per something—is called a probability density and \\(g(r)\\) itself is a probability density function.\nTo show the aptness of the word “density,” let’s switch to a graphic of a function that uses literal density of ink as the indicator of the function value. Figure 51.1) shows what the dart toss’s \\(g(r)\\) probability density function might look like:\nFigure 51.1: Showing a probability density function for the dart distance in two modes: 1) an ordinary function graph and 2) the density of ink.\nThe functions called for by the contest instructions are relative density functions. The “relative” means that the function indicates where the probability is more or less dense, but the function has not yet been scaled to be a probability density function. Suppose \\(h(x)\\) is a relative density function such that \\[\\int_{-\\infty}^\\infty h(x)\\, dx = A \\neq 1\\ .\\] Although \\(h(x)\\) is not a probability density function, the very closely related function \\(\\frac{1}{A} h(x)\\) will be a probability density function. We will use the term normalizing to refer to the simple process of turning a relative density function into a probability density function.\nA relative density function is entirely adequate for describing the distribution of probability. However, when comparing two or more probability distributions, it is important that they all be on the same scale. Normalizing the relative density functions to probability density functions accomplishes this. Figure 51.4 compares the three relative probability functions in Figure 51.3. Johnny makes the density large over a narrow domain and zero elsewhere, while Louisa specifies a small density over a large domain. All three competitors’ functions have an area-under-the-curve of dimensionless 1.\nFigure 51.4: Comparing the contest entries by normalizing each of them to a probability density function.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Probability and evidence</span>"
    ]
  },
  {
    "objectID": "Manifestations/B4-probability.html#probability-density",
    "href": "Manifestations/B4-probability.html#probability-density",
    "title": "51  Probability and evidence",
    "section": "",
    "text": "Flipping a coin is an event where the possible outcomes of H and T.\nTaking a medical screening test is an event where the outcomes are “positive” or “negative.”\nThrowing a dart at a bullseye is an event where the outcome is the distance of the impact point from the center of the bullseye.\n\n\n\n\n\nclose to the bullseye: \\(0 \\leq r \\leq a\\)\nfar from the bullseye: \\(b &lt; r\\)\nnot close but not far: \\(a &lt; r \\leq b\\)\n\n\n\n\n\nConsider a simple competition of the sort you might encounter at a fund-raising fair. There is a jar on display, filled with coins that have been donated by one of the fair’s sponsors. You pay $1 (which goes to a good cause) to enter the contest. Your play is to describe how much money is in the jar, writing your description down along with your name on an entry form. At the end of the day, an official will open the jar, count the money, and announce who made the best estimate. The winner gets the money in the jar.\n\n\n\n\n\n\n\n\n\nIn the usual way these contests are run, the contestants each write down a guess for the amount they think is in the jar, say $18.63. The winner is determined by seeing whose guess was closest to the actual value of the coins in the jar.\nIn reality, hardly anyone believes they can estimate the amount in the jar to the nearest penny. The person guessing $18.63 might prefer to be able to say, “between 18 and 19 dollars.” Or, maybe “$18 \\(\\pm\\) 3.” To communicate what you know about the situation, it is best to express a range of possibilities that you think likely.\nIn our more mathematical contest, we ask the participants to specify a function that describes their beliefs about the money in the jar. The instructions state, “On the graph-paper axes below, sketch a continuous function expressing your best belief about how much money is in the jar. The only requirement is that the function value must be zero or greater for all inputs.”\n\n\n\n\n\n\n\n\n\nFigure 51.2: The entry form for the money-in-the-jar contest.\n\n\n\n\nTake a minute to look at the picture of the jar and draw your function on the axes shown above. Think about why the contest form appropriately does not ask you to scale the vertical axis.\nHere are contest entries from three competitors.\n\n\n\n\n\n\n\n\nFigure 51.3: Three contestants’ contest entries.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Probability and evidence</span>"
    ]
  },
  {
    "objectID": "Manifestations/B4-probability.html#three-density-functions",
    "href": "Manifestations/B4-probability.html#three-density-functions",
    "title": "51  Probability and evidence",
    "section": "51.2 Three density functions",
    "text": "51.2 Three density functions\nThree commonly used families of probability density functions are:\n\nthe gaussian density function\nthe exponential density function\nthe uniform density function.\n\nFigure 51.5 shows their shapes.\n\n\n\n\n\n\n\n\n\n\n\n(a) Uniform density\n\n\n\n\n\n\n\n\n\n\n\n(b) Gaussian density\n\n\n\n\n\n\n\n\n\n\n\n(c) Exponential density\n\n\n\n\n\n\n\nFigure 51.5: Three probability density functions that are often used in applied work.\n\n\n\nThe uniform density function, \\(u(x, a, b)\\) is more or less the equivalent of the constant function. The family has two parameters \\(a\\) and \\(b\\) with the function defined as: \\[\\text{unif}(x, a, b) \\equiv \\left\\{\\begin{array}{cl}\\frac{1}{b-a} & \\text{for}\\ a \\leq x \\leq b\\\\0& \\text{otherwise} \\end{array}\\right.\\] This function is used to express the idea of “equally likely to be any value in the range \\([a, b]\\).” For instance, to describe a probability that a January event is equally likely to occur at any point in the month, you can use \\(u(x, 0, 31)\\) where \\(x\\) and the parameters \\(a\\) and \\(b\\) have dimension T and are in units of days. Notice that the density itself has dimension T-1 and units “per day.”\nThe gaussian density function, \\(\\dnorm(x, \\text{mean}, \\text{sd})\\) is familiar to you from previous blocks in this book: the bell-shaped function. It is known also as the normal distribution because it is so frequently encountered in practice. It is a way of expressing, “The outcome of the event will likely be close to this particular value.” The parameter named mean specifies “this particular value.” The parameter sd specifies what’s mean by “close.” The gaussian density function is smooth. It is never zero, but \\(\\lim_{x \\rightarrow \\pm \\infty} \\dnorm(x, \\text{mean}, \\text{sd}) = 0\\).\nTo use an analogy between physical density (e.g., kg per cubic-meter), where density times size gives mass, we can say that the total mass of a probability density function is always 1. For the gaussian density, 68% of of the total mass is within \\(\\pm 1\\)sd of the mean, 95% is within \\(\\pm 2\\)sd of the mean, 99.7% within \\(\\pm 3\\)sd, and 99.99% within \\(\\pm 4\\)sd.\nThe exponential probability density is shaped just like an exponential function \\(e^{-kx}\\). It is used to describe events that are equally likely to happen in any interval of the input quantity, and describes the relative probability that the first event to occur will be at \\(x\\).",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Probability and evidence</span>"
    ]
  },
  {
    "objectID": "Manifestations/B4-probability.html#sec-expected_value",
    "href": "Manifestations/B4-probability.html#sec-expected_value",
    "title": "51  Probability and evidence",
    "section": "51.3 Expectation value, mean and variance",
    "text": "51.3 Expectation value, mean and variance\nProbability theory was originally motivated by problems in gambling, specifically, figuring out what casino games are worth betting on. A feature of casino games—roulette, slot machines, blackjack, Texas hold’em, etc.—is that they are played over and over again. In any one round of play, you might win or you might lose, that is, your “earnings” might be positive or they might be negative. Over many plays, however, the wins and loses tend to cancel out. One way to summarize the game itself, as opposed to the outcome of any single play, is by the average earnings per play. This is called the expected value of the game.\nThis logic is often applied to summarizing a probability density function. If \\(x\\) is the outcome of the random event described by a probability density \\(f(x)\\), the expected value of the probability density is defined as \\[\\mathbb{E}\\!\\left[{\\strut} x\\right] \\equiv \\int_{-\\infty}^\\infty x\\, f(x) \\, dx\\ .\\] In Section 53.4, we will see this same form of integral for computing the center of mass of an object.\n\n\n\n\n\n\nWhy square braces for expected value?\n\n\n\nWhy are you using square braces \\(\\left[\\strut\\ \\ \\right]\\) rather than parentheses \\(\\left(\\strut \\ \\  \\right)\\).\nWe always used parentheses to indicate that the enclosed quantity is the input to a function. But \\(\\mathbb{E}\\!\\left[{\\strut} x\\right]\\) is not a function, let alone a function of \\(x\\). Instead, \\(\\mathbb{E}\\!\\left[{\\strut} x\\right]\\) is a numerical summary of a probability density function \\(f(x)\\).\n\n\n\nFind the expected value of the gaussian probability density \\(\\dnorm(x, \\text{mean}=6.3, \\text{sd}= 17.5)\\). Using the R/mosaic Integrate() function, we have\n\nIntegrate(x * dnorm(x, 6.3, 17.5) ~ x, bounds(x=-Inf:Inf))\n## Loading required namespace: cubature\n## [1] 6.3\n\nThe expected value of a gaussian is the same as the parameter called mean which describes the argmax of the gaussian.\n\nAnother important quantity to describe data or probability distributions is the variance, which is the average of the square distance from the mean. In math notation, this looks like \\[\\mathbb{E}\\!\\left[{\\large\\strut} (x - \\mathbb{E}[x])^2\\right] = \\int_{-\\infty}^{\\infty} \\left(\\strut x - \\text{mean}\\right)^2\\, \\dnorm(x, \\text{mean}, \\text{sd})\\, dx\\ .\\]\n\nCompute the variance of a gaussian probability density \\(\\dnorm(x, \\text{mean}=6.3, \\text{sd}= 17.5)\\).\nTo do this, we must first know the mean, then we can carry out the integration.\n\nIntegrate((x-6.3)^2 * dnorm(x, mean=6.3, sd=17.5) ~ x, bounds(x=-Inf:Inf))\n## [1] 306.25\n\nAgain, you might have anticipated this result, since the variance is the square of the standard deviation (sd) and we were using a particular gaussian distribution with sd equaling 17.5. Of course, \\(17.5^2 = 306.25\\).\n\nTo illustrate the calculations in another setting, we will use an exponential probability function. Just as the R function dnorm() gives the density of the “normal”/gaussian distribution, the R function dexp() outputs the density of the exponential distribution. We used \\(k\\) as the parameter in the exponential distribution. In R, the parameter is framed in terms of the rate at which events happen, that is, the expected number of events per unit time. For instance, the following integrals compute the mean and standard deviation of an exponential process where events happen on average twice per time unit.\n\nIntegrate(x * dexp(x, rate=2) ~ x, bounds(x=0:Inf))\n## [1] 0.5\n\nThe result shouldn’t surprise you. If events are occurring on average twice per unit time, the average time between events should be 0.5 time units.\nHere’s the variance of the same distribution\n\nIntegrate((x-0.5)^2 * dexp(x, rate=2) ~ x, bounds(x=0:Inf))\n## [1] 0.25\n\nIt works out that for an exponential distribution with parameter \\(k\\), the mean is \\(1/k\\) and the standard deviation (square root of the variance) is also \\(1/k\\).\nFinally, let’s look at the mean and variance of a uniform distribution with, say, \\(a=0\\) and \\(b=10\\). We can do this symbolically or numerically. For the mean: \\[\\int_{-\\infty}^\\infty x\\  \\text{unif}(x, 0, 10)\\, dx = \\int_0^{10} \\frac{x}{10-0}\\, dx = \\left.{\\Large\\strut} \\frac{x^2}{20}\\right|_{x=0}^{10} \\\\= \\frac{100}{20} - \\frac{0}{20} = 5\\] For the variance, \\[\\begin{eqnarray}\n\\ \\ \\ & \\!\\!\\!\\!\\!\\!\\int_{-\\infty}^\\infty (x-5)^2\\  \\text{unif}(x, 0, 10)\\, dx\\\\\n& = \\int_0^{10} \\frac{(x-5)^2}{10-0}\\, dx\\\\\n& = \\left.{\\Large\\strut}\\frac{(x-5)^3}{30}\\right|_{x=0}^{10}\\\\\n& =\\frac{5^3}{30} - \\frac{(-5)^3}{30} = \\frac{125}{30} - \\frac{-125}{30} = 8 \\tiny{\\frac{1}{3}}\n\\end{eqnarray}\\]\nOr, numerically1\n\nIntegrate(x * dunif(x, 0, 10) ~ x, bounds(x=0:Inf))\n## [1] 5.000001\nIntegrate((x-5)^2 * dunif(x, 0, 10) ~ x, bounds(x=0:Inf))\n## [1] 8.333336",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Probability and evidence</span>"
    ]
  },
  {
    "objectID": "Manifestations/B4-probability.html#likelihood-and-data",
    "href": "Manifestations/B4-probability.html#likelihood-and-data",
    "title": "51  Probability and evidence",
    "section": "51.4 Likelihood and data",
    "text": "51.4 Likelihood and data\nIn this section, we will examine the accepted technique for combining data with probability density functions to combine previous knowledge with new observations. The technique, called Bayesian inference, is used throughout science and engineering.\nRecall that a relative density function is a format to describe the relatively likeliness of possible outcomes from a random event. The domain for a relative density function is the complete set of possible outcomes from the event. An example: The distance of a dart’s impact from the bullseye.\nThe output of a relative density function is a non-negative number. For an expert dart thrower, the relative density will be high for small distances and low for large distances. This is just a way of quantifying that the expert’s is likely to hit close to the bullseye.\nIn comparing two relative density functions, for instance the function for an expert dart thrower versus that for an amateur, it is helpful to normalize them so that the integral of the relative density over the entire domain is dimensionless 1. The normalized version of a relative density function is called a probability density functions. Note that the probability density function contains the same information as the relative density function.\nIn this section, we introduce a new type of function that is important in probability calculations involving data. This new type of function is, perhaps confusingly, called a likelihood function.\nLikelihood functions always involve hypothetical reasoning. The idea is to construct a model world whose characteristics are exactly known. In that world, we can imagine constructing a function that gives the probability or probability density of any possible value of a measurement.\nFor instance, Johnny, Louisa, and Geoff each created hypothetical worlds that describe the amount of money in the jar. For each contestant, their personal hypothesis states a probability density over all the theoretically possible amounts of money in the jar.\nThe domain of a likelihood function is all the competing hypotheses. Take a moment to digest that. The domain of money-in-jar likelihood function is not the amount of money in the jar, it is instead the three hypotheses: Johnny’s, Louisa’s, and Geoff’s.\nIt is conventional to denote name a likelihood function \\({\\cal L}()\\). For the competition, a likelihood function will be \\({\\cal L}(\\text{contestant})\\), where \\(\\text{contestant}\\) will be one of “Johnny” or “Louisa” or “Geoff” in our example.\nThere are many likelihood functions that might be relevant to the money-in-jar situation. There is one likelihood function for each possible amount of money in the jar. For instance, \\({\\cal L}_{\\$10}(\\text{contestant})\\) is relevant if there were ten dollars in the jar. Another likelihood function \\({\\cal L}_{\\$11.50}(\\text{contestant})\\) would be relevant if there were eleven dollars and fifty cents in the jar.\nThis notation of naming functions using a subscript can get awkward when there are a huge number of functions. For instance, for the money-in-jar contest there will be a likelihood function for $0.01, $0.02, $0.03, and all other possibilities such as $21.83 or \\(47.06\\). If we want to be able to refer to the whole set of likelihood functions, better to replace the dollar amount in the subscript with a symbol, say \\(m\\) for money. Then the whole set of likelihood functions potentially relevant to the contest would be written \\({\\cal L}_m(\\text{contestant})\\).\n\nThere is another style for notation that you may encounter in your future work. In the alternative style, for example, instead of \\({\\cal L}_m(\\text{contestant})\\) the likelihood function would be written \\({\\cal L}(\\text{contestant}\\, {\\mathbf |} m )\\). The vertical bar is pronounced “given” and is part of a notational system often used in probability calculations.\n\nSince the output of any likelihood function is a probability or a probability density depending on context, we know that the output will be a non-negative quantity.\nLikelihood functions provide the link between data and hypotheses. The idea is that when data become available, it is possible to choose the relevant likelihood function.\nTo illustrate, let’s return to the jar-of-money contest and the three competitors’ entries as shown in Figure 51.4. For convenience, that Figure is reproduced here:\n\n\n\n\n\n\n\n\nFigure 51.6: The contest entries shown in Figure 51.4.\n\n\n\n\n\nThe functions shown in the Figure are not likelihood functions. But we can use them to construct whatever likelihood function turns out to be relevant in the money-in-jar contest.\n\nIt is time to calculate who won the jar-of-coins contest! That is, we will calculate whose entry is best. The word “best” should remind you of optimization and indeed the winner of the contest will be the argmax of the relevant likelihood function. At this point, remember that the likelihood functions are \\({\\cal L}_m(\\text{contestant})\\), so the argmax will be one of the contestants!\nFirst, we need to pick the relevant likelihood function. Common sense tells us that you can only pick a winner when the jar has been opened and the money counted. That is, we need some data.\nHere’s the data: The officials have opened the jar and carefully counted the money. There was $32.14 in the jar. This tells us that the relevant likelihood function is \\({\\cal L}_{\\$32.14}(\\text{contestant})\\).\nThe output of \\({\\cal L}_{\\$32.14}(\\text{contestant})\\) is the probability density assigned by the contestant to the observed value $32.14. You can read this from ?fig-jar-functions2). For your convenience, the observation \\(32.14\\) has been annotated with a faint brown vertical line.\nHere’s a tabular version of \\({\\cal L}_{\\$32.14}(\\text{contestant})\\).\n\n\n\n\\(\\text{contestant}\\)\n\\({\\cal L}_{\\$32.14}(\\text{contestant})\\)\n\n\n\n\nJohnny\n0.000 per dollar\n\n\nLouisa\n0.010 per dollar\n\n\nGeoff\n0.066 per dollar\n\n\n\nIn statistics, likelihood functions are used to describe how to estimate a quantity given some data about the quantity. The techique is called maximum likelihood estimation: the estimate is the argmax of the likelihood function. For the coins-in-jar contest, the argmax is Geoff. Therefore, Geoff wins!\nIn the spirit of “Monday morning quarterbacking,” let’s look carefully at Johnny’s entry. If his bar-shaped probability density function were shifted just a little to the right, he would have won. This illustrates a weakness in Johnny’s logic in constructing his probability density function. The function indicates that he thought the probability of the amount being $23 was the same as being 30 dollars. In other words, he was uncertain to a considerable extent. But given this uncertainty, why would he insist that $30.01 is impossible (that is, has probability density 0 per dollar). Wouldn’t it make more sense to admit nonzero density for $30.01, and similarly for $30.02 and upward, with the density gradually decreasing with the amount of money. This is why, absent very specific knowledge about the circumstances, probability densities are so often framed as Gaussian distributions, as in Geoff’s entry.\n\nThe previous example is intended to give you an idea about what a likelihood function is. In that example, we use the calculus operator argmax to find the contest winner.\nLet’s turn now to another important use of likelihood functions: their role in the Bayesian inference process. The example concerns figuring out the risk of disease transmission.\n\nApplication area 51.1 —Functions suited to modeling risk.\n\n\n\n\n\n\n\nApplication area 51.1 Beliefs about risk\n\n\n\nConsider the situation in November 2019 at the start of the COVID-19 pandemic. At that time, there was almost no information about the illness or how it spreads. In the US and many other countries, most people assumed that the spread of illness outside its origin in Wuhan, China, would be prevented by standard public health measures such as testing, contact tracing, quarantine, and restrictions on international travel. Intuitively, most people translated this assumption into a sense that the personal risk of illness was small.\nIn communicating with the public about risk, it is common to present risk as a number: a probability. This is an adequate presentation only when we have a solid idea of the risk. To form a solid idea, we need evidence.\nBefore there is enough evidence responsibly to form a solid idea, it is best to present risk not as a probability but as a probability density function. To illustrate, Figure 51.7) shows three different examples of what such probability density functions might look like for vague, preliminary ideas of risk.\n\n\n\n\n\n\n\n\nFigure 51.7: Three different opinions about the risk of a disease.\n\n\n\n\n\nPanel (A) in Figure 51.7 is a strong statement that the risk is believed to be small. Even so, the density function is non-zero even for values of the risk near 100%. This is an honest admission that, as with COVID-19, something that we don’t know might be going on. In the case of COVID-19, what most people didn’t realize is 1) that the reported numbers were completely unrepresentative of the extent of spread, since most cases are asymptomatic and 2) that the illness can spread even by those who are asymptomatic. Epidemiologists and other public health workers knew enough from previous experience to be aware of their lack of knowledge about (1) and (2), but the rest of us, including many policy makers, didn’t even know what they didn’t know. The word “unk-unk” is sometimes used by engineers to refer to such an “unknown unknown”.\nPanel (B) says, “I have no idea!” This can often be an honest, useful appraisal of the situation. But experts who are honest in this way are often regarded by the public and policy makers as lacking credibility.\nPanel (C) expresses the belief that the risk might well be small but also might be large.\nAny of the three probability density functions would be reasonable statements about what we knew and didn’t know about COVID-19 at the very beginning of the pandemic, before there was much data. Such statements are called priors; summaries of what we know up to the present.\nIn Bayesian inference, as data become available we can revise or update the priors, giving a better informed description of the risk.\nFor COVID-19, data eventually came in many different forms: estimates of incubation periods, testing to determine what fraction of cases are asymptomatic, and so on.\nFor our presentation of Bayesian reasoning, we will consider a simplified situation where data come in only one form: screening tests for the illness. Imagine that you are conducting a contact-tracing study. Whenever a patient presents with COVID-19 symptoms and has a positive PCR test, that patient’s close contacts are given a screening test for COVID. The objective is to estimate how transmissible the virus is by figuring out what proportion of close contacts become infected.\nWe cannot know which of the three priors in Figure 51.7 is most appropriate. After all, until rich enough data become available, each prior is just an opinion. So we will repeat the update-with-data analysis for each of the three priors. If, in the end, the results from the three priors substantially agree, then we can conclude that the data is shaping the results, rather than the prior.\nThe unknown here is the risk \\(R\\) of transmission. We will denote the three priors as \\(\\text{prior}_A (R)\\), \\(\\text{prior}_B (R)\\), and \\(\\text{prior}_C (R)\\). But, in general, we will write \\(\\text{prior}(R)\\) to stand for any of those three specific priors.\n\n\nIn Bayesian inference, the prior represents the starting point for what we know (or, more precisely, “believe”) about the risk of transmission. It has the form of a relative density function. As data come in, we update our prior beliefs on the basis of the data.\nAfter we have updated our prior, our state of knowledge is called a posterior belief. Think of the prior as “pre-data” belief and the posterior as “post-data” belief. The posterior also has the form of a relative density function.\nThe formula for updating is called Bayes’ Rule: posterior is likelihood times prior. \\[\\text{posterior}(R) = {\\cal L}_\\text{data}(R) \\times \\text{prior}(R)\\ .\\] Recall that the output of a likelihood function is a non-negative quantity. Since the prior is a relative density function, it too is non-negative for all \\(R\\). Therefore the posterior will have a non-negative output and be a valid relative density function.\n\nMost texts prefer to define priors and posteriors as probability density functions rather than relative density functions. The only difference, of course, is the normalization. But that can be performed at any time, so to streamline the updating process, we will let posteriors and priors be relative density functions.\n\nNotice that the posterior has just one input, the parameter \\(R\\). That is because the \\(\\text{data}\\) is fixed by our observations: the posterior only makes sense once we have the data available to choose the relevant likelihood function.\nOur task now is to construct the appropriate likelihood function that reflects how the screening test works. To outline the process, let’s consider a group of 1000 people who are taking the screening test. If we knew the parameter \\(R\\), we could split those 1000 people into two groups: one group with the illness and one group without.\n\nWhole group of 1000, made up of\n\n1000 \\(R\\) with the illness\n1000 \\((1-R)\\) without the illness\n\n\nFor instance, if \\(R=0.2\\), then out of the whole group of 1000 people, 200 would have the illness and 800 would not.\nAfter taking the screening test, each person will have either a positive test result (we will write this “+”) or a negative test result (we will write “-”).\nto make sense of a screening test, you need to know two probabilities. These are:\n\nThe probability of a + test in a group of people with the disease. We will call this \\(p_d(+)\\).\nThe probability of a - test in a group of people without the disease. we will call this \\(p_h(-)\\).\n\nNote that the subscript indicates whether we are referring to the probability in the has-the-illness group (\\(p_d\\)) or in the no-illness (“healthy”) group (\\(p_h\\)).\nYou may know that the result of a screening test is not definitive. That is, a person with a \\(+\\) result may not have the illness. Likewise, a \\(-\\) result is no guarantee that the person does not have the illness. The word “screening” is meant to emphasize the imperfections of such tests. But often the imperfect test is the best we have available.\nAfter the screening test has been taken by the 1000 people in our example group, we can divide them further\n\nWhole group of 1000, made up of\n\n1000 \\(R\\) with the illness, made up of\n\n1000 \\(R\\ p_d(+)\\) who had a correct positive test result\n1000 \\(R\\ (1-p_d(+))\\) who had a negative result despite having the illness\n\n1000 \\((1-R)\\) without the illness, made up of\n\n1000 \\((1-R)\\ (1-p_h(-))\\) who had a positive test result despite being healthy\n1000 \\((1-R)\\ p_h(-)\\) who had a correct negative result.\n\n\n\n\nSuppose that \\(R=0.2\\), \\(p_d(+) = 80\\%\\), and \\(p_h(-) = 70\\%\\). Then the division would be:\n\nWhole group of 1000, made up of\n\n200 with the illness, of whom\n\n160 who got a correct \\(+\\) result\n40 who got a \\(-\\) result, despite having the illness\n\n800 without the illness, of whom\n\n240 who got a \\(+\\) result, despite not having the illness\n560 who got a correct \\(-\\) result.\n\n\n\nThere are two likelihood functions reflecting the two different possible test results: \\({\\cal L}_+ (R)\\) and \\({\\cal L}_- (R)\\). We will need to construct both of these functions since the test result is going to be \\(+\\) for some people and \\(-\\) for others.\nRecall now that each likelihood function is based on a hypothesis about the world. In this case, the hypothesis is a particular value for \\(R\\). Let’s look at the situation for the hypothesis that \\(R=0.2\\). We can figure out the values of both \\({\\cal L}_+ (R=0.2)\\) and \\({\\cal L}_- (R=0.2)\\) from the breakdown given in the above example.\n\\({\\cal L}_+ (R=0.2)\\) is the probability of observing the given data (\\(+\\)) in the hypothetical world where \\(R=0.2\\). Out of the 1000 people, 160 will get a correct test result \\(+\\) and 240 people will get a \\(+\\) despite not having the illness. Therefore, \\[{\\cal L}_+ (R=0.2) = \\frac{160+240}{1000} = 40\\%\\ .\\]\nSimilarly, \\({\\cal L}_- (R=0.2)\\) is the probability of observing the given data (\\(-\\)) in the hypothetical world where \\(R=0.2\\). In the above breakdown, altogether 600 people received a \\(-\\) result: 560 of these were indeed healthy and 40 had the illness but nonetheless got a \\(-\\) result. So, \\[{\\cal L}_- (R=0.2) = \\frac{40+560}{1000} = 60\\%\\ .\\]\n\nThe above example calculated the output of the likelihood function for both \\(+\\) and \\(-\\) results when \\(R=0.2\\). We can repeat the calculation for any other value of \\(R\\). The results, as you can confirm yourself, are\n\\[{\\cal L}_+(R) = p_d(+)\\ R + (1-p_h(-))\\ (1-R) \\] and\n\\[{\\cal L}_-(R) = (1-p_d(+))\\ R + p_h(-)\\, (1-R)\\ .\\] In a real-world situation, we would have to do some experiments to measure \\(p_h(-)\\) and \\(p_d(+)\\). For our example we will set \\(p_d(+) = 0.8\\) and \\(p_h(-) = 0.7\\).\nNow that we have constructed the likelihood functions for the two possible observations \\(+\\) and \\(-\\), we can use them to update the priors.\nSuppose our first observations are the results of screening tests on ten randomly selected individuals.\n\n\n\nSubject ID\nTest outcome\n\n\n\n\n4349A\n\\(+\\)\n\n\n7386A\n\\(-\\)\n\n\n6263E\n\\(+\\)\n\n\n5912C\n\\(-\\)\n\n\n7361C\n\\(-\\)\n\n\n9384C\n\\(-\\)\n\n\n6312A\n\\(-\\)\n\n\n3017C\n\\(+\\)\n\n\n1347B\n\\(-\\)\n\n\n9611D\n\\(-\\)\n\n\n\nTo summarize: Three \\(+\\) tests out of ten.\nAfter the first test outcome is available we can calculate the posterior: \\[\\text{posterior}_1 (R) = {\\cal L}_+(R) \\times \\text{prior(R)}\\ .\\] After the second test outcome, the new posterior is \\[\\text{posterior}_2 (R) = {\\cal L}_-(R) \\times {\\cal L}_+(R) \\times \\text{prior(R)}\\ .\\] And after the third (a \\(+\\) result!) it will be \\[\\text{posterior}_3 (R) = {\\cal L}_+(R) \\times {\\cal L}_-(R) \\times {\\cal L}_+(R) \\times \\text{prior(R)}\\ .\\] We continue on in this way through all ten rows of the data to get the posterior distribution after all 10 test results have been incorporated.\nFigure 51.9 shows the posterior after the 10 rows of data have been considered for each of the three priors from Figure 51.7).\n\n\n\n\n\n\n\n\nFigure 51.8: Posteriors (blue) after the first screening test, which was \\(+\\), for each of the priors in Figure 51.7. The prior itself is drawn in gray.\n\n\n\n\n\nWith just one row of data considered, the posteriors depend very much on the particular prior selected. This shouldn’t be a surprise; one test result from an imperfect screening test is not going to tell us much. Let’s process all of the\n\n\n\n\n\n\n\n\nFigure 51.9: Posteriors (blue) after the first ten rows of data, for each of the priors in Figure 51.7. The prior itself is drawn in gray.\n\n\n\n\n\nAfter the first 10 rows of data have been considered, the posteriors are similar despite the different priors.\n\n\n\n\n\n\n\n\nFigure 51.10: Posteriors (blue) after 100 subjects have been screen, with 30 \\(+\\) results.\n\n\n\n\n\nAs data accumulates, the priors become irrelevant; the knowledge about the risk of disease is being driven almost entirely by the data.\nRemarkably, even though 30% of the tests were positive, all the posteriors place almost all the probability density on transmission risks less than 20%. This is because the likelihood functions correctly take into account the imperfections of the screening test.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Probability and evidence</span>"
    ]
  },
  {
    "objectID": "Manifestations/B4-probability.html#footnotes",
    "href": "Manifestations/B4-probability.html#footnotes",
    "title": "51  Probability and evidence",
    "section": "",
    "text": "Numerical integrals from \\(-\\infty\\) to \\(\\infty\\) of functions that are zero almost everywhere are challenging. The computer has to figure out where, out of the whole number line, the function has non-zero output. We’ve given the computer a head start by using 0 in the limits of integration. This would not be a problem for the exponential or gaussian distribution, which are non-zero everywhere (for the gaussian) or for half the number line (for the exponential).↩︎",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Probability and evidence</span>"
    ]
  },
  {
    "objectID": "Manifestations/B4-future-value.html",
    "href": "Manifestations/B4-future-value.html",
    "title": "52  Present and future value",
    "section": "",
    "text": "52.1 Present value\nPeople and institutions often have to make decisions about undertakings where the costs and benefits are spread out over time. For instance, a person acquiring an automobile or home is confronted with a large initial outlay. The outlay can be financed by agreeing to pay amounts in the future, often over a span of many years.\nAnother example: Students today are acutely aware of climate change and the importance of taking preventive or mitigating actions such as discouraging fossil fuel production, investing in renewable sources of energy and the infrastructure for using them effectively, and exploring active measures such as carbon sequestration. The costs and benefits of such actions are spread out over decades, with the costs coming sooner than the benefits. Policy makers are often and perhaps correctly criticized for overvaluing present-day costs and undervaluing benefits that accrue mainly to successive generations. There are many analogous situations on a smaller scale, such as setting Social Security taxes and benefits or the problem of underfunded pension systems and the liability for pension payments deferred to future taxpayers.\nThe conventional mechanism for condensing an extended time stream of benefits and costs is called discounting. Discounting is based on the logic of financing expenditures via borrowing at interest. For example, credit cards are a familiar mechanism for financing purchases by delaying the payment of money until the future. An expense that is too large to bear is “carried” on a credit card so that it can be paid off as funds become available in the future. This incurs costs due to interest on the credit-card balance. Typical credit-card interest rates are 18-30% per year.\nAs notation, consider a time stream of income \\({\\cal M}(t)\\), as with the apartment building example. We will call \\({\\cal M}(t)\\) the nominal income stream with the idea that the money in \\({\\cal M}(t)\\) is to be counted at face value. “Face value” is the literal amount of the money. In the case of cash, a $20 bill has a face value of $20 regardless of whether it becomes available today or in 50 years. The word “nominal” refers to the “name” on the bill, for instance $20.\nThe big conceptual leap is to understand that the present value of income in a future time is less than the same amount of income at the present time. In other words, we discount future money compared to present money. For example, if we decide that an amount of money that becomes available 10 years into the future is worth only half as much as that same amount of money if it were available today, we would be implying a discounting to 50%. In comparison, if that money were available 20 years in the future, it would make sense to discount it by more strongly to, say, 25%.\nTo represent the discounting to present value as it might vary with the future time horizon, we multiply the nominal income stream \\({\\cal M}(t)\\) by a discounting function \\({\\cal D} (t)\\). The function \\({\\cal D}(t)\\, {\\cal M}(t)\\) gives the income stream as a present value rather than a nominal value. It is sensible to insist that \\({\\cal D} (t=0) \\equiv 1\\) which is merely to say that the present value of money available today is the same as the nominal value.\nThe net present value (NPV) of a nominal income stream \\({\\cal M}(t)\\) is simply the sum of the stream discounted to the present value. Since we are imagining that the income stream is a function of continuous time, the sum amounts to an integral: \\[\\text{NPV} \\int_\\text{now}^\\text{forever} {\\cal M}(t)\\ {\\cal D}(t)\\, dt\\ .\\]",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Present and future value</span>"
    ]
  },
  {
    "objectID": "Manifestations/B4-future-value.html#sec-discount-functions",
    "href": "Manifestations/B4-future-value.html#sec-discount-functions",
    "title": "52  Present and future value",
    "section": "52.2 Discounting functions",
    "text": "52.2 Discounting functions\nWhat should be the shape of the discounting function?\nRecall that the purpose of the discounting function is to help us make comparisons between different income streams, that is, between the various options available to an entrepreneur. Each individual can in principle have his or her own, personal discounting function, much as each voter is entirely free to weight the different attributes of the candidates when deciding whom to vote for. As a silly example, a person might decide that money that comes in on a Tuesday is lucky and therefore worth more than Thursday money. We won’t consider such personalized forms further and instead emphasize discounting functions that reflect more standard principles of finance and economics.\nAs a thought experiment, consider the net present value of an income stream, that is \\[\\text{NPV}_\\text{original} = \\int_0^\\infty {\\cal M}(t)\\ {\\cal D}(t)\\, dt\\ .\\] Imagine now that it has been proposed to delay the income stream by \\(T=10\\) years. This new, delayed income stream is \\({\\cal M}(t-T)\\) and also has a net present value: \\[\\text{NPV}_\\text{delayed} = \\int_0^\\infty {\\cal M}(t - T)\\ {\\cal D}(t)\\, dt\\ .\\] There are at least two other ways to compute \\(\\text{NPV}_\\text{delayed}\\) that many people would find intuitively reasonable:\n\nSimply discount the original NPV to account for it becoming available \\(T\\) years in the future, that is, \\[\\text{NPV}_\\text{delayed} = {\\cal D}(T)\\ \\text{NPV}_\\text{original} = \\int_0^\\infty {\\cal M}(t)\\ {\\cal D}(T)\\ {\\cal D}(t)\\, dt\\ \\]\nApply to \\({\\cal M}(t)\\) a discount that takes into account the \\(T\\)-year delay. That is:\n\\[\\text{NPV}_\\text{delayed} = \\int_0^\\infty {\\cal M}(t)\\ {\\cal D}(t + T)\\, dt\\ .\\] For (1) and (2) to be the same, we need to restrict the form of \\({\\cal D}_r(t)\\) so that \\[{\\cal D}(T)\\ {\\cal D}(t) = {\\cal D} (t+T)\\ .\\] The form of function that satisfies this restriction is the exponential, that is \\({\\cal D}(t) \\equiv e^{kt}\\).",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Present and future value</span>"
    ]
  },
  {
    "objectID": "Manifestations/B4-future-value.html#compound-interest",
    "href": "Manifestations/B4-future-value.html#compound-interest",
    "title": "52  Present and future value",
    "section": "52.3 Compound interest",
    "text": "52.3 Compound interest\nA more down-to-earth derivation of the form of \\({\\cal D}(t)\\) is to look at how financial transactions take place in the everyday world: borrowing at interest. Suppose that a bank proposes to lend you money at an interest rate of \\(r\\) per year. To receive from the bank one dollar now entails that you pay the bank \\(1+r\\) dollars at the end of the year.\nFor this proposition to be attractive to you, the present value of \\(1+r\\) dollars to be paid in one year must be less than or equal to the present value of one dollar today. In other words, \\[{\\cal D}_\\text{you}(1)\\ (1+r) \\leq 1\\ .\\] From the bank’s perspective, the present value of your payment of \\((1+r)\\) dollars in a year’s time must be greater than one dollar today. That is\n\\[1 \\leq {\\cal D}_\\text{bank}(1)\\ (1+r) \\ .\\] It is perfectly reasonable for you and the bank to have different discounting functions, just as it is perfectly legitimate for you and another voter to have different opinions about the candidates. It is convenient, though to imagine that the two discounting functions are the same, which will be the case if \\[{\\cal D}(1) = \\frac{1}{1+r}\\ .\\]\nIf you were to borrow money for two years, the bank would presumably want to charge more for the loan. A typical practice is to charge compound interest. Compound interest corresponds to treating the loan as having two phases: first, borrow one dollar for a year and owe \\(1+r\\) dollars at the end of that year. At that point, you will borrow \\((1+r)\\) dollars at the interest rate \\(r\\). At the end of year two you will owe \\((1+r) (1+r) = (1+r)^2\\) dollars. In general, if you were to borrow one dollar for \\(t\\) years you would owe \\((1+r)^t\\) dollars. In order for the loan to be attractive to both you and the bank, the discounted value of \\((1+r)^t\\) should be one dollar: \\[{\\cal D}(t) = \\frac{1}{(1+r)^t} = (1 + r)^{-t}\\ .\\] ::: {#thm-building-apartments2 style=“display: none;”} —Is the interest rate too high? ::: ::: {.callout-note icon=false data-latex=““} ## ?thm-building-apartments2 Building apartments (2)\nLet’s return to the entrepreneur considering the apartment-building project in Application area 52.1.\nThe entrepreneur goes to the bank with her business plan and financial forecasts. The bank proposes to lend money for the project at an interest rate of 7% per year.\n\n\n\n\n\n\n\n\nFigure 52.2: Blue curve: The apartment project income stream discounted at 7% per year. Black curve: The undiscounted income stream.\n\n\n\n\n\nFigure 52.2 shows the income stream discounted at 7% per year. The net present value of the income stream for the apartment project is the accumulation of the discounted income stream: \\[\\text{NPV}(7\\%) = \\int_0^{30} (1 + 0.07)^{-t} {\\cal M}(t) \\ dt \\ .\\] This works out to be negative $1,095,000. As things stand, the apartment building will be a money pit.\nThe entrepreneur responds to the bank’s offer with some business jargon. “I’ll have to go back to the drawing board, put the team’s heads together, and see how to get the numbers to work. I’ll circle back with you tomorrow.” This might involve reconsidering her model of the income stream. Perhaps she should have taken into account yearly rent increases. Maybe she can re-negotiate with the city about zoning height restrictions and build a 12-unit building, which will cost $4 million, lowering the cost per apartment?\n\n## Loading required namespace: cubature\n\n:::",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Present and future value</span>"
    ]
  },
  {
    "objectID": "Manifestations/B4-future-value.html#mortgages",
    "href": "Manifestations/B4-future-value.html#mortgages",
    "title": "52  Present and future value",
    "section": "52.4 Mortgages",
    "text": "52.4 Mortgages\nA mortgage is a form of loan where you pay back the borrowed amount at a steady rate, month by month. At the end of a specified period, called the term of the mortgage, your have completely discharged the debt.\nSuppose you decide to buy a product that costs \\(\\$\\cal P\\), say \\(\\cal P=\\) $10,000 for a used car. Let’s say you need the car for your new job, but you don’t have the money. So you borrow it.\nYour plan is to pay an amount \\(A\\) each month for 30 months—the next \\(2\\,\\small\\frac{1}{2}\\) years. What should that rate be?\nYou can put the purchase on your credit card at an interest rate of \\(r=2\\%\\) per month. This corresponds to \\(k = \\ln(1+r) = 0.0198\\) per month. The net present value of your payments over 30 months at a rate of \\(A\\) per month will be\n\\[\\int_0^{30} A {\\cal D}_r(t)\\, dt = \\int_0^{30} e^{-kt} A = -\\frac{A}{k} e^{-kt}\\left.{\\Large\\strut}\\right|_{t=0}^{30}\\\\ =- A \\left(\\strut \\frac{e^{-30k}}{k} - \\frac{e^{-0k}}{k} \\right)\\\\ = - A\\left(\\strut27.88 - 50.50\\right) = 22.62 A\\]\nThe loan will be in balance if the net present value of the payments is the same as the amount \\({\\cal P}\\) you borrowed. For this 30-month loan, this amounts to \\[22.62 A = {\\cal P}\\ ,\\] from which you can calculate your monthly payment \\({\\cal A}\\). For the $10,000 car loan, your monthly payment will be $10,000/22.62 or $442.09.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Present and future value</span>"
    ]
  },
  {
    "objectID": "Manifestations/B4-mechanics.html",
    "href": "Manifestations/B4-mechanics.html",
    "title": "53  Mechanics",
    "section": "",
    "text": "53.1 Work\n“Work” is a familiar, everyday concept, but a nuanced one; one person’s work can be another person’s play. In mechanics, work has a much more specific meaning stemming from the study of simple machines. A lever, for instance, can be used to move an object that is otherwise too heavy to handle. It still takes toil and effort to move the object, but the effort is eased by the mechanics of the lever.\nOur intuitive sense of work is perhaps rooted in physiology: effort, fatigue, muscle pain. For instance, it takes work to pick up a heavy object, but it is also work to hold the object steady even without moving it. Generations of thinking about machines has brought us to a different notion of work that does not involve human subjectivity. In mechanics, holding an object steady, no matter how heavy, does not involve work. Although a human tasked to hold a heavy load will become exhausted, the same duty can be accomplished by placing the load on a table, completely eliminating the effort. In mechanics, work and motion go hand in hand; without motion there is no mechanical work.\nThe table holding the heavy load does no work. Work is done only when the load is moved, and the amount of work depends on how the load is moved. For instance, moving a block along level ground involves a lot of work, but pulling a cart filled with blocks can be almost effortless. In mechanics, work combines both the amount of motion and the force needed to accomplish the motion.\nConsider, for instance, the work involved in lifting a mass \\(m\\) to table height \\(h\\).\nknitr::include_graphics(\"www/mass-on-table.png\")\nThe lifting is accomplished by applying an upward force to counter the force of gravity. The gravitational force on the mass is \\(m g\\), where \\(g\\) is the instantaneous acceleration of an object released to fall freely (about 9.8 m/s2 near the Earth’s surface). The distance traveled is \\(h\\). So the work performed on the mass is \\(m g h\\).\nNotice that the mechanical work has nothing to do with the speed with which the mass is moved up to the table. Lift it fast or lift it slow, it amounts to the same mechanical work. (Of course, to human perception, lifting an object very slowly up to table height involves more effort than snapping it up quickly. But human effort is only peripherally related to mechanical work.)\nLet’s introduce a machine to the situation in the form of a ramp or a pulley. The purpose of the machine is to ease human labor by changing the strength or direction of forces. You can perhaps intuit that rolling the mass up the ramp will be an easier task than lifting it. How so?\nknitr::include_graphics(\"www/mass-up-ramp.png\")\nThe ramp can be seen as a sort of partial table. The ramp does most of what’s needed to hold the mass up. To keep the mass in place on the ramp the human worker need only supply a modest additional force parallel to the ramp surface. Calculating that modest additional force can be accomplished by a basic mathematical technique in mechanics: decomposing a vector.\nYou encountered vectors (in Section 24.003) in the context of the gradient vector of a function, say, \\(f(x,y)\\). At any given input \\((x,y)\\) the gradient vector, written \\(\\nabla f(x,y)\\), points in the steepest uphill direction of the function \\(f(x,y)\\). Recall that the gradient vector was written as a set of values; the partial derivative of \\(f()\\) with respect to each of its inputs in turn. That is, \\[\\nabla f(x,y) = \\left({\\large\\strut} \\partial_x f(x,y),\\ \\  \\partial_y f(x,y)\\right)\\ .\\] In this representation, the vector \\(\\nabla f(x, y)\\) is decomposed into two components: \\(\\partial_x f(x,y)\\) and \\(\\partial_y f(x,y)\\).\nTo decompose the vector of gravitational forces, we can place a coordinate grid over the gravity vector. In Figure 53.1 this grid has been arranged so that one cardinal direction is aligned with the ramp itself and the other is perpendicular—that is, “normal”—to the ramp. Merely by noting the coordinates of the gravitational vector in the coordinate grid, we decompose that vector into two components, one along the surface of the ramp and the other perpendicular to the ramp.\nFigure 53.1: Decomposing the vector of gravitational force into two perpendicular components, one tangent to the ramp and the other perpendicular to it. For clarity, the right triangle of decomposition is shown twice, once without the grid.\nWe return to the idea of vector decomposition in much more detail in Block 5 of this course; it has a major (though perhaps unexpected) role to play in fitting models to data. But for now, we will simply examine the right triangle in Figure 53.1. In that right triangle, the gravitational force vector \\(F_{gravity} = m g\\) is the hypotenuse. The component tangential to the ramp is \\(m \\sin(\\theta) g\\). The worker pushing the mass up the ramp need provide only tangential component of force which is smaller than the force imposed on the worker picking up the mass without a ramp. Thus human effort is reduced by the machine.\nWhat about the mechanical work? Is that also reduced? Remember that mechanical work is the product of force times distance. The force has been reduced to \\(m \\sin(\\theta) g\\), but the distance \\(D_{ramp}\\) along the ramp is much longer than the distance \\(h\\) from floor to table top.\nAgain, referring to the ramp itself as a right triangle, you can see that \\(D_{ramp}\\sin(\\theta) = h\\) or, \\(D_{ramp} = h / \\sin(\\theta)\\). The total mechanical work, the product of applied force times distance moved is \\[m \\sin(\\theta) g \\times D_{ramp} = m \\sin(\\theta) g \\times \\frac{h}{\\sin(\\theta)} = m g h\\ .\\] The ramp does nothing to reduce the mechanical work needed to lift the mass!\nWe usually think of ramps as an inclined plane. But, from Blocks 1 to 3 we have the tools to figure out the work for a (smooth) ramp with any shape at all. We will do this not because odd-shaped ramps are encountered frequently, but to provide an example in a relatively familiar setting of some techniques we will use elsewhere in this chapter.\nThe ramp we have in mind has a surface whose height \\(f(x)\\) is zero at the foot (\\(x=a\\)) and reaches \\(f(x=b) = h\\) where it joins the table.\nThe slope of the ramp at any location \\(x\\) is, as you know, \\(\\partial_x f(x)\\). It is helpful to convert this rise/run formulation of slope into the slope-angle form we used to study the simple ramp. As you can see from the diagram, which zooms in on one place on the ramp, rise over run amounts to \\(L\\sin(\\theta) / L\\cos(\\theta) = \\partial_x f(x) = \\tan(\\theta)\\), with the result: \\[\\theta = \\arctan({\\large\\strut}\\partial_x f(x))\\ .\\] Consequently, the force that needs to be applied parallel to the ramp’s surface is \\(m \\sin(\\arctan(\\partial_x f(x))) g = m \\sin(\\theta) g\\). To find the work done in pushing the mass an infinitesimal distance along the ramp we need to know the instantaneous length of the ramp. This is potentially confusing to the reader since we’ve already said that the distance is infinitesimal. As you know, infinitesimal is different from zero. We will write \\(dx\\) as an infinitesimal increment along the floor, but the zoomed-in length \\(dL\\) of the corresponding part of the ramp is the hypotenuse of a right triangle where one leg has length \\(dx\\) and the other leg has length \\(\\partial_x f(x) dx\\): slope times distance.\nThe hypotenuse of the infinitesimal segment of the ramp has length \\(dL = \\sqrt{\\strut dx + \\partial_x f(x) dx}\\), or \\(dL = \\sqrt{\\strut 1 + \\partial_x f(x)}\\ dx\\). Things are a bit simpler if we write \\(dL\\) in terms of the slope angle \\(\\theta(x)\\). Since \\(dx = \\cos(\\theta(x)) dL\\), we know \\(dL = dx/\\cos(\\theta(x))\\). Consequently the infinitesimal of work is \\[dW \\ = \\ m g \\frac{\\sin(\\theta(x))}{\\cos(\\theta(x))}\\ dx\\  = \\ m g \\tan(\\theta(x)) dx \\ .\\]\nThe total work is the accumulation of \\(dW\\) over the extent of the ramp. In other words, \\[\\int_a^b m g \\tan(\\theta(x))\\ dx\\ = \\ \\int_a^b m g \\tan(\\arctan(\\partial_x f(x)))\\ dx = \\int_a^b m g \\partial_x f(x) dx\\ ,\\] where we’ve used the formula \\(\\theta(x) = \\arctan(\\partial_x f(x))\\). From the “fundamental theorem of calculus” we know that\n\\[\\int_a^b m g\\ \\partial_x f(x)\\ dx \\ = \\ \\left.m g \\ f(x){\\Large\\strut}\\right|_a^b = mg \\left[\\strut f(b) - f(a)\\right] = mg h\\ .\\]\nWhat’s remarkable is that pushing the mass up the \\(f(x)\\)-shaped ramp involves an amount of work, \\(m g h\\), that does not depend on \\(f(x)\\), only on \\(f(b) - f(a)\\), the net height comprised by the ramp.\nWe haven’t yet said what this notion of work is good for and we’ve given no detailed justification for the definition of mechanical work as force times distance. You could imagine a dictatorial authority deciding to measure work as the square-root of force times distance squared. But … that particular measure is not going to make sense if we think about the dimension of the quantity. Force has dimension [force] = M L T-2. Square root of force times length squared would have dimension [sqrt(force) \\(\\times\\) length-squared] = M1/2 L5/2 T-2. The non-integer exponents mean that this is not a legitimate physical quantity.\nThe dimension of force-times-length are straightforward: [force \\(\\times\\) length] = M L2 T-2, that is, energy. The particular definition of work as force times length will make sense in the context of a more comprehensive mechanical theory of energy. The significance of energy itself is that, as a fundamental proposition of physics, the various forms of energy are interchangeable but conserved; energy is neither created nor destroyed, just moved around from one form to another and one place to another.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Mechanics</span>"
    ]
  },
  {
    "objectID": "Manifestations/B4-mechanics.html#work",
    "href": "Manifestations/B4-mechanics.html#work",
    "title": "53  Mechanics",
    "section": "",
    "text": "Work is force times displacement.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNear the surface of the Earth, gravitational acceleration is approximately constant regardless of latitude or longitude. But gravity varies with distance \\(r\\) from the Earth’s center. Newton’s law of universal gravitation gives the force on an object of mass \\(m\\) due to the Earth’s gravity as \\[F = \\frac{m M_e G}{r^2}\\] where \\(M_e = 5.972 \\times 10^{24}\\) kg is the mass of the Earth and \\(G = 6.674 \\times 10^{-11}\\) N m2 / kg2 is the universal gravitational constant. The Earth’s radius is roughly \\(6,370,000\\) m, so the force on a 1 kg object near the surface of the Earth is \\(F = 1 \\text{kg} (5.972 \\times 10^{24} \\text{kg}) (6.674 \\times 10^{-11})/ (6.37 \\times 10^6 \\text{m})^2\\) N m2 kg-2. Carrying out the arithmetic and consolidating the units gives \\[F = 9.823 N\\] for the 1 kg object.\nSuppose we want to lift the 1 kg object from the Earth’s surface to 10000 km away, that is, to a distance of 1,6370,000 m from the center of the Earth. For the purpose of the example, we will ignore the gravitational force exerted by the Sun, Moon, planets, and other galaxies, etc. The work performed in the lifting is\n\\[\\int_{6.47\\times 10^6}^{16.47\\times 10^6} \\frac{1 \\text{kg}\\ M_e\\ G}{r^2}\\ dr = -\\left.  {\\Large\\strut}\\frac{1 \\text{kg}\\ M_e\\ G}{r}\\right|_{6.47\\times 10^6}^{16.47\\times 10^6} \\\\\\ \\\\\\ \\\\=\n-\\ 3.986 \\times 10^{14}\\left[\\strut \\frac{1}{16.47 \\times 10^6} - \\frac{1}{6.47 \\times 10^6}\\right] \\text{N m} \\\\\\ \\\\\\ \\\\= 37,405,840\\ \\text{J}.\\]\nA Newton-meter (N m) is also known as a Joule (J), a unit of energy. With 37,000,000 J, you could toast about 2000 pieces of bread. (A toaster uses about 300 W of power and takes about 60 seconds to process a slice of bread. \\(\\text{300 W} \\times \\text{60 s} = 18,000 \\text{J}\\).",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Mechanics</span>"
    ]
  },
  {
    "objectID": "Manifestations/B4-mechanics.html#energy",
    "href": "Manifestations/B4-mechanics.html#energy",
    "title": "53  Mechanics",
    "section": "53.2 Energy",
    "text": "53.2 Energy\nMechanical work, as discussed in the previous section, is a form of energy. When we lift a object, we put energy into the object. But we cannot say from examining the object how much work was done to place it on the table. The amount of work depends on how the object came to be on the table: lifted from the floor (positive work; force is positive upward, displacement is also positive upward) or perhaps lowered from a helicopter (negative work: force exerted by the cable is positive upward but the displacement is downward, therefore negative). We might call the work energy latent, the word meaning “unobservable,” “hidden,” “concealed,” “dormant.” To have an operational meaning, the work-energy that we assign to an object at rest must be with respect to some “ground state.” A convenient ground state here is to imagine the object resting on the ground. The assigned energy will then be the work that would have to be performed to raise the object to table height. Once at table height, the energy is again latent.\nHow then to measure the work energy that is latent in the object resting on the table? The idea is to return the object to its ground state, which we could do by lowering it—a negative displacement—to the ground, measuring the force needed to support the object (upward, so positive) and multiplying this by the displacement.\nAnother idea for measuring the latent energy is to let the object fall freely back toward its ground state and see what changes about the object. Perhaps you have already caught on to what will happen: the object’s speed increases steadily until the instant before it hits the ground.\n“Latent” is an apt but unusual word to express the energy imbued in the object resting on the table. We might equally say that the energy is “associated with position (at the height the table),” or we could call it “gravitational energy.” The term that is generally used is a near synonym of “latent.” We call the energy of the stationary object on the table potential energy. More precisely it can be called gravitational potential energy to distinguish it from the potential energy created by other forms of work, for instance pulling apart magnets or electric charges or compressing a gas into a cylinder.\nThere is also a form of energy associated with motion. We could call this “energy of motion,” but the conventional term is kinetic energy. (A dictionary definition of “kinetic” is “relating to or resulting from motion.” so we might as well say simply that kinetic energy is “energy relating to motion.)\nVelocity is a good way to observe motion. We can use dimensional analysis to anticipate how velocity and kinetic energy are related. Recall that energy has dimension M L2 T-2 and velocity has dimension L/T. Consequently, if an object’s kinetic energy at any instant stems from its mass and its velocity, then the energy must be mass times velocity squared, perhaps multiplied by a scalar, that is: \\[E_{kinetic} = \\alpha\\, m v^2\\ .\\]\nTo find the scalar \\(\\alpha\\), we can use calculus and accumulation. We know that the acceleration of a free-falling object due to gravity is \\(-g\\) (where the negative sign reflects the downward direction). Starting from rest (that is zero velocity so zero kinetic energy) the newly released mass will have a velocity that is the accumulated acceleration over time. In other words: \\[v(t) = \\int_0^t - g\\ dt = -\\left.g \\ t{\\large\\strut}\\right|_0^t = -g\\ t\\ .\\] Correspondingly, the position at time \\(t\\) will be the accumulated velocity: \\[x(t) = x(t=0) + \\int_0^t v(t) dt \\\\ =\nh  + \\int_0^t -g\\ t\\ dt \\\\\n= h - \\frac{1}{2} \\left.g\\ t^2{\\Large\\strut}\\right|_0^t \\ \\ =\\ \\  h - \\frac{1}{2} g\\ t^2 \\ .\\] The mass reaches the ground at time \\(t_g\\) such that \\(h - \\frac{1}{2} g\\ t_g^2 = 0\\). Solving this for \\(t_g\\) gives \\(t_g = \\sqrt{\\strut 2 h/g}\\).\nNow that we know the time when the object reaches its ground state, we can calculate the velocity at that instant: \\[v(t_g) = -g\\ t_g = - g\\ \\sqrt{\\strut 2 h / g} = - \\sqrt{\\strut 2 g h}\\] As the object reaches its ground state, its gravitation potential energy is zero (because it is at the ground state) and, since total energy is conserved, the kinetic energy will be the same size as the potential energy at \\(t=0\\) when the object was released from the table, that is \\[E_{kinetic}(t_g) = \\alpha\\ m\\ v(t_g)^2 =\n= \\alpha\\ m \\left(\\sqrt{\\strut 2 g h\\ }\\ \\right)^2 = \\\\\\ \\\\2\\, \\alpha\\, m\\, g\\, h\\  = m\\, g\\, h = E_{potential}(t=0)\\]\nSolving \\(2 \\alpha\\ m\\,g\\,h = m\\,g\\,h\\) gives \\(\\alpha = \\frac{1}{2}\\). Thus, the kinetic energy as a function of mass \\(m\\) and velocity \\(v\\) is \\(\\frac{1}{2} m\\, v^2\\).\n\nIn the previous section, we calculated the potential energy of a 1 kg object at an altitude of 10,000 km above the Earth’s surface: 37,405,840 J. How fast would the 1 kg object need to be moving to have this much kinetic energy?\n\\[\\frac{1}{2} (1 \\text{kg}) v^2 = 37,\\!405,\\!840 \\text{J} = 37,\\!405,\\!840 \\ \\text{kg}\\ \\text{m}^2\\ \\text{t}^{-2}\\]\nSolving for \\(v\\) we get \\(v^2 = 2 \\times 37,\\!405,\\!840 \\text{kg}\\ \\text{m}^2\\ \\text{t}^{-2}\\ \\text{kg}^{-1}\\) or \\[v = 8649.4\\ \\text{m}/\\text{s}\\ ,\\] about eight-and-a-half kilometers per second.\n\n\nApplication area 53.1 —Mechanical work always involves movement, even when it doesn’t seem like it.\n\n\n\n\n\n\n\nApplication area 53.1 Work without movement?\n\n\n\nFigure 53.2 shows a simple exercise: holding a dumbbell out horizontally.\n\n\n\n\n\n\nFigure 53.2: Is he working even when the barbells are held still? Of course! Photo source\n\n\n\nAs anyone who does this exercise can tell you, even when there is no movement of the dumbbell, there is a strong sense of work being done. Your muscle fatigues and, for most people, the dumbbells can be held in place for only a short time.\nWe’ve said that mechanical work always involves motion; no motion, no work. So how come the exercise feels like work even though the hands do not move?\nTo perform the exercise, you contract the muscles of the shoulder and upper arm. There is no skeletal joint that can be locked in place (unlike, say, the knee). It is only the muscle force that holds the arms in place.\nOn the size scale that we normally perceive, it can appear that nothing is moving during the exercise. But zoom in to the molecular scale to see the action by which force is generated by muscle. The functional unit of muscle force involves two proteins, actin and myosin, that interact in a complicated way. The animation (from the online textbook by Michael D. Mann, The Nervous System in Action, chapter 14) shows the situation. The “head” of a myosin unit (red) acts like an oar. It attaches to a site on the actin molecule (orange) causing the head to contract and pull on the actin. Once contracted, a molecule of ATP (green sphere) binds to the myosin, releasing the head and preparing it for another stroke. ATP is an organic molecule that serves as a primary energy carrier and is found in all known forms of life. Transformation of ATP to ADP releases the energy. The ADP is then cycled, though other metabolic processes, back into ATP. This happens rapidly. Humans recycle approximately their own body weight in ATP each day.\n\n\n\n\n\n\nFigure 53.3: Animation of the generation of force by the interaction of actin and myosin, from The Nervous System in Action.\n\n\n\nWhen muscle is under tension, the actin can slip back in between strokes of the myosin head. Thus, a constant-length muscle in tension on a macroscopic scale is steadily consuming energy, in much the same way as an oarsman on an anchored boat can do work via the movement of oars against the water even when the boat itself is not moving.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Mechanics</span>"
    ]
  },
  {
    "objectID": "Manifestations/B4-mechanics.html#momentum",
    "href": "Manifestations/B4-mechanics.html#momentum",
    "title": "53  Mechanics",
    "section": "53.3 Momentum",
    "text": "53.3 Momentum\nIn the previous sections we looked at force \\(\\times\\) distance. Dimensional analysis showed that [force \\(\\times\\) distance] = energy and, in the setting of lifting an object and letting it fall back toward its ground state, we traced out the conversion of the energy of position (“potential energy”) into the energy of velocity (“kinetic energy”).\nNow consider a somewhat different quantity: force \\(\\times\\) time. Dimensional analysis gives \\[\\underbrace{M^1 L^1 \\ T^{-2}}_\\text{[force]}\\  \\times\\ \\underbrace{T}_\\text{[time]} = \\underbrace{M^1}_\\text{[mass]} \\underbrace{L^1 T^{-1}}_\\text{[velocity]} \\] The product of force times time is dimensionally equivalent to the product of mass times velocity. The quantity is called momentum. Newton’s second law of motion, often written in terms of acceleration, \\(F = m a\\), is more fundamentally written in terms of momentum: \\(F = \\partial_t\\, m\\, v\\). The conservation of momentum refers to the situation when outside forces on a system are nil. In such case, momentum of the system does not change with time; momentum is constant or “conserved.”\nAn example of such a system is a deep-space probe, sufficiently far from other matter that gravitational force is negligible. to speed up or slow down (or turn), the probe is made to throw out fast moving molecules of burnt fuel. These particles have “new” momentum, but since momentum of the whole system is conserved, the body of the probe gains “new” momentum in the opposite direction. This is the operating principle of the rocket engine.\n\n\n\n\n\n\n\n\nFigure 53.4: Left: A turbojet engine uses air for combustion and emits a relatively low amount of mass at high velocity. Right: A turbofan engine uses a fan blade (1) to convert some of the combustion energy into a large mass of relatively slow velocity, unburnt air.\n\n\n\n\n\n\n\n\n\n\n\nFigure 53.5: Left: A turbojet engine uses air for combustion and emits a relatively low amount of mass at high velocity. Right: A turbofan engine uses a fan blade (1) to convert some of the combustion energy into a large mass of relatively slow velocity, unburnt air.\n\n\n\n\n\nAircraft jet engines work in a similar matter, burning fuel to create energy. Whereas the force generated by a rocket engine is entirely produced by the newly created momentum of the burnt fuel, aircraft engines have an additional material to work with: air. The earliest jet engines, turbojet engines, were small in diameter, bringing in air mainly as a fuel for combustion. (?fig-jet-engines (left)1 Today’s more efficient engines are large diameter: turbofan engines. (?fig-jet-engines (right)2) In addition to using air for combustion, they use large fan blades to convert the energy of combustion into a large mass of relatively slowly moving, uncombusted air. This moving air carries momentum; more than that contained in the fast moving particles generated directly through combustion.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Mechanics</span>"
    ]
  },
  {
    "objectID": "Manifestations/B4-mechanics.html#sec-center-of-mass",
    "href": "Manifestations/B4-mechanics.html#sec-center-of-mass",
    "title": "53  Mechanics",
    "section": "53.4 Center of mass",
    "text": "53.4 Center of mass\nIn considering a physical object of extended shape, it can be a great simplification to be able to treat the whole extended object as if it were a simple point object at a single location. For instance, Figure 53.6 imagines a space probe (orange dot) coasting through the edge of a galaxy.\n\n\n\n\n\n\n\n\nFigure 53.6: An imagined space probe (orange dot) on the outer edges of a galaxy.\n\n\n\n\n\nWhat is the gravitational attraction of the galaxy on the probe? One way to find this is by adding up the individual gravitational attractions of the individual stars. Another is to find the center of mass of the galaxy and calculate the force as if all the mass were at that point. The two calculations give the same answer.\nFor the galaxy, the center of mass is located at a point \\((\\bar{x},\\bar{y})\\) where \\[\\bar{x} \\equiv \\sum_\\text{galaxy} m_i x_i\\ \\ \\ \\text{and}\\ \\ \\ \\ \\bar{y} \\equiv \\sum_\\text{galaxy} m_i y_i\\]\n\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nFigure 53.7: An irregular shape used in the example. The \\((x,y)\\) coordinates of closely spaced points on the boundary are available as Blob1 in the sandbox software.\n\n\n\n\n\nFor a continuous shape, such as in Figure 53.7 (left) we can describe the center-of-mass calculation as an accumulation of the mass-density function \\(\\rho(x, y)\\) over the entire shape \\(S\\). The mass of the object is the accumulation of mass-density itself \\[M = \\int_\\text{S} \\rho(x,y)\\ d\\text{S}\\] while the components of the center of mass are the accumulation of \\(x\\ \\rho(x,y)\\) and \\(y\\ \\rho(x,y)\\), that is: \\[\n\\bar{x} = \\int_\\text{S} x\\ \\rho(x,y)\\ d\\text{S} / M\\\\\n\\bar{y} = \\int_\\text{S} y\\ \\rho(x,y)\\ d\\text{S} / M\n\\] where \\(S\\) refers to the whole object and \\(d\\)S is a differential of the object, that is, a tiny piece of the object.\nThere are many ways to split an object up into differentials so that they can be accumulated to give the whole integral. One simple way, shown in Figure 53.7 (right), is to divide the object into a set of discrete, non-overlapping, adjacent rectangles (or cubes for a three-dimensional object). Then, as with adding up the stars, just add up \\(x \\rho(x, y) d\\)S or \\(y \\rho(x,y) d\\)S contained in each of the rectangular \\(d\\)A regions. For the rectangle located at $(x_i, y_i), the mass \\(m_i\\) will be \\(m_i = \\rho(x_i, y_i) d\\)S: density times area of each rectangle. This turns the integrals in Eq. @ref(eq:cm-integral) into a sum:\n\\[\\bar{x} \\approx \\sum_\\text{rectangles} m_i x_i/ M\\ \\ \\ \\text{and}\\ \\ \\ \\ \\bar{y} \\approx \\sum_\\text{rectangles} m_i y_i / M\\] where \\[M = \\sum_\\text{rectangles} m_i\\ .\\]\n\n\n\n\n\n\n\n\nFigure 53.8: A continuous shape can be approximated by a set of rectangles within the borders of the shape. Integrating over the shape is a matter of adding up across all of the rectangles the relevant quantity for each rectangle.\n\n\n\n\n\nFor the center of mass calculation, the relevant quantity for \\(\\bar{x}\\) for each rectangle is the mass times the \\(x\\)-position. Similarly, for \\(\\bar{y}\\) the relevant quanty is the mass times the \\(y\\)-position.\n\n\n\n\n\n\n\n\nFigure 53.9: For the \\(y\\)-component of the center of mass (left panel), the \\(x\\)-coordinate of each rectangle is irrelevant. It is as if all the rectangles were moved to \\(x=0\\). Similarly for the \\(x\\)-component of the center of mass (right panel).\n\n\n\n\n\n\nCompute the center of mass of the object Blob1 shown in Figure 53.7, assuming the mass-density \\(\\rho(x,y) = 10\\).\nThe mass of the object is \\[M = \\int_\\text{Blob1} \\rho(x, y)\\, dA\\] The \\(x\\)-component of the center of mass is\n\\[\\bar{x} = \\int_\\text{Blob1} x \\rho(x, y)\\, dA / M\\] and similarly for \\(\\bar{y}\\).\nTo find the center of mass, we first need to know the total mass of the object. We will carry out the calculation by dividing the object into a series of rectangles, computing the mass of each rectangle, then adding together the masses. The R/mosaic function box_set() takes as input the density function, a data frame with points on the boundary of the object, and a size for the boxes, which we will set to \\(dx=0.1\\).\n\nBoxes &lt;- box_set(10 ~ x + y, Blob1, dx=0.1)\n\nGive this command in a sandbox and look at the resulting data frame Boxes. Each row is one box. The x and y columns give the location of the center of that box, dx and dy are the lengths of the box sides in the \\(x\\) and \\(y\\) directions. The value ofthe function being accumulated is in the column labelled .output. column dA gives the area of each box (which is simply \\(dA = dx\\, dy\\)).\nAs the notation \\[\\int_\\text{Blob1} \\rho(x, y) dx dy\\] suggests, to accumulate the results for the individual boxes we just multiply the .output. by dA and sum.\n\nmass &lt;- with(Boxes, sum(.output. * dA))\n\n\n## [1] 67.3\n\nComputing the \\(x\\)-component of the center of mass, \\(\\bar{x}\\), is much the same but now the function being integrated is \\(x \\rho(x,y)\\) instead of just \\(\\rho(x,y)\\):\n\nBoxes2 &lt;- box_set(10*x ~ x + y, Blob1, dx=0.1)\nxbar &lt;- with(Boxes2, sum(.output. * dA)) / mass\n\n\n## [1] -0.1543015\n\nThe \\(y\\) component of the center of mass, \\(\\bar{y}\\) is computed almost identically, but substituting 10*y ~ x & y as the function to be integrated. In the next line, we will tell box_set() to do the summation over all the boxes directly, instead of our having to do it with the with(..., sum(.output. * dA)) command.\n\nybar &lt;- box_set(10*y ~ x + y, Blob1, dx=0.1, sum=TRUE) / mass\n\n\n## [1] -0.2371817\n\n\nRecall that the summation over the boxes provides an approximation to the integral. The quality of the approximation depends on the boxes being small enough. It is responsible to check the result by using smaller box size:\n\nybar &lt;- box_set(10*y ~ x + y, Blob1, dx = 0.01, sum=TRUE) / mass\nybar\n## [1] -0.2323201\nbox_set(10*y ~ x + y, Blob1, dx = 0.001, sum=TRUE) / mass\n## [1] -0.2322872\n\nFrom this, we conclude that a box size dx = 0.01 gives 4 digits precision, but dx = 0.1 was not small enough.\nRepeat the calculation for \\(\\bar{x}\\) to get the same precision:\n\nxbar  &lt;- box_set(10*x ~ x + y, Blob1, dx = 0.01, sum=TRUE) / mass\n\n\n## [1] -0.1513999\n\n\nThere is more than one way to describe the perimeter of an object, and the manner of integration has to be selected to match the description.\nConsider this shape that might be the design of a panel in a large sculpture. The panel will be cut out of 3mm thick sheet aluminum \\(x\\) and \\(y\\) are given in meters. In order for the panel to be balanced, it will be mounted at its center of mass\n\n\n\n\n\n\n\n\n\nFigure 53.10: A panel to locate center of mass\n\n\n\n\nThe shape is defined by two functions, \\(f(x)\\) and \\(g(x)\\), one of which sets the top edge and the other the bottom edge. The side edges are defined by the leftmost and rightmost values of \\(x\\). Here, that is \\(x_\\text{left} = - 1.5\\) and \\(x_\\text{right} = 4.0\\).\nThe area of the object can be found using the integration techniques from Block 3. For the purpose of finding the center of mass, we need to calculate the mass of the object. For 3mm sheet aluminum the density is about 8.1 kg/m2. Here, that is simply \\[\\text{mass} = \\int_{-1.5}^{4.0} 8.1 \\left[\\strut f(x) - g(x)\\right] dx \\approx 880 \\text{kg}\\ .\\]\nWhat about the center of mass? Because the shape is not described as a set of boxes, as we did earlier in this section, we need a way to perform the accumulation that uses only the information in the functions.\nThe differential \\(8.1 \\left[\\strut f(x) - g(x)\\right] dx\\) gives the mass of each vertical slice of the object, several of which are shown in \\(\\color{magenta}{\\text{magenta}}\\) in ?fig-cm-fun-diff.The tops and bottoms of those slices don’t align exactly with the boundaries of the object. That is because we’ve drawn them at a finite width so that you can see them. But the actual differentials being accumulated will have negligible width, and so will fit exactly.\n\n\n\n\n\n\n\n\nFigure 53.11: The panel can be divided into vertical slices, a few of which are shown here. The vertical mid-point of each slice is marked with a blue dash. Accumulating the slices’ mid-point coordinates times the slices’ mass, and dividing by the panel’s mass, gives the \\(y\\)-component of the center of mass.\n\n\n\n\n\nThe horizontal positions of the vertical slices are given by \\(x\\). The \\(x\\)-component of the center of mass of each individual vertical slice will be \\[x \\left[\\strut \\text{top}(x) - \\text{bottom}(x)\\right] dx\\ .\\] The center of mass of the entire object will be the accumulation \\[\\bar{x}= \\frac{1}{\\text{mass}}\\int_{-1.5}^{4.0} x\\ \\text{density} \\left[\\strut \\text{top}(x) - \\text{bottom}(x)\\right] dx \\approx 1.72 \\text{m}\\ .\\]\nFinding the \\(y\\)-component of the center of mass could be done in a similar way, by constructing horizontal slices. However, we would have to calculate the functions \\(\\text{left}(y)\\) and \\(\\text{right}(y)\\) that bound each slice.\nAn easier way is to find the vertical center of each slice. For a slice at position \\(x\\), the vertical center is \\[y_{\\text{mid}}(x) \\equiv\\left[\\strut \\text{top}(x) + \\text{bottom}(x)\\right]/ 2\\ ,\\] the average of the top and bottom positions. (These are marked in \\(\\color{blue}{\\text{blue}}\\) in Figure 53.11.) The \\(y\\)-component of the center of mass is \\[\\begin{eqnarray}\n\\bar{y} &=& \\frac{1}{\\text{mass}} \\int_{-1.5}^{4.0} \\text{density}\\ y_\\text{mid}(x)\\ \\left[\\strut \\text{top}(x) - \\text{bottom}(x)\\right] dx = \\\\\n&=&\\frac{1}{\\text{mass}} \\int_{-1.5}^{4.0} \\text{density}\\ \\frac{\\left[\\strut \\text{top}(x) + \\text{bottom}(x) \\right]}{2}\\ \\left[\\strut \\text{top}(x) - \\text{bottom}(x)\\right] dx =\\\\\n&=&\\frac{1}{\\text{mass}} \\int_{-1.5}^{4.0} \\text{density}\\ \\frac{\\left[\\strut \\text{top}(x)^2 - \\text{bottom}(x)^2 \\right]}{2}\\ \\approx -5.43 \\text{m}\\ .\n\\end{eqnarray}\\]\nThe center of mass, \\((x=1.72, y=-5.43)\\), is plotted as \\(\\color{blue}{\\Large\\mathbf{\\text{+}}}\\) on the object.\n\n53.5 Angular momentum and torque\nThe relationship between force and momentum is familiar: \\[F = \\partial_t\\, m\\, v =\\ \\underbrace{m \\ \\partial_t\\  v}_\\text{if mass is constant}\\ .\\] Of course, the derivative of velocity with respect to time is also called “acceleration.”\nConsider the following situation. A space probe is being acted on by a constant force, as in Figure 53.12. The mass of the probe is \\(m\\), the thrust from the rocket engine provides the force \\(F\\). Starting from velocity \\(\\partial_t y(t=0)\\) and position \\(y(t=0) = 0\\), the thrust produces an acceleration \\(\\partial_{tt} y(t) = F/m\\). Integrating the acceleration gives the velocity as a function of time \\[\\partial_t y(t) = \\frac{F}{m} t + C\\ .\\]\n\n\n\n\n\n\n\n\n\nFigure 53.12: A space probe accelerating along a linear course. The position at time \\(t\\) can be written as \\(y(t)\\) or as $ heta(t)$.\n\n\n\n\nThe function \\(y(t)\\) is not the only way to represent where the probe is as a function of \\(t\\). Suppose that the probe is being observed by a telescope which measures the angle \\(\\theta(t)\\) with respect to the equatorial plane. If the distance to the probe is \\(D(t)\\), then the pair \\(\\left(\\strut\\theta(t), D(t)\\right)\\) gives the position of the probe. As \\(y(t)\\) increases, so do \\(\\theta(t)\\) and \\(D(t)\\).\nWe know the laws of motion in terms of \\(y(t)\\). Can we translate these laws to an expression in terms of \\(\\theta(t)\\) and \\(D(t)\\)? That is, can we find the function \\[\\partial_{t} \\theta(t) = {\\Large ?}\\] If we can find the laws of motion in terms of \\(\\theta(t)\\) and \\(D(t)\\), we will have a way to describe the motion of spinning bodies.\n\n\n\nThe derivation of \\(\\partial_{t} \\theta(t)\\) will not be obvious, but you will be able to see how calculus operations come into play.\nThree points in the diagram describe a right triangle: the probe’s position at \\(t=0\\), the center of the planet, and the probe’s position at time \\(t\\). The length of the horizontal leg of the triangle is \\(D(t=0)\\) which not a function of time, so we will drop the unnecessary parentheses and write it as \\(D_0\\). The vertical leg has length \\(y(t)\\), and the hypotenuse has length \\(D(t)\\). The Pythagorean theorem tells us that \\[D(t)^2 = y(t)^2 + D_0^2\\ .\\]\nStep 1: Differentiate both sides with respect to \\(t\\): \\[\\partial_t \\left[\\strut D(t)^2\\right] = \\partial_t \\left[\\strut y(t)^2\\right] \\ . \\] Using the chain rule and the fact that \\(D_0\\) does not depend on \\(t\\), gives \\[2\\, D(t)\\, \\partial_t D(t) = 2\\, y(t)\\ \\partial_t y(t)\\ \\ \\implies\\ \\ \\partial_t y(t) = \\frac{D(t)}{y(t)}\\,\\partial_t D(t)\\ .\\]\nStep 2: Trigonometry allows us to see a relationship among the functions \\(y(t)\\), \\(\\theta(t)\\), and \\(D(t)\\):\n\\[y(t) = D(t) \\sin\\left(\\strut\\theta(t)\\right) \\ .\\] Plugging this form of \\(y(t)\\) into the equation for \\(\\partial_t y(t)\\) in Step 1 produces \\[\\partial_t y(t) = \\frac{1}{\\sin(\\theta(t))} \\partial_t D(t)\\] and \\[D_0 = D(t) \\cos\\left(\\strut\\theta(t)\\right)\\ \\ \\implies\\ \\ D(t) = \\frac{D_0}{\\cos\\left(\\strut\\theta(t)\\right)}\\]\nStep 3: Another fact from trigonometry is that \\[D(t) = D_0/\\cos(\\theta(t))\\ .\\] Differentiating both sides (chain rule again!) with respect to \\(t\\) gives another form for \\(\\partial_t D(t)\\): \\[\\partial_t D(t) = D_0\\, \\partial_t \\left(\\frac{1}{\\cos\\left(\\strut\\theta(t)\\right)}\\right) = D_0 \\frac{\\sin\\left(\\strut\\theta(t)\\right)}{\\cos\\left(\\strut\\theta(t)\\right)^2}\\, \\partial_t \\theta(t)\\]\nStep 4: Combining the results from Steps 1 and 3 gives \\[\\partial_t y(t) =  \\frac{1}{\\sin\\left(\\strut\\theta(t)\\right)}D_0 \\frac{\\sin\\left(\\strut\\theta(t)\\right)}{\\cos\\left(\\strut\\theta(t)\\right)^2}\\, \\partial_t \\theta(t)\\ .\\] Cancelling out the \\(\\sin(\\theta(t))\\) terms and remembering that \\(\\partial_t y(t) = \\frac{F}{m} t\\) gives\n\\[\\frac{F}{m} t = \\frac{D_0}{\\cos\\left(\\strut \\theta(t)\\right)^2}\\, \\partial_t\\theta(t) \\] Multiplying both sides by \\(D_0\\) and recalling that \\(D(t) = D_0/\\cos(\\theta(t))\\) we arrive at \\[\\frac{F D_0}{m} t = \\frac{D_0^2}{\\cos\\left(\\strut \\theta(t)\\right)^2}\\, \\partial_t\\theta(t) = D(t)^2 \\partial_t \\theta(t)\\ . \\]\nAgain re-arranging to find \\(\\partial_t\\, \\theta(t)\\): \\[\\partial_t\\,\\theta(t) = \\frac{F\\, D_0}{m D(t)^2}\\ t\\ .\\]\nTo summarize, we have two equivalent expressions for the dynamics of the space probe:\n\\[\\partial_t y(t) = \\frac{\\overbrace{\\ F}^\\text{force}}{\\underbrace{M}_\\text{mass}} t \\ \\ \\text{and}\\ \\ \\partial_t \\theta(t) = \\frac{\\overbrace{F\\, D_0}^\\text{torque}}{\\underbrace{m D(t)^2}_\\text{moment of inertia}}\\ t\\ .\\] In rectangular \\((x,y)\\) coordinates, the velocity is the accumulation of force divided by mass: the usual statement of Newton’s second law of motion. In the angular coordinates \\((\\theta, D)\\) the angular velocity is the accumulation of torque divided by ***moment of inertia.\nThe angular coordinate representation is helpful when studying the rotation of objects. To illustrate, imagine a different configuration for the system than that in in Figure 53.12 where the space probe was free to accelerate in a straight line. Instead, suppose the rocket is mounted on a carousel, that is a wheel whose axle goes through the center as in Figure 53.13.\n\n\n\n\n\n\n\n\n\nFigure 53.13: The space probe mounted on a rigid wheel that can spin around its center.\n\n\n\n\nIn the rocket-on-wheel configuration, \\(D(t)\\) is a constant; the rocket stays the same distance from the center. Therefore, the moment of inertia is \\(m D_0^2\\). Following the previous formula, \\[\\partial_t \\theta(t) = \\frac{F D_0}{m D_0^2} t\\] which is easily differentiated to give the angular acceleration \\[\\partial_{tt} \\theta(t) = \\frac{F D_0}{m D_0^2}\\ .\\]\nIn a typical wheel, there is mass density throughout the wheel, not just at distance \\(D_0\\). To find the moment of inertia of such a distributed mass system, break the wheel down into small pieces and find the moment of inertia due to each bit. Then accumulate the pieces’ moments of inertia to find the total moment of inertia:\n\\[\\text{moment of inertia} = \\int_\\text{wheel} \\rho(x,y) \\left(\\strut x^2 + y^2\\right) d \\text{wheel}\\ .\\]\n\nCompute the moment of inertia of Blob1.\nIt is not enough to say, “compute the moment of inertia.” We also have to specify what is the reference location—the wheel axle in the configuration. We will first do the calculation around the center of mass \\((\\bar{x}, \\bar{y})\\) which we computed earlier as xbar and ybar:\n\n# moment of inertia\nbox_set(10*((x - xbar)^2 + (y-ybar)^2)~ x + y, \n        Blob1, dx = 0.01, sum=TRUE) \n## [1] 76.67827",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Mechanics</span>"
    ]
  },
  {
    "objectID": "Manifestations/B4-mechanics.html#angular-momentum-and-torque",
    "href": "Manifestations/B4-mechanics.html#angular-momentum-and-torque",
    "title": "53  Mechanics",
    "section": "53.5 Angular momentum and torque",
    "text": "53.5 Angular momentum and torque\nThe relationship between force and momentum is familiar: \\[F = \\partial_t\\, m\\, v =\\ \\underbrace{m \\ \\partial_t\\  v}_\\text{if mass is constant}\\ .\\] Of course, the derivative of velocity with respect to time is also called “acceleration.”\nConsider the following situation. A space probe is being acted on by a constant force, as in Figure 53.12. The mass of the probe is \\(m\\), the thrust from the rocket engine provides the force \\(F\\). Starting from velocity \\(\\partial_t y(t=0)\\) and position \\(y(t=0) = 0\\), the thrust produces an acceleration \\(\\partial_{tt} y(t) = F/m\\). Integrating the acceleration gives the velocity as a function of time \\[\\partial_t y(t) = \\frac{F}{m} t + C\\ .\\]\n\n\n\n\n\n\n\n\n\nFigure 53.12: A space probe accelerating along a linear course. The position at time \\(t\\) can be written as \\(y(t)\\) or as $ heta(t)$.\n\n\n\n\nThe function \\(y(t)\\) is not the only way to represent where the probe is as a function of \\(t\\). Suppose that the probe is being observed by a telescope which measures the angle \\(\\theta(t)\\) with respect to the equatorial plane. If the distance to the probe is \\(D(t)\\), then the pair \\(\\left(\\strut\\theta(t), D(t)\\right)\\) gives the position of the probe. As \\(y(t)\\) increases, so do \\(\\theta(t)\\) and \\(D(t)\\).\nWe know the laws of motion in terms of \\(y(t)\\). Can we translate these laws to an expression in terms of \\(\\theta(t)\\) and \\(D(t)\\)? That is, can we find the function \\[\\partial_{t} \\theta(t) = {\\Large ?}\\] If we can find the laws of motion in terms of \\(\\theta(t)\\) and \\(D(t)\\), we will have a way to describe the motion of spinning bodies.\n\n\n\nThe derivation of \\(\\partial_{t} \\theta(t)\\) will not be obvious, but you will be able to see how calculus operations come into play.\nThree points in the diagram describe a right triangle: the probe’s position at \\(t=0\\), the center of the planet, and the probe’s position at time \\(t\\). The length of the horizontal leg of the triangle is \\(D(t=0)\\) which not a function of time, so we will drop the unnecessary parentheses and write it as \\(D_0\\). The vertical leg has length \\(y(t)\\), and the hypotenuse has length \\(D(t)\\). The Pythagorean theorem tells us that \\[D(t)^2 = y(t)^2 + D_0^2\\ .\\]\nStep 1: Differentiate both sides with respect to \\(t\\): \\[\\partial_t \\left[\\strut D(t)^2\\right] = \\partial_t \\left[\\strut y(t)^2\\right] \\ . \\] Using the chain rule and the fact that \\(D_0\\) does not depend on \\(t\\), gives \\[2\\, D(t)\\, \\partial_t D(t) = 2\\, y(t)\\ \\partial_t y(t)\\ \\ \\implies\\ \\ \\partial_t y(t) = \\frac{D(t)}{y(t)}\\,\\partial_t D(t)\\ .\\]\nStep 2: Trigonometry allows us to see a relationship among the functions \\(y(t)\\), \\(\\theta(t)\\), and \\(D(t)\\):\n\\[y(t) = D(t) \\sin\\left(\\strut\\theta(t)\\right) \\ .\\] Plugging this form of \\(y(t)\\) into the equation for \\(\\partial_t y(t)\\) in Step 1 produces \\[\\partial_t y(t) = \\frac{1}{\\sin(\\theta(t))} \\partial_t D(t)\\] and \\[D_0 = D(t) \\cos\\left(\\strut\\theta(t)\\right)\\ \\ \\implies\\ \\ D(t) = \\frac{D_0}{\\cos\\left(\\strut\\theta(t)\\right)}\\]\nStep 3: Another fact from trigonometry is that \\[D(t) = D_0/\\cos(\\theta(t))\\ .\\] Differentiating both sides (chain rule again!) with respect to \\(t\\) gives another form for \\(\\partial_t D(t)\\): \\[\\partial_t D(t) = D_0\\, \\partial_t \\left(\\frac{1}{\\cos\\left(\\strut\\theta(t)\\right)}\\right) = D_0 \\frac{\\sin\\left(\\strut\\theta(t)\\right)}{\\cos\\left(\\strut\\theta(t)\\right)^2}\\, \\partial_t \\theta(t)\\]\nStep 4: Combining the results from Steps 1 and 3 gives \\[\\partial_t y(t) =  \\frac{1}{\\sin\\left(\\strut\\theta(t)\\right)}D_0 \\frac{\\sin\\left(\\strut\\theta(t)\\right)}{\\cos\\left(\\strut\\theta(t)\\right)^2}\\, \\partial_t \\theta(t)\\ .\\] Cancelling out the \\(\\sin(\\theta(t))\\) terms and remembering that \\(\\partial_t y(t) = \\frac{F}{m} t\\) gives\n\\[\\frac{F}{m} t = \\frac{D_0}{\\cos\\left(\\strut \\theta(t)\\right)^2}\\, \\partial_t\\theta(t) \\] Multiplying both sides by \\(D_0\\) and recalling that \\(D(t) = D_0/\\cos(\\theta(t))\\) we arrive at \\[\\frac{F D_0}{m} t = \\frac{D_0^2}{\\cos\\left(\\strut \\theta(t)\\right)^2}\\, \\partial_t\\theta(t) = D(t)^2 \\partial_t \\theta(t)\\ . \\]\nAgain re-arranging to find \\(\\partial_t\\, \\theta(t)\\): \\[\\partial_t\\,\\theta(t) = \\frac{F\\, D_0}{m D(t)^2}\\ t\\ .\\]\nTo summarize, we have two equivalent expressions for the dynamics of the space probe:\n\\[\\partial_t y(t) = \\frac{\\overbrace{\\ F}^\\text{force}}{\\underbrace{M}_\\text{mass}} t \\ \\ \\text{and}\\ \\ \\partial_t \\theta(t) = \\frac{\\overbrace{F\\, D_0}^\\text{torque}}{\\underbrace{m D(t)^2}_\\text{moment of inertia}}\\ t\\ .\\] In rectangular \\((x,y)\\) coordinates, the velocity is the accumulation of force divided by mass: the usual statement of Newton’s second law of motion. In the angular coordinates \\((\\theta, D)\\) the angular velocity is the accumulation of torque divided by ***moment of inertia.\nThe angular coordinate representation is helpful when studying the rotation of objects. To illustrate, imagine a different configuration for the system than that in in Figure 53.12 where the space probe was free to accelerate in a straight line. Instead, suppose the rocket is mounted on a carousel, that is a wheel whose axle goes through the center as in Figure 53.13.\n\n\n\n\n\n\n\n\n\nFigure 53.13: The space probe mounted on a rigid wheel that can spin around its center.\n\n\n\n\nIn the rocket-on-wheel configuration, \\(D(t)\\) is a constant; the rocket stays the same distance from the center. Therefore, the moment of inertia is \\(m D_0^2\\). Following the previous formula, \\[\\partial_t \\theta(t) = \\frac{F D_0}{m D_0^2} t\\] which is easily differentiated to give the angular acceleration \\[\\partial_{tt} \\theta(t) = \\frac{F D_0}{m D_0^2}\\ .\\]\nIn a typical wheel, there is mass density throughout the wheel, not just at distance \\(D_0\\). To find the moment of inertia of such a distributed mass system, break the wheel down into small pieces and find the moment of inertia due to each bit. Then accumulate the pieces’ moments of inertia to find the total moment of inertia:\n\\[\\text{moment of inertia} = \\int_\\text{wheel} \\rho(x,y) \\left(\\strut x^2 + y^2\\right) d \\text{wheel}\\ .\\]\n\nCompute the moment of inertia of Blob1.\nIt is not enough to say, “compute the moment of inertia.” We also have to specify what is the reference location—the wheel axle in the configuration. We will first do the calculation around the center of mass \\((\\bar{x}, \\bar{y})\\) which we computed earlier as xbar and ybar:\n\n# moment of inertia\nbox_set(10*((x - xbar)^2 + (y-ybar)^2)~ x + y, \n        Blob1, dx = 0.01, sum=TRUE) \n## [1] 76.67827",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Mechanics</span>"
    ]
  },
  {
    "objectID": "Manifestations/B4-mechanics.html#footnotes",
    "href": "Manifestations/B4-mechanics.html#footnotes",
    "title": "53  Mechanics",
    "section": "",
    "text": "Source: Jeff Dahl, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=3235265↩︎\nSource: https://commons.wikimedia.org/wiki/File:Geared_Turbofan_NT.PNG↩︎",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Mechanics</span>"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "MOSAIC Calculus",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis project was initiated by the Mathematical Sciences department at the US Air Force Academy. They recognized that a traditional calculus introduction is ill-suited to the needs of STEM in the 21st century.\nCritical support was given by the ARDI Foundation which awarded the Holland H. Coors Chair in Education Technology to one of the project members, Daniel Kaplan. This made possible a year-long residency at USAFA during which time he was able to work unhindered on this project.\nMacalester College, where Kaplan is DeWitt Wallace Professor of Mathematics, Statistics, and Computer science, was the site where the overall framework and many of the materials for a STEM-oriented calculus were developed. Particularly important in the germination were David Bressoud and Jan Serie, respectively chairs of the Macalester math and biology departments, as well as Prof. Thomas Halverson and Prof. Karen Saxe, who volunteered to team teach with Kaplan the first prototype course. Early grant support from the Howard Hughes Medical Foundation and the Keck Foundation provided the resources to carry the prototype course to a point of development where it became the entryway to calculus for Macalester students.\nProfs. Randall Pruim (Calvin University) and Nicholas Horton (Amherst College) were essential collaborators in developing software to support calculus in R. They and Kaplan formed the core team of Project MOSAIC, which was supported by the US National Science Foundation (NSF DUE-0920350).\nJoel Kilty and Alex McAllister at Centre College admired the Macalester course and devoted much work and ingenuity to write a textbook, Mathematical Modeling and Applied Calculus (Oxford Univ. Press), implementing their own version. Their textbook enabled us to reduce the use of sketchy notes in the first offering of this course at USAFA.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "Preliminaries/06-describing-functions.html",
    "href": "Preliminaries/06-describing-functions.html",
    "title": "6  Describing functions",
    "section": "",
    "text": "6.1 Slope\nSlope describes whether the output goes up or down, and the extent of this rise or fall, as the input changes. Typically, except for the constant and straight-line functions the slope is different for different input values.\nFigure 6.2 graphs the sinusoid function (black curve). At numerous points in the domain, the function has been overlaid with a straight-line segment that has the same slope as does the function itself. For \\(x\\) near \\(-3\\) the slope is negative; for \\(x\\) near zero the slope is positive, then swings back to negative again for \\(x\\) near \\(3\\).\nWhen we speak of the slope of the sinusoid, or any other function, we mean the local slope as a function of the input. The value of the function does not enter into it, just the slope. Figure 6.3 shows only the slope of the sinusoid, without the sinusoid output at all. Each line segment has a horizontal “run” of \\(0.1\\), so you can measure the slope of each segment—rise over run—as the vertical extent \\(\\Delta y\\) of the segment divided by \\(0.1\\).\nFor instance, the \\(\\Delta y\\) for the slope segment at \\(x=0\\) is 0.1, so the slope at \\(x=0\\) is \\(\\Delta y/0.1 = 1\\). At \\(x=1\\), \\(\\Delta y \\approx 0.05\\), so the slope is 0.5. The graph colors the segment according to the slope, so large negative slopes are blue, slopes near zero are green, and large positive slopes are yellow.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Describing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/06-describing-functions.html#slope",
    "href": "Preliminaries/06-describing-functions.html#slope",
    "title": "6  Describing functions",
    "section": "",
    "text": "Figure 6.2: Short straight-line segments laid over a graph of the sinusoid. The slope of each line segment is selected to match the local slope of the sinusoid. The red annotations show several different slopes quantitatively.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.3: Showing just the slope of the sinusoid as a function of input \\(x\\). Top: representing the slope by the steepness of line segments. Bottom: The numerical value of the slope of each segment.\n\n\n\n\n\n\n\n\n\n\nDefinition: “change” versus “rate of change”\n\n\n\nA more general word than “slope” for describing functions is rate of change. It is absolutely crucial to distinguish between the change in the output value of a function and the rate of change of that output.\nTo illustrate, suppose we have a function \\(f(x) \\equiv x^2 + 3\\). When we talk about “change” we imagine a situation where we have to different values of the function input, say \\(x_1 = 3\\) and \\(x_2 = 6\\).\nThe “change” in output for these two different inputs is \\(f(x_2) - f(x_1)\\), or in this case \\(39 - 12 = 27\\).\nIn contrast, the “rate of change” is the change in output divided by the change in input, that is:\n\\[\\frac{f(x_2) - f(x_1)}{x_2 - x_1} = \\frac{27}{3} = 9\\ .\\]\nA “rate” in mathematics is a ratio: one measure divided by another. For instance, a heart rate is measured as beats-per-minute. To measure it, count the number of pulse waves in a given interval of time. A typical medical practice is to count for 15 seconds, an interval long enough to get a reliable count but short enough not to unduly prolong the process. If 18 pulse waves were counted in the 15 seconds, the heart rate is 18 beats per 15 seconds, more usually reported as 72 beats-per-minute.\nIn a rate of change, the ratio is the change in output divided by the change of input.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Describing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/06-describing-functions.html#sec-concavity-intro",
    "href": "Preliminaries/06-describing-functions.html#sec-concavity-intro",
    "title": "6  Describing functions",
    "section": "6.2 Concavity",
    "text": "6.2 Concavity\nThe slope of a function at a given input tells how fast the function output is increasing or decreasing as the input changes slightly. Concavity is not directly about how the function output changes, but about how the function’s slope changes. For instance, a function might be growing slowly in some region of the domain and then gradually shift to larger growth in an adjacent region. Or, a function might be decaying steeply and then gradually shift to a slower decay. Both of these are instances of positive concavity. The opposite pattern of change in slope is called negative concavity. If the slope does not change at all—only straight-line functions are this way— the concavity is zero.\nConcavity has a very clear appearance in a function graph. If a function is positive concave in a region, the graph looks like a smile or cup. Negative concavity looks like a frown. Zero concavity is a straight line.\nReferring to the three function examples in Figure 6.1, we will use the traditional terms concave up and concave down to refer to positive and negative concavity respectively.\n\nThe exponential is concave up everywhere in its domain.\nThe sinusoid alternates back and forth between concave up and concave down.\nThis particular power law \\(x^{-1}\\) is concave up for \\(0 &lt; x\\) and concave down for \\(x &lt; 0\\).\n\nWhen a function switches between positive concavity and negative concavity, as does the sinusoid as well as the gaussian and sigmoid functions, there is an input value where the switch occurs and the function has zero concavity. (Continuous functions that pass from negative to positive or vice versa must always cross zero.) Such in-between points of zero concavity are called inflection points. A function can have zero, one, or many inflection points. For instance, the sinusoid has inflection points at \\(x = \\ldots, -\\pi, 0, \\pi, 2\\pi, \\ldots\\). In contrast, the exponential and the reciprocal functions do not have any inflection points.\n\n\n\n\n\n\nFigure 6.4: Mnemonics for remembering the meaning of “concave up” and “concave down.” In functions like the sinusoid, which have adjacent concave up and concave down regions, there is a point inbetween where the slope doesn’t change. This is called the “inflection point.”\n\n\n\n“Inflection point” appears in news stories, so it is important to know what it means in context. The mathematical definition is about the change in the direction of curvature of a graph. In business, however, it generally means something less esoteric, “a time of significant change in a situation” or “a turning point.”1 The business sense effectively means that the function—say profits as a function of time, or unemployment as a function of time—has a non-zero concavity, up or down. It is about the existence of concavity rather than about the change in the sign of concavity.\nOne of the benefits of learning calculus is to gain a way to think about the previous paragraph that is systematic, so it is always easy to know whether you are talking about the slope of a function or the change in slope of a function.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Describing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/06-describing-functions.html#sec-continuity-intro",
    "href": "Preliminaries/06-describing-functions.html#sec-continuity-intro",
    "title": "6  Describing functions",
    "section": "6.3 Continuity",
    "text": "6.3 Continuity\nA function is continuous if you can trace out the graph of the function without lifting pencil from the page. A function is continuous on an interval (a,b) if you can trace the function over that whole interval.\nAll of the pattern-book functions are continuous over any interval in their domain except for power-law functions with negative exponents. (This includes the reciprocal since it is a power-law with a negative exponent: \\(1/x = x^{-1}\\).) Those exceptions are not defined at \\(x=0\\).\nOn any interval (a,b) that does not include 0, the reciprocal function is continuous. For inputs \\(x &lt; 0\\), the function is negative. For inputs \\(0 &lt; x\\), the function is positive. But, on an interval that includes \\(x=0\\) the function jumps discontinuously from negative to positive.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Describing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/06-describing-functions.html#sec-monotonicity",
    "href": "Preliminaries/06-describing-functions.html#sec-monotonicity",
    "title": "6  Describing functions",
    "section": "6.4 Monotonicity",
    "text": "6.4 Monotonicity\nA function is monotonic on a domain when the sign of the slope never changes on that domain. Monotonic functions either steadily increase in value or, alternatively, steadily decrease in value.\nAnother way of thinking about monotonicity is to consider the order of inputs and outputs compared to a number line. If a function is monotonically increasing then it will preserve the order of inputs along the number line when it maps inputs to outputs, whereas a monotonically decreasing function will reverse the order. For instance, if the input \\(x\\) comes before an input \\(y\\) (i.e., \\(x&lt;y\\)), then \\(f(x)&lt;f(y)\\) for monotonically increasing functions (the order is preserved), but \\(f(y)&lt;f(x)\\) for monotonically decreasing functions (the order of outputs is reversed).\nOf the pattern-book functions in Figure 6.1: both the exponential and the logarithm function are monotonic: the exponential grows monotonically as does the logarithm. The sinusoid is not monotonic over any domain longer than half a cycle: the function switches between positive slope and negative slope in different parts of the cycle (as is evident in Figure 6.3).",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Describing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/06-describing-functions.html#periodicity",
    "href": "Preliminaries/06-describing-functions.html#periodicity",
    "title": "6  Describing functions",
    "section": "6.5 Periodicity",
    "text": "6.5 Periodicity\nA phenomenon is periodic if it repeats a pattern over and over again. The pattern that is repeated is called a cycle; the periodic function as a whole is one cycle placed next to the previous one and so forth. The day-night cycle is an example of a periodic phenomenon, as is the march of the seasons. The period is the duration of one complete cycle; the period of the day-night cycle is 24 hours, the period of the seasonal progression is 1 year.\nReal-world periodic phenomena often show some slight variation from one cycle to the next. Of the pattern-book functions, only the sinusoid is periodic. And it is exactly periodic, repeating the same cycle over and over again. The period—that is, the length of an input interval that contains exactly one cycle—has a value of \\(2\\pi\\) for the pattern-book sinusoid. When used to model a periodic phenomenon, the model function needs to be tailored to match the period of the phenomena.\nThe idea of representing with sinusoids phenomena that are almost but not exactly periodic, for instance a communications signal or a vibration, is fundamental to many areas of physics and engineering.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Describing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/06-describing-functions.html#asymptotic-behavior",
    "href": "Preliminaries/06-describing-functions.html#asymptotic-behavior",
    "title": "6  Describing functions",
    "section": "6.6 Asymptotic behavior",
    "text": "6.6 Asymptotic behavior\nAsymptotic refers to two possible situations depending on whether the input or output is being considered:\n\nWhen the input to a function gets bigger and bigger in size, going to \\(\\infty\\) or \\(-\\infty\\). If, as the input changes in this way the output gets closer and closer to a specific value, the function is said to have a horizontal asymptote of that value.\n\nLook at the graph of the exponential function in Figure 6.1. As \\(x \\rightarrow -\\infty\\), that is, as \\(x\\) goes more and more to the left of the domain, the output of the exponential function tends asymptotically to zero.\n\nWhen the output of a function gets bigger and bigger in size, going to \\(\\infty\\) or \\(-\\infty\\) without the input doing likewise. The visual appearance on a graph is like a sky-rocket: the output changes tremendously fast even though the input changes only a little. The vertical line that the skyrocket approaches is called a vertical asymptote. The power-law function \\(x^{-1}\\) has a vertical asymptote at \\(x=0\\). If you were to consider inputs closer and closer to \\(x=0\\), the outputs would grow larger and larger is magnitude, tending toward \\(\\infty\\) or \\(-\\infty\\).\n\nSeveral of the pattern-book functions have horizontal or vertical asymptotes or both. For instance, the reciprocal function (\\(x^{-1}\\)) has a horizontal asymptote of zero for both \\(x \\rightarrow \\infty\\) and \\(x \\rightarrow -\\infty\\).\nThe sinusoid has neither a vertical nor a horizontal asymptote. As input \\(x\\) increases either to \\(-\\infty\\) or \\(\\infty\\), the output of the sinusoid continues to oscillate, never settling down to a single value. And, of course, the output of the sinusoid is everywhere \\(-1 \\leq \\sin(x) \\leq 1\\), so there is no possibility for a vertical asymptote.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Describing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/06-describing-functions.html#sec-local-extremes",
    "href": "Preliminaries/06-describing-functions.html#sec-local-extremes",
    "title": "6  Describing functions",
    "section": "6.7 Locally extreme points",
    "text": "6.7 Locally extreme points\nMany continous functions have a region of the input domain where the output is gradually growing, then reaches a peak, then gradually diminishes. This peak is called a local maximum. “Maximum” because the output reaches a peak at a particular input, “local” because in the neighborhood of the peak the function output is smaller than at the peak.\n\n\n\nLikewise, functions can have a local minimum: the bottom of a bowl rather than the top of a peak.\nOf the three pattern-book functions in ?fig-for-words, only the sinusoid has a local maximum, and, being periodic, it repeats that every cycle. The sinusoid similarly has a local minimum in every cycle..\nMany modeling applications involve finding an input where the function output is maximized. Such an input is called an argmax. “Argument” is a synonym for “input” in mathematical and computer functions, so “argmax” refers to the input at which the function reaches a maximum output. For instance, businesses attempt to set prices to maximize profit. At too low a price, sales are good but income is low. At too high a price, sales are too low to bring in much income. There is a sweet spot in the middle.\nOther modeling applications involve finding an argmin, the input for which the output is minimized. For instance, aircraft have a speed at which fuel consumption is at a minimum for the distance travelled. All other things being equal, it is best to operate at this speed.\nThe process of finding an argmin or an argmax is called optimization. And since maxima and minima are very much the same mathematically, collectively they are called extrema.\nAny function that has an extremum cannot possibly be monotonic, since the growth is positive on one side of the extremum and negative on the other side.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Describing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/06-describing-functions.html#footnotes",
    "href": "Preliminaries/06-describing-functions.html#footnotes",
    "title": "6  Describing functions",
    "section": "",
    "text": "Google dictionary, provided by Oxford Languages↩︎",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Describing functions</span>"
    ]
  },
  {
    "objectID": "Modeling/09-assembling-functions.html#thm-transient-vibration-transient-vibration",
    "href": "Modeling/09-assembling-functions.html#thm-transient-vibration-transient-vibration",
    "title": "9  Assembling functions",
    "section": "9.4 Application area 9.2 Transient vibration",
    "text": "9.4 Application area 9.2 Transient vibration\nA guitar string is plucked to produce a note. The sound is, of course, vibrations of the air created by vibrations of the string.\nAfter plucking, the note fades away. An important model of this is a sinusoid (of the correct period to correspond to the frequency of the note) times an exponential.\nFunction multiplication is used so often in modeling that you will see it in many modeling situations. Here’s one example that is important in physics and communication: the wave packet. Overall, the wave packet is a localized oscillation as in Figure 9.2. The packet can be modeled with the product of two pattern-book functions: a gaussian times a sinusoid.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assembling functions</span>"
    ]
  },
  {
    "objectID": "Modeling/09-assembling-functions.html#division-by-zero-on-the-computer",
    "href": "Modeling/09-assembling-functions.html#division-by-zero-on-the-computer",
    "title": "9  Assembling functions",
    "section": "9.6 Division by zero on the computer",
    "text": "9.6 Division by zero on the computer\nIt can be tedious to make sure that you are on the right side of the law when dealing with functions whose domain is not the whole number line. The designers of the hardware that does computer arithmetic, after several decades of work, found a clever system to make it easier. It is a standard part of such hardware that whenever a function is handed an input that is not part of that function’s domain, one of two special “numbers” is returned. To illustrate:\n\nsqrt(-3)\n## [1] NaN\n(-2)^0.9999\n## [1] NaN\n1/0\n## [1] Inf\n\nNaN stands for “not a number.” Just about any calculation involving NaN will generate NaN as a result, even those involving multiplication by zero or cancellation by subtraction or division.1 For instance:\n\n0 * NaN\n## [1] NaN\nNaN - NaN\n## [1] NaN\nNaN / NaN\n## [1] NaN\n\nDivision by zero produces Inf, whose name is reminiscent of “infinity.” Inf infiltrates any calculation in which it takes part:\n\n3 * Inf\n## [1] Inf\nsqrt(Inf)\n## [1] Inf\n0 * Inf\n## [1] NaN\nInf + Inf\n## [1] Inf\nInf - Inf\n## [1] NaN\n1/Inf\n## [1] 0\n\nTo see the benefits of the NaN / Inf system let’s plot out the logarithm function over the graphics domain \\(-5 \\leq x \\leq 5\\). Of course, part of that graphics domain, \\(-5 \\leq x \\leq 0\\) is not in the domain of the logarithm function and the computer is entitled to give us a slap on the wrists. The NaN provides some room for politeness.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assembling functions</span>"
    ]
  }
]