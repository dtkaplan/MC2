[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MOSAIC Calculus",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#welcome-to-calculus",
    "href": "index.html#welcome-to-calculus",
    "title": "MOSAIC Calculus",
    "section": "Welcome to calculus",
    "text": "Welcome to calculus\nCalculus is the set of concepts and techniques that form the mathematical basis for dealing with motion, growth, decay, and oscillation. The phenomena can be as simple as a ball arcing ballistically through the air or as complex as turbulent airflow over a wing generating lift. Calculus is used in biology and business, chemistry, physics and engineering. It is the foundation for weather prediction and understanding climate change. It is the basis for the algorithms for heart rate and blood oxygen measurement by wristwatches. It is a key part of the language of science. The electron orbitals of chemistry, the stresses of bones and beams, and the business cycle of recession and rebound are all understood primarily through calculus.\nCalculus has been central to science from the very beginnings. It is no coincidence that the scientific method was introduced and the language of calculus was invented by the same small group of people during the historical period known as the Enlightenment in the late 17th century. Learning calculus has always been a badge of honor and an entry ticket to professions. Millions of students’ career ambitions have been enhanced by passing a calculus course or thwarted by lack of access to one.\nIn the 1880s, a hit musical featured “the very model of a modern major general.” One of his claims for modernity: “I’m very good at integral and differential calculus.”\n\n\n\n\n\n\nVideo 1: A Gilbert and Sullivan opera pays homage to Calculus.\n\n\n\nWhat was modern in 1880 is not modern anymore. Yet, amazingly, calculus today is every bit as central to science and technology as it ever. Indeed, calculus remains central to fields that were not even imagined in 1880, such as AI, logistics, economics, and data science. One reason is that science, engineering, and society have now fully adopted the computer for almost all aspects of work, study, and life. The collection and use of data is growing dramatically. Machine learning has become the way human decision makers interact with such data.\nThink about what it means to become “computerized.” To take an everyday example, consider video. Over the span of a human life, we moved from a system which involved people going to theaters to watch the shadows recorded on cellulose film to the distribution over the airwaves by low-resolution television, to the introduction of high-def broadcast video, to on demand streaming from huge libraries of movies. Just about anyone can record, edit, and distribute their own video. The range of topics (including calculus) on which you can access a video tutorial or demonstration is incredibly vast. All of this recent progress is owed to computers.\nThe “stuff” on which computers operate, transform, and transmit is always mathematical representations stored as bits. The creation of mathematical representations of objects and events in the real world is essential to every task of any sort that any computer performs. Calculus is a key component of inventing and using such representations.\nYou may be scratching your head. If calculus is so important, why is it that many of your friends who took calculus came away wondering what it is for? What’s so important about “slopes” and “areas” and how come your high-school teacher might have had trouble telling you what calculus is for?\nThe disconnect between the enthusiasm expressed in the preceding paragraphs and the lived experience of students is very real. There are two major reasons for that disconnect, both of which we tackle head-on in this book.\nFirst, teachers of mathematics have a deep respect for tradition. Such respect has its merits, but the result is that almost all calculus is taught using methods that were appropriate for the era of paper and pencil—not for the computer era. As you will see, in this book we express the concepts of calculus in a way that carries directly over to the uses of calculus on computers and in working out answers to real-world problems.\nSecond, the uses of calculus are enabled not by the topics of Calc I and Calc II alone, but the courses for which Calc I/II are preliminary: linear algebra and dynamics. Only a small fraction of students who start in Calc I ever reach the parts of calculus that are the most useful. Fortunately, there is a large amount of bloat and rote in the standard textbook topics of Calc I/II. This can be removed to make room for the more important topics.\nThe computer language used in this book is R. This is a mainstream language in high demand by employers in many fields. The small amount of R that you need to learn for this book will open doors to much greater possibilities. We have augmented R with functions designed to simplify access to calculus-related computations. These functions are provided by the {mosaicCalc} R package and build on other widely used, open-source software for R. We will often refer to the software as a whole as “R/mosaic.”\nFor convenience, we place “Active R Chunks” within the text. Often, as in Active R chunk 1, these will be presented with working R/mosaic commands already included. Just press “Run Code” to have the commands evaluated.\n\n\n\nActive R chunk 1\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nSometimes, you will be asked to make modifications to the commands in a chunk so that you can see for yourself how the output changes.\n\n\n\n\n\n\nAccessing software\n\n\n\nThis book, as well as the exercises in the MOSAIC Calculus Workbook, provide immediate access to R/mosaic software via interactive elements such as Active R chunk 1 that run in your browser.\nThere are also versions of R that are provided as stand-alone applications or web services. The most popular of these, RStudio, includes rich facilities for managing large projects and constructing complex documents. (This book is written using RStudio and the quarto document system.)\nFor most students learning Calculus with MOSAIC Calculus, the run-in-your-browser version of R will be sufficient. (The same applies to statistics students working with Lessons in Statistical Thinking.) Instructors may prefer to work within the RStudio document system. Some instructors even introduce all their students to RStudio for writing reports, etc. Wherever you run the software—in-the-browser or using RStudio—it’s all the same language and works the same way.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#instructors-preface",
    "href": "index.html#instructors-preface",
    "title": "MOSAIC Calculus",
    "section": "Instructor’s preface",
    "text": "Instructor’s preface\nThe “MOSAIC” in the book’s title refers to a movement in undergraduate mathematics to integrate Modeling, Statistics and data science, Computing, and Calculus. Skill in all these areas is needed for successful work in the technical and scientific world. Traditionally, teaching does not honor the strong links among these areas and ignores the advantages of teaching them in a unified way. Indeed, modeling, though often mentioned in calculus textbook blurbs, is hardly taught at all. Introductory statistics courses, even those with a formal pre-requisite of calculus, do not draw on calculus concepts beyond the mention of “area under a curve.” Few and far between are introductory computing courses that reinforce calculus and statistics topics, or calculus courses that develop and build on computing and data skills.\nThe isolation of mathematical calculus from disciplines that ought to be considered allies leads to a devastating gap in the education of students. The half-life of a student in a calculus sequence is, roughly speaking, one course. (The decay in participation starts in algebra and trigonometry.) The result is that only a small fraction of students see relevant mathematics of multiple variables and hardly any encounter the powerful concepts of spaces, vectors, and matrix factorizations traditionally reserved for a junior-year linear algebra course. Contemporary statistical modeling techniques, including machine learning, draw heavily on a small core of linear-algebra topics.\nThe design of the mosaic serves the goal of providing for all students access to a broad, directly useful education in the mathematical sciences. One component of access is keeping the program small enough to fit in with a plausible student schedule. Our working definition of “small enough” is one-quarter of the first two years of university. Fitting within this envelope, students ought to achieve basic competence in computing, statistics, and calculus.\nCalculus provides opportunities to prepare students well for both modern statistics and computing. The most direct ties to statistics are modeling, functions of multiple variables, and concepts of linear spaces. The relevant links of calculus to computing are extensive and go both ways. To give but one example: It’s impractical for students to construct contour plots without a computer. With a computer at hand, students can learn about mathematical ideas such as gradients and iterative optimization. For instance, constrained optimization is hardly a topic of standard introductory calculus and Lagrange multipliers are mysterious even to many professors. The essential concepts become much clearer when they can be presented using graphical tools of contours and gradient fields.\nA book that attempts to build on the connections among historically distinct disciplines must also resolve inconsistencies in nomenclature and notation. We draw the instructor’s attention to some of these and the policies adopted in the book:\n\nVariable. In mathematics “variable” is used generically, sometimes meaning “unknown” and other times referring to a known quantity. In computing “variable” is used colloquially to mean “the name of an object,” and in statistics a “variable” refers to data: a column of a data frame or, more generally, a specific attribute of the units of observation that form the rows of the data frame. (See 7.1 Data frames for definitions of “data frame” and “unit of observation.”) We reserve “variable” to be used in the statistical sense. Consequently, a “function of several variables” becomes a “function with several inputs.”\nOutput. Evaluating a function, either a mathematically or on a computer, produces an output. Functions take inputs and produce outputs. Typical names for inputs are, following tradition, the last few letters of the alphabet: \\(t, u, v, w, x, y, z\\).\nFunction names. Functions always have a name. \\(f()\\), \\(g()\\), \\(h()\\) are the pronouns for discussing functions in general, but in specific applications functions often have more descriptive names, e.g. population() or elev() or risk(). The empty parentheses are a reminder that the thing being named is a function and not an input or parameter. Since \\(y\\) is used as an input name, we never use it for the name of a function or to identify the output of a function. So, \\(y=mx+b\\) is not an esteemed phrase in this book. Instead, when we want to define a straight-line function we write \\(g(x) \\equiv a x + b\\) or some other parameterization, for instance \\(g(x) \\equiv a (x - x_0)\\).\nSpecial inputs. Often, a problem or application context requires the identification of some special values for inputs to a function, for instance, argmaxes or zero crossings or starting time. These are often constructed by using an output name (often \\(t\\) through \\(z\\)) with a subscript or a non-numerical superscript as in \\(y^\\star\\).\nOutput (part 2). When we mean something like “the output of the function \\(f()\\) at its argmax,” we write \\(f(x_\\star)\\), or something similar. When we mean, “the output of function \\(f()\\) at some as yet unspecified input named \\(x\\),” we write \\(f(x)\\).\nFormulas. An expression like \\(ax + b\\) is a formula. One of the most common ways to define a function is by using a formula. But creating a function from a formula requires some special syntax, as demonstrated earlier with \\(g(x) \\equiv a x + b\\). The names used within the parentheses on the left side of \\(\\equiv\\) are the input names. Other symbols in the formula are called parameters.\nTilde expression. We use the R language in this book. Those familiar with R know that there is a special kind of expression called a “formula,” for instance a*x + b ~ x. One of the main uses for R formulas is to represent a mathematical formula when creating a function. “Formulas representing formulas” can lead to confusion. We address this by violating the technical vocabulary of R and calling an expression like a*x + b ~ x a “tilde expression.” This name properly draws attention to the ~ (“tilde”) character that is an essential component of R-language “formulas.” A typical use for a tilde expression is to create a computer version of a function. The computer version of \\(g(x) \\equiv a x + b\\) is g &lt;- makeFun(a*x + b ~ x). In this use, the ~ x part of the tilde expression identifies the input name, just as does the \\(x\\) in \\(g(x) \\equiv ...\\). You’ll also use tilde expressions for graphics and operations such as differentiation and anti-differentiation.\n\nThose familiar with R may be tempted to use the native function-building syntax, which looks like\n\ng &lt;- function(x, a, b) {\n  a*x + b\n}\n\nWe strongly encourage you to use makeFun() instead. One reason is to reinforce the use of tilde expressions which are needed to identify the “with-respect-to” input in differentiation and anti-differentiation, as well as the frame of a graph. Another reason has to do with rules of scoping in computer languages, which have no obvious analog in mathematical notation. We prefer to leave scoping to a computer science class, rather than making it a pre-requisite for calculus. makeFun() sidesteps the scoping difficulties.\n\nDifferential notation. Historically, Leibniz’s lovely ratio notation, for instance, \\[\\frac{dy}{dx}\\ ,\\] helped generations of students learn differential calculus and see the connections to integral calculus. It is, however, wordy, which is why other notations—\\(f'\\) or \\(\\dot{x}\\) or \\(f^{(1)}\\) so often appear. But Leibniz could hardly have anticipated a future in which writing is done mainly with keyboards and linear sequences of characters. There is no mainstream computer language in which df/dx or f' or \\dot{x} or f^{(1)} are valid names. To simplify the use of the computer, we use \\(\\partial_x y\\) notation for differentiation. This can be easily morphed into a legal computer name: dx_y. We use \\(\\partial\\) instead of the Latin \\(d\\), partly to mark differentiation as something special and partly because we will use notation like \\(\\partial_{xt} g\\) when dealing with functions of multiple variables, or \\(\\partial_{xx} f\\) for second derivatives.\n\nThe book is designed to support six to ten credit hours of calculus study. The algebra pre-requisites are kept to a minimum. Trigonometric identities are never used. In the starting “Preliminaries” part of the book nine “pattern-book” algebraic functions are introduced that form the basis for modeling work. “Preliminaries” also introduces computing notation, particularly that used for graphing functions.\n“Modeling,” Block I, introduces non-calculus topics that are essential to the rest of the book. It is worth spending considerable class time on Block I, regardless of the previous experience of students. Block II, “Differentiation,” is self-explanatory to a calculus instructor. Absence of extensive drill on symbolic differentiation of obscure functions or the use of Taylor polynomials or l’Hopital’s rule to provide even more drill is entirely intentional. Our goal in MOSAIC Calculus is to ensure that students understand what a derivative is and—importantly—what it is for. Symbolic differentiation is limited to the “pattern-book” functions and other functions constructed as compositions, products, and linear combinations of the pattern-book function.\nBlock III, “Vectors and linear combinations,” does not depend on previously covering differentiation. The sequence Preliminaries-Modeling-Vectors could make a suitable 3- or 4-credit course for students entering data science. Block III might have been reasonably titled “Linear algebra.” But the universal emphasis on determinants and inverses of square matrices in a conventional linear algebra course is not a suitable introduction for working with data, and we did not want to suggest that all the conventional topics of linear algebra are included.\nBlock IV, “Accumulation,” builds on Block II, where differentiation is treated less as an algebraic process than as a relationship between functions. Our focus is on occasions when anti-differentiation is a useful modeling tool for extracting certain forms of information from a function. Instructors are advised to minimize or wholly avoid the area-under-a-curve metaphor. Like cigarettes, that metaphor is addictive and creates dis-ease with the more important roles for accumulation in contexts like dynamics. The final chapter of Block IV is about symbolic integration of functions constructed from from the pattern-book functions. Including it is a concession to administrative practices at universities where topics like “integration by parts” are included in hard-to-change course-catalog copy. Those techniques are not used elsewhere in the book.\nBlock V, “Dynamics,” introduces systems, that is, wholes made of multiple connected parts. Although the context used is differential equations, techniques for finding solutions are not central. More important are the phenomena (e.g. oscillation), the opportunities for modeling and showing how simple mathematical models can provide insight to otherwise seemingly complex natural and social systems, extension of the linear-algebra material from Block III to eigenvalues and eigenvectors, and a glimpse at the surprisingly close connection between exponentials and sinusoids.\nThe last block, “Manifestations,” is arranged along different lines than the previous chapters. The point is to show how calculus operations show themselves in a wide variety of contexts. We are not making up opportunities for more drill in symbolic differentiation and anti-differentiation. In the “Probability” chapter, to give an example, what’s central is the relationships between functions. One of these relationships is standard in calculus books, that between what’s called the CDF and the PDF. Less basic and not at all standard, the relationship between prior, likelihood, and posterior functions. It is not required to cover all the chapters in “Manifestations.” They do not much depend one on the other. Choose the ones that are best suited to the directions in which your students are heading. And the topics need not be delayed to the end of the course. For instance, the constrained optimization topic in “Optimization” can be handled with the material up to Block IV.\nMany highly expert calculus instructors have taught with these materials. Some of their experiences may be relevant to instructors who haven’t yet used MOSAIC Calculus. First, the experts are surprised by how many esteemed, traditional calculus topics are given short shrift or even omitted altogether, and even more surprised that their exclusion does not diminish the course. Second, many experts find that the calculus in this book is not calculus as they have been trained to think about it, but that nonetheless “it works.” Third, instructors who try to avoid spending class time on the computational elements of the book find that their students echo the avoidance. As these instructors go through their first year, they discover that there are only a handful of computational patterns (e.g. tilde expressions, domains) and that students would have avoided many headaches by facing them head-on from the start of the course. Some advice: Don’t think that you have to learn R before you can master using R to teach this book. The mosaic software that powers this book is a better place to start than with the many more general introductions and tutorials on R computing available in printed and video form.\nMost universally, even the expert instructors find they are unfamiliar with much of the material. Leading examples are dimensions of measurement, splines, mechanics (e.g. torque), the uses of orthogonalization, and, broadly, dynamics. Teaching unfamiliar material is admittedly stressful, but highly beneficial to yourself and your students. Dimensions and units, in particular, are a great guide to thinking; take every opportunity to ask your class what are the dimensions of the inputs to and outputs from a function and whether an operation makes sense in terms of dimensions. Consistently, even tradition-minded experts find that thinking about physical dimension gives them unexpected insight into the tasks and methods of calculus.\n\nDaniel Kaplan, Saint Paul, Minnesota, October 2024",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "MOSAIC Calculus",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis project was initiated by the Mathematical Sciences department at the US Air Force Academy. They recognized that a traditional calculus introduction is ill-suited to the needs of STEM in the 21st century.\nCritical support was given by the ARDI Foundation which awarded the Holland H. Coors Chair in Education Technology to one of the project members, Daniel Kaplan. This made possible a year-long residency at USAFA during which time he was able to work unhindered on this project.\nMacalester College, where Kaplan is DeWitt Wallace Professor of Mathematics, Statistics, and Computer science, was the site where the overall framework and many of the materials for a STEM-oriented calculus were developed. Particularly important in the germination were David Bressoud and Jan Serie, respectively chairs of the Macalester math and biology departments, as well as Prof. Thomas Halverson and Prof. Karen Saxe, who volunteered to team teach with Kaplan the first prototype course. Early grant support from the Howard Hughes Medical Foundation and the Keck Foundation provided the resources to carry the prototype course to a point of development where it became the entryway to calculus for Macalester students.\nProfs. Randall Pruim (Calvin University) and Nicholas Horton (Amherst College) were essential collaborators in developing software to support calculus in R. They and Kaplan formed the core team of Project MOSAIC, which was supported by the US National Science Foundation (NSF DUE-0920350).\nJoel Kilty and Alex McAllister at Centre College admired the Macalester course and devoted much work and ingenuity to write a textbook, Mathematical Modeling and Applied Calculus (Oxford Univ. Press), implementing their own version. Their textbook enabled us to reduce the use of sketchy notes in the first offering of this course at USAFA.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preliminaries-part.html",
    "href": "preliminaries-part.html",
    "title": "PRELIMINARIES",
    "section": "",
    "text": "Calculus is about change, and change is about relationships. Consider the complex and intricate network of relationships that determine climate: a changing climate implies that there is a relationship between, say, global average temperature and time. Scientists know temperature changes with levels of CO2 and methane which themselves change due to their production or elimination by atmospheric and geological processes. A change in one component of climate (e.g., ocean acidification or pH level) provokes change in others.\nTo describe and use the relationships we find in the natural or designed world, we build mathematical representations of them. We call these mathematical models. Models provide the link between the real world and the abstractions of mathematics.\nTo be concise …\n\nA model is “a representation for a purpose.”\n\nA blueprint describing the design of a building is an everyday example of a model. The blueprint represents the building but in a way that is utterly different from the building itself. Blueprints are much easier to construct or modify than buildings, they can be carried and shared easily. Two of the purposes of a blueprint is to aid in the design of buildings and to communicate that design to the people securing the necessary materials and putting them together into the building itself.\nDefining the purpose for your model is a crucial first step in building a mathematical representation that will serve that purpose. Useful models of the same real-world setting can be very different, depending on the purpose. For instance, one routine use for a model is to make a prediction. But other models are intended for exploring the connections among the components of the system being modeled.\n\nApplication area 1 Modeling is crucial to atmospheric science.\n\n\n\n\n\n\n\nApplication area 1 Modeling the climate\n\n\n\nAtmospheric scientists build climate models whose purpose is to explore scenarios for the future emission of greenhouse gasses. The model serves as a stand-in for the Earth, enabling predictions in a few hours of decades of future change in the climate. This is essential for the development of policies to stabilize the climate.\n\n\nDesigning a building or modeling the climate requires expertise and skill in a number of areas. Nonetheless, constructing and using a model is easy compared to the alternative of working directly with the object of interest. For instance, a blueprint gives a comprehensive overview of a building in a way that is hard to duplicate just by walking around an actual building.\nModels are easy to manipulate compared to reality, easy to implement (think “draw a blueprint” versus “construct a building”), and easy to extract information from. We can build multiple models and compare and contrast them to gain insight into the real-world situation behind the models.\nA mathematical model is a model made out of mathematical and computational stuff. Example: a bank’s account books are a model made mostly out of numbers. But in technical areas—science and engineering are obvious examples, but there are many other fields, too—numbers don’t get you very far. By learning calculus, you gain access to important mathematical and computational concepts and tools for building models and extracting information from them. The chapters in this Preliminaries section of this book introduce some of the fundamental mathematical entities that are the heart of modeling.",
    "crumbs": [
      "PRELIMINARIES"
    ]
  },
  {
    "objectID": "Preliminaries/01-quant-fun-space.html",
    "href": "Preliminaries/01-quant-fun-space.html",
    "title": "1  Quantity, function, space",
    "section": "",
    "text": "1.1 Quantity vs number\nOne good definition1 of “mathematics” is “the abstract science of number, quantity, and space.” The chapter title, however, doesn’t include the word “number.” Why not? The answer lies in another word in the definition: “abstract.”\n“Abstract” means “based on general ideas and not on any particular person, thing, or situation.” Numbers are abstract; they can be used to represent all sorts of things, sheep, blood pressure, age, and so on without end. The mechanics of working with numbers is the same regardless of the kind of thing involved. In other words, abstract.\nThis book is about applied mathematics: the topics and methods of mathematics that are particularly useful in representing—that is, “modeling”—particular objects or situations. The topics and methods of applied mathematics are abstract in the sense that they can be usefully applied to a huge range of situations. For this book, we have carefully selected topics that are particulary useful in modeling situations and drawing conclusions from those models.\nAs mentioned previously, a “model” is a representation of an object or situation for a given purpose. A blueprint is a model of a building. A doll is a model of a person. We construct models because they are easier to manipulate than the real object being represented. Want to take down a wall between rooms? To do this in the blueprint model doesn’t require sweat or a sledgehammer; just erase the lines corresponding to the wall. Time to play? Just take the doll out of the toy chest and you’re ready to go.\nWe will focus on mathematical models, the kind of model made out of mathematical concepts and constructs. Three of these are listed in the chapter title, but there are others we will encounter later. Mathematical models make it particularly easy to perform manipulations such as asking and answering “what if” questions, predicting outcomes, and creating effective designs of objects and mechanisms. A good example is the use of spreadsheet software to figure out a budget.\nAs you know, a spreadsheet is a device for calculating with numbers. The word “calculate” comes from the Latin “calculus” which originally meant “stone” or “pebble.” Thousands of years before spreadsheets were invented, people were using tables with divots holding pebbles. Moving the pebbles around was the mechanism for calculation. An “abacus” is a particularly clever re-arrangement that replaces the pebbles with beads on a stick. For the last 250 years, however, “Calculus” has come to refer to a set of mathematical concepts and techniques that emerged as part of the Enlightenment and proved spectacularly useful for creating and manipulating mathematical models in physics, chemistry, engineering, and a host of other fields such as economics.\nA mathematical quantity is an amount. How we measure amounts depends on the kind of stuff we are measuring. The real-world stuff might be time, speed, force, price inflation, physiological and ecological systems … anything to which arithmetic can be applied!\nEveryone learns that arithmetic provides a set of patterns and rules for manipulating numbers. A number is a special, abstract kind of quantity. Among people who work with applied mathematics, experience has shown that numbers lack something that is important in many modeling situations: units. To fill the gap, we use “quantity” to indicate a structure with two parts: (1) a number and (2) the units of measurement. From this perspective, a number is merely a quantity that has no units.\nWe learn about units in elementary school, where we study the kinds of units appropriate for measuring tangible stuff. For example, the capacity of a water bottle might be presented in liters, cups, fluid ounces, gallons, bushels, and so on. The area of a field or apartment can be measured in square-meters, hectares, square-feet, square yards, acres, and so on. One of the things school-children learn about units is to identify what kind of stuff is measured by any given unit. For example, a gallon and a cup measure the same kind of stuff: volume. A meter and an inch measure the same kind of stuff: length. Some students also learn how to convert between units. For example, a meter is about 40 inches, a gallon is exactly 16 cups and a cup is exactly 48 teaspoons. But there is no way to convert, say, an inch into a cup. These lessons on units are useful in everyday life, even if most adults have only a limited recollection of them.\nMoving beyond stuff like length, area, volume, and time can be difficult. You probably recognize kilometers-per-hour (or miles-per-hour) as a unit of velocity, but many student struggle when they first encounter something like meters-per-second-per-second, which is a unit for acceleration.\nOne of the great uses of Calculus is to convert between different kinds of stuff. A simple, everyday example is relevant to transportation. You can convert from speed (say, miles-per-hour) to distance travelled. The conversion is accomplished by multiplying speed by the duration of movement. Or, for some purposes, you might need to convert into speed two different measurements: distance travelled and time taken. In later chapters, we will encounter many situations where such conversions between types of stuff is important.\nKeeping track of the type of stuff is an important habit when using Calculus for such conversions. In other words, you need to keep track of the units of each kind of stuff.\nWhen talking about the methods of Calculus, however, there is a shorthand that enables you to see what kind of conversion is being done. This shorthand can help you select the appropriate arithmetical or Calculus method for whatever model manipulation you need to do, and it is especially helpful for spotting and correcting errors. The shorthand involves explicitly noting the “dimension” of the quantity.\nUnits and dimensions are closely related. Knowing the units, you can readily figure out the dimension. But knowing the dimension makes it easier to design and check your calculations. Chapter 15 will describe units and dimension more thoroughly. For now, however, we will give just a hint, enough to understand why knowing and understanding dimensions will help tremendously in your calculation.\nIn particular, we want to emphasize the reason to think about using quantities rather than mere numbers. Whenever you see a quantity, you should expect to see units or their shorthand, dimension.\nAlthough there are hundreds of different kinds of units, we will need only four basic dimensions for most of the applications presented in this book. These are:\nConsider the familiar rules of arithmetic. The basic actions—addition, subtraction, multiplication, division—apply to any two numbers (although division by zero is not allowed). So 17 + 1.3 is 18.3, whatever those numbers are meant to represent. But the two quantities 17 inches and 1.3 seconds (L and T, respectively) cannot be meaningfully added. To apply addition and subtraction, the two quantities must be in the same unit, hence the same dimension. If you encounter someone adding quantities with different dimensions, you know you have spotted an error.\nOn the other hand, division and multiplication can work with any two quantities, regardless of their respective dimensions. In fact, division and multiplication are the basic arithmetic that enables us to construct new kinds of stuff from old kinds of stuff. For insight into the use of the word “dimension,” consider that a length times a length (L \\(\\times\\) L) gives an area (L2) and an area times a length (L2 \\(\\times\\) L) gives a volume (L^3). These correspond to one-, two- and three-dimensional objects respectively.\nThe mathematics of units and dimension are to the technical world what common sense is in our everyday world. For instance (and this may not make sense at this point), if people tell me they are taking the square root of 10 liters, I know immediately that either they are just mistaken or that they haven’t told me essential elements of the situation. It is just as if someone said, “I swam across the tennis court.” You know that person either used the wrong verb—walk or run would work—or that it wasn’t a tennis court, or that something important was unstated, perhaps, “During the flood, I swam across the tennis court.”",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quantity, function, space</span>"
    ]
  },
  {
    "objectID": "Preliminaries/01-quant-fun-space.html#sec-quant-vs-number",
    "href": "Preliminaries/01-quant-fun-space.html#sec-quant-vs-number",
    "title": "1  Quantity, function, space",
    "section": "",
    "text": "time, denoted T\nlength, denoted L\nmass, denoted M\nmoney, denoted V (for “value”)",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quantity, function, space</span>"
    ]
  },
  {
    "objectID": "Preliminaries/01-quant-fun-space.html#sec-functions",
    "href": "Preliminaries/01-quant-fun-space.html#sec-functions",
    "title": "1  Quantity, function, space",
    "section": "1.2 Functions",
    "text": "1.2 Functions\nFunctions, in their mathematical and computing sense, are central to calculus. The introduction to this Preliminaries Block states, “Calculus is about change, and change is about relationships.” The idea of a mathematical function gives a definite perspective on this. The relationship represented by a function is between the function’s input and the function’s output. The input might be day-of-year2 and the output cumulative rainfall up to that day. Every day it rains, the cumulative rainfall increases.\n\nApplication area 1.1 —Temperature as a function of height, brightness as a function of time past noon.\n\n\n\n\n\n\n\nApplication area 1.1 Two examples of input/output relationships\n\n\n\n\nThe input is the altitude on your hike up Pikes Peak to its peak elevation of 14,115 feet; the output is the air temperature. Typically, as you gain altitude the temperature goes down.\nThe input is the number of hours past noon; the output is the brightness of sunlight. As the afternoon progresses, the light grows dimmer, but only to a point.\n\n\n\nA function is a mathematical concept for taking one or more inputs and returning an output. In calculus, we will deal mainly with functions that take one or more quantities as inputs and return another quantity as output.\n\n\n\n\n\n\nvariable\n\n\n\nBe aware of our use of “input” and “output” in place of the vague, but commonly used, “variable.” Try to put the word “variable” out of mind for the present, until we get to discussing the nature of data.\n\n\nBut sometimes we will work with functions that take functions as input and return a quantity as output. And, perhaps surprisingly, there will be functions that take a function as an input and return a function as output.\nIn a definition like \\(f(x) \\equiv \\sqrt{\\strut x}\\), think of \\(x\\) as the name of an input. So far as the definition is concerned, \\(x\\) is just a name. We could have used any other name; it is only convention that leads us to choose \\(x\\). The definition could equally well have been \\(f(y) \\equiv \\sqrt{\\strut y}\\) or \\(f(\\text{zebra}) \\equiv \\sqrt{\\strut\\text{zebra}}\\).\nNotation like \\(f(x)\\) is also used for something completely different from a definition. In particular, \\(f(x)\\) can mean apply the function \\(f()\\) to a quantity named \\(x\\). You can always tell which is intended—function definition or applying a function—by whether the \\(\\equiv\\) sign is involved in the expression.\n\n\n\n\n\n\nComing attraction … the pattern-book functions\n\n\n\nLater in this Preliminaries Block, we will introduce the “pattern-book functions.” These always take a pure number as input and return a pure number as output. In the Modeling Block, we will turn to functions that take quantities—which generally have units—as input and return another quantity as output. The output quantity also generally has units.\n\n\nOne familiar sign of applying a function is when the contents of the parentheses are not a symbolic name but a numeral. For example, when we write \\(\\sin(7.3)\\) we give the numerical value \\(7.3\\) to the sine function. The sine function then does its calculation and returns the value 0.8504366. In other words, \\(\\sin(7.3)\\) is utterly equivalent to 0.8504366.\nIn contrast, using a name on it is own inside the parentheses indicates that the specific value for the input is being determined elsewhere. For example, when defining a function we often will be combining two or more functions, like this: \\[g(x) \\equiv \\exp(x) \\sin(x)\\] or \\[h(y,z) \\equiv \\ln(z) \\left(\\strut\\sin(z) - \\cos(y)\\right)\\ .\\] The \\(y\\) and \\(z\\) on the left side of the definition are the names of the inputs to \\(h()\\).3 The right side describes how to construct the output, which is being done by applying \\(\\ln()\\), \\(\\sin()\\) and \\(\\cos()\\) to the inputs. Using the names on the right side tells us which function is being applied to which input. We won’t know what the specific values those inputs will have until the function \\(h()\\) is being applied to inputs, as with \\[h(y=1.7, z=3.2)\\ .\\]\nOnce we have specific inputs, we (or the computer) can plug them into the right side of the definition to determine the function output: \\[\\ln(3.2)\\left(\\sin(3.2) - \\strut \\cos(1.7)\\right) = 1.163(-0.0584 + 0.1288) =-0.2178\\ .\\]\n\n\n\n\n\n\nTip\n\n\n\nSection 1.3 introduces the idea of “spaces.” A function maps each point in the function’s input space into a single point in the function’s output space. The input and output spaces are also known respectively as the “domain” and “range” of the function.\n\n\n\nApplication area 1.2 —An early application of computing\n\n\n\n\n\n\n\nApplication area 1.2 Functional gunnery\n\n\n\nThe various mathematical functions that we will be studying in this book are in the service of practical problems. But there are so many such problems, often involving specialized knowledge of a domain or science, engineering, economics, and so on, that an abstract mathematical presentation can seem detached from reality.\nVideo 1.1 is a training cartoon from 1945 for gunners in B-29 bombers. The gunner tries to position the gun (the function output) so that a shell and the plane will intersect. There are many inputs to the function, which has been implemented by electronics. The function itself is literally a black box. The inputs are provided by a human gunner training a telescope on a target and setting control dials. The ultimate output is the deflection of the guns in a remote turret. The main function is composed of several others, such as a function that outputs target range given the target size based on knowledge of the size of the target and how large it appears in the telescopic sight.\n\n\n\n\n\n\n\nVideo 1.1: A training video from World War II: Gunnery in the B-29: How to Shoot.\n\n\n\nDividing the gunnery task into a set of inputs and a computed output allows for a division of labor. The gunner can provide the skills properly trained humans are good at, such as tracking a target visually. The computer provides the capabilities—mathematical calculation—to which electronics are well suited. Combining the inputs with the calculation provides an effective solution to a practical problem.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quantity, function, space</span>"
    ]
  },
  {
    "objectID": "Preliminaries/01-quant-fun-space.html#sec-space-intro",
    "href": "Preliminaries/01-quant-fun-space.html#sec-space-intro",
    "title": "1  Quantity, function, space",
    "section": "1.3 Spaces",
    "text": "1.3 Spaces\nCalculus is largely about change, and change involves movement. To move, as you know, means to change location. But we often use location as a metaphor for other things. Consider, for example, the everyday expression, “The temperature is getting higher.” This is not pointing to a thermometer rising up in the air, but to a quantity as it changes.\nTo represent a quantity as it changes we need to provide scope for movement. One way to do this is familiar from schooling: the number line. Each location on the number line corresponds to a possible value for a quantity. The particular value of that quantity at a specific moment in time is represented as a tiny bead on that line. The line as a whole encompasses many other locations. The changing quantity is the movement of the bead.\nFigure 1.1 shows a number line as conventionally drawn. I have placed two different colored dots on the line to correspond to two different quantities: \\(\\color{blue}{6.3^\\circ\\ \\text{C}}\\) and \\(\\color{magenta}{-4.5^\\circ \\text{C}}\\). (You only know that the line is about temperature because I told you.)\n\n\n\n\n\n\nFigure 1.1: A number line drawn in the conventional style suitable for representing temperatures near freezing.\n\n\n\nThe graphical element representing the set of possibilities is the horizontal line segment. The tick marks and labels are added to enable you to translate any given location into the corresponding quantity. In mathematics texts, it’s common to put arrowheads at the ends of the line segment, perhaps to remind you that a line is infinite in length. But in other disciplines, no infinity or arrowheads are needed: the picture is just a scale to help people translate location into quantity.\nI could have placed many dots on the line to represent many different particular quantitities. Of course, all the dots would be representing temperatures in \\(^\\circ\\text{C}\\) since that is the sort of quantity that the number line in Figure 1.1 represents.\nA more general way for representing pairs of quantities is the coordinate plane, which Figure 1.2 shows in the mathematics-text style. (Chapter 4 switches to another style more commonly used across disciplines.)\n\n\n\n\n\n\nFigure 1.2: The coordinate plane drawn in the style common to mathematics texts. Source: Wikipedia\n\n\n\nEvery location in the coordinate plane is a possibility. A specific pair of quantities is displayed by placing a dot. Figure 1.2 has four such dots, corresponding to four distinct pairs of quantities.\nThe space annoted by the coordinate axes and grid is two-dimensional, analogous to a table-top or a piece of paper or a computer display’s surface. Two-dimensional space accommodates change in each of the two quantities.\nEveryday life acquaints us well with three-dimensional spaces, where each location corresponds to a possible value for each of three quantities. Displaying a three-dimensional space is difficult to do well because conventional displays show only two dimensions. Figure 1.3 shows one style that uses perspective and shading to create the impression of a 3-D scene.\n\n\n\n\n\n\nFigure 1.3: One of many styles for displaying a three-dimensional space. Source\n\n\n\nDrawings of one-dimensional space (Figure 1.1) or two-dimensional space (Figure 1.2) make it straightforward to read off the quantitative value corresponding to any location. Already in 3-dimensional space, reading quantitative coordinates is difficult and requires conscious mental effort. We will make only limited use of 3-D displays.\nConsider now what the different dimensional spaces permit in terms of movement, that is, how quantities can change. The number line permits one kind of movement, left-right in Figure 1.1. Even though we use two words to name the kind of movement—“left” and “right”—we still consider movement in either opposing direction as one kind of movement. Left is the opposite of right. The coordinate plane permits two kinds of movement: left-right and up-down. A three-dimensional space permits three kinds of movement: left-right, up-down, nearer-farther. We often say that each type of movement is along an axis. as is conventional for one- and two-dimensional spaces. The number line has one axis, the coordinate plane has two axes, and three-dimensional space has three.\nFigures 1.1, 1.2, and 1.3 are conventional drawings of one-, two-, and three-dimensional spaces respectively. What about four- or higher-dimensional spaces? Many people put their foot down here and refuse to accept such a thing. Some others will point to the Theory of Relativity where an essential concept is “space-time,” a four-dimensional space sometimes denoted as \\((x, y, z, t)\\). True though this be, it does not much appeal to intuition and for a good reason. We are free to move objects in their x-, y-, and z-coordinates, but we have no control over time.\nEngineers, statisticians, physicists, and others often use the phrase “degree of freedom” to refer to a type of movement. In English, we have many phrases for different types of movement: in-and-out, back-and-forth, clockwise-and-counter-clockwise, nearer-and-farther, up-and-down, left-and-right, north-and-south, east-and-west, and so on. When imagining time machines, or showing photos from our recent trip, we speak of going forward in time or going back in time: forward-and-back. (Note the word “going,” which emphasizes the idea of movement rather than position.)\nHealth professions learn additional names for kinds of movement: adduction-abduction, flexion-extension, internal-vs-external rotation.\nA depiction of even four-, five-, and higher-dimensional space can be expressed as possibilities for movement. Video 1.2 shows the movement of a robotic hand with six degrees of freedom. Each of these is a different kind of movement.\n\n\n\nTable 1.1: Six degrees of freedom for the robot hand.\n\n\n  1. swiveling of the base\n2-4. rotation around each of the three knuckles\n  5. swiveling at the wrist\n  6. fingers moving closer together or farther apart\n\n\n\n\n\n\n\n\n\nVideo 1.2: A robot hand with six degrees of freedom. Source\n\n\n\nThere can be multiple ways of forming coordinates for the same space. The configuration of the mechanism for the robotic hand can be specified by the six degrees of freedom enumerated in Table 1.1. But from the point of view of a person training the robot, it might be more convenient to think about the configuration in terms of the six quantities given in Table 1.2.\n\n\n\nTable 1.2: A different way of describing the configuration of the robot hand that is better suited for a user training the robot in some action.\n\n\n1-3. The location of the wrist in everyday three-dimensional space. (x,y,z)\n4-5. The direction that the fingers point in. (Two angles often called “azimuth” and “elevation.”)\n  6. The distance between the fingers (d).\n\n\n\nTo train the robot to perform a specific movement, the engineer specifies a sequence of configurations. Each of the configurations corresponds to a dot in the six-dimensional space. An important Calculus method, introduced in Chapter 49, is to create a smooth, continuous path connecting the dots called a trajectory which, in this case, depicts movement through six-dimensional space.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quantity, function, space</span>"
    ]
  },
  {
    "objectID": "Preliminaries/01-quant-fun-space.html#all-together-now",
    "href": "Preliminaries/01-quant-fun-space.html#all-together-now",
    "title": "1  Quantity, function, space",
    "section": "1.4 All together now",
    "text": "1.4 All together now\nThe three mathematical concepts we’ve been discussing—quantities, functions, spaces—are used together.\nA quantity can be a specific value, like 42.681\\(^\\circ\\)F. But you can also think of a quantity more broadly, for instance, “temperature.” Naturally, there are many possible values for temperature. The set of all possible values is a space. And, using the metaphor of space, the specific value 42.681\\(^\\circ\\)F is a single point in that space.\nFunctions relate input quantities to a corresponding output quantity. A way to think of this—which will be important in Chapter 4 —is that a function is a correspondence between each point in the input space and a corresponding point in the output space. By mathematical convention, the output space in Calculus is always one-dimensional.\nEvery function has a set of legitimate potential inputs, a region in the input space. This input-space region is called the domain of the function. For some functions, the possible output values occupy the whole of the one-dimensional output space. Other functions use only regions of the one-dimensional output space. The set of possible outputs is called the range of the function.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quantity, function, space</span>"
    ]
  },
  {
    "objectID": "Preliminaries/01-quant-fun-space.html#footnotes",
    "href": "Preliminaries/01-quant-fun-space.html#footnotes",
    "title": "1  Quantity, function, space",
    "section": "",
    "text": "Source: Oxford Languages↩︎\n“Day-of-year” is a quantity with units “days.” It starts at 0 on midnight of New Year’s Eve and ends at 365 at the end of day on Dec. 31.↩︎\nSometimes, we will use both a name and a specific value, for instance \\(\\sin(x=7.3)\\) or \\(\\left.\\sin(x)\\Large\\strut\\right|_{x=7.3}\\)↩︎",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quantity, function, space</span>"
    ]
  },
  {
    "objectID": "Preliminaries/02-notation.html",
    "href": "Preliminaries/02-notation.html",
    "title": "2  Notation",
    "section": "",
    "text": "2.1 Functions, inputs, parameters\nGood notation makes the writer’s intent clear to the reader. In our study of calculus, each component of notation will refer to a mathematical object of some kind. Consequently, the notation should, on its own, indicate the kind of object.\nIn the previous chapter, we described three different kinds of objects: functions, quantities, and spaces. We introduced mathematical and computer notation to make clear what is the name of a function and the names of inputs. In this chapter, we will introduce constants, parameters, and special inputs. We will see how input names, like \\(x\\) in \\(g(x) \\equiv x^3 + 3\\) refer to a kind of space called a domain.\nIt makes obvious sense to use a mathematical notation with which you are already familiar. We will supplement this notation with simple rules for naming, intended to make it clear just from the name what kind of mathematical object is being named.\nSince you will be using computing extensively, it will pay to be aware of how traditional mathematical notation sometimes conflicts with the requirements of computing. (Chapter 3 introduces computing with the R language.)\nWe will attempt to use mathematical notation in a way that limits the conflict between tradition and computer notation. This conflict is particularly acute when it comes to the idea of an “equation,” so widely used in high-school mathematics but not a component of mainstream computer languages.\nOur style of notation will be to give functions and their inputs explicit names. The basic principle is that a function name is a sequence of letters followed by an empty pair of parentheses, for instance, sin() or ln(). The parentheses provide clear indication that this is a function name.\nChapter 1 provided notation for defining a function includes the names of the parameters. In our mathematical notation, both the name of the function and the names of the inputs are shown on the left-hand side of the \\(\\equiv\\) symbol. For instance, \\[g(u, z) \\equiv u\\,\\cos(z)\\] defines a function named \\(g()\\) that takes two inputs. The input names are listed in the parentheses following the function name.\nThe right-hand side of a function definition is a formula. The formula specifies how each of the inputs will get used in a computation of the function output. When a function has more than one input, the input names serve to indicate where each input goes in the formula defining the calculation. For instance: \\[h(x, y) \\equiv x^2 e^y\\ .\\] \\(h()\\) is a completely different function than, say, \\(f(x, y) \\equiv y^2 e^x\\).\nA sensible person will define a function because they are planning to use it later on, perhaps multiple times. “Using” a function might mean including it in the formula in the definition of another function. But there is also a more specific sense of “using” to which we need to give a precise name. To apply a function means providing specific input quantities so that the output of the function can be calculated. An equivalent phrase is evaluate a function on an input(s). Consider, for instance, a simple function \\[z(t) \\equiv t^2\\ .\\] To apply the function \\(z()\\) to the input quantity 3, any of the following notation styles can be used: \\[z(3)\\ \\ \\ \\text{or}\\ \\ \\ \\ z(t=3) \\ \\ \\ \\text{or}\\ \\ \\ \\ z(t)\\left.\\Large\\strut\\right|_{t=3}\\ .\\] Remember that \\(z(3)\\) or its equivalents are not themselves functions. They are the quantity that results from applying a the function to a specific input quantity, namely \\(3\\) in this example.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Notation</span>"
    ]
  },
  {
    "objectID": "Preliminaries/02-notation.html#functions-inputs-parameters",
    "href": "Preliminaries/02-notation.html#functions-inputs-parameters",
    "title": "2  Notation",
    "section": "",
    "text": "Tip\n\n\n\nIn defining a function, say, \\(z(t) \\equiv t^2\\), one uses specific names for the arguments and writes the function formula using these same names. The application of a function to an input is different. When a function is being applied, the argument can be a numeral or any name that contains the value to serve as input. For instance, any of \\(z(b)\\), \\(z(\\text{age})\\), or \\(z(\\text{orbit\\_duration})\\) can be correct ways to apply \\(z()\\).\n\n\n\n2.1.1 Input names\nTo reduce human cognitive load in parsing function definitions, we will often select input names from a small set. This way, the name itself is a clue to what role that name is playing. Here are the most commonly used input names.\n\n\\(x\\) or \\(y\\) or \\(z\\).\n\\(t\\). This name is typically used when the input is meant to represent a time or duration.\nLess frequently, \\(u\\), \\(v\\), \\(w\\) when the other arguments are already in use.\n\nOf course, it’s often appropriate to use other names for arguments. In modeling, to make clearer the relationship of functions and the real-world setting, it is a good idea to use more descriptive names, like \\(T\\) for “temperature” or \\(V\\) for volume, or even \\(\\text{altitude}\\) (which helpfully describes itself).\n\n\n\n\n\n\nDefinition: “argument”\n\n\n\nIn everyday speech, an “argument” is a discussion between people with differing views. But in mathematics and computing, argument means something else entirely: it is a synonym for “input to a function.”\n\n\nOften, the functions we define will have formulas that include quantities other than the inputs. For instance, we might define: \\[h(t) \\equiv A \\sin(t) + B\\ .\\] This definition explicitly identifies \\(t\\) as the name of the function input. The quantities named \\(A\\) and \\(B\\) that appear in the formula are not listed as inputs on the left side of \\(\\equiv\\) but they are nonetheless essential for evaluating the function \\(h()\\).\nThere is a case to be made for identifying as inputs to the function all quantities needed for evaluating the function. In this style, the function would be defined as \\(h(t,A,B) \\equiv A \\sin(t) + B\\).\nIn writing mathematical notation for the human reader, there is a tradition of distinguishing between quantities that will differ from one evaluation to another and quantities that will be the same each time the function is evaluated. These latter quantities are called parameters.\nIn reading a definition such as \\[h(t) \\equiv A \\sin(t) + B\\ ,\\] the named quantities that are not listed inside the parentheses on the left-hand side of the definition—\\(A\\) and \\(B\\) in this example—will be the parameters. By writing a name in the parameter style, we are signaling that these quantities will not be changing when we apply the function. That leaves unstated what are the values of the parameters, a source of confusion for many newcomers to calculus.\n\n\n\n\n\n\nInput or parameter?\n\n\n\nThere is no absolute rule for identifying a named quantity used in a function’s formula as a parameter rather than as an input. It is a matter of style and the conventions of the field in which you’re working. When we get to the computer notation for defining functions, you will see that we simplify things by considering all named quantities used in a function formula as inputs.\n\n\n\nApplication area 2.1 —The period of a pendulum swing.\n\n\n\n\n\n\n\nApplication area 2.1 Period of swing\n\n\n\nA pendulum is a device that swings back and forth from a fixed pivot. The period of a pendulum is the time it takes to go through one complete cycle of motion—one “back” and one “forth.” It happens that it is simple to compute the period of a pendulum, \\[\\text{period}(L) \\equiv \\sqrt{\\strut L/g\\ }\\] where \\(L\\) is the length of the pendulum, \\(g\\) is the “acceleration due to gravity.”\n\nWe could have written the function as \\(\\text{period}(L, g) \\equiv \\sqrt{\\strut L/g\\ }\\), treating both quantities \\(L\\) and \\(g\\) as inputs. We wrote instead \\(\\text{period}(L)\\) to signify something to the human reader: that we are anticipating the user of \\(\\text{period}()\\) to be calculating the periods of various pendula, with different \\(L\\), but all in about the same location. That location will presumably be near the surface of the Earth, where \\(g \\approx 9.8\\) m/s2. In other words, the definition of \\(\\text{period}(L)\\) treats the acceleration due to gravity as a parameter rather than an input.\nOf course, you might be the kind of person who puts pendula in elevators or on Mars. If so, you would need to use a different value for \\(g\\) than \\(9.8\\) m/s2.\nYou will see much more use of parameters in Block 11 when we use parameters to “fit” functions to data.\n\n\n\n\n2.1.2 Parameter names\nTo make it easy to recognize parameters, we will use names like \\(a\\), \\(b\\), \\(c\\), \\(\\ldots\\), or their upper-case cousins \\(A\\), \\(B\\), \\(\\ldots\\). For instance, here is a definition of a function called a “cubic polynomial”: \\[h(x)\\equiv a + b x + c x^2 + d x^3\\ .\\]\nBut there will be occasions where we need to compare two or more functions and run out of appropriate names from the start of the alphabet. A way to keep things organized is to use subscripts on the letters, for instance comparing \\[g(x) \\equiv a_0 + a_1 x^2 + a_2 x^2 + a_3 x^3 + a_4 x^4\\] to \\[f(x) \\equiv b_0 + b_1 x^2 + b_2 x^2\\ .\\]\n\n\n\n\n\n\nTip\n\n\n\nPronounce names \\(a_0\\) or \\(b_3\\) as “a-sub-zero” and “b-sub-three” respectively.\n\n\n\n\n\n\n\n\nCalculus history: Letters from A to Z and \\(\\alpha\\) to \\(\\omega\\).\n\n\n\nThe tradition of using letters from the start of the alphabet as parameter names dates from the time of Isaac Newton.\nMathematicians and scientists often reach further back in history and use Greek letters as parameter names: \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\), \\(\\delta\\), … In this book, we minimize the use of Greek, but mastering the Greek alphabet is an important form of socialization when becoming a scientist.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Notation</span>"
    ]
  },
  {
    "objectID": "Preliminaries/02-notation.html#special-inputs",
    "href": "Preliminaries/02-notation.html#special-inputs",
    "title": "2  Notation",
    "section": "2.2 Special inputs",
    "text": "2.2 Special inputs\nWe will create functions as models of a real-world situation. Once created, we generally have to extract information from the function that informs the real-world choice, decision, or understanding that we need to make or develop.\nThere are many forms that the extracted information will take, depending on circumstance. With surprising frequency, two types of information turn out to be useful:\n\nThe set of inputs that produces a maximum or minimum output.\nInputs that produce a specific output.\n\nWe will call these special inputs and will study the techniques for determining them later in the book. Here, we focus on the notation we will use so that you can spot when a special input is being used.\nAs we’ve stated before, the names of inputs will tend to be letters from the back of the alphabet: \\(t\\), \\(u\\), \\(v\\), \\(x\\), \\(y\\), \\(z\\). Each such name refers to the entire set of possible values, that is, a space in the sense of Chapter 1.\nWhen we want to refer to a specific input value that describes a particular feature of a function, we will use the standard input names with a superscript—for instance, \\(x^\\star\\)—or a subscript like \\(y_1\\) or \\(u_0\\).",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Notation</span>"
    ]
  },
  {
    "objectID": "Preliminaries/02-notation.html#footnotes",
    "href": "Preliminaries/02-notation.html#footnotes",
    "title": "2  Notation",
    "section": "",
    "text": "Original word: “ramify”↩︎",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Notation</span>"
    ]
  },
  {
    "objectID": "Preliminaries/03-computing.html",
    "href": "Preliminaries/03-computing.html",
    "title": "3  Computing with R",
    "section": "",
    "text": "3.1 Commands and evaluation\nMathematical notation evolved for the purpose of communication among people. With the introduction of programmable computers in the middle of the 20th century, a notation was needed to communicate between people and hardware. It turns out that traditional mathematical notation in calculus is not fully adequate for this purpose. This chapter introduces the notation used in R/mosaic.\nComputers need to distinguish between declarative and imperative statements. A declarative statement, like \\(g(z) \\equiv z \\cos(z)\\) defines and describes a relationship. An imperative statement is a direction to do some action. For instance, “The store is on the next block,” is declarative. “Bring some crackers from the store,” is imperative. Evaluating a function calls for an imperative statement.\nThe names and format of such instructions—e.g. make a mathematical function from a formula, draw a graph of a function, plot data—are given in the same name/parentheses notation we use in math. For example, makeFun() constructs a function from a formula, slice_plot() graphs a function, gf_point() makes one style of data graphic. These R entities saying “do this” are also called “functions.”\nWhen referring to such R “do this” functions, we will refer to the stuff that goes in between the parentheses as “arguments.” The word “input” would also be fine. The point of using “input” for math functions and “argument” for R “do-this” functions is merely to help you identify when we are talking about mathematics and when we are talking about computing.\nWith computers, writing an expression in computer notation goes hand-in-hand with evaluating the notation. In the mode we will use in this book, you enter your commands in an editor block that we call an “active R chunk.” An example is Active R chunk 3.1, where the editor has been pre-populated with a simple arithmetic command: 2 + 3.\nTo evaluate the command, press the “Run Code” button. You can edit the command in the usual way, placing the cursor in the editor block and typing.\nIn Active R chunk 3.1 (as it initially appears) there is a 1 at the far left. This is a line number, not part of a command. Active R chunks can contain multiple lines and even multiple commands.\nThe R language is set up to format the printing of returned values with an index, which is helpful when the value of the expressions is a large set of numbers. In the case here, with just a single number in the result of evaluating the expression, the index is simply stating the obvious.\nStrictly speaking, we can say that running the code sqrt(17) is invoking the function sqrt() on the input value 17. It’s very common in the world of computing to use the word “calling” rather than “invoking.” So, in Try it! 3.2 you have “called sqrt().\nAn important form of R expression is storage, a declarative statement. Storage uses a symbolic name and the &lt;- token. Active R chunk 3.2 gives a simple example using the name b.\nWe call &lt;- the “storage arrow.” Think of it as stowing the left-hand value in a box labelled with the name on the right-hand side. When storing a value, R is designed not to display the value as happened in the imperative statements in Try it 3.1 and [-try-sqrt-17].\nWithin a document, for instance a chapter of this book or from the MOSAIC Calculus Workbook all the active R chunks have access to the work of the other chunks in the document. Understandably, you need to run the code in a chunk in order for other chunks to access it.\nWhenever you refresh or re-open a document, the active R chunks will all in a not-yet-run state. You have to run the code individually for each chunk you want to use or which are used by other chunks. By and large, we write each R chunks to avoid the need to run earlier chunks … but not always!",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Computing with R</span>"
    ]
  },
  {
    "objectID": "Preliminaries/03-computing.html#commands-and-evaluation",
    "href": "Preliminaries/03-computing.html#commands-and-evaluation",
    "title": "3  Computing with R",
    "section": "",
    "text": "\\(\\equiv\\) and \\(=\\) are declarative\n\n\n\nNoting the distinction between declarative and imperative helps to better understand mathematical notation. In a mathematical statement like \\(h(x) \\equiv 3 x + 2\\), the \\(\\equiv\\) indicates that the statement is declarative. Similarly, the equal sign is used for declarative statements, as with \\[\\pi = 3.14159\\ldots\\] On the other hand, applying a function to a value, as in \\(h(3)\\), is an imperative statement.\n\n\n\n\n\n\n\n\nActive R chunk 3.1\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\n\n\nImportant 3.1: Simple editing\n\n\n\nModify the contents of Active R chunk 3.1 to subtract 22 from 108. Confirm that running the code produces an output of 86.\n\n\n\n\n\n\n\n\n\nImportant 3.2: “Calling” a function\n\n\n\nLeaving the command from Try it! 3.1 in place, add a second line to Active R chunk 3.1 to calculate \\(\\sqrt{17}\\). (In R, the square-root function is named sqrt().) When you run the code, you will see two outputs: one for the first line and the other for the second line.\n\n\n\n\n\n\n\nActive R chunk 3.2\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\n\nImportant 3.3: Accessing stored values by name\n\n\n\nThe purpose of storage is to provide access to a value later on. In order to retrieve the value stored under a name, simply give the name as a command, as in Active R chunk 3.3.\n\n\n\nActive R chunk 3.3\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nSome students may have gotten a response from Active R chunk 3.3 in the form of an “error message” such as Error: object ‘b’ not found. Even experts get such error messages; it’s easy to make a mistake when forming a computer command. It’s important for newbies to understand that error messages are intended to help you fix things rather than to frighten you. Perhaps being gentler would help here—Sorry, but the object ‘b’ wasn’t found.—but “errors” are part of computing culture.\nThe message “object ‘b’ not found” is an indication that nothing has yet been stored under the name b. This would be the case if you had neglected to run the code in Active R chunk 3.2. If that’s your situation, go back to Active R chunk 3.2 and run the code, then try Active R chunk 3.3 again.\n\n\n\n\n\n\n\n\n\n\nUsing R as a stand-alone app\n\n\n\nProfessionals are used to R coming in the form of a stand-alone app. The most popular such app is RStudio, available as free, open-source software produced by a public benefit corporation, Posit PBC.\nThere are many tutorials on installing and using RStudio. People do this especially when they need to document and preserve their work or integrate computing into documents. (This book is an example of such a document.) MOSAIC Calculus is written so that you do not need to use anything but the active R chunks. Still, you might prefer to use the many helpful features that RStudio adds to work with the R language. (Advantages of the active R chunks: the student doesn’t need to install any software. It’s nice as well to be able to refer to computations by pointing to a specific active R chunk like Active R chunk 3.1.)\nIf you do use RStudio, note that in addition to the app, you need to install several packages from the R ecosystem. You do this from within R. The following R command will do the job:\n\ninstall.packages(\"mosaicCalc\")\n\nThen, each time you open RStudio, you need to tell R that you are planning to use the {mosaicCalc} package. Do this as follows:\n\nlibrary(mosaicCalc)",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Computing with R</span>"
    ]
  },
  {
    "objectID": "Preliminaries/03-computing.html#sec-makefun",
    "href": "Preliminaries/03-computing.html#sec-makefun",
    "title": "3  Computing with R",
    "section": "3.2 Defining mathematical functions in R/mosaic",
    "text": "3.2 Defining mathematical functions in R/mosaic\nAs you progress through this book you will meet a dozen or so functions from R/mosaic. For instance, Chapter 4 introduces functions for drawing graphs. Chapter 7 shows how to plot data.\nIn this section, we introduce the R/mosaic function for creating mathematical functions.\nAs you know, our mathematical notation for defining a function looks like this: \\[h(t) \\equiv 1.5\\, t^2 - 2\\ .\\]\nThe R/mosaic equivalent to this is\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nLaying out the two notation forms side by side lets us label the elements they share:\n\nFor the human reading the mathematical notation, you know that the statement defines a function because you have been taught so. Likewise, the computer needs to be told what to do with the provided information. That is the point of makeFun(). There are other R/mosaic commands that could take the same information and do something else with it, for example create a graph of the function or (for those who have had some calculus) create the derivative or the anti-derivative of the function.\nAt its simplest, makeFun() takes only one argument. That argument must be in a form called a tilde expression. This name comes from the character tilde (~) in the middle. On the right-hand side of the tilde goes the name of the input: ~ t. On the left-hand side is the R expression for the formula to be used. The formula is written using the input name and whatever parameters are used. For instance, Active R chunk 3.4 defines a parameterized version of \\(h()\\).\n\n\n\nActive R chunk 3.4: Arguments are always separated by commas.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn Active R chunk 3.4, three arguments are being provided:\n\nThe tilde expression a*t^2 - b ~ t\nA default value for parameter a.\nA default value for parameter b.\n\n\n\n\n\n\n\n\n\n\nImportant 3.4: Evaluating h() at an input.\n\n\n\nIn Active R chunk 3.5, write each of the following commands.\n\nEvaluate h() for the input \\(t = 1\\).\nEvaluate h() for the input \\(t = 2\\).\nRepeat (i) and (ii), but override the default value of b, setting it instead to be \\(b=100\\).\n\nNaturally, you need to run the code in Active R chunk 3.4 before you can evaluate h().\n\n\n\nActive R chunk 3.5\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Computing with R</span>"
    ]
  },
  {
    "objectID": "Preliminaries/03-computing.html#names-and-assignment",
    "href": "Preliminaries/03-computing.html#names-and-assignment",
    "title": "3  Computing with R",
    "section": "3.3 Names and assignment",
    "text": "3.3 Names and assignment\nThe command\n\nh &lt;- makeFun(1.5*t^2 - 2 ~ t)\n\ngives the name h to the function created by makeFun(). Good choice of names makes your commands much easier for the human reader.\n\n\n\n\n\n\nValid and invalid names in R\n\n\n\nThe R language puts some restrictions on the names that are allowed. Keep these in mind as you create R names in your future work:\n\nA name is the only thing allowed on the left side of the storage arrow &lt;-. (For experts … there are additional allowed forms for the left-hand side. We will not need them in this book.)\nA name must begin with a letter of the alphabet, e.g. able, Baker, and so on.\nNumerals can be used after the initial letter, as in final4 or g20. You can also use the period . and underscore _ as in third_place. No other characters can be used in names: no minus sign, no @ sign, no / or +, no quotation marks, and so on.\n\nFor instance, while third_place is a perfectly legitimate name in R, the following are not: 3rd_place, third-place. But it is OK to have names like place_3rd or place3, etc., which start with a letter.\nR also distinguishes between letter case. For example, Henry is a different name than henry, even though they look the same to a human reader.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Computing with R</span>"
    ]
  },
  {
    "objectID": "Preliminaries/03-computing.html#formulas-in-r",
    "href": "Preliminaries/03-computing.html#formulas-in-r",
    "title": "3  Computing with R",
    "section": "3.4 Formulas in R",
    "text": "3.4 Formulas in R\nThe constraint of the keyboard means that computer formulas are written in a slightly different way than the traditional mathematical notation. This is most evident when writing multiplication and exponentiation. Multiplication must always be indicated with the * symbol, for instance \\(3 \\pi\\) is written 3*pi. For exponentiation, instead of using superscripts like \\(2^3\\) you use the “caret” character, as in 2^3. The best way to learn to implement mathematical formulas in a computer language is to read examples and practice writing them. Several examples are given in Table 3.1.\n\n\n\nTable 3.1: Arithmetic operations used frequently in this book. Here, the inputs are all numbers. But in the rest of the book we will often use names as the arguments.\n\n\n\n\n\nTraditional notation\nR notation\n\n\n\n\n\\(3 + 2\\)\n3 + 2\n\n\n\\(3 \\div 2\\)\n3 / 2\n\n\n\\(6 \\times 4\\)\n6 * 4\n\n\n\\(\\sqrt{\\strut4}\\)\nsqrt(4)\n\n\n\\(\\ln 5\\)\nlog(5)\n\n\n\\(2 \\pi\\)\n2 * pi\n\n\n\\(\\frac{1}{2} 17\\)\n(1 / 2) * 17\n\n\n\\(17 - 5 \\div 2\\)\n17 - 5 / 2\n\n\n\\(\\frac{17 - 5}{\\strut 2}\\)\n(17 - 5) / 2\n\n\n\\(3^2\\)\n3^2\n\n\n\\(e^{-2}\\)\nexp(-2)",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Computing with R</span>"
    ]
  },
  {
    "objectID": "Preliminaries/04-graphs-and-graphics.html",
    "href": "Preliminaries/04-graphs-and-graphics.html",
    "title": "4  Visualizing functions",
    "section": "",
    "text": "4.1 Graphics for functions with a single input\nAlthough functions are often defined using a formula, humans are particularly adept at interpreting functions when presented graphically. This chapter is about two basic modes of function graphics, that is graphics which present the relationship between function input(s) and the output.\nThe first graphical model, called a graph, is most appropriate when the function has only one input. The second graphical model, called a contour plot, works well for functions with two inputs.\nWhen it comes to functions with more than two inputs, a good strategy is to simplify by taking a one- or two-dimensional slice through the input space. On this slice, the function can be presented using either a graph or a contour plot.\nMost readers will have encountered graphs of functions in high-school or earlier. Figure 4.1 shows an example: a graph of a function named g(). There is only one input to g().\nMany calculus concepts can be made intuitive by referring to the shapes of function graphs. For now, however, we focus on a simple task: finding from a graph the output values corresponding to a given input. At the risk of telling you what you already know, the process is:\nActive R chunk 4.1 shows an R/mosaic graphics command that draws a graph of a function.\nYou see the typical components of an R/mosaic command: a function name (slice_plot()) followed by a pair of parentheses. Inside the parentheses are two arguments, both of which are required. As it happens, the second argument is being placed on a new line, indented to align with the first argument, but this is solely for human readability. The two arguments are:\nA tilde expression suitable for slice_plot() must have a single input name on the right-hand side, as with the ~ z in Active R chunk 4.1. Other than this restriction, the tilde expression is the same sort you have already used in makeFun().\nRemember that a function’s domain is the space of all possible valid inputs to the function. The domain for most of the functions you studied in high school is infinite, for instance extending from \\(-\\infty\\) to \\(\\infty\\). A graphical domain, in contrast, is usually a finite part of the function domain.\nYou set the graphical domain via the second argument to slice_plot(). This argument will always involve the use of domain(), a function which takes its own argument, which takes the form of a named argument, here z = 0:10. In Active R chunk 4.1 we use z as the name of the argument because z has already been specified by the tilde expression as the input to the function being graphed. The name of the argument to domain() will always correspond to the name used on the right-hand side of the tilde expression. The low and high bounds of the graphical domain are given as numbers. In domain(), the two numbers are separated by a colon, as in 0:10, that is, “low : high.”1\nSometimes the tilde expression will involve parameters, as with 3 * y + b ~ y. You can tell that b is a parameter because it is not listed as an input on the righthand side of the tilde.\nWhen plotting a parameterized function, you must give a numerical value for each parameter. Active R chunk 4.2 gives and example of one what to do this, including the parameter values as named arguments after the domain, that is, outside of the parentheses used with domain().",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/04-graphs-and-graphics.html#graphics-for-functions-with-a-single-input",
    "href": "Preliminaries/04-graphs-and-graphics.html#graphics-for-functions-with-a-single-input",
    "title": "4  Visualizing functions",
    "section": "",
    "text": "Figure 4.1: Graph of g(), a function with one input.\n\n\n\n\n\nFind the horizontal location that corresponds to the given input and draw a vertical line through that location.\nMark the point where the vertical line in (1) intersects the curve.\nRead off the output as the vertical location of the point in (2).\n\n\n\n\n\nActive R chunk 4.1\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\na tilde expression for the function you wish to graph\nthe graphical domain to be covered by the horizontal axis\n\n\n\n\n\n\n\n\n\nActive R chunk 4.2\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/04-graphs-and-graphics.html#sec-drawing-contour-plots",
    "href": "Preliminaries/04-graphs-and-graphics.html#sec-drawing-contour-plots",
    "title": "4  Visualizing functions",
    "section": "4.2 Graphics for functions with two inputs",
    "text": "4.2 Graphics for functions with two inputs\nIn general, the functions we use in modeling can have multiple inputs. So best not to get to fixated on the format in Active R chunk 4.1. That format is appropriate only for functions that have one input.\nOur preferred visual format for a function of two inputs is the “contour plot.” You might be familiar with the idea if you have ever had to use a topographic map for hiking. Active R chunk 4.3 gives an example:\n\n\n\nActive R chunk 4.3: A contour plot\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nNotice that the righthand side of the tilde expression includes the names of two inputs. Consequently the graphical domain needs to be a space with two dimensions. We signal this by giving two arguments to domain(), one for each dimension of the input space. Of course, the names used within domain() have to match the names used in the tilde expression. Note that we use only one domain() call but it has two arguments: one for the horizontal axis, the other for the vertical axis.\nAs you move through MOSAIC Calculus you will learn to identify shapes found in contour plots. Ssome calculus concepts will be easily interpreted as shapes. But for now, consider a basic task that can be accomplished using contour plots: reading off the output value corresponding to given values of the inputs.\nTo understand this, note that the output values are not displayed as position along an axis. Since there are two inputs to the function, we need both axes for input values. The output values are displayed by the contour lines of the plot.\nHere’s the algorithm:\n\nFind the position in the input space at the coordinates of the given input values. For example, in the graph from ?fig-contour-1, the inputs \\(x=0, y=1\\) correspond to the point right in the middle of the graphics domain.\nRead off the output as the value of the contour line passing through that point. As it happens, the contour passing through \\(x=0, y=1\\) is not labeled. You need to infer the label from the labels on neighboring contours. These neighbors have labels “0” and “-2”. Correspondingly, the contour of interest corresponds to the value -1.\n\nWhat to do when there is no contour passing through the input point? For example, in Active R chunk 4.3 the position for input values \\(x=0, y = 1.5\\) does not have a contour. You can see, however, that the position lies between two contours, the one for value -2 and the (unlabelled) one for value -3. Thus, you can infer that the output value at the position \\(x=0, y = 1.5\\) is somewhere between -3 and -2.\nWhen starting out with contour plots, you might prefer to show more contours and make sure that every contour is labeled. You can accomplish this with additional arguments to contour_plot(), as in Active R chunk 4.4.\n\n\n\nActive R chunk 4.4\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nSuch graphics are tediously crowded. Some improvement can be found by skipping the labels on contours. For instance, try Active R chunk 4.4 with skip = 5.\n\n\n\n\n\n\nGiving functions names\n\n\n\nIt can be handy to give a function a name before plotting it. This also helps when evaluating the function at a specific set of inputs. Like this:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/04-graphs-and-graphics.html#spaces-and-graphs-of-functions",
    "href": "Preliminaries/04-graphs-and-graphics.html#spaces-and-graphs-of-functions",
    "title": "4  Visualizing functions",
    "section": "4.3 Spaces and graphs of functions",
    "text": "4.3 Spaces and graphs of functions\nWe are going to step back from the instructions for drawing graphics like Active R chunk 4.1 in order to give you a better perspective on the essentials of making graphical displays of functions. This will help in reading and interpreting graphics, especially graphics for functions of multiple inputs such as in Active R chunk 4.3.\nAs you know, the domain of a function is the set of all possible valid inputs to that function. Chapter 1 defines a space to be a set of possibilities. Thus, a function domain can be seen as a space, namely, the “input space” for the function.\nIn Active R chunk 4.1, the input space is a number line. Each point in the space is a possible input value to the function.\n\n\n\n\n\n\nFigure 4.2: The input space for the function graphed in Active R chunk 4.1.\n\n\n\nThere is also an output space for a function, the set of all possible outputs. Figure 4.3 shows a number line that is the output space for the function graphed in Active R chunk 4.1.\n\n\n\n\n\n\nFigure 4.3: A space suitable for representing output values.\n\n\n\nThe function itself tells us, for every point in the input space, what is the corresponding point in the output space. Figure 4.4 gives an example. The input and output spaces are shown as number lines, while the function is indicated by the thin colored lines.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.4: A function relates points from the input space to their corresponding location in output space.\n\n\n\nFor example, the function displayed in Figure 4.4 translates an input value of -4 (see the green line) into an output value of 3.7.\nThe display in Figure 4.4 is very hard to interpret. Also, to avoid the display being filled up with colored ink, we can show only a few of the input/output pairs.\nA function graph (like Active R chunk 4.1) is much easier to read. But let’s be clear about where the spaces are shown. In a function graph, the input space is shown horizontally and the output space is shown vertically as in Figure 4.5.\n\n\n\n\n\n\nFigure 4.5: A function graph shows the input space horizontally and the output space vertically.\n\n\n\nNow for every point in the input space the function specifies a corresponding point in the output space. We mark the correspondence with a dot. The horizontal coordinate tells us what is the input value. The vertical coordinate tells us what is the corresponding output value. To show the function as a whole—the output corresponding to every input—we would need a lot of dots! So many dots that they collectively give an appearance of a thin curve, the curve seen in Active R chunk 4.1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.6: The mapping from Figure 4.4 translated into the form of a graph. The input space is marked by the horizontal axis and the output space by the vertical axis. Each of the arrows in Figure 4.4 is represented by a point, whose x-coordinate is the position of the tail of the arrow in the input space and whose y-coordinate is the position of the head of the arrow in the output space. This is the graph of the function.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/04-graphs-and-graphics.html#input-and-output-spaces",
    "href": "Preliminaries/04-graphs-and-graphics.html#input-and-output-spaces",
    "title": "4  Visualizing functions",
    "section": "4.4 Input and output spaces",
    "text": "4.4 Input and output spaces\nThis background about spaces is important for understanding functions because, for functions with two or more inputs, the input space and the output space can be depicted in different ways. For example, in the contour plot of Active R chunk 4.3, the input space is the two-dimensional space in the plane of the display (that is, the paper or screen depending on how you are reading this). The output space for the contour plot is depicted using curves, colors, and labels. There is one curve for each output value. Only a handful of output values are shown, but you can get pretty close to estimating the output value even for inputs not on the curve.\nThe space of all possibilities (y, z, output) is three-dimensional, but very few of those possibilities are consistent with the function to be graphed. You can imagine our putting dots at all of those consistent-with-the-function points, or our drawing lots and lots of continuous curves through those dots, but the cloud of dots forms a surface; a continuous cloud of points floating over the (y, z) input space.\nFigure 4.7 displays this surface. Since the image is drawn on a two-dimensional screen, we have to use painters’ techniques of perspective and shading. In the interactive version of the plot, you can move the viewpoint for the image which gives many people a more solid understanding of the surface.\nMath textbooks—but not so much this one!—often display functions with two inputs using a three-dimensional space. This space is made by laying the two-dimensional input space on a table, and sticking the one-dimensional output space perpendicular to the table. Each point in the input space has a corresponding value in the output space which could, in principle, be marked with a dot. The whole set of dots, one for each value in the input space, appears as a surface floating over the table.\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.7: A function g(x, y) ~ x & y drawn as a surface. The function output value, for any given \\((x, y)\\) pair, is the height of the surface above the \\((x, y)\\)-plane. Such graphs are best appreciated by interacting with them. When you move the cursor over the graph, a rectangular frame appears from which you can read out coordinates numerically. Dragging while pressing will rotate the graph. Press the  icon to return to the original view.\n\n\n\nA variety of drawing techniques such as transparency, color, and interactive annotation are used to help us perceive a two-dimensional surface embedded in a three-dimensional space. (Place your cursor within the space delimited by the x, y, z axes to see the annotations.)\nPretty as such surfaces are, a contour plot (Figure 4.8) provides a good view of the same function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.8: The function from Figure 4.7 shown in contour-plot format.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/04-graphs-and-graphics.html#slicing-input-spaces",
    "href": "Preliminaries/04-graphs-and-graphics.html#slicing-input-spaces",
    "title": "4  Visualizing functions",
    "section": "4.5 Slicing input spaces",
    "text": "4.5 Slicing input spaces\nSaying “graph” for a display of \\(f(x)\\) versus \\(x\\) is correct and reasonable. But in MOSAIC Calculus we have another point to make.\nAlmost always, when mathematically modeling a real-world situation or phenomenon, we do not try to capture every nuance of every relationship that might exist in the real world. We leave some things out. Such simplifications make modeling problems easier to deal with and encourage us to identify the most important features of the most important relationships.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.9: The function that we will be taking slices of in Figures 4.10 and 4.11.\n\n\n\nIn this spirit, it is useful always to assume that our models are leaving something out and that a more complete model involves a function with more inputs than the present candidate. The present candidate model should be considered as a slice of a more complete model. Our slice leaves out one or more of the input quantities in a more complete model.\nTo illustrate this, suppose that the actual system involves relationships among three quantities, which we represent in the form of a function of two inputs, as shown in Figure 4.9. (The third quantity in the relationship is the output of the function.)\nThe most common forms of slice involve constructing a simpler function that has one input but not the other. For example, our simpler function might ignore input 22. There are different ways of collapsing the function of two inputs into a function of one input. An especially useful way in calculus is to take the two-input function and set one of the inputs to a constant value.\nFor instance, suppose we set input 22 to the constant value 1.5. This means that we can consider any value of input 1, but input 2 has been replaced by 1.5. In Figure 4.10, we’ve marked in red the points in the contour plot that give the output of the simplified function.\n\n\n\n\n\n\n\n\n\n(a) Contour plot of a function with two inputs. The red path shows points in the input space where input_2 is held constant at 1.5.\n\n\n\n\n\n\n\n\n\n(b) Values of the function at the points along the red path in (a). Since there is effectively one input, the function can be presented as a graph.\n\n\n\n\n\n\n\nFigure 4.10: A slice through the function in Figure 4.9,\n\n\n\nEach point along the red line in Figure 4.10(a) corresponds to a specific value of input #1. From the contours, we can read the output corresponding to each of those values of input #1. This relationship, output versus input #1 can be drawn as a mathematical graph (to the right of the contour plot). Study that graph until you can see how the rising and falling parts of the graph correspond to the contours being crossed by the red line.\nSlices can be taken in any direction or even along a curved path! The blue line in Figure 4.11 shows the slice constructed by letting input 2 vary and holding input 1 at the constant value 0.\n\n\n\n\n\n\n\n\n\n(a) The same function as shown in Figure 4.10 but with a slicing path drawn by holding input_1 at zero.\n\n\n\n\n\n\n\n\n\n(b) A graph of the value of the function along the blue slice. Note the violation of convention: The graph has been flipped on its side so that the input axis aligns with the direction of the slice.\n\n\n\n\n\n\n\nFigure 4.11: Another one-dimensional slice through the graph of a function with two inputs.\n\n\n\nIn much the same way, a function with three or more inputs can be graphed by taking a one- or two-dimensional slice.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/04-graphs-and-graphics.html#footnotes",
    "href": "Preliminaries/04-graphs-and-graphics.html#footnotes",
    "title": "4  Visualizing functions",
    "section": "",
    "text": "R experts should note that the colon has a different meaning within domain() than it does generally in R.↩︎",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/05-pattern-book-functions.html",
    "href": "Preliminaries/05-pattern-book-functions.html",
    "title": "5  Pattern-book functions",
    "section": "",
    "text": "5.1 Exponential and logarithm functions\nOne of the major tasks in modeling is to construct functions that represent a situation or phenomenon of interest. As you will see in later chapters, there are many ways to build a function. For example, you may have data about a relationship. Or, sometimes, you can figure out the shape of a function by asking simple questions about the situation or phenomenon.\nIn this Chapter, we introduce the pattern-book functions—a brief list of basic mathematical functions that apply in a surprisingly large range of situations. Think of the items in the pattern-book list as different actors, each of whom is skilled in portraying an archetypal character: hero, outlaw, lover, fool, comic. A play brings together different characters, costumes them, and relates them to one another through dialog or other means.\nSimilarly, we will start with a pattern set of functions that have been collected from generations of experience. To remind us of their role in modeling, we will call these pattern-book functions. These pattern-book functions are useful in describing diverse aspects of the real world and have simple calculus-related properties that make them relatively easy to deal with. There are just a handful of pattern-book functions from which untold numbers of useful modeling functions can be constructed. Mastering calculus is in part a matter of becoming familiar with the mathematical connections among the pattern-book functions, just as understanding a play calls for becoming familiear with the relationships among the characters.\nTable 5.1 is a list of our pattern-book functions written traditionally and in R. The list of pattern-book functions is short. You should memorize the names and be able easily to associate each name with the traditional notation.\nThe input name used in the table, \\(x\\), is entirely arbitrary. You can (and will) use the pattern-book functions with other quantities—\\(y\\), \\(z\\), \\(t\\), and so on, even zebra if you like.\nIn addition to the names of the pattern-book functions, you should be able to draw their shapes easily. Figure 5.1 provides a handy guide.\nThese pattern-book functions are widely applicable. But nobody would confuse the pictures in a pattern book with costumes that are ready for wear. Each pattern must be tailored to fit the actor and customized to fit the theme, setting, and action of the story. Chapter 8 shows how this is done.\nEach of the pattern-book functions takes a number as input and produces a number as output. Naturally, a proper graph of a pattern-book function should display specific numerical values for input and output. This section shows such quantitative graphs.\nWhen used in modeling, however, we need to modify the functions so that their inputs and outputs are quantities. Chapters 8 and 9 present the main techniques for doing this. This allows us to use the shape of a pattern-book function while customizing the quantitative input-output relationship to serve the situation at hand. In practice, however, it suffices to memorize just a couple of input/output pairs for each function\nYou may wonder why to take the trouble to make a function whose output is always the same. After all, in a formula it shows up simply as the number 1, not looking like a formula at all. But when we start to combine functions (Chapter 9), the constant function will almost always have a role to play.\nThe remaining pattern-book functions all have curved shapes.\nThe Gaussian function shows up so often in modeling that it has another widely used name: the normal function. But “normal” has additional meanings in mathematics, so we will not use that name in this book.\nIf you studied trigonometry, you may be used to the sine of an angle in the context of a right triangle. That is the historical origin of the idea. For our purposes, think of the sinusoid just as a function that takes an input and returns an output. Starting in the early 1800s, oscillatory functions found a huge area of application in representing phenomena such as the diffusion of heat through solid materials and, eventually, radio communications.\nIn trigonometry, the sine has the cosine as a partner. But the two functions \\(\\sin()\\) and \\(\\cos()\\) are so closely connected that we will not often need to make the distinction, calling both of them simply “sinusoids.”\nIt’s a common English phrase to say that something is “growing exponentially.” For instance, at the outbreak of an epidemic there are few cases and slow growth in the number of cases. But as the number of cases increases, the growth gets faster.\nThere is another way to think about exponential growth that makes it easier to comprehend. Again, imagine an epidemic in its eary stages. Suppose there are 20 cases. Time goes by and eventually the number of cases reaches 40, double the initial value. Suppose it takes one week for this doubling. That is, the doubling time is one week.\nNow a second week goes by. If the growth were proportional to the input, after the second week the case number would be 60. For exponential growth, by the end of the second week the output will double. So, there will be 80 cases after two weeks.\nAt the end of the third week the output of the exponential will double again, to 160 cases. Fourth week: 320 cases. Fifth week: 640 cases. To summarize, Table 5.2 shows the number of cases as a function of weeks elapsed.\nNote: You might speculate about where the epidemic will go in the long run. Most things that grow exponentially do so only for a short time. After that, other things come into play that limit or reverse the growth. So, over long periods of time the pattern can be better described as gaussian or sigmoidal.\nI suspect that most people are comfortable with the idea of exponential growth. But many people, even highly-educated professionals, react with fear to the word “logarithmic.” [This fear might originate in the way logarithms are typically taught in high-school is heavily algebraic and taken out of any meaningful contemporary context. Logarithms were invented around 1600 to facilitate by-hand mathematical operations like multiplication and exponentiation. Nowadays, we have computers to perform such calculations.] Chapter 14 argues logarithms are an important tool for understanding quantities that can range from very, very small to very, very large. Table 5.2 gives an example: the left column is a logarithm of the right column.\nThe exponential and logarithmic functions are intimate companions. You can see the relationship by taking the graph of the logarithm, and rotating it 90 degrees, then flipping left for right as in Figure 5.11. (Note in Figure 5.11 that the graph is shown as if it were printed on a transparency which we are looking at from the back.)",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Pattern-book functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/05-pattern-book-functions.html#exponential-and-logarithm-functions",
    "href": "Preliminaries/05-pattern-book-functions.html#exponential-and-logarithm-functions",
    "title": "5  Pattern-book functions",
    "section": "",
    "text": "The exponential function has important applications throughout science, technology, and the economy. For large negative inputs, the value is very close to zero in much the same way as for the Gaussian or sigmoid functions. But the output increases faster and faster as the input gets bigger. Note that the output of the exponential function is never negative for any input.\n\n\n\n\n\n\n\n\n\nFigure 5.9: The output of the exponential function grows faster and faster as the input increases.\n\n\n\n\n\n\n\n\n\n\n\nTable 5.2: The number of cases as it increases over time for an epidemic that grows exponentially with a doubling time of one week..\n\n\n\n\n\nTime (weeks)\nNumber of cases\n\n\n\n\n0 (initial time)\n20\n\n\n1\n40\n\n\n2\n80\n\n\n3\n160\n\n\n4\n320\n\n\n5\n640\n\n\n6\n1280\n\n\n…\n…\n\n\n10\n20,480\n\n\n…\n…\n\n\n15 weeks\n655,360\n\n\n\n\n\n\n\n\n\n\nThe logarithmic function is defined only for positive inputs. As the input increase from just above zero, the output t constantly grows but at a slower and slower rate. It never levels out.\n\n\n\n\n\n\n\n\n\nFigure 5.10: The domain of the logarithm is the positive numbers.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.11: A flipped-over version of Figure 5.10. The exponential and the logarithmic functions are twins, related by reversing the roles of the input and output. The flipped over logarithm function is the same as the exponential function.\n\n\n\n\n\n\n\n\n\nWhy is it called the “logarithm?”\n\n\n\nThe name “logarithm” is anything but descriptive. The name was coined by the inventor, John Napier (1550-1617), to emphasize the original purpose of his invention: to simplify the work of multiplication and exponentiation. The name comes from the Greek words logos, meaning “reasoning” or “reckoning,” and arithmos, meaning “number.” A catchy marketing term for the new invention, at least for those who speak Greek!\nAlthough invented for the practical work of numerical calculation, the logarithm function has become central to mathematical theory as well as modern disciplines such as thermodynamics and information theory. The logarithm is key to the measurement of information and magnitude. As you know, there are units of information used particularly to describe the information storage capacity of computers: bits, bytes, megabytes, gigabytes, and so on. Very much in the way that there are different units for length (cm, meter, kilometer, inch, mile, …), there are different units for information and magnitude. For almost everything that is measured, we speak of the “units” of measurement. For logarithms, instead of “units,” by tradition another word is used: the base of the logarithm. The most common units outside of theoretical mathematics are base-2 (“bit”) and base-10 (“decade”). But the unit that is most convenient in mathematical notation is “base e,” where \\(e = 2.71828182845905...\\). This is genuinely a good choice for the units of the logarithm, but that is hardly obvious to anyone encountering it for the first time. To make the choice more palatable, it is marketed as the “base of the natural logarithm.” In this book, we will be using this natural logarithm as our official pattern-book logarithm.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Pattern-book functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/05-pattern-book-functions.html#sec-power-law-family",
    "href": "Preliminaries/05-pattern-book-functions.html#sec-power-law-family",
    "title": "5  Pattern-book functions",
    "section": "5.2 The power-law family (optional)",
    "text": "5.2 The power-law family (optional)\nFour of the pattern-book functions—\\(1\\), \\(1/x\\), \\(x\\), \\(x^2\\)— belong to an infinite family called the power-law functions. Some other examples of power-law functions are \\(x^3, x^4, \\ldots\\) as well as \\(x^{1/2}\\) (also written \\(\\sqrt{x}\\)), \\(x^{1.36}\\), and so on. Some of these also have special (albeit less frequently used) names, but all of the power-law functions can be written as \\(x^p\\), where \\(x\\) is the input and \\(p\\) is a number.\nYou have been using power-law functions from early in your math and science education. Some examples:1\n\n\n\nTable 5.3: Examples of power-law relationships\n\n\n\n\n\n\n\n\n\n\nSetting\nFunction formula\nexponent\n\n\n\n\nCircumference of a circle\n\\(C(r) = 2 \\pi r\\)\n1\n\n\nArea of a circle\n\\(A(r) = \\pi r^2\\)\n2\n\n\nVolume of a sphere\n\\(V(r) = \\frac{4}{3} \\pi r^3\\)\n3\n\n\nDistance traveled by a falling object\n\\(d(t) = \\frac{1}{2} g t^2\\)\n2\n\n\nGas pressure versus volume\n\\(P(V) = \\frac{n R T}{V}\\)\n\\(-1\\)\n\n\n… perhaps less familiar …\n\n\n\n\nDistance traveled by a diffusing gas\n\\(X(t) = D \\sqrt{\n\\strut t}\\)\n\\(1/2\\)\n\n\nAnimal lifespan (in the wild) versus body mass\n\\(L(M) = a M^{0.25}\\)\n0.25\n\n\nBlood flow versus body mass\n\\(F(M) = b M^{0.75}\\)\n0.75\n\n\n\n\n\n\nOne reason why power-law functions are so important in science has to do with the logic of physical quantities such as length, mass, time, area, volume, force, power, and so on. We introduced this topic in Chapter 1 and will return to it in more detail in Chapter 15.\nWithin the power-law family, it is helpful to know and be able to distinguish between several groups:\n\nThe monomials. These are power-law functions such as \\(m_0(x) \\equiv x^0\\), \\(m_1(x) \\equiv x^1\\), \\(m_2(x) \\equiv x^2\\), \\(\\ldots\\), \\(m_p(x) \\equiv x^p\\), \\(\\ldots\\), where \\(p\\) is a whole number (i.e., a non-negative integer). Of course, \\(m_0()\\) is the same as the constant function, since \\(x^0 = 1\\). Likewise, \\(m_1(x)\\) is the same as the identity function since \\(x^1 = x\\). As for the rest, they have just two general shapes: both arms up for even powers of \\(p\\) (like in \\(x^2\\), a parabola); one arm up and the other down for odd powers of \\(p\\) (like in \\(x^3\\), a cubic). Indeed, you can see in Figure 5.12 that \\(x^4\\) has a similar shape to \\(x^2\\) and that \\(x^5\\) is similar in shape to \\(x^3\\). For this reason, high-order monomials are rarely needed in practice.\n\n\n\n\n\n\n\n\n\n\n\\(x^0\\), that is, 1\n\n\n\n\n\n\n\n\n\n\\(x^1\\), that is, \\(x\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(x^2\\)\n\n\n\n\n\n\n\n\n\n\\(x^3\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(x^4\\)\n\n\n\n\n\n\n\n\n\n\\(x^5\\)\n\n\n\n\n\n\n\nFigure 5.12: The first six monomial functions: \\(x^0\\), \\(x^1\\), \\(x^2\\), \\(x^3\\), \\(x^4\\), and \\(x^5\\). In each plot the dashed magenta line marks zero output.\n\n\n\n\nThe negative powers. These are power-law functions where \\(p&lt;0\\), such as \\(f(x) \\equiv x^{-1}\\), \\(g(x) \\equiv x^{-2}\\), \\(h(x) \\equiv x^{-1.5}\\). For negative powers, the size of the output is inversely proportional to the size of the input. In other words, when the input is large (not close to zero) the output is small, and when the input is small (close to zero), the output is very large. This behavior happens because a negative exponent like \\(x^{-2}\\) can be rewritten as \\(\\frac{1}{x^2}\\); the input is inverted and becomes the denominator, hence the term “inversely proportional”.\n\n\n\n\n\n\n\n\n\n\n\\(x^{-1}\\), that is, \\(1/x\\)\n\n\n\n\n\n\n\n\n\n\\(x^{-2}\\), that is, \\(1/x^2\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(x^{-3}\\), that is, \\(1/x^3\\)\n\n\n\n\n\n\n\n\n\n\\(x^{-4}\\), that is, \\(1/x^4\\)\n\n\n\n\n\n\n\nFigure 5.13: Graphs of power-law functions with negative integer exponents. The arrows point to the output being very large when \\(x\\) is near zero.\n\n\n\n\nThe non-integer powers, e.g. \\(f(x) = \\sqrt{x}\\), \\(g(x) = x^\\pi\\), and so on. When \\(p\\) is either a fraction or an irrational number (like \\(\\pi\\)), the real-valued power-law function \\(x^p\\) can only take non-negative numbers as input. In other words, the domain of \\(x^p\\) is \\(0\\) to \\(\\infty\\) when \\(p\\) is not an integer. You have likely already encountered this domain restriction when using the power law with \\(p=\\frac{1}{2}\\) since \\(f(x)\\equiv x^{1/2}=\\sqrt{x}\\), and the square root of a negative number is not a real number. You may have heard about the imaginary numbers that allow you to take the square root of a negative number, but for the moment, you only need to understand that when working with real-valued power-law functions with non-integer exponents, the input must be non-negative. (The story is a bit more complicated since, algebraically, rational exponents like \\(1/3\\) or \\(1/5\\) with an odd-valued denominator can be applied to negative numbers. Computer arithmetic, however, does not recognize these exceptions.)\n\n\n\n\n\n\n\n\n\n\n\\(x^{1/2}\\)\n\n\n\n\n\n\n\n\n\n\\(x^\\pi\\)\n\n\n\n\n\n\n\nFigure 5.14: The domain of power-law functions with non-integer power is \\(0 \\leq x &lt; \\infty\\).\n\n\n\n\n\n\n\n\n\nComputing power-law functions\n\n\n\nWhen a function like \\(\\sqrt[3]{x}\\) is written as \\(x^{1/3}\\) make sure to include the exponent in grouping parentheses: x^(1/3). Similarly, later in the book you will encounter power-law functions where the exponent is written as a formula. Particularly common will be power-law functions written \\(x^{n-1}\\) or \\(x^{n+1}\\). In translating this to computer notation, make sure to put the formula within grouping parentheses, for instance x^(n-1) or x^(n+1).",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Pattern-book functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/05-pattern-book-functions.html#domains-of-pattern-book-functions",
    "href": "Preliminaries/05-pattern-book-functions.html#domains-of-pattern-book-functions",
    "title": "5  Pattern-book functions",
    "section": "5.3 Domains of pattern-book functions",
    "text": "5.3 Domains of pattern-book functions\nEach of our basic modeling functions, with two exceptions, has a domain that is the entire number line \\(-\\infty &lt; x &lt; \\infty\\). No matter how big or small is the value of the input, the function has an output. Such functions are particularly nice since we never have to worry about the input going out of bounds.\nThe two exceptions are:\n\nthe logarithm function, which is defined only for \\(0 &lt; x\\).\nsome of the power-law functions: \\(x^p\\).\n\nWhen \\(p\\) is negative, the output of the function is undefined when \\(x=0\\). You can see why with a simple example: \\(g(x) \\equiv x^{-2}\\). Most students had it drilled into them that “division by zero is illegal,” and \\(g(0) = \\frac{1}{0} \\frac{1}{0}\\), a double law breaker.\nWhen \\(p\\) is not an integer, that is \\(p \\neq 1, 2, 3, \\cdots\\) the domain of the power-law function does not include negative inputs. To see why, consider the function \\(h(x) \\equiv x^{1/3}\\).\n\n\n\n\n\n\n\n\nExponentials are not power-laws\n\n\n\nIt is essential that you recognize that the exponential function is utterly different from the functions from the power-law family.\nAn exponential function, for instance, \\(e^x\\) or \\(2^x\\) or \\(10^x\\) has a constant quantity raised to a power set by the input to the function.\nA power-law function works the reverse way: the input is raised to a constant quantity, as in \\(x^2\\) or \\(x^10\\).\nA mnemonic phrase for exponentials functions is\n\nExponential functions have \\(x\\) in the exponent.\n\nOf course, the exponential function can have inputs with names other than \\(x\\), for instance, \\(f(y) \\equiv 2^y\\), but the name “x” makes for a nice alliteration in the mnemonic.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Pattern-book functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/05-pattern-book-functions.html#footnotes",
    "href": "Preliminaries/05-pattern-book-functions.html#footnotes",
    "title": "5  Pattern-book functions",
    "section": "",
    "text": "The animal lifespan relationship is true when comparing species. Individual-to-individual variation within a species does not follow this pattern.↩︎",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Pattern-book functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/06-describing-functions.html",
    "href": "Preliminaries/06-describing-functions.html",
    "title": "6  Describing functions",
    "section": "",
    "text": "6.1 Slope\nEach pattern-book function is like a unique Lego block. With Lego, you build a castle or an airplane by combining the basic blocks together. Similarly, Chapters 8 and 9 introduce ways to build customized functions out of the pattern-book functions.\nBefore we start to build such customized functions, it is helpful to have a word vocabulary for describing the various features we might want to include in a customized function. Knowing and correctly using a handful of phrases about functions with a single input goes a long way in being able to communicate with other people . Often, the words make sense in everyday speech (“steep”, “growing”, “decaying”, “goes up”, “goes down”, “flat”).\nSometimes the words are used in everyday speech but the casual person isn’t sure exactly what they mean. For instance, you will often hear the phrase “growing exponentially.” The graph of the exponential function illustrates exactly this sort of growth: flat for small \\(x\\) and growing steadily steeper and steeper as \\(x\\) increases.\nStill other words are best understood by those who learn calculus. “Concave up,” “concave down”, “approaching 0 asymptotically,” “continuous”, “discontinuous”, “smooth”, “having a minimum at …,” “having a minimum of …”, “approaching \\(\\infty\\) asymptotically,” “having a vertical asymptote.”\nThe next short sections describe seven simple function-shape concepts:\nEach of these concepts has the idea of a function at the core, because each one depends on how the function output changes as the input is changed.\nFor the sake of illustration, I’ll use three pattern-book functions to illustrate the function-shape concepts. (Figure 6.1) However, that the shape words can be applied to almost every function you will see in this book. (To be more precise, the shape words apply to every continuous function in this book. See Section 6.3.)\nSlope describes whether the output goes up or down, and the extent of this rise or fall, as the input changes. Typically, except for the constant and straight-line functions the slope is different for different input values.\nFigure 6.2 graphs the sinusoid function (black curve). At numerous points in the domain, the function has been overlaid with a straight-line segment that has the same slope as does the function itself. For \\(x\\) near \\(-3\\) the slope is negative; for \\(x\\) near zero the slope is positive, then swings back to negative again for \\(x\\) near \\(3\\).\nWhen we speak of the slope of the sinusoid, or any other function, we mean the local slope as a function of the input. The value of the function does not enter into it, just the slope. Figure 6.3 shows only the slope of the sinusoid, without the sinusoid output at all. Each line segment has a horizontal “run” of \\(0.1\\), so you can measure the slope of each segment—rise over run—as the vertical extent \\(\\Delta y\\) of the segment divided by \\(0.1\\).\nFor instance, the \\(\\Delta y\\) for the slope segment at \\(x=0\\) is 0.1, so the slope at \\(x=0\\) is \\(\\Delta y/0.1 = 1\\). At \\(x=1\\), \\(\\Delta y \\approx 0.05\\), so the slope is 0.5. The graph colors the segment according to the slope, so large negative slopes are blue, slopes near zero are green, and large positive slopes are yellow.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Describing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/06-describing-functions.html#slope",
    "href": "Preliminaries/06-describing-functions.html#slope",
    "title": "6  Describing functions",
    "section": "",
    "text": "Figure 6.2: Short straight-line segments laid over a graph of the sinusoid. The slope of each line segment is selected to match the local slope of the sinusoid. The red annotations show several different slopes quantitatively.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.3: Showing just the slope of the sinusoid as a function of input \\(x\\). Top: representing the slope by the steepness of line segments. Bottom: The numerical value of the slope of each segment.\n\n\n\n\n\n\n\n\n\n\nDefinition: “change” versus “rate of change”\n\n\n\nA more general word than “slope” for describing functions is rate of change. It is absolutely crucial to distinguish between the change in the output value of a function and the rate of change of that output.\nTo illustrate, suppose we have a function \\(f(x) \\equiv x^2 + 3\\). When we talk about “change” we imagine a situation where we have to different values of the function input, say \\(x_1 = 3\\) and \\(x_2 = 6\\).\nThe “change” in output for these two different inputs is \\(f(x_2) - f(x_1)\\), or in this case \\(39 - 12 = 27\\).\nIn contrast, the “rate of change” is the change in output divided by the change in input, that is:\n\\[\\frac{f(x_2) - f(x_1)}{x_2 - x_1} = \\frac{27}{3} = 9\\ .\\]\nA “rate” in mathematics is a ratio: one measure divided by another. For instance, a heart rate is measured as beats-per-minute. To measure it, count the number of pulse waves in a given interval of time. A typical medical practice is to count for 15 seconds, an interval long enough to get a reliable count but short enough not to unduly prolong the process. If 18 pulse waves were counted in the 15 seconds, the heart rate is 18 beats per 15 seconds, more usually reported as 72 beats-per-minute.\nIn a rate of change, the ratio is the change in output divided by the change of input.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Describing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/06-describing-functions.html#sec-concavity-intro",
    "href": "Preliminaries/06-describing-functions.html#sec-concavity-intro",
    "title": "6  Describing functions",
    "section": "6.2 Concavity",
    "text": "6.2 Concavity\nThe slope of a function at a given input tells how fast the function output is increasing or decreasing as the input changes slightly. Concavity is not directly about how the function output changes, but about how the function’s slope changes. For instance, a function might be growing slowly in some region of the domain and then gradually shift to larger growth in an adjacent region. Or, a function might be decaying steeply and then gradually shift to a slower decay. Both of these are instances of positive concavity. The opposite pattern of change in slope is called negative concavity. If the slope does not change at all—only straight-line functions are this way— the concavity is zero.\nConcavity has a very clear appearance in a function graph. If a function is positive concave in a region, the graph looks like a smile or cup. Negative concavity looks like a frown. Zero concavity is a straight line.\nReferring to the three function examples in Figure 6.1, we will use the traditional terms concave up and concave down to refer to positive and negative concavity respectively.\n\nThe exponential is concave up everywhere in its domain.\nThe sinusoid alternates back and forth between concave up and concave down.\nThis particular power law \\(x^{-1}\\) is concave up for \\(0 &lt; x\\) and concave down for \\(x &lt; 0\\).\n\nWhen a function switches between positive concavity and negative concavity, as does the sinusoid as well as the gaussian and sigmoid functions, there is an input value where the switch occurs and the function has zero concavity. (Continuous functions that pass from negative to positive or vice versa must always cross zero.) Such in-between points of zero concavity are called inflection points. A function can have zero, one, or many inflection points. For instance, the sinusoid has inflection points at \\(x = \\ldots, -\\pi, 0, \\pi, 2\\pi, \\ldots\\). In contrast, the exponential and the reciprocal functions do not have any inflection points.\n\n\n\n\n\n\nFigure 6.4: Mnemonics for remembering the meaning of “concave up” and “concave down.” In functions like the sinusoid, which have adjacent concave up and concave down regions, there is a point inbetween where the slope doesn’t change. This is called the “inflection point.”\n\n\n\n“Inflection point” appears in news stories, so it is important to know what it means in context. The mathematical definition is about the change in the direction of curvature of a graph. In business, however, it generally means something less esoteric, “a time of significant change in a situation” or “a turning point.”1 The business sense effectively means that the function—say profits as a function of time, or unemployment as a function of time—has a non-zero concavity, up or down. It is about the existence of concavity rather than about the change in the sign of concavity.\nOne of the benefits of learning calculus is to gain a way to think about the previous paragraph that is systematic, so it is always easy to know whether you are talking about the slope of a function or the change in slope of a function.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Describing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/06-describing-functions.html#sec-continuity-intro",
    "href": "Preliminaries/06-describing-functions.html#sec-continuity-intro",
    "title": "6  Describing functions",
    "section": "6.3 Continuity",
    "text": "6.3 Continuity\nA function is continuous if you can trace out the graph of the function without lifting pencil from the page. A function is continuous on an interval (a,b) if you can trace the function over that whole interval.\nAll of the pattern-book functions are continuous over any interval in their domain except for power-law functions with negative exponents. (This includes the reciprocal since it is a power-law with a negative exponent: \\(1/x = x^{-1}\\).) Those exceptions are not defined at \\(x=0\\).\nOn any interval (a,b) that does not include 0, the reciprocal function is continuous. For inputs \\(x &lt; 0\\), the function is negative. For inputs \\(0 &lt; x\\), the function is positive. But, on an interval that includes \\(x=0\\) the function jumps discontinuously from negative to positive.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Describing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/06-describing-functions.html#sec-monotonicity",
    "href": "Preliminaries/06-describing-functions.html#sec-monotonicity",
    "title": "6  Describing functions",
    "section": "6.4 Monotonicity",
    "text": "6.4 Monotonicity\nA function is monotonic on a domain when the sign of the slope never changes on that domain. Monotonic functions either steadily increase in value or, alternatively, steadily decrease in value.\nAnother way of thinking about monotonicity is to consider the order of inputs and outputs compared to a number line. If a function is monotonically increasing then it will preserve the order of inputs along the number line when it maps inputs to outputs, whereas a monotonically decreasing function will reverse the order. For instance, if the input \\(x\\) comes before an input \\(y\\) (i.e., \\(x&lt;y\\)), then \\(f(x)&lt;f(y)\\) for monotonically increasing functions (the order is preserved), but \\(f(y)&lt;f(x)\\) for monotonically decreasing functions (the order of outputs is reversed).\nOf the pattern-book functions in Figure 6.1: both the exponential and the logarithm function are monotonic: the exponential grows monotonically as does the logarithm. The sinusoid is not monotonic over any domain longer than half a cycle: the function switches between positive slope and negative slope in different parts of the cycle (as is evident in Figure 6.3).",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Describing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/06-describing-functions.html#periodicity",
    "href": "Preliminaries/06-describing-functions.html#periodicity",
    "title": "6  Describing functions",
    "section": "6.5 Periodicity",
    "text": "6.5 Periodicity\nA phenomenon is periodic if it repeats a pattern over and over again. The pattern that is repeated is called a cycle; the periodic function as a whole is one cycle placed next to the previous one and so forth. The day-night cycle is an example of a periodic phenomenon, as is the march of the seasons. The period is the duration of one complete cycle; the period of the day-night cycle is 24 hours, the period of the seasonal progression is 1 year.\nReal-world periodic phenomena often show some slight variation from one cycle to the next. Of the pattern-book functions, only the sinusoid is periodic. And it is exactly periodic, repeating the same cycle over and over again. The period—that is, the length of an input interval that contains exactly one cycle—has a value of \\(2\\pi\\) for the pattern-book sinusoid. When used to model a periodic phenomenon, the model function needs to be tailored to match the period of the phenomena.\nThe idea of representing with sinusoids phenomena that are almost but not exactly periodic, for instance a communications signal or a vibration, is fundamental to many areas of physics and engineering.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Describing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/06-describing-functions.html#asymptotic-behavior",
    "href": "Preliminaries/06-describing-functions.html#asymptotic-behavior",
    "title": "6  Describing functions",
    "section": "6.6 Asymptotic behavior",
    "text": "6.6 Asymptotic behavior\nAsymptotic refers to two possible situations depending on whether the input or output is being considered:\n\nWhen the input to a function gets bigger and bigger in size, going to \\(\\infty\\) or \\(-\\infty\\). If, as the input changes in this way the output gets closer and closer to a specific value, the function is said to have a horizontal asymptote of that value.\n\nLook at the graph of the exponential function in Figure 6.1. As \\(x \\rightarrow -\\infty\\), that is, as \\(x\\) goes more and more to the left of the domain, the output of the exponential function tends asymptotically to zero.\n\nWhen the output of a function gets bigger and bigger in size, going to \\(\\infty\\) or \\(-\\infty\\) without the input doing likewise. The visual appearance on a graph is like a sky-rocket: the output changes tremendously fast even though the input changes only a little. The vertical line that the skyrocket approaches is called a vertical asymptote. The power-law function \\(x^{-1}\\) has a vertical asymptote at \\(x=0\\). If you were to consider inputs closer and closer to \\(x=0\\), the outputs would grow larger and larger is magnitude, tending toward \\(\\infty\\) or \\(-\\infty\\).\n\nSeveral of the pattern-book functions have horizontal or vertical asymptotes or both. For instance, the reciprocal function (\\(x^{-1}\\)) has a horizontal asymptote of zero for both \\(x \\rightarrow \\infty\\) and \\(x \\rightarrow -\\infty\\).\nThe sinusoid has neither a vertical nor a horizontal asymptote. As input \\(x\\) increases either to \\(-\\infty\\) or \\(\\infty\\), the output of the sinusoid continues to oscillate, never settling down to a single value. And, of course, the output of the sinusoid is everywhere \\(-1 \\leq \\sin(x) \\leq 1\\), so there is no possibility for a vertical asymptote.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Describing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/06-describing-functions.html#sec-local-extremes",
    "href": "Preliminaries/06-describing-functions.html#sec-local-extremes",
    "title": "6  Describing functions",
    "section": "6.7 Locally extreme points",
    "text": "6.7 Locally extreme points\nMany continous functions have a region of the input domain where the output is gradually growing, then reaches a peak, then gradually diminishes. This peak is called a local maximum. “Maximum” because the output reaches a peak at a particular input, “local” because in the neighborhood of the peak the function output is smaller than at the peak.\nLikewise, functions can have a local minimum: the bottom of a bowl rather than the top of a peak.\nOf the three pattern-book functions in Figure 6.1, only the sinusoid has a local maximum, and, being periodic, it repeats that every cycle. The sinusoid similarly has a local minimum in every cycle..\nMany modeling applications involve finding an input where the function output is maximized. Such an input is called an argmax. “Argument” is a synonym for “input” in mathematical and computer functions, so “argmax” refers to the input at which the function reaches a maximum output. For instance, businesses attempt to set prices to maximize profit. At too low a price, sales are good but income is low. At too high a price, sales are too low to bring in much income. There is a sweet spot in the middle.\nOther modeling applications involve finding an argmin, the input for which the output is minimized. For instance, aircraft have a speed at which fuel consumption is at a minimum for the distance travelled. All other things being equal, it is best to operate at this speed.\nThe process of finding an argmin or an argmax is called optimization. And since maxima and minima are very much the same mathematically, collectively they are called extrema.\nAny function that has an extremum cannot possibly be monotonic, since the growth is positive on one side of the extremum and negative on the other side.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Describing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/06-describing-functions.html#footnotes",
    "href": "Preliminaries/06-describing-functions.html#footnotes",
    "title": "6  Describing functions",
    "section": "",
    "text": "Google dictionary, provided by Oxford Languages↩︎",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Describing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/07-data-functions-graphics.html",
    "href": "Preliminaries/07-data-functions-graphics.html",
    "title": "7  Data, functions, graphics",
    "section": "",
    "text": "7.1 Data frames\nThe decade of the 1660s was hugely significant in the emergence of science, although no one realized it at the time. 1665 was a plague year, the last major outbreak of bubonic plague in England. The University of Cambridge closed to wait out the plague.\nIsaac Newton, then a 24-year old Cambridge student, returned home to Woolsthorpe where he lived and worked in isolation for two years. Biographer James Gleich wrote: “The plague year was his transfiguration. Solitary and almost incommunicado, he became the world’s paramount mathematician.” During his years of isolation, Newton developed what we now call “calculus” and, associated with that, his theory of Universal Gravitation. He wrote a tract on his work in 1669, but withheld it from publication until 1711.\nNewton (1642-1726) was elected to the prestigious Royal Society at an early age. There, he may well have met another member of the Society, John Graunt(1620-1674). Graunt was well known for collecting data on causes of death in London. Figure 7.1 shows his data for one week in 1665.\nNeither Newton nor Graunt could have anticipated the situation today. Masses of data are collected and analyzed. Mathematical functions are essential to the analysis of complex data. To give one example: Artificial intelligence products such as large-language models (e.g. Chat-GPT) are based on the construction of mathematical functions (called “neural nets”) that take large numbers of inputs and process them into an output. The construction methods are rooted in optimization, one of the topics in Newton’s first calculus book.\nSince data is so important, and since Calculus underlies data analysis techniques, it makes sense that a course on Calculus should engage data analysis as a core topic. This book emphasizes data-related aspects of Calculus although traditional books, (even in the 21st century) mainly follow an old, data-free curriculum.\nThis chapter introduces some basic techniques for working with data. This will enable you to apply important data-oriented methods of Calculus that we will encounter in later chapters.\nThe organization of data as a table (as in Figure 7.1) is almost intuitive. However, data science today draws on much more sophisticated structures that are amenable to computer processing.\nData science is strikingly modern. Relational databases, the prime organization of data used in science, commerce, medicine, and government was invented in the 1960s. All data scientists have to master working with relational databases, but we will use only one component of them, the data frame.\nLet’s consider what Graunt’s 1665 data might look like in a modern data frame. Remember that the data in Figure 7.1 covers only one week (Aug 15-22, 1665) in only one place (London). A more comprehensive data frame might include data from other weeks and other places:\nAs you can see, the data frame is organized into columns and rows. Each column is called a variable and contains entries that are all the same kind of thing. For example, the location variable has city names. The deaths variable has numbers.\nEach row of the table corresponds to a unique kind of thing called a unit of observation. It’s not essential that you understand exactly what this means. It suffices to say that the unit of observation is a “condition of death during a time interval in a place.” Various everyday words are used for a single row: instance, case, specimen (my favorite), tupple, or just plane row. In a data frame, all the rows must be the same kind of unit of observation.\nThe modern conception of data makes a clear distinction between data and the construction of summaries of that data for human consumption. Such summaries might be graphical, or in the form of model functions, or even in the form of a set of tables, such as seen in the Bill of Mortality. Learning how to generate such summaries is an essential task in statistics and data science. The automatic construction of model functions (without much human intervention) is a field called machine learning, one kind of “artificial intelligence.”\nA data scientist would know how to process (or, “wrangle”) such data, for instance to use the begins and stops variables to calculate the duration of the interval covered. She would also be able to “join” the data table to others that contain information such as the population of the city or the mean temperature during the interval.\nTechnology allows us to store very massive data frames along with allied data. For example, a modern “bill of mortality” might have as a unit of observation the death of an individual person, including date, age, sex, occupation, and so on. Graunt’s bill of mortality encompasses 5319 deaths. Given that the population of the world in the 1660s was about 550 million, a globally comprehensive data frame on deaths covering only one year would have about 20 million rows. (Even today, there is no such globally comprehensive data, and in many countries births and deaths are not uniformly recorded.)\nA modern data wrangler would have no problem with 20 million rows, and would easily be able to pull out the data Graunt needed for his Aug. 15-22, 1665 report, summarizing it by the different causes of death and even breaking it down by age group. Such virtuosity is not needed for our purposes.\nThe basics that you need for our work with data are:",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data, functions, graphics</span>"
    ]
  },
  {
    "objectID": "Preliminaries/07-data-functions-graphics.html#sec-data-frames",
    "href": "Preliminaries/07-data-functions-graphics.html#sec-data-frames",
    "title": "7  Data, functions, graphics",
    "section": "",
    "text": "Table 7.1: A data frame organization of some data from Figure 7.1. Additional imagined “data” (in italics) has been added to illustrate why so many columns are needed, rather than Graunt’s two-column layout.\n\n\n\n\n\ncondition\ndeaths\nbegins\nstops\nlocation\n\n\n\n\nkingsevil\n10\n1665-08-15\n1665-08-22\nLondon\n\n\nlethargy\n1\n1665-08-15\n1665-08-22\nLondon\n\n\npalsie\n2\n1665-08-15\n1665-08-22\nLondon\n\n\nplague\n3880\n1665-08-15\n1665-08-22\nLondon\n\n\nspotted feaver\n190\n1665-07-12\n1665-07-19\nParis\n\n\nconsumption\n219\n1665-07-12\n1665-07-19\nParis\n\n\ncancer\n5\n1665-07-12\n1665-07-19\nParis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData frames are accessed by a file name.\nIndividual columns are accessed by a variable name.\nWe will use tilde expressions to identify one variable as the “response” variable and other variables as “explanatory variables.” The response corresponds to the output of a function, the explanatory variables are the inputs. See Tip 7.2.\nWe will use software to construct functions that capture important patterns in the data, but that is a topic for later chapters.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data, functions, graphics</span>"
    ]
  },
  {
    "objectID": "Preliminaries/07-data-functions-graphics.html#accessing-data-tables",
    "href": "Preliminaries/07-data-functions-graphics.html#accessing-data-tables",
    "title": "7  Data, functions, graphics",
    "section": "7.2 Accessing data tables",
    "text": "7.2 Accessing data tables\nFor our work, you can access the data frames we need directly by name in R. For instance, the Engines data frame (Table 7.2) records the characteristics of several internal combustion engines of various sizes:\n\n\n\nTable 7.2: Various attributes of internal combustion engines, from the very small to the very large. Engines has 39 rows; only 8 are seen here.\n\n\n\n\n\nEngine\nmass\nBHP\nRPM\nbore\nstroke\n\n\n\n\nWebra Speed 20\n0.25\n0.78\n22000\n16.5\n16\n\n\nEnya 60-4C\n0.61\n0.84\n11800\n24.0\n22\n\n\nHonda 450\n34.00\n43.00\n8500\n70.0\n58\n\n\nJacobs R-775\n229.00\n225.00\n2000\n133.0\n127\n\n\nDaimler-Benz 609\n1400.00\n2450.00\n2800\n165.0\n180\n\n\nDaimler-Benz 613\n1960.00\n3120.00\n2700\n162.0\n180\n\n\nNordberg\n5260.00\n3000.00\n400\n356.0\n407\n\n\nCooper-Bessemer V-250\n13500.00\n7250.00\n330\n457.0\n508\n\n\n\n\n\n\nThe fundamental questions to ask first about any data frame are:\n\nWhat constitutes a row?\nWhat are the variables and what do they stand for?\n\nThe answers to these questions, for the data frames we will be using, are available via R documentation. To bring up the documentation for Engines, for instance, give the command:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWhen working with data, it is common to forget for a moment what are the variables, how they are spelled, and what sort of values each variable takes on. Two useful commands for reminding yourself are (illustrated here with Engines):\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn RStudio, the command View(Engines) is useful for showing a complete table of data in printed format. This may be useful for our work in this book, but is only viable for data frames of moderate size.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data, functions, graphics</span>"
    ]
  },
  {
    "objectID": "Preliminaries/07-data-functions-graphics.html#plotting-data",
    "href": "Preliminaries/07-data-functions-graphics.html#plotting-data",
    "title": "7  Data, functions, graphics",
    "section": "7.3 Plotting data",
    "text": "7.3 Plotting data\nWe will use just one graphical format for displaying data: the point plot. In a point plot, also known as a “scatterplot,” two variables are displayed, one on each graphical axis. Each case is presented as a dot, whose horizontal and vertical coordinates are the values of the variables for that case. For instance:\n\n\n\nActive R chunk 7.1: Running the code will create a point plot showing the relationship between engine stroke and bore. Each individual point is one row of the data frame\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nLater in MOSAIC Calculus, we will discuss ways to construct functions that are a good match to data using the pattern-book functions. Here, our concern is graphing such functions on top of a point plot. So, without explanation (until later chapters), we will construct a power-law function, called, stroke(bore), that might be a good match to the data. The we will add a second layer to the point-plot graphic: a slice-plot of the function we’ve constructed.\n\n\n\nActive R chunk 7.2: Code to make a graphic composed of two layers: 1) a point plot; 2) a slice plot of a power-law function named stroke() fitted to the data.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nThe second layer is made with an ordinary slice_plot() command. To place it on top of the point plot we connect the two commands with a bit of punctuation called a “pipe”: |&gt;.\n\n\n\n\n\n\nTip 7.1: Pipe punctuation\n\n\n\nThe pipe punctuation can never go at the start of a line. Usually, we will use the pipe at the very end of a line; think of the pipe as connecting one line to the next.\n\n\nslice_plot() is a bit clever when it is used after a previous graphics command. Usually, you need to specify the interval of the domain over which you want to display the function, as with …\n\nslice_plot(stroke_model(bore) ~ bore, domain(bore=0:1000))\n\n\n\n\n\n\n\n\nYou can do that also when slice_plot() is the second layer in a graphics command. But slice_plot() can also infer the interval of the domain from previous layers as in …\n\n1gf_point(stroke ~ bore, data = Engines) |&gt;\n2  slice_plot(stroke_model(bore) ~ bore)\n\n\n1\n\nThe graphics domain is set by the data.\n\n2\n\nslice_plot() inherits the domain from the layer created in (1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip 7.2: Response/Explanatory, Output/Inputs\n\n\n\nIn previous chapters, you have seen tilde expressions in use for two purposes:\n\nConstructing a function from a formula. e.g. g &lt;- makeFun(a*x + b ~ x)\nDirecting a slice plot, e.g. slice_plot(stroke(bore) ~ bore)\n\nThis chapter expands the use of tilde expressions to two new tasks:\n\nPlotting data, e.g. gf_point(stroke ~ bore)\nFitting a function to data, e.g. fitModel(stroke ~ A*bore^b, data=Engines)\n\nThere is an important pattern shared by all these tasks. In the tilde expression, the output is on the left-hand side of ~ while the inputs are on the right-hand side. That is:\n      function output          input(s)\nFor historical reasons, mathematics and data science/statistics use different terms. In data science, the terms “response” and “explanatory” are used instead of “input” and “output.”\n      response variable          explanatory variable(s)\nIn the tilde expression for fitModel() used above, the response variable is stroke while the explanatory variable is named in the RHS of the expression. As it happens, in fitModel() the RHS needs to do double duty: (1) name the explanatory variables (2) specify the formula for the function produced by fitModel().",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data, functions, graphics</span>"
    ]
  },
  {
    "objectID": "Preliminaries/07-data-functions-graphics.html#sec-functions-as-data",
    "href": "Preliminaries/07-data-functions-graphics.html#sec-functions-as-data",
    "title": "7  Data, functions, graphics",
    "section": "7.4 Functions as data",
    "text": "7.4 Functions as data\nIt is helpful to think of functions, generally, as a sort of data storage and retrieval device that uses the input value to locate the corresponding output and return that output to the user. Any device capable of this, such as a table or graph with a human interpreter, is a suitable way of implementing a function.\nTo reinforce this idea, we ask you to imagine a long corridor with a sequence of offices, each identified by a room number. The input to the function is the room number. To evaluate the function for that input, you knock on the appropriate door and, in response, you will receive a piece of paper with a number to take away with you. That number is the output of the function.\nThis will sound at first too simple to be true, but … In a mathematical function each office gives out the same number every time someone knocks on the door. Obviously, being a worker in such an office is highly tedious and requires no special skill. Every time someone knocks on the worker’s door, he or she writes down the same number on a piece of paper and hands it to the person knocking. What that person will do with the number is of absolutely no concern to the office worker.\n\n\n\n\n\n\nFigure 7.2: Part of the first table of logarithms, published by Henry Briggs in 1624.\n\n\n\nThe reader familiar with floors and corridors and office doors may note that the addresses are discrete. That is, office 321 has offices 320 and 322 as neighbors. But Calculus is mainly about functions with a continuous domain.\nFortunately, it is easy to create a continuous function out of a discrete table by adding on a small, standard calculation called “interpolation.” The simplest form, called “linear interpolation,” works like this: for an input of, say, 321.487… the messenger goes to both office 321 and 322 and collects their respective outputs. Let’s imagine that they are -14.3 and 12.5 respectively. All that is needed is a small calculation, which in this case will look like \\[-14.3 \\times (1 - 0.487...)   + 12.5 \\times 0.487...\\]\nChapter 49 introduces a modern, sophisticated form of interpolation that makes smooth functions.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data, functions, graphics</span>"
    ]
  },
  {
    "objectID": "modeling-part.html",
    "href": "modeling-part.html",
    "title": "BLOCK I. Modeling",
    "section": "",
    "text": "A model is a representation of something—for instance, a building—for a specific purpose. A model of a building might take several different forms, depending on the purpose. For example, a blueprint plan shows the layout of rooms, corridors, windows, etc. for the purpose of exploring the design and guiding the construction. A three-dimensional balsa-wood model of a building helps to examine how the building appears from different perspectives. A list of components in the building is a model whose purpose is to facilitate ordering those components and checking whether everything needed is at hand during construction.\nA mathematical model is a model made up of mathematical stuff. Balsa-wood and blueprint paper are not mathematical stuff. This Block introduces some of the different kinds of mathematical stuff that we will use in constructing mathematical models. Of primary importance will be mathematical functions. The block describes various aspects of functions: parameters and the ways to set their values, ways to construct complicated functions out of simpler components, a particularly useful type of function—polynomials—that are a type of modeling “clay.”\nUsing mathematical models often involves performing a mathematical operation on a model. One type of operation emphasized in high-school math is solving which takes information in one form (e.g., a function) and gives information in another form (e.g., the inputs that will cause the function output to take on a specific value). Other kinds of operations are optimization and iteration, which will be introduced in this Block.\nAlthough functions are fundamental to mathematical modeling, there are other mathematical and scientific concepts that make it much easier to think about how a model relates to the real world. Two of these are the mathematics of magnitude and the units and dimension of quantities.\nFinally, the block considers the process of building models and techniques that help a human modeler to get started and then refine the model until it can serve its purpose.",
    "crumbs": [
      "BLOCK I. Modeling"
    ]
  },
  {
    "objectID": "Modeling/08-parameters.html",
    "href": "Modeling/08-parameters.html",
    "title": "8  Parameters",
    "section": "",
    "text": "8.1 Matching numbers to quantities\nThe variety of shapes of the nine pattern-book functions means that, often, one or another will be suitable for the modeling situation in hand. But, even if the shape of the function used is appropriate, the pattern still needs to be “adjusted” so that the units of output and input are well matched to the phenomenon being modeled. Let’s consider data from the outbreak of COVID-19 as an example. Figure 8.1 shows, day-by-day, the number of officially confirmed COVID-19 cases as the in the US in March 2020.\nDuring the outbreak, case numbers increased with time. As time went on, the rate of case-number increase itself grew faster and faster. This is the same pattern provided by the exponential function.\nAlongside the case-number data Figure 8.1 shows the function \\(\\text{cases}(t) \\equiv e^t\\) plotted as a magenta curve.\nThere is an obvious mismatch between the data and the function \\(e^t\\). Does this mean the COVID pattern is not exponential?\nThis chapter will introduce how modelers stretch and shift the individual patter-book functions so that they can be used in models of real-world situations such as the outbreak of COVID-19.\nThe coordinate axes in Figure 8.1 represent quantities. On the horizontal axis is time, measured in days. The vertical axis is denominated in “10000 cases,” meaning that the numbers on the vertical scale should be multiplied by 10000 to get the number of cases.\nThe exponential function takes as input a pure number and produces an output that is also a pure number. This is true for all the pattern-book functions. Since the graph axes don’t show pure numbers, it is no surprise then that the pattern-book exponential function doesn’t align with the COVID case data.\nIf we want the input to the model function \\(\\text{cases}(t)\\) to be denominated in days, we will have to convert \\(t\\) to a pure pure number (e.g. 10, not “10 days”) before the quantity is handed off as the argument to \\(\\exp()\\). We do this by introducing a parameter.\nThe standard parameterization for the exponential function is \\(e^{kt}\\). The parameter \\(k\\) will be a quantity with units of “per-day.” Suppose we set \\(k=0.2\\) per day. Then \\(k\\, t{\\LARGE\\left.\\right|}_{t=10 days} = 2\\). This “2” is a pure number because the units on the 0.2 (“per day”) and on the 10 (days) cancel out: \\[0.2\\, \\text{day}^{-1} \\cdot 10\\, \\text{days} = 2\\ .\\] The use of a parameter like \\(k\\) does more than handle the formality of converting input quantities into pure numbers. Having a choice for \\(k\\) allows us to stretch or compress the function to align with the data. Figure 8.2 plots the modeling version of the exponential function to the COVID-case data:",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parameters</span>"
    ]
  },
  {
    "objectID": "Modeling/08-parameters.html#matching-numbers-to-quantities",
    "href": "Modeling/08-parameters.html#matching-numbers-to-quantities",
    "title": "8  Parameters",
    "section": "",
    "text": "Recall that pure numbers, like 17.32, do not have units. Quantities, on the other hand, usually do have units, as in 17.3 days or 34 meters.\n\nIn every case, these parameters are arranged to translate a with-units quantity into a pure number suitable as an input to the pattern-book function. Similarly, parameters will translate the pure-number output from the pattern-book function into a quantity with units.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.2: Using the function form \\(A e^{kt}\\) with parameters \\(k=0.19\\) per day and \\(A = 0.0573\\) cases (in 10000s) matches the COVID-case data well.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parameters</span>"
    ]
  },
  {
    "objectID": "Modeling/08-parameters.html#parallel-scales",
    "href": "Modeling/08-parameters.html#parallel-scales",
    "title": "8  Parameters",
    "section": "8.2 Parallel scales",
    "text": "8.2 Parallel scales\nAt the heart of how we use the pattern-book functions to model the relationship between quantities is the idea of conversion between one scale and another. Consider these everyday objects: a thermometer and a ruler.\n\n\n\n\n\n\n\n\n\n\n\n(a) A thermometer\n\n\n\n\n\n\n\n\n\n\n\n(b) A ruler\n\n\n\n\n\n\n\nFigure 8.3: Two everyday objects that facilitate conversion from one scale to another.\n\n\n\nEach object presents a read-out of what’s being measured—temperature or length—on two different scales. At the same time, the objects provide a way to convert one scale to another.\nA function gives the output for any given input. We represent the input value as a position on a number line—which we call an “axis”—and the output as a position on another output line, almost always drawn perpendicular to one another. But the two number lines can just as well be parallel to one another. To evaluate the function, find the input value on the input scale and read off the corresponding output.\nWe can translate the correspondance between one scale and the other into the form of a straight-line function. For instance, if we know the temperature in Fahrenheit (\\(^\\circ\\)F) and want to convert it to Celsius (\\(^\\circ C\\)) we have the following function: \\[C(F) \\equiv {\\small\\frac{5}{9}}(F-32)\\ .\\] Similarly, converting inches to centimeters can be accomplished with \\[\\text{cm(inches)} \\equiv 2.54 \\, (\\text{inches}-0)\\ .\\] Both of these scale conversion functions have the form of the straight-line function, which can be written as \\[f(x) \\equiv a x + b\\ \\ \\ \\text{or, equivalently as}\\ \\ \\ \\ f(x) \\equiv a(x-x_0)\\ ,\\] where \\(a\\), \\(b\\), and \\(x_0\\) are parameters.\nSection 8.3 will use the \\(ax + b\\) form of scale conversion to scale the input to pattern-book functions. But we could equally well have used \\(a(x-x_0)\\).\nSection 8.4 introduces a second scale conversion function, for the output from pattern-book functions. That scaling will also be in the form of a straight-line function: \\(A x + B\\). The use of the lower-case parameter names (\\(a\\), \\(b\\)) versus the upper-case parameter names (\\(A\\), \\(B\\)) will help us distinguish the two different uses for scale conversion, namely input scaling versus output scaling.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parameters</span>"
    ]
  },
  {
    "objectID": "Modeling/08-parameters.html#sec-input-scaling",
    "href": "Modeling/08-parameters.html#sec-input-scaling",
    "title": "8  Parameters",
    "section": "8.3 Input scaling",
    "text": "8.3 Input scaling\nFigure 8.4 is based on the data frame RI-tide, a minute-by-minute record of the tide level in Providence, Rhode Island (USA) for the period April 1 to 5, 2010. The level variable is measured in meters; the hour variable gives the time of the measurement in hours after midnight at the start of April 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.4: Tide levels oscillate up and down over time. This is analogous to the \\(\\\\sin(t)\\) pattern-book function.\n\n\n\nThe pattern-book \\(\\sin()\\) and the function \\(\\color{magenta}{\\text{level}}\\color{blue}{(hour)}\\) have similar shapes, so it seems reasonable to model the tide data as a sinusoid. However, the scale of the axes is different on the two graphs.\nTo model the tide with a sinusoid, we need to modify the sinusoid to change the scale of the input and output. First, let’s look at how to accomplish the input scaling. Specifically, we want the pure-number input \\(t\\) to the sinusoid be a function of the quantity \\(hour\\). Our framework for this re-scaling is the straight-line function. We will replace the pattern-book input \\(t\\) with a function \\[t(\\color{blue}{hour}) \\equiv a\\, \\color{blue}{hour} + b\\ .\\]\nThe challenge is to find values for the parameters \\(a\\) and \\(b\\) that will transform the \\(\\color{blue}{\\mathbf{\\text{blue}}}\\) horizontal axis into the black horizontal axis, like this:\n\n\n\n\n\n\n\n\n\nBy comparing the two axes, we can estimate that \\(\\color{blue}{10} \\rightarrow 4\\) and \\(\\color{blue}{100} \\rightarrow 49\\). With these two coordinate points, we can find the straight-line function that turns \\(\\color{blue}{\\mathbf{\\text{blue}}}\\) into black by plotting the coordinate pairs \\((\\color{blue}{0},1)\\) and \\((\\color{blue}{100}, 51)\\) and finding the straight-line function that connects the points.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.5: The input scaling function must transform 10 into 4 and transform 100 into 49 to properly arrange the time scale with the scale for the pattern-book function.\n\n\n\nYou can calculate for yourself that the function that relates \\(\\color{blue}{\\mathbf{\\text{blue}}}\\) to black is \\[t(\\color{blue}{time}) = \\underbrace{\\frac{1}{2}}_a \\color{blue}{time}  \\underbrace{-1\\LARGE\\strut}_b\\]\nReplacing the pure number \\(t\\) as the input to pattern-book \\(\\sin(t)\\) with the transformed \\(\\frac{1}{2} \\color{blue}{time}\\) we get a new function: \\[g(\\color{blue}{time}) \\equiv \\sin\\left(\\strut {\\small\\frac{1}{2}}\\color{blue}{time} - 1\\right)\\ .\\] Figure 11.6 plots \\(g()\\) along with the actual tide data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.6: The sinusoid with input scaling (black) aligns nicely with the tide-level data.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parameters</span>"
    ]
  },
  {
    "objectID": "Modeling/08-parameters.html#sec-output-scaling",
    "href": "Modeling/08-parameters.html#sec-output-scaling",
    "title": "8  Parameters",
    "section": "8.4 Output scaling",
    "text": "8.4 Output scaling\nJust as the natural input needs to be scaled before it reaches the pattern-book function, so the output from the pattern-book function needs to be scaled before it presents a result suited for interpreting in the real world.\n\n\n\n\n\n\nFigure 8.7: Natural quantities must be scaled to pure numbers before being suited to the pattern-book functions. The output from the pattern-book function is a pure number which is scaled to the natural quantity of interest.\n\n\n\nThe overall result of input and output scaling is to tailor the pattern-book function so that it is ready to be used in the real world.\nLet’s return to Figure 11.6 which shows that the function \\(g(\\color{blue}{time})\\), which scales the input to the pattern-book sinusoid, has a much better alignment to the tide data. Still, the vertical axes of the two graphs in the figure are not the same.\nThis is the job for output scaling, which takes the output of \\(g(\\color{blue}{time})\\) (bottom graph) and scales it to match the \\(\\color{magenta}{level}\\) axis on the top graph. That is, we seek to align the black vertical scale with the \\(\\color{magenta}{\\mathbf{\\text{magenta}}}\\) vertical scale. To do this, we note that the range of the \\(g(\\color{blue}{time})\\) is -1 to 1, whereas the range of the tide-level is about 0.5 to 1.5. The output scaling will take the straight-line form \\[{\\color{magenta}{\\text{level}}}({\\color{blue}{time}}) = A\\, g({\\color{blue}{time}}) + B\\] or, in graphical terms\n\n\n\n\n\n\n\n\n\nWe can figure out parameters \\(A\\) and \\(B\\) by finding the straight-line function that connects the coordinate pairs \\((-1, \\color{magenta}{0.5})\\) and \\((1, \\color{magenta}{1.5})\\) as in Figure 8.8.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.8: Finding the straight-line function that converts \\(-1 \\rightarrow \\color{magenta}{0.5}\\) and converts \\(1 \\rightarrow \\color{magenta}{1.5}\\)\n\n\n\nYou can confirm for yourself that the function that does the job is \\[{\\color{magenta}{\\text{level}}} = 0.5 g({\\color{blue}{time}}) + 1\\ .\\]\nPutting everything together, that is, scaling both the input to pattern-book \\(\\sin()\\) and the output from pattern-book \\(\\sin()\\), we get\n\\[{\\color{magenta}{\\text{level}}}({\\color{blue}{time}}) = \\underbrace{0.5}_A \\sin\\left(\\underbrace{\\small\\frac{1}{2}}_a {\\color{blue}{time}}  \\underbrace{-1}_b\\right) + \\underbrace{1}_B\\]",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parameters</span>"
    ]
  },
  {
    "objectID": "Modeling/08-parameters.html#a-procedure-for-building-models",
    "href": "Modeling/08-parameters.html#a-procedure-for-building-models",
    "title": "8  Parameters",
    "section": "8.5 A procedure for building models",
    "text": "8.5 A procedure for building models\nWe’ve been using pattern-book functions as the intermediaries between input scaling and output scaling, using this format.\n\\[f(x) \\equiv A e^{ax + b} + B\\ .\\] We can use the other pattern-book functions—the gaussian, the sigmoid, the logarithm, the power-law functions—in the same way. That is, the basic framework for modeling is this:\n\\[\\text{model}(x) \\equiv A\\, {g_{pattern\\_book}}(ax + b) + B\\ ,\\] where \\(g_{pattern\\_book}()\\) is one of the pattern-book functions. To construct a basic model, you task has two parts:\n\nPick the specific pattern-book function whose shape resembles that of the relationship you are trying to model. For instance, we picked \\(e^x\\) for modeling COVID cases versus time (at the start of the pandemic). We picked \\(\\sin(x)\\) for modeling tide levels versus time.\nFind numerical values for the parameters \\(A\\), \\(B\\), \\(a\\), and \\(b\\). Chapter 11 shows some ways to make this part of the task easier.\n\nIt is remarkable that models of a very wide range of real-world relationships between pairs of quantities can be constructed by picking one of a handful of functions, then scaling the input and the output. As we move on to other Blocks in MOSAIC Calculus, you will see how to generalize this to potentially complicated relationships among more than two quantities. That is a big part of the reason you’re studying calculus.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parameters</span>"
    ]
  },
  {
    "objectID": "Modeling/08-parameters.html#other-formats-for-scaling",
    "href": "Modeling/08-parameters.html#other-formats-for-scaling",
    "title": "8  Parameters",
    "section": "8.6 Other formats for scaling",
    "text": "8.6 Other formats for scaling\nOften, modelers choose to use input scaling in the form \\(a (x - x_0)\\) rather than \\(a x + b\\). The two are completely equivalent when \\(x_0 = - b/a\\). The choice between the two forms is largely a matter of convention. But almost always the output scaling is written in the format \\(A y + B\\).\n\nApplication area 8.1 —Shifting an exponential horizontally is the same as scaling vertically.\n\n\n\n\n\n\n\nApplication area 8.1 The start of COVID\n\n\n\nFor the COVID case-number data shown in Figure 8.2, we found that a reasonable match to the data can be had by input- and output-scaling the exponential: \\[\\text{cases}(t) \\equiv  \\underbrace{573}_A e^{\\underbrace{0.19}_a\\ t}\\ .\\]\nYou might wonder why the parameters \\(B\\) and \\(b\\) aren’t included in the model. One reason is that cases and the exponential function already have the same range: zero and upwards. So there is no need to shift the output with a parameter B.\nAnother reason has to do with the algebraic properties of the exponential function. Specifically, \\[e^{a x + b}= e^b e^{ax} = {\\cal A} e^{ax}\\] where \\({\\cal A} \\equiv e^b\\).\nIn the case of exponentials, writing the input scaling in the form \\(e^{a(x-x_0)}\\) can provide additional insight.\nA bit of symbolic manipulation of the model can provide some additional insight. As you know, the properties of exponentials and logarithms are such that \\[A e^{at} = e^{\\lb(A)} e^{at} = e^{a t + \\ln(A)} = e^{a\\left(\\strut t + \\lb(A)/a\\right)} = e^{a(t-t_0)}\\ ,\\] where \\[t_0 = - \\ln(A)/a = - \\ln(593)/0.19 = -33.6\\ .\\] You can interpret \\(t_0\\) as the starting point of the pandemic. When \\(t = t_0\\), the model output is \\(e^{k 0} = 1\\): the first case. According to the parameters we matched to the data for March, the pandemic’s first case would have happened about 33 days before March 1, which is late January. We know from other sources of information, the outbreak began in late January. It is remarkable that even though the curve was constructed without any data from January or even February, the data from March, translated through the curve-fitting process, pointed to the start of the outbreak. This is a good indication that the exponential form for the model is fundamentally correct.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parameters</span>"
    ]
  },
  {
    "objectID": "Modeling/08-parameters.html#parameterization-conventions",
    "href": "Modeling/08-parameters.html#parameterization-conventions",
    "title": "8  Parameters",
    "section": "8.7 Parameterization conventions",
    "text": "8.7 Parameterization conventions\nThere are conventions for the symbols used for input-scaling parameterization of the pattern-book functions. Knowing these conventions makes it easier to read and assimilate mathematical formulas. In several cases, there is more than one conventional option. For instance, the sinusoid has a variety of parameterization forms that get used depending on which feature of the function is easiest to measure. Table 8.1 list several that are frequently used in practice.\n\n\n\nTable 8.1: Some standard forms of input scaling parameterizations\n\n\n\n\n\n\n\n\n\n\n\nFunction\nWritten form\nParameter 1\nParameter 2\n\n\n\n\nExponential\n\\(e^{kt}\\)\n\\(k\\)\nNot used\n\n\nExponential\n\\(e^{t/\\tau}\\)\n\\(\\tau\\) “time constant”\nNot used\n\n\nExponential\n\\(2^{t/\\tau_2}\\)\n\\(\\tau_2\\) “doubling time”\nNot used\n\n\nExponential\n\\(2^{-\\tau_{1/2}}\\)\n\\(-\\tau_{1/2}\\) “half life”\nNot used\n\n\nPower-law\n\\([x - x_0]^p\\)\n\\(x_0\\) x-intercept\nexponent\n\n\nSinusoid\n\\(\\sin\\left(\\frac{2 \\pi}{P} (t-t_0)\\right)\\)\n\\(P\\) “period”\n\\(t_0\\) “time shift”\n\n\nSinusoid\n\\(\\sin(\\omega t + \\phi)\\)\n\\(\\omega\\) “angular frequency”\n\\(\\phi\\) “phase shift”\n\n\nSinusoid\n\\(\\sin(2 \\pi \\omega t + \\phi)\\)\n\\(\\omega\\) “frequency”\n\\(\\phi\\) “phase shift”\n\n\nGaussian\ndnorm(x, mean, sd)\n“mean” (center)\nsd “standard deviation”\n\n\nSigmoid\npnorm(x, mean, sd)\n“mean” (center)\nsd “standard deviation”\n\n\nStraight-line\n\\(mx + b\\)\n\\(m\\) “slope”\n\\(b\\) “y-intercept”\n\n\nStraight-line\n\\(m (x-x_0)\\)\n\\(m\\) “slope”\n\\(x_0\\) “center”",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parameters</span>"
    ]
  },
  {
    "objectID": "Modeling/09-assembling-functions.html",
    "href": "Modeling/09-assembling-functions.html",
    "title": "9  Assembling functions",
    "section": "",
    "text": "9.1 Linear combination\nWhen a modeler needs a new function for some purpose, there are three tactics. The best one depends on the circumstances and the availability of data.\nThis tactic is well represented in Calculus textbooks. I think this is not so much a statement about the power of the tactic, but the general tendency to avoid working with data. For the applications of applied math, data is a valuable resource. When it’s available, as is often the case, there are alternatives to (1) that are available.\nThis chapter focusses on the mathematical framework for (1). Chapter 11 introduces the incorporation of data used in tactic (2). As for (3), we will need several core concepts of calculus as a basis for understanding, so we mainly defer the tactic to Chapter 49.\nSection 8.4 introduced the notation of input and output scaling. For instance, Figure 11.6 illustrates how the pattern-book \\(\\sin()\\) function can be customized to model a particular situation. Recall that the input scaling involves inserting a straight-line function inside the parentheses, as in\n\\[f_1(t) \\equiv \\sin\\left( a t + b\\right) \\tag{9.1}\\]\nYou may recognize \\(at + b\\) as a straight-line function. Possibly, your recognition would be easier if we wrote \\(at + b\\) using different names for the input and the first parameter: \\(m x + b\\). You have been trained to pronounce \\(m\\) as the “slope” of the line and \\(b\\) as the “y-intercept.”\nUsing both input and output scaling gives a more general kind of function:\n\\[f_2(t) \\equiv A \\sin(a t + b) + B \\tag{9.2}\\]\nMath expression 9.2 is an example of a linear combination of two functions. The two functions are \\(g_1(t) \\equiv 1\\) and \\(g_2(t) \\equiv sin(a t + b)\\).\nIt might be easier to see Math expression 9.2 as a linear combination if the function were written explicitly using the two functions being combined, that is, as\n\\[f_2 \\equiv A g_2(t) + B g_1(t)\\]\nThe combination is made by scaling each of the functions involved then adding the scaled functions together. The two scaling factors, \\(A\\) and \\(B\\), could be called “parameters of \\(f_2()\\),” which indeed they are. It would be good to get used to another word that’s used specifically for the parameters in a linear combination: “coefficients.” The advantage of “coefficients” as a name lies in it marking the parameters as those involved in a linear combination, rather than any of the other ways parameters can be used.\nNote that “coefficients” always refers to parameters that are not inside the parentheses of a function. In contrast, often parameters are inside the parentheses as in Math expression 9.1. The parameters \\(a\\) and \\(b\\) in \\(\\sin(a t + b)\\) are inside the parentheses. Consequently, \\(a\\) and \\(b\\) shouldn’t be called “coefficients.” Sometimes, to emphasize this, parameters in parentheses are called “nonlinear parameters” to distinguish them from coefficients like \\(A\\) and \\(B\\).\nTo illustrate how linear combination is used to create new functions, consider polynomials, for instance, \\[f(x) \\equiv 3 x^2 + 5 x - 2\\ . \\tag{9.3}\\] In high school, polynomials are often presented as puzzles—factor them to find the roots! In calculus, however, polynomials are used as functions for modeling. They are a kind of modeling “clay,” which can be shaped as needed.\nThere are three pattern-book functions in Math expression 9.3. In polynomials the functions being combined are all power-law functions: \\(g_0(x) \\equiv 1\\), \\(g_1(x) \\equiv x\\), and \\(g_2(x) \\equiv x^2\\). With these functions defined, we can write the polynomial \\(f(x)\\) as \\[f(x) \\equiv 3 g_2(x) + 5 g_1(x) - 2 g_0(x)\\] Each of the functions is being scaled by a quantity: 3, 5, and -2 respectively. Then the scaled functions are added up. That is a linear combination; scale and add.\nLinear combination is an extremely important tactic that quantitative workers use throughout their careers. For instance, many physical systems are described by linear combinations. For instance, the motion of a vibrating molecule, a helicopter in flight, or a building shaken by an earthquake are described as simple “modes” which are linearly combined to make up the entire motion. More down to Earth, the timbre of a musical instrument is set by the scalars in a linear combination of pure tones. And throughout work with data in science, commerce, government and other fields a primary data analysis method—called “regression”—is about finding the best linear combination of a set of explanatory variables to create a model function of the response variable.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assembling functions</span>"
    ]
  },
  {
    "objectID": "Modeling/09-assembling-functions.html#linear-combination",
    "href": "Modeling/09-assembling-functions.html#linear-combination",
    "title": "9  Assembling functions",
    "section": "",
    "text": "Identical vs proportional vs straight-line\n\n\n\nOne of the pattern book functions is very simple; the output is identical to the input:\n\\[\\text{identity}(x) \\equiv x\\] Multiplying the identity() function by a parameter gives a function that can well be called “proportional().”\n\\[\\text{proportional}(x) \\equiv a\\ \\text{identity}(x) = a\\ x\\] The parameter \\(a\\) is often called the “constant of proportionality.”\nIt’s common to call this closely related function the “linear” function, but a better name is the “straight-line” function. “Straight-line” is the name we shall use in this book.\n\\[\\text{straight\\_line}(x) \\equiv a\\ x + b\\]",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assembling functions</span>"
    ]
  },
  {
    "objectID": "Modeling/09-assembling-functions.html#sec-function-composition",
    "href": "Modeling/09-assembling-functions.html#sec-function-composition",
    "title": "9  Assembling functions",
    "section": "9.2 Function composition",
    "text": "9.2 Function composition\nFunction composition refers to combining functions by taking the input of one function and feeding it as input to another. “\\(g()\\) composed with \\(h()\\)” means \\(g(h(x))\\).\nTo illustrate, consider again the function defined in Math expression 9.2: \\[f(t) \\equiv A \\sin\\left( a t + b\\right) + B\\]\nYou’ve already seen how Math expression 9.2 is a linear combination of two functions \\(f_1(t) \\equiv 1\\) and \\(f_2(t) \\equiv sin(a t + b)\\). But \\(f_2()\\) is not a linear combination. Instead, it is a function composition. The two functions being composed are \\(sin(x)\\) and \\(a t + b\\), producing \\(sin(a t + b)\\). Here, \\(\\sin()\\) is the outer function in the composition and \\(at + b\\) is the inner function.\nIn function composition, the order of the functions matters: \\(f(g(x))\\) and \\(g(f(x))\\) are in general completely different functions.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assembling functions</span>"
    ]
  },
  {
    "objectID": "Modeling/09-assembling-functions.html#sec-function-multiplication",
    "href": "Modeling/09-assembling-functions.html#sec-function-multiplication",
    "title": "9  Assembling functions",
    "section": "9.3 Function multiplication",
    "text": "9.3 Function multiplication\nMultiplication is the third in our repertoire of methods for making new functions. To review, here are the first two methods involving combining a function \\(f()\\) with a function $g():\n\nLinear combinations. Example: \\(n_1(t) \\equiv 5 f(t) + 1.2 g(t)\\).\nFunction composition. There are two possibilities which produce two distinct functions:\n\n\\(c_1(t) \\equiv f(g(t))\\), that is, \\(g()\\) is the inner function.\n\\(c_2(t) \\equiv g(f(t))\\), that is, \\(g()\\) is the outer function.\n\n\nNow a new method:\n\nMultiplication of the output of two functions. Example: \\(m(t)\\equiv f(t) g(t)\\). This is perfectly ordinary multiplication. Multiplication is commutative, as illustrated by the equality of \\(5 \\times 2\\) and \\(2 \\times 5\\). Owing to the commutativity of multiplication, it doesn’t matter whether \\(f()\\) is first or \\(g()\\) is first.\n\n\\[\\underbrace{f(x) \\times g(x)}_\\text{multiplication}\\ \\ \\ \\ \\underbrace{{\\Large f(}g(x){\\Large)} \\ \\ \\text{or}\\ \\ \\ {\\Large g(}f(x){\\Large)}}_\\text{composition}\\]\nIn function composition, only one of the functions—the interior function is applied to the overall input, \\(x\\) in the above example. The exterior function is fed its input from the output of the interior function.\nIn multiplication, each of the functions is applied to the input individually. Then their outputs are multiplied to produce the overall output.\n\nApplication area 9.1 —What goes up may come down.\n\n\n\n\n\n\n\nApplication area 9.1 Modeling rise and fall\n\n\n\nThe initial rise in popularity of the social media platform Yik Yak was exponential. Then popularity leveled off, promising a steady, if static, business into the future. But, the internet being what it is, popularity collapsed to near zero and the company closed.\nOne way to model this pattern is by multiplying a sigmoid by an exponential. (See Figure 9.1.)\n\n\n\n\n\n\n\n\n\nA sigmoid (orange) and an exponential function, shifted to be centered in mid-2014\n\n\n\n\n\n\n\n\n\nMultiplying the sigmoid and exponential produces a hump.\n\n\n\n\n\n\n\nFigure 9.1: Building a model of a steep rise and gentler fall by multiplying a sigmoid by an exponential. Subscriptions to the web messaging service Yik Yak grew exponentially in 2013 and 2014, then collapsed. The company closed in 2017.\n\n\n\n\n\n\nApplication area 9.2 —Sounds of short duration can be modeled by multiplying sines by a local function such as the gaussian.\n\n\n\n\n\n\n\nApplication area 9.2 Transient vibration\n\n\n\nA guitar string is plucked to produce a note. The sound is, of course, vibrations of the air created by vibrations of the string.\nAfter plucking, the note fades away. An simple model of this is a sinusoid (of the correct period to correspond to the frequency of the note) times a gaussian.\n\n\n\n\n\n\n\n\n\nThe two components of the wave packet: an envelope and an oscillation.\n\n\n\n\n\n\n\n\n\nThe wave packet constructed by multiplication\n\n\n\n\n\n\n\nFigure 9.2: A wave packet constructed by multiplying a sinusoid and a gaussian function.\n\n\n\n\n\nFunction multiplication is used so often in modeling that you will see it in many modeling situations. Here’s one example that is important in physics and communication: the wave packet. Overall, the wave packet is a localized oscillation as in Figure 9.2. The packet can be modeled with the product of two pattern-book functions: a gaussian times a sinusoid.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assembling functions</span>"
    ]
  },
  {
    "objectID": "Modeling/09-assembling-functions.html#sec-piecewise-intro",
    "href": "Modeling/09-assembling-functions.html#sec-piecewise-intro",
    "title": "9  Assembling functions",
    "section": "9.4 Splitting the domain",
    "text": "9.4 Splitting the domain\nConsider the familiar absolute-value function, written \\[\\text{abs}(x) \\equiv \\left|x\\right|\\] in traditional notation. Written this way, the definition of abs() is a tautology: unless you already know what \\(\\left|x\\right|\\) means, you will have no clue what’s going on.\nCan we assemble abs() out of pattern-book functions? What’s distinctive about \\(abs(x)\\) is the break at \\(x=0\\). There is no similarly sharp transition in any of the pattern-book functions.\nDefining a function piecewise, that is, constructing it from two different functions on two non-overlapping domains, provides the means to create a sharp transition. For the absolute value function, one domain is the negative inputs: \\(x &lt; 0\\). The other domain is the positive inputs: \\(0 \\leq x\\). Note that the domains do not overlap.\n\n\nThe absolute value function has two pieces, one to the left of input 0 for the function \\(-x\\), the other to the right of 0 for the function \\(x\\). The two pieces meet at input 0. \\[\\text{abs}(x) \\equiv \\left\\{\n\\begin{array}{rl}  x & \\text{for}\\ 0 \\leq x \\\\\n-x & \\text{otherwise}\\\\\\end{array}\n\\right.\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.3: The absolute value function\n\n\n\n\n\nA function defined separately on different pieces of its domain is called a piecewise function. In the conventional mathematical notation, there is a large \\(\\LARGE\\left\\{\\right.\\) followed by two or more lines. Each line gives a formula for that part of the function and indicates to which interval the formula applies.\n\n\nAnother piecewise function widely used in technical work, but not as familiar as \\(abs()\\) is the Heaviside function, which has important uses in physics and engineering. \\[\\text{Heaviside}(x) \\equiv \\left\\{\n\\begin{array}{cl} 1 & \\text{for}\\ 0 \\leq x \\\\0 & \\text{otherwise}\\end{array}\n\\right.\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.4: The Heaviside function\n\n\n\n\n\nThe Heaviside function is defined on the same two pieces of the number line as \\(abs()\\). To the right of zero, Heaviside is identical to constant(). To the left, it is identical to 0 times constant\\(()\\).\nThe vertical gap between the two pieces of the Heaviside function is called a discontinuity. Intuitively, you cannot draw a discontinuous function without lifting the pencil from the paper. The Heaviside’s discontinuity occurs at input \\(x=0\\).\n\n\nThe ramp function is closely related to the Heaviside function. The ramp has output 0 when the input is negative. For positive inputs, the ramp is the identity function. \\[\\text{ramp}(x) \\equiv \\left\\{\n\\begin{array}{rl}  x & \\text{for}\\ 0 \\leq x \\\\\n0 & \\text{otherwise}\\\\\\end{array}\n\\right.\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.5: The ramp function\n\n\n\n\n\n\n9.4.1 Computing notation\nThe usual mathematical notation for piecewise functions, spread out over multiple lines that are connected with a tall brace, is an obvious non-candidate for computer notation. In R, the stitching together of the two pieces can be done with the function ifelse(). The name is remarkably descriptive. The ifelse() function takes three arguments. The first is a question to be asked, the second is the value to return if the answer is “yes,” and the third is the value to return for a “no” answer.\nTo define abs() or Heaviside() the relevant question is, “Is the input on the right or left side of zero on the number line?” In widely-used computing languages such as R, the format for asking a question does not involve a question mark. For example, to ask the question, “Is 3 less than 2?” use the expression:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn mathematics notation, \\(3 &lt; 2\\) is a declarative statement and is an impossibility. More familiar would be \\(x &lt; 2\\), which is again a declarative statement putting a restriction on the possible values of the quantity \\(x\\).\nIn computing notation, 3 &lt; 2 or x &lt; 2 is not a declaration, it is an imperative statement that directs the computer to do the calculation to find out if the statement is true or false for the particular values given.\nHere’s a definition of Heaviside() written with ifelse().\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nSince Heaviside() is a function, the question \\(0 \\leq x\\) does not get answered at the time the function is defined. Instead, when the function is evaluated we will know the value of input \\(x\\), can answer the \\(0 \\leq x\\) question, and use the answer to select either 1 or 0 as the output.\nTable 9.1 shows computer notation for some common sorts of questions.\n\n\n\nTable 9.1: Each of these imperative statements in R asks a question about numbers.\n\n\n\n\n\n\n\n\n\nR notation\nEnglish\n\n\n\n\nx &gt; 2\n“Is \\(x\\) greater than 2?”\n\n\ny &gt;= 3\n“Is \\(y\\) greater than or equal to 3?”\n\n\nx == 4\n“Is \\(x\\) exactly 4?”\n\n\n2 &lt; x & x &lt; 5\n“Is \\(x\\) between 2 and 5?” Literally, “Is \\(x\\) both greater than 2 and less than 5?”\n\n\nx &lt; 2 | x &gt; 6\n“Is \\(x\\) either less than 2 or greater than 6?”\n\n\nabs(x-5) &lt; 2\n“Is \\(x\\) within two units of 5?”\n\n\n\n\n\n\n\nApplication area 9.3 —Two uses of gas—heating and cooking—call for a piecewise function.\n\n\n\n\n\n\n\nApplication area 9.3 Heating with gas\n\n\n\nFigure 9.6 is a graph of monthly natural gas use in the author’s household versus average temperature during the month. (Natural gas is measured in cubic feet, abbreviated ccf.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.6: The amount of natural gas used for heating the author’s home varies with the outside temperature.\n\n\n\nThe graph looks somewhat like a hockey stick. A sloping straight-line dependence of ccf on temperature for temperatures below \\(60^\\circ\\)F and constant for higher temperatures. The shape originates from the dual uses of natural gas. Gas is used for cooking and domestic hot water, the demand for which is more or less independent of outdoor temperature at about 15 ccf per month. Gas is also used for heating the house, but that is needed only when the temperature is less than about \\(60^\\circ\\)F.\nWe can accomplish the hockey-stick shape with a linear combination of the ramp() function and a constant. The ramp function represents gas used for heating, the constant is the other uses of gas (which are modeled as not depending on temperature. Overall, the model is \\[\\text{gas}(x) \\equiv 4.3\\,  \\text{ramp}(62-x)  + 15\\ .\\]\nNote that the input to ramp() is 62 - \\(x\\). This input scaling (Section 8.3) turns the ramp around so that it rises to the left. The transition between the flat and ramp sections occurs at \\(x=62\\).",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assembling functions</span>"
    ]
  },
  {
    "objectID": "Modeling/09-assembling-functions.html#computing-outside-the-domain",
    "href": "Modeling/09-assembling-functions.html#computing-outside-the-domain",
    "title": "9  Assembling functions",
    "section": "9.5 Computing outside the domain",
    "text": "9.5 Computing outside the domain\nEach of our pattern-book functions, with two exceptions, has a domain that is the entire number line \\(-\\infty &lt; x &lt; \\infty\\). No matter how big or small is the value of the input, the function has an output. Such functions are particularly nice to work with since we never have to worry about the input going out of bounds.\nThe two exceptions are:\n\nthe logarithm function, which is defined only for \\(0 &lt; x\\).\nsome of the power-law functions: \\(x^p\\).\n\nWhen \\(p\\) is negative, the output of the function is undefined when \\(x=0\\). You can see why with a simple example: \\(g(x) \\equiv x^{-2}\\). Most students had it drilled into them that “division by zero is illegal,” and \\(g(0) = \\frac{1}{0} \\frac{1}{0}\\), a double law breaker.\nWhen \\(p\\) is not an integer, that is \\(p \\neq 1, 2, 3, \\cdots\\) the domain of the power-law function does not include negative inputs. To see why, consider the function \\(h(x) \\equiv x^{1/3}\\).\n\n\n\n\n\n\n\n\nDivision by zero on the computer\n\n\n\nIt can be tedious to make sure that you are on the right side of the law when dealing with functions whose domain is not the whole number line. The designers of the hardware that does computer arithmetic, after several decades of work, found a clever system to make it easier. It is a standard part of such hardware that whenever a function is handed an input that is not part of that function’s domain, one of two special “numbers” is returned. To illustrate:\n\nsqrt(-3)\n## [1] NaN\n(-2)^0.9999\n## [1] NaN\n1/0\n## [1] Inf\n\nNaN stands for “not a number.” Just about any calculation involving NaN will generate NaN as a result, even those involving multiplication by zero or cancellation by subtraction or division.1 For instance:\n\n0 * NaN\n## [1] NaN\nNaN - NaN\n## [1] NaN\nNaN / NaN\n## [1] NaN\n\nDivision by zero produces Inf, whose name is reminiscent of “infinity.” Inf infiltrates any calculation in which it takes part:\n\n3 * Inf\n## [1] Inf\nsqrt(Inf)\n## [1] Inf\n0 * Inf\n## [1] NaN\nInf + Inf\n## [1] Inf\nInf - Inf\n## [1] NaN\n1/Inf\n## [1] 0\n\nTo see the benefits of the NaN / Inf system let’s plot out the logarithm function over the graphics domain \\(-5 \\leq x \\leq 5\\). Of course, part of that graphics domain, \\(-5 \\leq x \\leq 0\\) is not in the domain of the logarithm function and the computer is entitled to give us a slap on the wrists. The NaN provides some room for politeness.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assembling functions</span>"
    ]
  },
  {
    "objectID": "Modeling/09-assembling-functions.html#footnotes",
    "href": "Modeling/09-assembling-functions.html#footnotes",
    "title": "9  Assembling functions",
    "section": "",
    "text": "One that does produce a number is NaN^0.↩︎",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assembling functions</span>"
    ]
  },
  {
    "objectID": "Modeling/10-functions-with-multiple-inputs.html",
    "href": "Modeling/10-functions-with-multiple-inputs.html",
    "title": "10  Functions with multiple inputs",
    "section": "",
    "text": "10.1 Linear combinations\nSection 4.2 showed how to construct contour plots of functions taking two inputs. Let’s consider an example to motivate why functions taking multiple inputs are important.\nFigure 10.1 shows a biological phenomenon with which we are all familiar. There are 10,000 dots, each of which corresponds to a person selected randomly from the US population. The black lines show a model function fitted to the data by methods to be presented in Section 16.7.\nWe all know that children tend to get heavier as they age through childhood and puberty. It’s also true that adult women tend to be lighter than adult men, at least for those over 18 years old. The model function in Figure 10.1 takes two inputs: age and gender.\nIt’s also true that humans differ in weight for reasons beyond age and sex. For example, health condition affects weight, as does height, skeletal size, caloric intake, and energy expenditure, among other factors. A more complete model of weight could take such factors into account, becoming a model taking multiple inputs. Such multi-input model functions are often constructed from data using the fitting techniques you will meet in Section 16.7.\nEspecially in science and engineering, many functions are described by formulas. Consider, for instance, the gravitational force \\(F_g\\) on an object such as a automobile or person. Since Isaac Newton’s time, we have known that this force is determined by the mass of the Earth, the mass of the object, and the distance of the object from the center of the Earth, that is, three inputs:\n\\[F_g(M_\\text{Earth}, M_\\text{object}, \\text{dist}) \\equiv\n\\frac{G M_\\text{Earth} M_\\text{object}}{\\text{dist}^2} \\tag{10.1}\\]\n\\(G\\) is a parameter called the “universal gravitational constant”: \\(G = 6.6743 \\times 10^{-11} \\text{m}^3 \\text{kg}^{−1} \\text{s}^{−2}\\). Thinking about the construction of Math expression 10.1 as a combination, it is a multiplication of two functions, \\(g_1(x) \\equiv x\\) and \\(g_2(x) \\equiv x^{-2}\\), that is,\n\\[F_g(M_\\text{Earth}, M_\\text{object}, \\text{dist}) \\equiv G \\times g_1(M_\\text{Earth}) \\times g_1(M_\\text{object}) \\times g_2(\\text{dist}) \\tag{10.2}\\]\nIt happens that the function \\(g_1()\\) plays two roles in \\(F_g()\\), once with \\(M_\\text{Earth}\\) as the input and again with \\(M_\\text{object}\\) as the input. This use of construction by function multiplication is common in formulas. When the functions being multiplied take different inputs, the overall function becomes one that takes multiple inputs.\nIn other areas of quantitative work, especially where there is no accepted physical law at hand, linear combinations are a valuable way of constructing model functions with multiple inputs.\nTo illustrate, housing prices are determined by several (or many!) factors. Translating the previous sentence into the language of functions, we can say that the price is a function of multiple inputs. Plausible inputs to the function include the amount of living area and the number of bedrooms and bathrooms. The inputs may also include quality of the neighborhood, length of commute, and so on.\nOften, the starting point for building a function with multiple inputs is a data frame whose variables include the function output (say, price) and the various inputs to the function.\nA large fraction of work in the data-oriented quantitative sciences uses just the identity function in the linear combination. Recall that the identify function merely echos as output whatever input is provided, that is, \\(\\text{identity}(x) \\equiv x\\).\nThere is no point in constructing a linear combination of identity functions that take the same input. For example, the linear combination\n\\[4.3\\ \\text{identity}(x) + 1.7\\ \\text{identity}(x) - 2.6\\ \\text{identity}(x)\\]\nis merely a long-winded way of saying \\(3.4\\ \\text{identity}(x)\\).\nWhere a linear combination of identity functions becomes useful is when the inputs to the various functions are different, for example, consider this function of three inputs, \\(x\\), \\(y\\), and \\(z\\).\n\\[4.3\\ \\text{identity}(x) + 1.7\\ \\text{identity}(y) - 2.6\\ \\text{identity}(z)\\]\nBy convention, we rarely write \\(\\text{identity}()\\) preferring instead just to write the name of the input, as with\n\\[4.3\\ x + 1.7\\ y - 2.6\\ z \\tag{10.3}\\]\nEven though there are no parentheses used in Math expression 10.3, it is still a linear combination of functions. Thinking of it this way prompts consideration of what other, non-identity function might have been included, for instance, \\(x^2\\) or \\(x\\times y\\).\nTo illustrate the use of linear combinations of identity functions (with different inputs), consider the following model of house prices:\n\\(\\text{price}(\\text{livingArea}, \\text{bedrooms}, \\text{bathrooms}) \\equiv\\)\n\\[\\ \\ \\ 21000 + 105\\,\\text{livingArea} - 13000\\, \\text{bedrooms} + 26000\\, \\text{bathrooms}\\]\nwhere each of the coefficients is in units of dollars. These coefficients are based on data in the SaratogaHouses data frame which records the sales price of 1728 houses in Saratoga County, New York, USA, in 2006\nThe model function is a simple linear combination, but it effectively quantifies how different aspects of a house contribute to its sales price. The model indicates that an additional square foot of living area is worth about 105 dollars per foot2. An extra bathroom is worth about $25,000. Bedrooms, strangely, are assigned a negative value by the model.\nPossibly you already understand what is meant by “an additional square foot” or “an extra bathroom.” These ideas can be intuitive, but they can be best understood with a grounding in calculus, which we turn to in Block II. For instance, the negative scalar on bedrooms will make sense when you understand “partial derivatives,” the subject of Chapter 25.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Functions with multiple inputs</span>"
    ]
  },
  {
    "objectID": "Modeling/10-functions-with-multiple-inputs.html#function-multiplication-fx-times-gt",
    "href": "Modeling/10-functions-with-multiple-inputs.html#function-multiplication-fx-times-gt",
    "title": "10  Functions with multiple inputs",
    "section": "10.2 Function multiplication: f(x) times g(t)",
    "text": "10.2 Function multiplication: f(x) times g(t)\nWhen a guitar string is at rest it forms a straight line connecting its two fixed ends: one set by finger pressure along the neck of the guitar and the other at the bridge near the center of the guitar body. When the string is plucked, its oscillations follow a sinusoid pattern of displacement. With the right camera and lighting setup, we can see these oscillations in action:\n\n\n\n\n\n\nVideo 10.1: The displacement of each guitar string is a function of position along the string and time. Figure 10.2 shows a single frame from the video, making it easier to see the string’s displacement as a function of position.\n\n\n\n\n\n\n\n\n\nFigure 10.2: The displacement of a vibrating guitar string is a function of both time and space. In the still picture taken from Video 10.1, you see a slice of that function taken at a fixed moment of time.\n\n\n\nFor a string of length \\(L\\), the string displacement is a function of position \\(x\\) along the string and is a linear combination of functions of the form \\[g_k(x) \\equiv \\sin(k \\pi x /L)\\] where \\(k\\) is an integer. A few of these functions are graphed in Figure 10.3 with \\(k=1\\), \\(k=2\\), and \\(k=3\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10.3: The overall shape of a guitar string at a fixed instant is complicated. It is a linear combination of three functions, each of which is called a “mode” of the string.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Functions with multiple inputs</span>"
    ]
  },
  {
    "objectID": "Modeling/11-fitting-features.html",
    "href": "Modeling/11-fitting-features.html",
    "title": "11  Fitting features",
    "section": "",
    "text": "11.1 Gaussian\nFor more than three centuries, there has been a standard calculus model of an everyday phenomenon: a hot object such as a cup of coffee cooling off to room temperature. The model, called Newton’s Law of Cooling, posits that the rate of cooling is proportional to the difference between the object’s temperature and the ambient temperature. The technology for measuring temperature (Figure 11.1) was rudimentary in Newton’s era, raising the question of how Newton formulated a quantitative theory of cooling. (Chapter 12 returns to this question.)\nUsing today’s much more precise technology, Prof. Stan Wagon of Macalester College investigated the accuracy of Newton’s “Law.” Figure 11.2 shows some of Wagon’s data from experiments with cooling water. He poured boiling water from a kettle into an empty room-temperature mug (26 degrees C) and measured the temperature of the water over the next few hours.\nThis chapter is about fitting, finding parameters and coefficients that will align a function with data such as in Figure 11.2. This chapter covers the exponential, sinusoid, and gaussian functions. Chapter 14 considers the power-law and logarithm functions.\nIn every instance, the first step, before finding parameters, is to determine that the pattern shown in the data is a reasonable match to the shape of the function you are considering. Here’s a reminder of the shapes of the functions we will be fitting to data in this chapter. If the shapes don’t match, there is little point in looking for the parameters to fit the data!\nThe ability to perceive color comes from “cones”: specialized light-sensitive cells in the retina of the eye. Human color perception involves three sets of cones. The L cones are most sensitive to relatively long wavelengths of light near 570 nanometers. The M cones are sensitive to wavelengths near 540 nm, and the S cones to wavelengths near 430nm.\nThe current generation of Landsat satellites uses nine different wavelength-specific sensors. This makes it possible to distinguish features that would be undifferentiated by the human eye.\nBack toward Earth, birds have five sets of cones that cover a wider range of wavelengths than humans. (Figure 11.4) Does this give them a more powerful sense of the differences between natural features such as foliage or plumage? One way to answer this question is to take photographs of a scene using cameras that capture many narrow bands of wavelengths. Then, knowing the sensitivity spectrum of each set of cones, new “false-color” pictures can be synthesized recording the view from each set.1\nCreating the false-color pictures on the computer requires a mathematical model of the sensitivities of each type of cone. The graph of each sensitivity function resembles a Gaussian function.\nThe Gaussian has two parameters: the “mean” and the “sd” (short for standard deviation). It is straightforward to estimate values of the parameters from a graph, as in Figure 11.5.\nThe parameter “mean” is the location of the peak. The standard deviation is, roughly, half the width of the graph at a point halfway down from the peak.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting features</span>"
    ]
  },
  {
    "objectID": "Modeling/11-fitting-features.html#gaussian",
    "href": "Modeling/11-fitting-features.html#gaussian",
    "title": "11  Fitting features",
    "section": "",
    "text": "Figure 11.3: Two views of the same scene synthesized by combining the output of different types of cones. The top picture uses V, M, and S cones; the bottom only S, M, and L cones. The dark geen leaves revealed in the top picture are not distinguishable in the bottom picture. (Source: Tedore and Nilsson)\n\n\n\n\n\n\n\n\n\n\nFigure 11.4: Sensitivity to wavelength for each of the five types of bird cones. [Source: Tedore and Nilsson]\n\n\n\n\n\n\n\n\n\n## Warning: All aesthetics have length 1, but the data has 101 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 101 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 101 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 101 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n\n\n\n\n\n\n\n\n\nFigure 11.5: A Gaussian function annotated to identify the parameters mean (location of peak of graph) and sd (half-width at half-height).",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting features</span>"
    ]
  },
  {
    "objectID": "Modeling/11-fitting-features.html#sinusoid",
    "href": "Modeling/11-fitting-features.html#sinusoid",
    "title": "11  Fitting features",
    "section": "11.2 Sinusoid",
    "text": "11.2 Sinusoid\nWe will use three parameters for fitting a sinusoid to data: \\[A \\sin\\left(\\frac{2\\pi}{P}\\right) + B\\] where\n\n\\(A\\) is the “amplitude”\n\\(B\\) is the “baseline”\n\\(P\\) is the period.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.6: A reproduction of the data originally shown in Figure 8.4. The baseline for the sinusoid is midway between the top of the oscillation and the bottom.\n\n\n\nThe baseline for the sinusoid is the value mid-way between the top of the oscillations and the bottom. For example, Figure 8.4 shows the sinusoidal-like pattern of tide levels. Dashed horizontal lines (\\(\\color{brown}{\\text{brown}}\\)) have been drawn roughly going through the top of the oscillation and the bottom of the oscillation. The baseline (magenta) will be halfway between these top and bottom levels.\nThe amplitude is the vertical distance between the baseline and the top of the oscillations. Equivalently, the amplitude is half the vertical distance between the top and the bottom of the oscillations.\nIn a pure, perfect sinusoid, the top of the oscillation—the peaks—is the same for every cycle, and similarly with the bottom of the oscillation—the troughs. The data in Figure 8.4 is only approximately a sinusoid so the top and bottom have been set to be representative. In Figure 11.6, the top of the oscillations is marked at level 1.6, the bottom at level 0.5. The baseline is therefore \\(B \\approx = (1.6 + 0.5)/2 = 1.05\\). The amplitude is \\(A  = (1.6 - 0.5)/2 = 1.1/2 = 0.55\\).\nTo estimate the period from the data, mark the input for a distinct point such as a local maximum, then count off one or more cycles forward and mark the input for the corresponding distinct point for the last cycle. For instance, in Figure 11.6, the tide level reaches a local maximum at an input of about 6 hours, as marked by a black dotted line. Another local maximum occurs at about 106 hours, also marked with a black dotted line. In between those two local maxima you can count \\(n=8\\) cycles. Eight cycles in \\(106-6 = 100\\) hours gives a period of \\(P = 100/8 = 12.5\\) hours.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting features</span>"
    ]
  },
  {
    "objectID": "Modeling/11-fitting-features.html#sec-exponential-water",
    "href": "Modeling/11-fitting-features.html#sec-exponential-water",
    "title": "11  Fitting features",
    "section": "11.3 Exponential",
    "text": "11.3 Exponential\nTo fit an exponential function, such as the ones in Figure 11.7, we estimate the three parameters: \\(A\\), \\(B\\), and \\(k\\) in \\[A \\exp(kt)+ B\\]\n\n\n\n\n\n\n\n\n\n\n\nExp. growth\n\nExp. decay\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.7: Exponential decay is a left-to-right flip of exponential growth.\n\n\n\nThe data in Figure 11.2 illustrates the procedure. The first question to ask is whether the pattern shown by the data resembles an exponential function. After all, the exponential pattern book function grows in output as the input gets bigger, whereas the water temperature is getting smaller—the word decaying is used—as time increases. To model exponential decay, use \\(\\exp(-k t)\\), where the negative sign effectively flips the pattern-book exponential left to right.\nThe exponential function has a horizontal asymptote for negative inputs. The left-to-right flipped exponential \\(\\exp(-k t)\\) also has a horizontal asymptote, but for positive inputs.\nThe parameter \\(B\\), again called the “baseline,” is the location of the horizontal asymptote on the vertical axis. Figure 11.2 suggests the asymptote is located at about 25 deg. C. Consequently, the estimated value is \\(B \\approx 25\\) deg C.\n\n11.3.1 Estimating A\nThe parameter \\(A\\) can be estimated by finding the value of the data curve at \\(t=0\\). In Figure Figure 11.8 that is just under 100 deg C. From that, subtract off the baseline you estimated earlier: (\\(B = 25\\) deg C). The amplitude parameter \\(A\\) is the difference between these two: \\(A = 99 - 25 = 74\\) deg C.\n\n\n11.3.2 Estimating k\nThe exponential has a unique property of “doubling in constant time” as described in Section 11.3.2. We can exploit this to find the parameter \\(k\\) for the exponential function.\n\nThe procedure starts with your estimate of the baseline for the exponential function. In Figure 11.8 the baseline has been marked in magenta with a value of 25 deg C.\nPick a convenient place along the horizontal axis. You want a place such that the distance of the data from the baseline to be pretty large. In Figure 11.8 the convenient place was selected at \\(t=25\\).\n\n\n\n\n\n## Warning: All aesthetics have length 1, but the data has 222 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n\n\n\n\n\n\n\n\n\nFigure 11.8: Determining parameter \\(k\\) for the exponential function using the doubling time.\n\n\n\n\nMeasure the vertical distance from the baseline at the convenient place. In Figure 11.8 the data curve has a value of about 61 deg C at the convenient place. This is \\(61-25 = 36\\) deg C from the baseline.\nCalculate half of the value from (c). In Figure 11.8 this is \\(36/2=18\\) deg C. But you can just as well do the calculation visually, by marking half the distance from the baseline at the convenient place.\nScan horizontally along the graph to find an input where the vertical distance from the data curve to the baseline is the value from (d). In Figure 11.8 that half-the-vertical-distance input is at about \\(t=65\\). Then calculate the horizontal distance between the two vertical lines. In Figure 11.8 that is \\(65 - 25 = 40\\) minutes. This is the doubling time. Or, you might prefer to call it the “half-life” since the water temperature is decaying over time.\nCalculate the magnitude \\(\\|k\\|\\) as \\(\\ln(2)\\) divided by the doubling time from (e). That doubling time is 40 minutes, so \\(\\|k\\|= \\ln(2) / 40 = 0.0173\\). We already know that the sign of \\(k\\) is negative since the pattern shown by the data is exponential decay toward the baseline. So, \\(k=-0.0173\\).",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting features</span>"
    ]
  },
  {
    "objectID": "Modeling/11-fitting-features.html#graphics-layers",
    "href": "Modeling/11-fitting-features.html#graphics-layers",
    "title": "11  Fitting features",
    "section": "11.4 Graphics layers",
    "text": "11.4 Graphics layers\nWhen fitting a function to data, it is wise to plot out the resulting function on top of the data. This involves making graphics with two layers, as described in Chapter 7. As a reminder, here is an example comparing the cooling-water data to the exponential function we fitted in Section 11.3.\nThe fitted function we found was \\[T_{water}(t) \\equiv 74 \\exp(-0.0173 t) + 25\\] where \\(T\\) stands for “temperature.”\nTo compare \\(T_{water}()\\) to the data, we will first plot out the data with gf_point(), then add a slice plot of the function. We will also show a few bells and whistles of plotting: labels, colors, and such.\n\n\n\n\nT_water &lt;- makeFun(74*exp(-0.0173*t) + 25 ~ t)\ngf_point(temp ~ time, data = CoolingWater, alpha = 0.5 ) %&gt;%\n  slice_plot(T_water(time) ~ time, color = \"blue\") %&gt;%\n  gf_labs(x = \"Time (minutes)\", y = \"Temperature (deg. C)\")\n\n\n\n\n\n\n\n\n\n\nFigure 11.9: A graphic with two layers: one for the cooling-water data and the other with the exponential function fitted to the data.\n\n\n\nThe slice_plot() command inherited the domain interval from the gf_point() command. This happens only when the name of the input used in slice_plot() is the same as that in gf_point(). (it is time in both.) You can add additional data or function layers by extending the pipeline.\nBy the way, the fitted exponential function is far from a perfect match to the data. Chapter 16 returns to this mismatch in exploring the modeling cycle.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting features</span>"
    ]
  },
  {
    "objectID": "Modeling/11-fitting-features.html#fitting-other-pattern-book-functions",
    "href": "Modeling/11-fitting-features.html#fitting-other-pattern-book-functions",
    "title": "11  Fitting features",
    "section": "11.5 Fitting other pattern-book functions",
    "text": "11.5 Fitting other pattern-book functions\nThis chapter has looked at fitting the exponential, sinusoid, and Gaussian functions to data. Those are only three of the nine pattern-book functions. What about the others?\n\n\n\n\nTable 11.2: Shapes of the pattern-book functions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconst\nprop\nsquare\nrecip\ngaussian\nsigmoid\nsinusoid\nexp\nln\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn Blocks 2 and 4, you will see how the Gaussian and the sigmoid are intimately related to one another. Once you see that relationship, it will be much easier to understand how to fit a sigmoid to data.\nThe remaining five pattern-book functions, the ones we haven’t discussed in terms of fitting, are the logarithm and the four power-law functions included in the pattern-book set. Chapter 14 introduces a technique for estimating from data the exponent of a single power-law function.\nIn high school, you may have done exercises where you estimated the parameters of straight-line functions and other polynomials from graphs of those functions. In professional practice, such estimations are done with an entirely different and completely automated method called regression. We will introduce regression briefly in Chapter 16. However, the subject is so important that all of Block 3 is devoted to it and its background.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting features</span>"
    ]
  },
  {
    "objectID": "Modeling/11-fitting-features.html#sec-polishing-promise",
    "href": "Modeling/11-fitting-features.html#sec-polishing-promise",
    "title": "11  Fitting features",
    "section": "11.6 Polishing parameters",
    "text": "11.6 Polishing parameters\nOften, fitting parameters to match a pattern seen in data can be done automatically (or semi-automatically) by software. When there are multiple inputs to the function, practicality demands that automated techniques be used. And even when it’s easy to estimate parameters by eye, as with the examples in this chapter, they can be improved by use of function-fitting software. We call this improvement in estimated parameters “polishing.”\nAn example of such automated fitting is given in Active R chunk 7.2. Chapter 16 covers the topic in more depth.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting features</span>"
    ]
  },
  {
    "objectID": "Modeling/11-fitting-features.html#footnotes",
    "href": "Modeling/11-fitting-features.html#footnotes",
    "title": "11  Fitting features",
    "section": "",
    "text": "Cynthia Tedore & Dan-Eric Nilsson (2019) “Avian UV vision enhances leaf surface contrasts in forest environments”, Nature Communications 10:238 11.3 and 11.4 ↩︎",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting features</span>"
    ]
  },
  {
    "objectID": "Modeling/12-low-order-polynomials.html",
    "href": "Modeling/12-low-order-polynomials.html",
    "title": "12  Low-order polynomials",
    "section": "",
    "text": "12.1 Polynomials\nChapter 11 looked at the task of modifying a pattern-book function to display a desired pattern, focusing on patterns originating in graphs of data. The procedure involved identifying an appropriate pattern-book function, then using input and output scaling to stretch, flip, and lift that function so that it overlays, as much as possible, the desired pattern.\nIn this chapter, we will take on a different strategy for constructing appropriately shaped functions using linear combinations of a handful of simple functions: the monomials.\nRecall that the monomials are the power-law functions with non-negative, integer exponents: \\(x^0\\), \\(x^1\\), \\(x^2\\), \\(x^3\\), and so on. The “and so on” refers to even higher integer exponents such as \\(x^4\\) or \\(x^{51}\\) or \\(x^{213}\\), to name but a few. The more common name for a linear combination of monomials is polynomial.\nFor instance, a fifth-order polynomial consists of a linear combination of monomials up to order 5. That is, up to \\(x^5\\). This will have six terms because we count the order of the monomials starting with 0. \\[g(t) \\equiv a_0 + a_1 t + a_2 t^2 + a_3 t^3 + a_4 t^4 + a_5 t^5\\ . \\tag{12.1}\\]\nThe challenge in shaping a polynomial is to find the scalar multipliers—usually called coefficients when it comes to polynomials—that give us the shape we want. This might seem to be a daunting task, and it is for a human. But it can easily be handled using volumes of arithmetic, too much arithmetic for a human to take on but ideally suited for computing machines.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Low-order polynomials</span>"
    ]
  },
  {
    "objectID": "Modeling/12-low-order-polynomials.html#polynomials",
    "href": "Modeling/12-low-order-polynomials.html#polynomials",
    "title": "12  Low-order polynomials",
    "section": "",
    "text": "Tip\n\n\n\nMath expression 12.1 could be written entirely using exponents, like this: \\[ g(t) \\equiv a_0 t^0 + a_1 t^1 + a_2 t^2 + a_3 t^3 + a_4 t^4 + a_5 t^5\\ .\\] This form makes it easier to see that all of the terms in \\(g()\\) are monomials. By convention, nobody writes out explicitly the \\(t^0\\) function. Instead, recognizing that \\(t^0 = 1\\), we write the first term simply as \\(a_0\\). Similarly, rather than writing \\(t^1\\) in the second term, we write \\(a_1 t\\), without the exponent.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Low-order polynomials</span>"
    ]
  },
  {
    "objectID": "Modeling/12-low-order-polynomials.html#low-order-polynomial-models",
    "href": "Modeling/12-low-order-polynomials.html#low-order-polynomial-models",
    "title": "12  Low-order polynomials",
    "section": "12.2 Low-order polynomial models",
    "text": "12.2 Low-order polynomial models\nPolynomials in general can show a wide variety of snake-like patterns. A fifth-order polynomial can have up to four internal curves. A tenth-order polynomial can have 9 internal curves, and so. There is, however, rarely a need for generating functions with all those curves. Instead, a great deal of modeling work can be accomplished with just first-order polynomials (no internal curves) or second-order polynomials (one internal curve).\n\\[\\begin{eqnarray}\n\\textbf{First-order: }\\ \\ \\ \\ \\ & f_1(t) \\equiv b_0 + b_1 t\\\\\n\\textbf{Second-order: }\\ \\ \\ \\ \\ & f_2(t) \\equiv c_0 + c_1 t + c_2 t^2\n\\end{eqnarray}\\]\nYou may prefer to think about a first-order polynomial as a straight-line function. Similarly, a second-order polynomial is also known as a “quadratic” or “parabola.” Nonetheless, it is good to see them as polynomials distinguished by their order. This puts them into a general framework, all of which can be handled by the technology of linear combinations. And polynomials can also involve more than one input. For instance, here are three polynomial forms that involve inputs \\(x\\) and \\(y\\):\n\\[\n\\begin{eqnarray}\nh_a(x, y) &\\equiv & a_0 + a_x\\, x + a_y\\, y\\\\\nh_b(x, y) &\\equiv & b_0 + b_x\\, x + b_y\\, y + b_{xy}\\, x y\\\\\nh_c(x, y) &\\equiv & c_0 + c_x\\, x + c_y\\, y + c_{xy}\\, x y + c_{xx}\\, x^2 + c_{yy}\\, y^2\n\\end{eqnarray} \\tag{12.2}\\]\nThe reason to work with first- and second-order polynomials is rooted in the experience of modelers. Second-order polynomials provide a useful amount of flexibility while remaining simple and avoiding pitfalls.\n\n\n\n\n\n\nParameter names in polynomials\n\n\n\nThere are three different polynomials in Math expression 12.2. Modeling often involves constructing multiple models for the same setting, as described in Chapter 16. It’s conventional to name all the parameters in a given polynomial with a single letter: \\(a\\) is used for \\(h_a()\\), \\(b\\) for \\(h_b()\\), and so on.\nIn order to distinguish the parameters within a polynomial, we use subscripts, e.g. \\(a_0\\), \\(a_y\\), \\(c_x\\), \\(c_{xy}\\), \\(c_{yy}\\). The subscript on the coefficient name indicates which term it belongs to. For instance, the coefficient on the \\(y^2\\) term of the \\(h_c\\) polynomial is named \\(c_{yy}\\) while the coefficient on the \\(x y\\) term has the subscript \\(_{xy}\\). Always, the coefficients are constant quantities and not functions of \\(x\\) or any other input. The double-letter coefficients are used for second-order terms in the polynomial, e.g. \\(c_{xx}\\) is used for the \\(x^2\\) term.\nIn high-school mathematics, polynomials are often written without subscript, for instance \\(a x^2 + b x + c\\). This can be fine when working with only one polynomial at a time, but in modeling we often need to compare multiple, related polynomials.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Low-order polynomials</span>"
    ]
  },
  {
    "objectID": "Modeling/12-low-order-polynomials.html#eight-simple-shapes",
    "href": "Modeling/12-low-order-polynomials.html#eight-simple-shapes",
    "title": "12  Low-order polynomials",
    "section": "12.3 Eight simple shapes",
    "text": "12.3 Eight simple shapes\nAn easy way to think about how to use low-order polynomials in modeling is to think about the shapes of their graphs. Figure 12.1 shows eight simple shapes for functions with a single input that occur often in modeling.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.1: Eight simple shapes of localized functions with one input.\n\n\n\nRecall that Chapter 6 introduced terms such as concavity, monotonicity, and slope for describing functions. To choose among these shapes, consider your modeling context:\n\nis the relationship positive (slopes up) or negative (slopes down)?\nis the relationship monotonic or not?\nis the relationship concave up, concave down, or neither?\n\nEach of the eight simple shapes corresponds to a particular set of answers to these equations. Consider these modeling contexts as examples:\n\nHow many minutes can you run as a function of speed? Concave down and downward sloping: Shape (F). In everyday terms, you wear out faster if you run at high speed.\nHow much fuel is consumed by an aircraft as a function of distance? For long flights, the function is concave up and positive sloping: Shape (D). In everyday terms: fuel use increases with distance, but the amount of fuel you have to carry also increases with distance. A heavy aircraft uses more fuel per mile.\nHow far can you walk as a function of time? Steep-then-shallow and concave down: Shape (E). Your pace slows as you get tired.\nHow does the stew taste as a function of saltiness? There is a local maximum: Shape (H). The taste improves as the amount of salt increases … up to a point. Too much salt and the stew is unpalatable.\nThe incidence of an out-of-control epidemic versus time is concave up, but shallow-then-steep. As the epidemic is brought under control, the decline is steep-then-shallow and concave up. Over the whole course of an epidemic, there is a maximum incidence. Experience shows that epidemics can have a phase where incidence reaches a local minimum: a decline as people practice social distancing followed by an increase as people become complacent.\nIn micro-economic theory there are production functions that describe how much of a good is produced at any given price, and demand functions that describe how much of the good will be purchased as a function of price. As a rule, production increases with price and demand decreases with price.\nIn the short term, production functions tend to be concave down, since it is hard to squeeze increased production out of existing facilities. Production functions are Shape (E).\n\nIn the short term, some consumers have no other choice than to buy the product, regardless of price. Short-term demand functions are downward sloping, but concave up: Shape C in Figure 12.1. the long term, demand functions can be concave down as consumers find alternatives to the high-priced good. For example, high prices of gasoline may, in the long term, prompt a switch to more efficient cars, hybrids, or electric vehicles. This will push demand for gas guzzlers down steeply.\n\n\nRemarkably, all the eight simple shapes can be generated by appropriate choices for the coefficients in a second-order polynomial: \\(g(x) = a_0 + a_1 x + a_2 x^2\\). So long as \\(a_2 \\neq 0\\), the graph of the second-order polynomial will be a parabola.\n\nThe parabola opens upward if \\(0 &lt; a_2\\). That is the shape of a local minimum.\nThe parabola opens downward if \\(a_2 &lt; 0\\). That is the shape of a local maximum\n\nConsider what happens if \\(a_2 = 0\\). The function becomes simply \\(a_0 + a_1\\, x\\), the straight-line function.\n\nWhen \\(0 &lt; a_1\\) the line slopes upward.\nWhen \\(a_1 &lt; 0\\) the line slopes downward.\n\nTo produce the steep-then-shallow or shallow-then-steep shapes, you also need to restrict the function domain to be on one side or another of the turning point of the parabola as shown in Figure 26.2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.2: Four of the eight simple shapes correspond to the sides of the parabola. The labels refer to the graphs in Figure 12.1.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Low-order polynomials</span>"
    ]
  },
  {
    "objectID": "Modeling/12-low-order-polynomials.html#sec-low-order-two",
    "href": "Modeling/12-low-order-polynomials.html#sec-low-order-two",
    "title": "12  Low-order polynomials",
    "section": "12.4 Polynomials with two inputs",
    "text": "12.4 Polynomials with two inputs\nFor functions with two inputs, the low-order polynomial approximation looks like this:\n\\[g(x, y) \\equiv a_0 + a_x x + a_y y + a_{xy} x y + a_{yy} y^2 + a_{xx} x^2\\]\nIt helps to have different names for the various terms. It is not too bad to say something like, “the \\(a_{xy}\\) term.” (Pronunciation: “a sub x y” or “a x y”) But the proper names are: linear terms, quadratic terms, and interaction term. And a shout out to \\(a_0\\), the constant term.\n\\[g(x, y) \\equiv a_0 + \\underbrace{a_x x + a_y y}_\\text{linear terms} \\ \\ \\ +\n\\underbrace{a_{xy} x y}_\\text{interaction term} +\\ \\ \\  \\underbrace{a_{yy} y^2 + a_{xx} x^2}_\\text{quadratic terms}\\]\nThe interaction term arises in models of phenomena such as the spread of epidemics, the population dynamics of predator and prey animals, and the rates of chemical reactions. In each of these situations, one thing is interacting with another: a predator killing a prey animal, an infective individual meeting a person susceptible to the disease, one chemical compound reacting with another.\nUnder certain circumstances, modelers include one or both quadratic terms, as in \\[h_3(x, y) \\equiv c_0 + c_x\\, x + c_y\\, y + c_{xy}\\,x\\, y + \\underbrace{c_{yy}\\, y^2}_\\text{quadratic in y}\\] The skilled modeler can often deduce which terms to include from basic facts about the system being modeled. We will need some additional calculus concepts before we can explain this straightforwardly.\nA second-order polynomial with two inputs can take on any one of three shapes: a bowl, a hilltop, or a saddle.\n\n\n\n\n\n\n\n\n\nBowl\n\n\n\n\n\n\n\n\n\nHill\n\n\n\n\n\n\n\n\n\nSaddle\n\n\n\n\n\n\n\nFigure 12.3: The three forms for a second-order polynomial with two inputs.\n\n\n\nOther shapes for modeling can be extracted from these three basic shapes. For example, the lower-right quadrant of the Saddle has the shape of seats in an amphitheater.\n\nApplication area 12.1 — Even first-order can be worthwhile\n\n\n\n\n\n\n\nApplication area 12.1 Polynomials and Laws of Physics\n\n\n\nThe start of Chapter 11 introduced a little mystery. Newton introduced his Law of Cooling in the 17th century: The rate at which an object cools depends on the difference in temperature between the object and its ambient environment. But in the 17th century, there was no precise way to measure a rate of temperature change. So how did Newton do it?\nEven with primitive thermometers, one can confirm that a mug of hot water will cool and a glass of cold water will warm to room temperature and stay there. So Newton could deduce that the rate of temperature change is zero when the object’s temperature is the same as the environment. Similarly, it is easy to observe with a primitive thermometer that a big difference in temperature between an object and its environment produces a rapid change in temperature, even if you cannot measure the rate precisely. So the rate of cooling is a function of the temperature difference \\(\\Delta T\\) between object and environment.\nWhat kind of function?\nLow-order polynomials to the rescue! The simplest model is that the rate of cooling will be \\(a_0 + a_1 \\Delta T\\), a first-order polynomial. But we know that the rate of cooling is zero when \\(\\Delta T = 0\\), implying that \\(a_0=0\\). All that is left is the first-order term \\(\\Delta T\\), which you can recognize as the proportional() function.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Low-order polynomials</span>"
    ]
  },
  {
    "objectID": "Modeling/13-operations.html",
    "href": "Modeling/13-operations.html",
    "title": "13  Operations on functions",
    "section": "",
    "text": "13.1 Zero finding\nChapters 8 through 11 introduce concepts and techniques for constructing functions. This is an important aspect of building models, but it is not the only one. Typically, a modeler, after constructing appropriate functions, will manipulate them in ways that extract the information required to answer questions that motivated the modeling work. This chapter will introduce some of the operations and manipulations used to extract information from model functions.\nThere are five such operations that you will see many times throughout this book.\nThis chapter will introduce the first three of these. The remaining two—differentiation and integration—are the core operations of calculus. They will be introduced starting in Block II.\nWe take two different perspectives on the three operations: graphical and computational. Often, a graph lets you carry out an operation with sufficient precision for the purpose at hand. Graphs are relatively modern, coming into mainstream use only in the 1700s. Much of mathematics was developed before graphs were invented. It is often the case that the algebraic ways of implementing the operations are difficult, while graphical approaches can be very easy. This is especially true now that graphics are so easy to make. You can iterate (operation 3 above), zooming the graphics domain around an approximate answer.\nGraphics connect well with human cognitive strengths, but the computer can also automatically carry out the operations quickly and precisely. The computing algorithms and software used for zero-finding/inversion and optimization are often based on concepts from calculus that we have not yet encountered. Software provides another advantage: Experts in a field can communicate with newbies so that anyone can use the operation in practice without necessarily developing a complete theoretical understanding of the algorithm. At this stage, before the calculus concepts have been introduced, our computational focus will be on how to set up the calculation and how to interpret the results.\nA function is a mechanism for turning any given input into an output. Zero finding is about going the other way: given an output value, find the corresponding input. As an example, consider the exponential function \\(e^x\\). Given a known input, say \\(x=2.135\\) you can easily compute the corresponding output as in Active R chunk 13.1:\nDepending on the modeling context, information can come in different forms. For instance, suppose the situation is:\nThis sort of situation calls for “inverting” the \\(\\exp()\\) function. There are very general and simple methods for performing this sort of extraction. We call these methods “zero finding. They involve searching for the value of an input that will make the output of a given function be zero.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operations on functions</span>"
    ]
  },
  {
    "objectID": "Modeling/13-operations.html#zero-finding",
    "href": "Modeling/13-operations.html#zero-finding",
    "title": "13  Operations on functions",
    "section": "",
    "text": "Active R chunk 13.1\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\nGiven: We know that \\(\\exp(x_0) = 4.93\\) for some as yet unknown \\(x_0\\).\nInformation wanted: What is the value of \\(x_0\\)?\n\n\n\nApplication area 13.1 — Detecting earthquakes and heart disease have something in common.\n\n\n\n\n\n\n\nApplication area 13.1 Finding earthquakes and injured heart muscle.\n\n\n\nWe look at pretty simple functions in this book. But consider these two similar kinds of functions:\n\nAn earthquake occurs at some depth and location and involving a particular energy and direction of motion along the fault. The function is this: The motion creates waves that pass through the Earth’s crust and mantle and are refracted by the Earth’s core. In the end, they reach a seismometer at a geophysics research lab.\nA heart attack has damaged some heart muscle. This alters the electrical activity of the heart. In every heart beat, the electrical activity passes as waves through the complex structure of the thorax and muscles, and on to the arms and legs. The function is this: how the alteration in activity appears when measured as electrical voltages on the surface of the body, as in an electro-cardiogram (ECG).\n\nIn both these cases, it’s possible to approximate the function. Often, this is done with simulation: building a detailed model of the Earth or a body and passing waves through the model. The inversion is the process of finding what input to the model produces the measured output.\n\n\n\n\n\n\n\n\nInverting algebraically\n\n\n\nSome may already know a technique for finding the value of \\(x_0\\) in the Information/Extraction situation posed above. The process, learned in high-school algebra classes is to use the natural logarithm on the output 4.93.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nHow would you demonstrate that the result is correct? Feed the value you found for \\(x_0\\) back into the \\(\\exp()\\) function and verify that you get 4.93.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis process works because we happen to have a function at hand, the natural logarithm, that is perfectly set up to “undo” the action of the exponential function. In high school, you learned a handful of function/inverse pairs: exp() and log() as you’ve just seen, sin() and arcsin(), square and square root, etc.\nMemorizing the names of such function/inverse pairs is helpful in passing high-school algebra and for following derivations in texts and on the blackboard. But knowing the name is only part of the story. You have to be able to apply the functions to inputs. Typically, the only way you have to do this is with a machine such as a calculator or computer.\nA similar high-school situations involves quadratic polynomials. Almost all readers will be familiar with this situation:\n\nGiven: We know that \\(x_0^2 - 3.9 x_0 = -7.2\\) for some as yet unknown \\(x_0\\).\nInformation wanted: What is the value of \\(x_0\\)?\n\nHigh-school students learn to bring the \\(-7.2\\) to the left side of the equals sign and then find the “roots.” Applying the “quadratic formula” to the coefficients of the polynomial gives numerical values for the roots. which produces one or two numbers as a result. This is well and good so far as it goes, but the quadratic formula is no use for a slightly modified problem. For instance, suppose the situation is this:\n\nGiven: We know that \\(x_0^2 - 3.9 e^{x_0} = -7.2\\) for some as yet unknown \\(x_0\\).\nInformation wanted: What is the value of \\(x_0\\)?\n\nWouldn’t it be nice to have one simple technique that applies to all such problems? That’s what the zero-finding operation gives us.\n\n\n\nGraphical zero-finding\nConsider any function \\(h(x)\\) that you constructed by linear combination and/or function multiplication and/or function composition. To illustrate, we will work with the function \\(h(x)\\) graphed in Figure 13.1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 13.1: Finding an \\(x_0\\) such that \\(h(x_0) = 3\\)\n\n\n\nSuppose the output for which we want to find a corresponding input is 3, that is, we want to find \\(x_0\\) such that \\(h(x_0)=3\\).\nThe steps of the process are:\n\nGraph the function \\(h(x)\\) over a domain interval of interest.\nDraw a horizontal line located at the value on the right-hand side of the equation \\(h(x_0) = 3\\). (This is the magenta line in Figure 13.1.)\nFind the places, if any, where the horizontal line intersects the graph of the function. In Figure 13.1, there are two such values: \\(x_0 = -3.5\\) or \\(x_0 = 2.75\\).\n\n\n\n\n\n\n\nImportant 13.1: Graphical zero finding\n\n\n\nA function \\(g(t)\\) is graphed below. Find a value \\(t_0\\) such that \\(g(t_0) = 5\\).\n\n\n\n\n\n\n\n\n\n\nDraw a horizontal line at output level 5.\nFind the t-value where the horizontal line intersects the function graph. There is only one such intersection and that is at about \\(t=1.2\\).\n\nConsequently, \\(t_0 = 1.2\\), at least to the precision possible when reading a graph.\n\n\nThe graphical approach to zero finding is limited by your ability to locate positions on the vertical and horizontal axis. If you need more precision than the graph provides, you have two options:\n\nTake a step-by-step, iterative approach. Use the graph to locate a rough value for the result. Then refine that answer by drawing another graph, zooming in on a small region around the result from the first step. You can iterate this process, repeatedly zooming in on the result you got from the previous step.\nUse software implementing a numerical zero-finding algorithm. Such software is available in many different computer languages and a variety of algorithms is available, each with its own merits and demerits.\n\n\n\nNumerical zero finding\nIn this book, for consistency with our notation, we use the R/mosaic Zeros() function. The first argument to Zeros() is a tilde expression and the second argument an interval of the domain over which to search.\nZeros() is set up to find inputs where the mathematical function defined in the tilde expression produces zero as an output. But suppose you are dealing with a problem like \\(f(x) = 10\\)? You can modify the tilde expression so that it implements a slightly different function: \\(f(x) - 10\\). If we can find \\(x_0\\) such that \\(f(x_0) - 10 = 0\\), that will also be the \\(x_0\\) satisfying \\(f(x_0) = 10\\).\n\n\n\n\n\n\nImportant 13.2: Computationally finding zeros\n\n\n\nThe point of this example is to show how to use Zeros(), so we will define a function \\(f(x)\\) using doodle_fun() from R/mosaic. This constructs a function by taking a linear combination of other functions selected at random. The argument seed=579 determines which functions will be in the linear combination. The function is graphed in Figure 13.2.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe want to find the zeros of the function \\(f(x) - 10\\) which corresponds to solving \\(f(x) = 10\\). The function Zeros() handles this for us.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe output produced by Zeros() is a data frame with one row for each of the \\(x_0\\) found. Here, two values were found: \\(x_0 = -2.92\\) and \\(x_0 = 0.0795\\), shown as vertical lines in Figure 13.2. The .output column reports \\(f(x_0)\\). In principal, this should be exactly zero. However, computer arithmetic is not always exactly precise. Even so, numbers as small as those in the .output. column—\\(-3.4 \\times 10^{-8}\\) for example—are miniscule compared to the range of values seen in Figure 13.2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 13.2: A graph of \\(f()\\), marking the places where the output is 10.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operations on functions</span>"
    ]
  },
  {
    "objectID": "Modeling/13-operations.html#optimization",
    "href": "Modeling/13-operations.html#optimization",
    "title": "13  Operations on functions",
    "section": "13.2 Optimization",
    "text": "13.2 Optimization\nOptimization problems consist of both a modeling phase and a solution phase (that is, an information extraction phase). We use our knowledge of how the world works for the modeling phase. Then we extract information in the form we want from the solution phase.\nIn this chapter we will deal only with the solution phase. In real work, the modeling phase is essential\n\nGraphical optimization\nSimple. Look for local peaks, then read off the input that generates the value at the peak.\n\n\nNumerical optimization\nWhen it comes to functions, maximization is the process of finding an input to the function that produces a larger output than any of the other, nearby inputs.\nTo illustrate, Figure 13.3 shows a function f() with two peaks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 13.3: A function with two peaks\n\n\n\nJust as you can see a mountain top from a distance, so you can see where the function takes on its peak values. Draw a vertical line through each of the peaks. The input value corresponding to each vertical line is called an argmax, short for “the argument1 at which the function reaches a local maximum value.\nMinimization refers to the same technique, but where the vertical lines are drawn at the deepest point in each “valley” of the function graph. An input value located in one of those valleys is called an argmin.\nOptimization is a general term that covers both maximization and minimization.\n\n\n13.2.1 Numerical optimization\nThe R/mosaic argM() function finds a mathematical function’s argmax and argmin over a given domain. It works in exactly the same way as slice_plot(), but rather than drawing a graphic it returns a data frame giving the argmax in one row and the argmin in another. For instance, the function shown in Figure 13.3 is \\(f()\\), generated by doodle_fun():\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe x column holds the argmax and argmin, the .output. column gives the value of the function output for the input x. The concavity column tells whether the function’s concavity at x is positive or negative. Near a peak, the concavity will be negative; near a valley, the concavity is positive. Consequently, you can see that the first row of the data frame corresponds to a local minimum and the second row is a local maximum.\nargM() is set up to look for a single argmax and a single argmin in the domain interval given as the second argument. In Figure 13.3 there are two local peaks and two local valleys. argM() gives only the largest of the peaks and the deepest of the valleys.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operations on functions</span>"
    ]
  },
  {
    "objectID": "Modeling/13-operations.html#iteration",
    "href": "Modeling/13-operations.html#iteration",
    "title": "13  Operations on functions",
    "section": "13.3 Iteration",
    "text": "13.3 Iteration\nMany computations involve starting with a guess followed by a step-by-step process of refining the guess. A case in point is the process for calculating square roots. There isn’t an operational formula for a function that takes a number as an input and produces the square root of that number as the output. When we write \\(\\sqrt{\\strut x}\\) we aren’t saying how to calculate the output, just describing the sort of output we are looking for.\nThe function that is often used to calculate \\(\\sqrt{x}\\) is better():\n\\[\\text{better(guess)} = \\frac{1}{2}\\left( \\text{guess} + \\frac{x}{\\text{guess}}\\right)\\ .\\]\nIt may not be at all clear why this formula is related to finding a square root. Let’s put that matter off until the end of the section and concentrate our attention on how to use it.\nTo start, let’s define the function for the computer. Suppose we want to apply the square root function to the input 55, that is, calculate \\(\\sqrt{\\strut x=55}\\). The value we should assign to \\(x\\) is therefore 55.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nNotice that \\(x\\) is cast in the role of a parameter of the function rather than an input to the function.\nTo calculate better(guess) we need an initial value for the guess. What should be this value and what will we do with the quantity better(guess) when we’ve calculated it.\nWithout explanation, we will use guess = 1. Calculating the output …\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nNeither our guess 1 nor the output 28 are \\(\\sqrt{\\strut x=55}\\). (Having long-ago memorized the squares of integers, we know \\(\\sqrt{\\strut x=55}\\) will be somewhere between 7 and 8. Neither 1 nor 28 are in that interval.)\nThe people—more than two thousand years ago—who invented the ideas behind the better() function were convinced that better() constructs a better guess for the answer we seek. It is not obvious why 28 should be a better guess than 1 for \\(\\sqrt{\\strut x=55}\\) but, out of respect, let’s accept their claim.\nThis is where iteration comes in. Even if 28 is a better guess than 1, 28 is still not a good guess. But we can use better() to find something better than 28:\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nTo iterate an action means to perform that action over and over again. (“Iterate” stems from the Latin word iterum, meaning “again.”) A bird iterates its call, singing it over and over again. In mathematics, “iterate” has a twist. When we repeat the mathematical action, we will draw on the results of the previous angle rather than simply repeating the earlier calculation.\nContinuing our iteration of better(), plugging the output of each calculation as the input for the next …\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nIn the last step, the output of better() is practically identical to the input, so no reason to continue. We can confirm that the last output is a good guess for \\(\\sqrt{\\strut x=55}\\):\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nGraphical iteration\nTo iterate graphically, we graph the function to be iterated and mark the initial guess on the horizontal axis. For each iteration step, trace vertically from the current point to the function, then horizontally to the line of identity (blue dots). The result will be the starting point for the next guess.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 13.4: Three steps of iteration of better() starting with an initial guess of 1.\n\n\n\n\n\nNumerical iteration\nUse the R/mosaic Iterate() function. The first argument is a tilde expression defining the function to be iterated. The second is the starting guess. The third is the number of iteration steps. For instance:\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nThe output produced by Iterate() is a data frame. The initial guess is in the row with \\(n=0\\). Successive rows give the output, step by step, with each new iteration step.\n\n\n\n\n\n\nWhere does better() come from?\n\n\n\nFor calculating square roots, we used the function \\[\\text{better}(y) = \\frac{1}{2}\\left( y + \\frac{x}{y}\\right)\\ .\\] Let’s suppose you happened on a guess that is exactly right, that is \\(y = \\sqrt{x}\\). There is no way to improve on a guess that is exactly right, so the best better() can do is to return the guess unaltered. Indeed it does: \\[\\text{better}\\left(y=\\!\\!\\sqrt{\\strut x}\\ \\right) = \\frac{1}{2}\\left( \\sqrt{\\strut x} + \\frac{x}{\\sqrt{x}} \\right)\\ = \\frac{1}{2}\\left(\\sqrt{\\strut x} + \\sqrt{\\strut x}\\right) = \\sqrt{\\strut x}.\\]\nOf course, the initial guess \\(y\\) might be wrong. There are two ways to be wrong:\n\nThe guess is too small, that is \\(y &lt; \\sqrt{\\strut x}\\).\nThe guess is too big, that is \\(\\sqrt{\\strut x} &lt; y\\).\n\nThe formula for better() is the average of the guess \\(y\\) and another quantity \\(x/y\\). If \\(y\\) is too small, then \\(x/y\\) must be too big. If \\(y\\) is too big, then \\(x/y\\) must be too small.\nAs guesses, the two quantities \\(y\\) and \\(x/y\\) are equivalent in the sense that \\(\\text{better}(y) = \\text{better}(x/y)\\). The average of \\(y\\) and \\(x/y\\) will be closer to the true result than the worst of \\(y\\) or \\(x/y\\); the average will be a better guess.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operations on functions</span>"
    ]
  },
  {
    "objectID": "Modeling/13-operations.html#footnotes",
    "href": "Modeling/13-operations.html#footnotes",
    "title": "13  Operations on functions",
    "section": "",
    "text": "Also known as an input.↩︎",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operations on functions</span>"
    ]
  },
  {
    "objectID": "Modeling/14-magnitudes.html",
    "href": "Modeling/14-magnitudes.html",
    "title": "14  Magnitude",
    "section": "",
    "text": "14.1 Order of magnitude\nConsider a mathematical task that is a routine part of everyday life: comparing two numbers to determine which is bigger or selecting the biggest from a set of numbers. For instance:\nYou can see the answer at a glance; the task requires hardly any mental effort.\nBut for the Romans and Europeans up through the 13th century, numbers were hard to work with. For instance,\nIn Arabic notation, a number with more digits is always larger than another number that requires fewer digits. Consequently, the printed form of a number gives a visual clue about the numbers’ relative sizes.\nFor the Roman numerals, the length of the printed form gives no good hint about the size of the number. Indeed, the number represented by MLI is almost fifty times bigger than the number denoted by XXXIII.\nEven with Arabic numerals, the comparison task becomes harder when the numbers are either very small or very big. For instance:\nCounting the digits before the decimal point (for large numbers) or counting the leading zeros after the decimal point (for small numbers) is tedious and error prone.\nFor this reason, people who have routinely to deal with very large or very small numbers learn a different notation for writing numbers called scientific notation. Here is just about the same problem as the above, but with the numbers written in scientific notation.\nIn scientific notation, the first step in the comparison process involves just the exponent in 10?. Whichever of the two numbers has the larger exponent is the larger number. Only if the exponents are the same is there any need to look at the digits preceeding \\(\\times  10^{?}\\).\nThe exponent in a scientific-notation number can be called the “order of magnitude” of that number. As you will see in the following sections, “order of magnitude” is closely connected to one of our pattern-book functions: the logarithm. This is one reason that the logarithm is important in applied work.\nWe will refer to judging the size of numbers by their count of digits as reading the magnitude of the number. To get started, consider numbers that start with 1 followed by zeros, e.g. 100 or 1000. We will quantify the magnitude as the number of zeros: 100 has a magnitude of 2 and 1000 has a magnitude of 3. In comparing numbers by magnitude, we way things like, “1000 is an order of magnitude greater than 100,” or “1,000,000” is five orders of magnitude larger than 10.\nMany phenomena and quantities are better understood using magnitude rather than number. An example: Animals, including humans, go about the world in varying states of illumination, from the bright sunlight of high noon to the dim shadows of a half-moon. To be able to see in such diverse conditions, the eye needs to respond to light intensity across many orders of magnitude.\nThe lux is the unit of illuminance in the Système international. Table 14.1 shows the illumination in a range of familiar outdoor settings:\nFor a creature active both night and day, the eye needs to be sensitive over 7 orders of magnitude of illumination. To accomplish this, eyes use several mechanisms: contraction or dilation of the pupil accounts for about 1 order of magnitude, photopic (color, cones) versus scotopic (black-and-white, rods, nighttime) covers about 3 orders of magnitude, adaptation over minutes (1 order), squinting (1 order).\nMore impressively, human perception of sound spans more than 16 orders of magnitude in the energy impinging on the eardrum. The energy density of perceptible sound ranges from the threshold of hearing at 0.000000000001 Watt per square meter to a conversational level of 0.000001 W/m2 to 0.1 W/m2 in the front rows of a rock concert. But in terms of our subjective perception of loudness, each order of magnitude change is perceived in the same way, whether it be from street traffic to vacuum cleaner or from whisper to normal conversation. (The unit of sound measurement is the decibel (dB), with 10 decibels corresponding to an order of magnitude in the energy density of sound.)\n6, 60, 600, and 6000 miles-per-hour are quantities that differ in size by orders of magnitude. Such differences often point to a substantial change in context. A jog is 6 mph, a car on a highway goes 60 mph, a cruising commercial jet goes 600 mph, and a rocket passes through 6000 mph on its way to orbital velocity. From an infant’s crawl to highway cruising is 3 orders of magnitude in speed.\nOf course, many phenomena are not usefully represented by orders of magnitudes. For example, the difference between normal body temperature and high fever is 0.01 orders of magnitude in temperature.1 An increase of 1 order of magnitude in blood pressure from the normal level would cause instant death! The difference between a very tall adult and a very short adult is about 1/4 of an order of magnitude.\nOrders of magnitude are used when the relevant comparison is a ratio. “A car is 10 times faster than a person,” refers to the ratio of speeds. In contrast, quantities such as body temperature, blood pressure, and adult height are compared using a difference. Fever is 2\\(^\\circ\\)C higher in temperature than normal. A 30 mmHg increase in blood pressure will likely correspond to developing hypertension. A very tall and a very short adult differ by about 2 feet.\nOne clue that thinking in terms of orders of magnitude is appropriate is when you are working with a set of objects whose range of sizes spans one or many factors of 2. Comparing baseball and basketball players? Probably no need for orders of magnitudes. Comparing infants, children, and adults in terms of height or weight? Orders of magnitude may be useful. Comparing bicycles? Mostly they fit within a range of 2 in terms of size, weight, and speed (but not expense!). Comparing cars, SUVs, and trucks? Differences by a factor of 2 are routine, so thinking in terms of order of magnitude is likely to be appropriate.\nAnother clue is whether “zero” means “nothing.” Daily temperatures in the winter are often near “zero” on the Fahrenheit or Celcius scales, but that in no way means there is a complete absence of heat. Those scales are arbitrary. Another way to think about this clue is whether negative values are meaningful. If so, expressing those values as orders of magnitude is not likely to be useful.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Magnitude</span>"
    ]
  },
  {
    "objectID": "Modeling/14-magnitudes.html#order-of-magnitude",
    "href": "Modeling/14-magnitudes.html#order-of-magnitude",
    "title": "14  Magnitude",
    "section": "",
    "text": "Table 14.1: Illumination in lux in some outdoor settings. Source\n\n\n\n\n\nIlluminance\nCondition\n\n\n\n\n110,000 lux\nBright sunlight\n\n\n20,000 lux\nShade illuminated by entire clear blue sky, midday\n\n\n1,000 lux\nTypical overcast day, midday\n\n\n400 lux\nSunrise or sunset on a clear day (ambient illumination)\n\n\n0.25 lux\nA full Moon, clear night sky\n\n\n0.01 lux\nA quarter Moon, clear night sky\n\n\n\n\n\n\n\n\n\n\n\nTable 14.2: Energy density of sound in various situations. Sound at 85 dB, for extended periods, can cause permanent hearing loss. Exposure to sound at 120 dB over 30 seconds is dangerous.\n\n\n\n\n\nSituation\nEnergy level (dB)\n\n\n\n\nRustling leaves\n10 dB\n\n\nWhisper\n20 dB\n\n\nMosquito buzz\n40 dB\n\n\nNormal conversation\n60 dB\n\n\nBusy street traffic\n70 dB\n\n\nVacuum cleaner\n80 dB\n\n\nLarge orchestra\n98 dB\n\n\nEarphones (high level)\n100 dB\n\n\nRock concert\n110 dB\n\n\nJackhammer\n130 dB\n\n\nMilitary jet takeoff\n140 dB",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Magnitude</span>"
    ]
  },
  {
    "objectID": "Modeling/14-magnitudes.html#sec-counting-digits",
    "href": "Modeling/14-magnitudes.html#sec-counting-digits",
    "title": "14  Magnitude",
    "section": "14.2 Counting digits",
    "text": "14.2 Counting digits\nImagine having a digit counting function called digits(). It takes a number as input and produces a number as output. We have not yet presented a formula for digits(), but for some inputs, the output can be calculated just by counting. For numbers like 0.01 or 10 or 100000, we will define the number of digits to be the count of zeros before For example:\ndigits(10) \\(\\equiv\\) 1\ndigits(100) \\(\\equiv\\) 2\ndigits(1000) \\(\\equiv\\) 3\n… and so on …\ndigits(1,000,000) \\(\\equiv\\) 6\n… and on.\nFor numbers smaller than 1, like 0.01 or 0.0001, we define the number of digits to be the negative of the number of zeros before the 1.\ndigits(0.1) \\(\\equiv\\) -1\ndigits(0.01) \\(\\equiv\\) -2\ndigits(0.0001) \\(\\equiv\\) -4\nThe digits() function easily can be applied to the product of two numbers. For instance:\n\ndigits(1000 \\(\\times\\) 100) = digits(1000) + digits(100) = 3 + 2 = 5.\n\nSimilarly, applying digits() to a ratio gives the difference of the digits of the numerator and denominator, like this:\n\ndigits(1,000,000 \\(\\div\\) 10) = digits(1,000,000) - digits(10) = 6 - 1 = 4\n\nIn practice, digits() is so useful that it could well have been one of our basic modeling functions. Actually, this is very nearly the case: the logarithm is proportional to the number of digits.\nTo illustrate, consider these three calculations of logarithms:\n\n\n\nActive R chunk 14.1: The output is the order of magnitude of the number given as an argument to log().\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nHere is a formula definition of the digits() function.\n\\[\\text{digits}(x) \\equiv \\ln(x) / \\ln(10) \\tag{14.1}\\]\nIn R/mosaic, the analogous definition is:\n\n\n\nActive R chunk 14.2\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nYou may have guessed that digits() is handy for computing differences in orders of magnitude.\n\n\n\n\n\n\nImportant 14.1: Calculating differences in order of magnitude\n\n\n\n\nMake sure that the quantities are expressed in the same units.\nCalculate the difference between the digits() of the numerical part of the quantities.\n\nWhat is the order-of-magnitude difference in velocity between a snail and a walking human? A snail slides at about 1 mm/sec, a human walks at about 5 km per hour.\nThe first task is to put human speed in the same units as snail speed: \\[\\begin{eqnarray}5 \\frac{km}{hr} = \\left[\\frac{1}{3600} \\frac{hr}{sec}\\right] 5 \\frac{km}{hr} &=& \\\\\n\\left[10^6 \\frac{mm}{km}\\right] \\left[\\frac{1}{3600} \\frac{hr}{sec}\\right] 5 \\frac{km}{hr} &=& 1390 \\frac{mm}{sec}\n\\end{eqnarray}\\]\nThe second task is to find the difference between the number of digits()in each of the numbers.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nSo, about 3 orders of magnitude difference in speed: a factor of 1000. To a snail, we walking humans must seem like rockets on their way to orbit seem to us! (Think about it. We walk about 5 km/hour. A thousand time that is 5000 km/hour—rocket speed.)\n\n\nThe use of factors of 10 in counting orders of magnitude is arbitrary. A person walking and a person jogging are on the edge of being qualitatively different, although their speeds differ by a factor of only 2. Aircraft that cruise at 600 mph and 1200 mph are qualitatively different in design, although the speeds are only a factor of 2 apart. A professional basketball player (height 2 meters or more) is qualitatively different from a third grader (height about 1 meter).\n\n\n\n\n\n\nCalculus history—The “natural” logarithm\n\n\n\nYou may have noticed in Math expression 14.1 or Active R chunk 14.2 the terms \\(1/\\ln(10)\\) and / log(10) are used as a conversion factor. Similarly, in Active R chunk 14.1 the conversion factor 0.4342945 is used. Actually, 0.4342945 and 1 / log(10) are the same number:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nBut why is a conversion factor needed for the digits() calculation?\nThe point of the conversion factor is to have the “units” of the output of digits() correspond to a factor of 10. Such units are called “decades.” The unit of decade is dimensionless, just as the unit for angles—rads or degrees, as you preferr—is dimensionless.\nNaturally, we use decades because of our human habit of writing numbers in base 10, using the digits 0 to 9. I say “naturally” because base 10 is familiar to us. But different people have different notions of what is “natural.”\nWe could have used \\(1/\\ln(2)\\) as the conversion factor in digits() in which case each multiple of 2 in the input corresponds to a change in the output of 1. Or, I should say, “1 bit” because “bit” is the name given to the unit when \\(1/\\ln(2)\\) (that is, 1.442695) is used as the conversion factor.\nAn aesthetic widely admired in the field of mathematics is that having any such conversion factor at all is “unnatural.” The only mathematically pretty conversion factor is 1, that is, no conversion factor at all. If we had used 1 instead of \\(1/\\ln(10)\\) in digits(), the output would be in different units, not in decades.\nThe name mathematicians have given to the version of digits() where the conversion factor is 1 is the natural logarithm. It’s hard to understand the advantages of the natural logarithm until we get further into Calculus. For the natural logarithm, each increase in the input by a factor of 2.7182818281828… leads to an increase in the output by 1. To illustrate, run the code in Active R chunk 14.3.\n\n\n\nActive R chunk 14.3\n\n\n\nlog(63)\n## [1] 4.143135\nlog(2.718281828 * 63)\n## [1] 5.143135",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Magnitude</span>"
    ]
  },
  {
    "objectID": "Modeling/14-magnitudes.html#sec-magnitude-graphics",
    "href": "Modeling/14-magnitudes.html#sec-magnitude-graphics",
    "title": "14  Magnitude",
    "section": "14.3 Magnitude graphics",
    "text": "14.3 Magnitude graphics\nTo display a variable from data that varies over multiple orders of magnitude, it helps to plot the logarithm rather than the variable itself. Let’s illustrate using the Engine data frame, which contains measurements of many different internal combustion engines of widely varying sizes. For instance, we can graph engine RPM (revolutions per second) versus engine mass, as in Figure 14.1.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) linear axes\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) semi-log axes\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) log-log axes\n\n\n\n\n\n\n\n\nFigure 14.1: Engine RPM versus mass for 39 different enginges plotted\n\n\n\nIn the graph, most of the engines have a mass that is … zero. At least that is what it appears to be. The horizontal scale is dominated by the two huge 100,000-pound monster engines plotted at the right end of the graph.\nPlotting the logarithm of the engine mass spreads things out, as in Figure 14.1 (b).\nNote that the horizontal axis has been labeled with the actual mass (in pounds). The labels are evenly spaced in logarithm. This presentation, with the horizontal axis constructed this way, is called a semi-log plot.\nWhen both axes are labeled this way, we have a log-log plot, as shown in Figure 14.1 (c).\nSemi-log and log-log axes are widely used in science and economics, whenever data spanning several orders of magnitude need to be displayed. In the case of the engine RPM and mass, the log-log axis shows that there is a graphically simple relationship between the variables. Such axes are very useful for displaying data but can be hard for the newcomer to read quantitatively. For example, calculating the slope of the evident straight-line relationship in Figure 14.1 (c) is extremely difficult for a human reader and requires translating the labels into their logarithms.\n\n\n\n\n\n\nCalculus history—Boyle’s Law\n\n\n\nRobert Boyle (1627-1691) was a founder of modern chemistry and the scientific method in general. As any chemistry student already knows, Boyle sought to understand the properties of gasses. Famously, Boyle’s Law states that, at a constant temperature, the pressure of a constant mass of gas is inversely proportional to the volume occupied by the gas. Figure 14.2 shows a cartoon of the relationship.\n\n\n\n\n\n\nFigure 14.2: A cartoon illustrating Boyle’s Law. Source: NASA Glenn Research Center”\n\n\n\nThe data frame Boyle contains two variables from one of Boyle’s experiments as reported in his lab notebook: pressure in a bag of air and volume of the bag. The units of pressure are mmHg and the units of volume are cubic inches.2\nFigure 14.3 plots out Boyle’s actual experimental data in two different ways.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) linear axes\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) log-log axes\n\n\n\n\n\n\n\n\nFigure 14.3: Boyle’s pressure vs volume data plotted on linear and log-log axes.\n\n\n\nThe straight-line model fits nicely to the log-log plot in Figure 14.3 (b) showing that log-pressure and log-volume data are related as a straight-line function. In other words:\n\\[\\ln(\\text{Pressure}) = a + b \\ln(\\text{Volume})\\]\nYou can find the slope \\(b\\) and intercept \\(a\\) from the graph. For now, we want to point out the consequences of the straight-line relationship between logarithms.\nExponentiating both sides gives \\[e^{\\ln(\\text{Pressure})} = \\text{Pressure} = e^{a + b \\ln(\\text{Volume})}\\] \\[= e^a\\  \\left[e^{ \\ln(\\text{Volume})}\\right]^b = e^a\\, \\text{Volume}^b \\tag{14.2}\\] or, more simply (and writing the number \\(e^a\\) as \\(A\\))\nSince \\(a\\) is a parameter, the quantity \\(e^a\\) is effectively a parameter. We’ll use \\(A\\) to denote \\(e^a\\), simplifying Math expression 14.2 to \\[\\text{Pressure} = A\\times  \\text{Volume}^b\\ . \\tag{14.3}\\] A power-law relationship!",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Magnitude</span>"
    ]
  },
  {
    "objectID": "Modeling/14-magnitudes.html#sec-reading-log-axes",
    "href": "Modeling/14-magnitudes.html#sec-reading-log-axes",
    "title": "14  Magnitude",
    "section": "14.4 Reading logarithmic scales",
    "text": "14.4 Reading logarithmic scales\nPlotting the logarithm of a quantity gives a visual display of the magnitude of the quantity and labels the axis as that magnitude. A useful graphical technique is to label the axis with the original quantity, letting the position on the axis show the magnitude.\nTo illustrate, Figure 14.4 (a) is a log-log graph of horsepower versus displacement for the internal combustion engines reported in the Engines data frame. The points are admirably evenly spaced, but it is hard to translate the scales to the physical quantity. The right panel in Figure 14.4 (b) shows the same data points, but now the scales are labeled using the original quantity.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Log-log scales on the data\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Linear scales on the log data\n\n\n\n\n\n\n\n\nFigure 14.4: The same horsepower (BHP) versus engine displacement data plotted using two different scales. The data points themselves appear the same in both plots, but the parameters \\(b\\) and \\(e^a\\) (that is, \\(A\\) in Math expression 14.3) can be estimated as the slope and y-intercept respectively of a line fitted to Figure 14.4 (a) in the usual way.\n\n\n\nThe tick marks on the vertical axis in the left pane are labeled for 0, 1, 2, 3, and 4. These numbers do not refer to the horsepower itself, but to the logarithm (base 10) of the horsepower. The right pane has tick labels that are in horsepower at positions marked 1, 10, 100, 1000, and 10000.\nSuch even splits of a 0-100 scale are not appropriate for logarithmic scales. One reason is that 0 cannot be on a logarithmic scale in the first place since \\(\\ln(0) = -\\infty\\).\nAnother reason is that 1, 3, and 10 are pretty close to an even split of a logarithmic scale running from 1 to 10. It is something like this:\n\n1              2            3          5            10     x\n|----------------------------------------------------|\n0               1/3         1/2        7/10          1     log(x)\n\nIt is nice to have the labels show round numbers. It is also nice for them to be evenly spaced along the axis. The 1-2-3-5-10 convention is a good compromise; almost evenly separated in space yet showing simple round numbers.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Magnitude</span>"
    ]
  },
  {
    "objectID": "Modeling/14-magnitudes.html#footnotes",
    "href": "Modeling/14-magnitudes.html#footnotes",
    "title": "14  Magnitude",
    "section": "",
    "text": "we are using the Kelvin scale, which is the only meaningful scale for a ratio of temperatures.↩︎\nBoyle’s notebooks are preserved at the Royal Society in London. The data in the Boyle data frame have been copied from this source.)↩︎",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Magnitude</span>"
    ]
  },
  {
    "objectID": "Modeling/15-dimensions.html",
    "href": "Modeling/15-dimensions.html",
    "title": "15  Dimensions and units",
    "section": "",
    "text": "15.1 Mathematics of quantity\nNext time you’re at a family gathering with your 10-year-old cousin, give her the following math quiz.\nI don’t know your cousin, but I suspect she will have an easy time answering (a) and (b) correctly. As for (c), she might give the correct answer, “5 miles,” or just say “5.” If so, you will follow up with “5 what?” at which point she’ll respond, “miles.”\n10-year-olds are pretty creative, so I’m not sure how she’ll answer (e). But if you ask your Ph.D. aunt, she’ll answer along the lines of “silly question,” or “there is no such thing.” That is true.\nConsider these everyday quantities:\nHow would you measure such things?\nIt makes sense to multiply and divide different types of quantities: feet, gallons, kilometers, kilograms, pounds, hours, etc. But you won’t ever see a quantity constructed by adding or subtracting miles and hours or gallons and square feet. You can square feet and cube centimeters, but can you take the square root of a gallon? Does it make sense to raise 2 to the power of 3 yards?\nThis chapter is about the mathematical structure of combining quantities; which kinds of mathematical operations are legitimate and which are not.\nThe first step in understanding the mathematics of quantity is to make an absolute distinction between two concepts that, in everyday life, are used interchangeably: dimension and unit.\nLength is a dimension. Meters is a unit of length. We also measure length in microns, mm, cm, inches, feet, yards, kilometers, and miles, to say nothing of furlongs, fathoms, astronomical units (AU), and parsecs.\nTime is a dimension. Seconds is a unit of time. We also measure time in micro-seconds, milliseconds, minutes, hours, days, weeks, months, years, decades, centuries, and millenia.\nMass is a dimension. Kilograms is a unit of mass.\nLength, time, and mass are called fundamental dimensions. This is not because length is more important than area or volume. It is because you can construct area and volume by multiplying lengths together. This is evident when you consider units of area like square inches or cubic centimeters, but obscured in the names of units like acre, liter, gallon.\nWe use the notation L, T, and M to refer to the fundamental dimensions. Also useful are \\(\\Theta\\) (“theta”) for temperature, S for money, and P for a count of organisms such as the population of the US or the size of a sheep herd.\nThe square brackets \\([\\) and \\(]\\) signify that we are looking at the dimension of the quantity inside the brackets. For instance,\nAnother example: the population of the US state Colorado is about 5.8 million people. The dimension of this quantity is P.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dimensions and units</span>"
    ]
  },
  {
    "objectID": "Modeling/15-dimensions.html#mathematics-of-quantity",
    "href": "Modeling/15-dimensions.html#mathematics-of-quantity",
    "title": "15  Dimensions and units",
    "section": "",
    "text": "[1 yard] = L\n[1000 kg] = M\n[3 years] = T\n[10 \\(\\mu\\) (microns)] = L.\n\n\n\nApplication area 15.1 —Electrical current is another fundamental dimension.\n\n\n\n\n\n\n\n{Application area 15.1 Electro-magnetism}\n\n\n\nWe will not have many examples involving electro-magnetism. But for those with an interest, note that electrical current is a fundamental dimension often denoted I. Combine I with other fundamental dimensions to produce the dimension of other quantities, for instance:\n\nVoltage: M L2 T-3 I-1\nMagnetic field; M T^{-2} I",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dimensions and units</span>"
    ]
  },
  {
    "objectID": "Modeling/15-dimensions.html#sec-compound-dimensions",
    "href": "Modeling/15-dimensions.html#sec-compound-dimensions",
    "title": "15  Dimensions and units",
    "section": "15.2 Compound dimensions",
    "text": "15.2 Compound dimensions\nThere are other dimensions: volume, force, pressure, energy, torque, velocity, acceleration, and such. These are called compound dimensions because we represent them as combinations of the fundamental dimensions, L, T, and M. The notation for these combinations involves multiplication and division. For instance:\n\nVolume is L \\(\\times\\) L \\(\\times\\) L \\(=\\) L\\(^3\\), as in “cubic centimeters”\nVelocity is L/T, as in “miles per hour”\nForce is M L/T\\(^2\\), which is obscure unless you remember Newton’s Second Law that \\(\\text{F} = \\text{m}\\,\\text{a}\\): “force equals mass times acceleration.” In the notation of dimension, mass is M, acceleration is L/T\\(^2\\). Multiply the two together and you get the dimension “force.”\n\nMultiplication and division are used to construct a compound dimension from the fundamental dimensions L, T, and M.\nAddition and subtraction are never used to form a compound dimension.\nMuch of the work in understanding dimensions involves overcoming the looseness of everyday speech. Remember the weight scale graduated in pounds and kilograms. The unit kilograms is a way of measuring M, but the unit of pounds is a way of measuring force: M L/T\\(^2\\).\nWeight is not the same as mass. This makes no sense to most people and does not matter in everyday life. It is only when you venture off the surface of the Earth that the difference shows up. The Mars rover Perseverance weighs 1000 kg on Earth. It was weightless for most of its journey to Mars. After landing on Mars, Perseverence weighed just 380 kg. But the rover’s mass didn’t change at all.\nAnother source of confusion carried over from everyday life is that sometimes we measure the same quantity using different dimensions. You can measure a volume by weighing water; a gallon of water weighs 8 pounds; a liter of water has a mass of 1 kg. Serious bakers measure flour by weight; a casual baker uses a measuring cup. We can measure water volume with length because water has a (more-or-less) constant mass density. But 8 pounds of gasoline is considerably more than a gallon. It turns out that the density of flour varies substantially depending on how it is packed, humidity, etc. This is why it matters whether you weigh flour for baking or measure it by volume. You can measure time by the swing of a pendulum. To measure the same time successfully with different pendula they need to have the same length, not the same mass.\nA unit is a conventional amount of a quantity of a given dimension. All lengths are the same dimensionally, but they can be measured with different conventions: inches, yards, meters, … Units for the same dimension can all be converted unambiguously one into the other. A meter is the same quantity of length as 39.3701 inches, a mile is the same length as 1609.34 meters. Liters and gallons are both units of volume (L\\(^3\\)): a gallon is the same as 3.78541 liters.\nYou will hear it said that a kilogram is 2.2 pounds. That is not strictly true. A kilogram has dimension M and a pound has dimension ML/T\\(^2\\). Quantities with different dimensions cannot be “equal” or even legitimately compared to one another. Unless you bring something else into the game that physically changes the situation, for instance, gravity (dimension of acceleration due to gravity (dimension \\(\\text{L}/\\text{T}^2\\)). The weight of a kilogram on the surface of the Earth is 2.2 pounds because gravitational acceleration is (almost) the same everywhere on the surface of the Earth.\nIt is also potentially confusing that sometimes different dimensions are used to get at the same idea. For instance, the same car that gets 35 miles / gallon in the US (dimension \\(\\text{L}/\\text{L}^3 = 1/\\text{L}^2\\)) will use 6.7 liters per 100 kilometers (\\(\\text{L}^3 / L = \\text{L}^2\\)) in Europe. Same car. Same fuel. Different conventions using different dimensions.\nKeeping track of the various compound dimensions can be tricky. For many people, it is easier to keep track of the physical relationships involved and use that knowledge to put together the dimensions appropriately. Often, the relationship can be described using specific calculus operations, so knowing dimensions and units helps you use calculus successfully.\nEasy compound dimensions that you likely already know:\n\n[Area] \\(= \\text{L}^2\\). Some corresponding units to remind you: “square feet”, “square miles”, “square centimeters.”\n[Volume] \\(= \\text{L}^3\\). Units to remind you: “cubic centimeters”, “cubic feet”, “cubic yards.” (What landscapers informally call a “yard,” for instance “10 yards of topsoil” should properly be called “10 cubic-yards of topsoil.”)\n[Velocity] \\(= \\text{L}/\\text{T}\\). Units: “miles per hour,” “inches per second.”\n[Momentum] \\(= \\text{M}\\text{L}/\\text{T}\\). Units: “kilogram meters per second.”\n\nAnticipating that you will return to this section for reference, we’ve also added some dimensions that can be understood through the relevant calculus operations.\n\n[Acceleration] \\(= \\text{L}/\\text{T}^2\\). Units: “meters per second squared,” In calculus, acceleration is the derivative of velocity with respect to time, or, equivalently, the 2nd derivative of position with respect to time.\n[Force] \\(=  \\text{M}\\, \\text{L}/\\text{T}^2\\) In calculus: force is the derivative of momentum with respect to time.\n[Energy] or [Work] \\(=   \\text{M}\\, \\text{L}^2/\\text{T}^2\\) In calculus, energy is the integral of force with respect to length.\n[Power] \\(=  \\text{M}\\, \\text{L}^2/\\text{T}^3\\) In the language of calculus, power is the derivative of energy with respect to time.\n\n\nApplication area 15.2 —With respect to what? The several different meanings of density.\n\n\n\n\n\n\n\nApplication area 15.2 Density\n\n\n\nDensity sounds like a specific concept, but there are many different kinds of densities. These have in common that they are a ratio of a physical amount to a geometric extent:\n\na physical amount: which might be mass, charge, people, etc.\na geometric extent: which might be length, area, or volume.\n\nSome examples:\n\n“paper weight” is the mass per area, typically grams-per-square-meter\n“charge density” is the amount of electrical charge, usually per area or volume\n“lineal density of red blood cells” is the number of cells in a capillary divided by the length of the capillary. (Capillaries are narrow. Red blood cells go through one after the other.)\n“population density” is people per area of ground.\n\n\n\n\nApplication area 15.3 —The units for people.\n\n\n\n\n\n\n\nApplication area 15.3: A person as a unit\n\n\n\nThe theory of dimensions and units was developed for the physical sciences. Consequently, the fundamental dimensions are those of physics: length, mass, time, electrical current, and luminous intensity.\nSince proper use of units is important even outside the physical sciences, it is helpful to recognize the dimension of several other kinds of quantity.\n\n“people” / “passengers” / “customers” / “patients” / “cases” / “passenger deaths”: these are different different ways to refer to people. we will consider such quantities to have dimension P, for population.\n“money”: Units are dollars (in many varieties: US, Canadian, Australian, New Zealand), euros, yuan (synonym: renminbi), yen, pounds (many varieties: UK, Egypt, Syria, Lebanon, Sudan, and South Sudan), pesos (many varieties), dinar, franc (Swiss, CFA), rand, riyal, rupee, won, and many others. Conversion rates depend on the situation and national policy, but we will consider money a dimension, denoted by S (from the name of the first coinage, the Mesopotamian Shekel).\n\nExamples:\n\nPassenger-miles is a standard unit of transport.\nPassenger-miles-per-dollar is an appropriate unit of the economic efficiency of transport.\nPassenger-deaths per million passenger-mile is one way to describe the risk of transport.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dimensions and units</span>"
    ]
  },
  {
    "objectID": "Modeling/15-dimensions.html#arithmetic-with-dimensions",
    "href": "Modeling/15-dimensions.html#arithmetic-with-dimensions",
    "title": "15  Dimensions and units",
    "section": "15.3 Arithmetic with dimensions",
    "text": "15.3 Arithmetic with dimensions\nRecall the rules for arithmetic dimensioned quantities. We restate them briefly with the square-bracket notation for “the dimension of.” For instance, “the dimension of \\(b\\)” is written \\([b]\\). We also write \\([1]\\) to stand for the dimension of a pure number, that is, a quantity without dimension.\n\n\n\nTable 15.1: Conditions under which functions can be applied to dimensionful quantitities. Note that \\([a] = [b]\\) means that the dimension of \\(a\\) and of \\(b\\) are the same. For instance, even though 1 mm and 500 miles are very different distances, [1 mm]\\(=\\)[500 miles]. Both [1 mm] and [500 miles] are dimension L. {#tbl-allowed-operations}\n\n\n\n\n\n\n\n\n\n\n\nOperation\nResult\nOnly if satisfies\nMetaphor\n\n\n\n\nMultiplication\n\\([a \\times b] = [a]  \\times [b]\\)\nanything goes\npromiscuous\n\n\nDivision\n\\([a \\div b] = [a] \\div  [b]\\)\nanything goes\npromiscuous\n\n\nAddition\n\\([a + b] = [a]\\)\n\\([a] = [b]\\)\nmonogomous\n\n\nSubtraction\n\\([a - b] = [a]\\)\n\\([a] = [b]\\)\nmonogomous\n\n\nTrigonometric\n\\([\\sin(a)] = [1]\\)\n\\([a] = [1]\\)\ncelibate\n\n\nExponential\n\\([e^a] = [1]\\)\n\\([a] = [1]\\) (of course, \\([e] = [1]\\))\ncelibate\n\n\nPower-law\n\\([b  ^  a] = \\underbrace{[b]\\times[b]\\times ...\\times [b]}_{a\\ \\ \\text{times}}\\)\n\\([a]  = [1]\\) with \\(a\\) an integer\nexponent celibate\n\n\nSquare root\n\\([\\sqrt{b}] = [c]\\)\n\\([b] = [c\\times c]\\)\nidiosyncratic\n\n\nCube root\n\\([\\sqrt[3]{b}] = [c]\\)\n\\([b] = [c \\times c \\times  c]\\)\nidiosyncratic\n\n\nBump\n\\([\\text{bump}(a)] = [1]\\)\n\\([a] =  [1]\\)\ncelibate\n\n\nSigmoid\n\\([\\text{sigmoid}(a)] =  [1]\\)\n\\([a] = [1]\\)\ncelibate",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dimensions and units</span>"
    ]
  },
  {
    "objectID": "Modeling/15-dimensions.html#dimensionless-numbers",
    "href": "Modeling/15-dimensions.html#dimensionless-numbers",
    "title": "15  Dimensions and units",
    "section": "15.4 Dimensionless numbers",
    "text": "15.4 Dimensionless numbers\nIn many STEM fields, worker use special, usually named quantities that are dimensionless. Being dimensionless is different from having no units. For instance, an angle is a dimensionless quantity (arc length divided by radius length) but is stated in units. For instance, 90 degrees is the same as \\(\\pi\\) radians. (More obscurely, some fields use “grads.” A grad is one-hundredth of a right angle, that is, a little less than a degree.) Many people have heard of the “mach” and that 1 mach corresponds to the speed of sound. But speed has dimension—L/T, as in meters per second or miles per hour. And mach is a dimensionless quantity. How is mach constructed to be dimensionless and yet still be about speed?\nMach is actually a ratio of speeds, the speed of an object such as an airplane divided by the speed of sound. In the division, the dimensions of the two quantities cancel out to produce a quantity of dimension L/T / (L/T) = [1]. Dimensionless. Mach is useful in aerodynamics because the speed of an object is an important determiner of the pattern of flow through or around the object. It is simpler to describe the pattern of flow using rhw object’s speed relative to the speed of sound in the medium. This is why words like “supersonic” and “subsonic” (and even “transonic” and “hypersonic”) are often heard when talking about aircraft, missles, and meteoriods.\nOnly rarely do science workers invent a useful new dimensionless quantity, but this does sometimes happen and we all have a shared historical legacy of such inventions to build on. My goal here is to relate the motivation for dimensionless quantities.\nRecall from Chapter 8 the framework we use for turning pattern-book functions into models of the real world. All the pattern-book functions take a pure number (dimensionless) as input and produce a pure number (dimensionless) as output. We used input scaling to translate between a dimensionful quantity (e.g. time, speed, density, and so on) and the dimensionless input needed for a pattern-book function. Similarly, we used output scaling to translate between the dimensionless output of a pattern-book function and the dimensional quantity that we want to model with the function. This is an important technique that you will use often.\nBut consider an alternative framework for doing things. You will not need this except in this demonstration, but it does serve to motivate why dimensionless quantities are so important. In this alternative approach, we stipulate that modeling functions will always take dimensionless inputs and produce a dimensionless output. Such a modeling function, let’s call our example \\(g()\\) will do any input scaling internally, perhaps by parameters hidden to the user. To create a dimensionful output, we’ll continue to use output scaling. So, a model constructed with a generic function of this sort will look like \\(A g(x, y, z, ...) + B\\). The output scaling parameters will always have the same dimensions and units as the quantity we want to model as the output. But we insist—within this alternative framework—that all inputs, \\(x, y, z, ...\\) be dimensionless.\nFor the sake of demonstration, let’s build a model in this framework that describes the motion of a pendulum. The output variable will be the period of the pendulum, that is, the time to complete a full cycle or, more realistically for a pendulum, to go through a complete back-and-forth motion (the amplitude of which is often decreasing in the familiar way as time goes by). Because of this choice, the period is one of the dimensionful quantities that are involved in the model.\n\nperiod \\(p\\), dimension T, units might be seconds.\n\nBut there are other dimensionful quantities that characterize a pendulum. Let’s list the main ones:\n\nlength of the rod, \\(r\\), with dimension L, units might be meters\nmass of the bob at the end of the rod, \\(m\\), units might be kg\nthe force of gravity on the bob, \\(m g\\), with units M L T-2. The quantity \\(g\\) is called the “acceleration due to gravity.” On Earth, the value of this is roughly 9.8 m/s2 (meters per second-squared). The force, as Newton defined, is mass times acceleration.\nthe angle the pendulum swings through, \\(\\theta\\).\n\nAs for (v), Galileo famously established, by measuring with his pulse the period of several swinging candelabras in Pisa Cathedral, some of which had a wide swing and others with a narrow swing, that \\(\\theta\\) doesn’t enter into things. And, naturally, there are undoubtedly other quantities that influence the period of the pendulum such as air resistence, the Earth’s spin, and so on. For simplicity, we will ignore these as we start to build our model. Then, once we have a model to work with, we can use the techniques in Chapter 16 to see if the discrepancies between our model results and observations in the real world are so substantial that we need to take them into account. If not, the simple model will do.\nIn the input-scaling framework from Chapter 8, we would be tempted to write down the name and arguments of the model function, choose an appropriate pattern-book or composed function and then think about input scaling. That is, the model function of period might be defined as \\[period(r, m, m g) \\equiv \\text{some function of } r, m, \\text{and } g.\\] But right now we are using the pure-number input framework: \\[A\\ g(x, y, z) + B\\] where \\(x\\), \\(y\\) and \\(z\\) are dimensionless quantities.\nThe challenge is to figure out how to construct the dimensionless inputs (\\(x, y, z\\)) out of the dimensionful quantities that we can measure from a real-world pendulum on Earth.\nThe main way to create something of a new dimension is to multiply (or divide) things with the old dimensions. The things we are working with are\n\n\n\nquantities:\n\\(\\text{period}\\)\n\\(r\\)\n\\(m\\)\n\\(m \\ g\\)\n\n\n\n\ndimension:\nT\nL\nM\nM L T-2\n\n\n\nLet’s get to work. To start, we’ll see what happens when we divide period by the rod length. The dimension of this will be T / L. That’s not dimensionless! Neither would be period times rod length or period times mass or period times gravitational force.\nPerhaps there is no combination of the four quantities that will come out as dimensionless. Surprisingly, if this were the case it would be a hint that we have left an important quantity out of our model, or that we have included a quantity that doesn’t contribute. (This is an amazing power of dimensional analysis, to be able to check whether the set of input quantities we have selected is self-consistent!)\nSystematic search reveals that there is a dimensionless quantity to be made by combining period, rod length, mass of the bob, and gravitational force \\(m g\\). This is\nperiod2 \\(\\times\\) gravitational force / (rod length \\(\\times\\) mass)\nWritten out using fundamental dimensions, this is\n(T2 \\(\\times\\) M L / T2 ) / (L \\(\\times\\) M) , or, rearranging, T2/T^2 \\(\\times\\) M / M $L / L. That is, [1]: dimensionless.\nThis tells us that there can be only one dimensionless input to \\(g()\\). This dimensionless input must be\n\\[\\frac{\\text{period}^2 m\\ g}{m\\ r}\\ \\text{where } \\left[\\frac{\\text{period}^2 m\\ g}{m\\ r}\\right] = [1].\\]\nSince we create new dimensions by multiplying and dividing old ones, it’s the case that\n\\(\\frac{\\left[\\text{period}\\right]^2 [m]\\ [g]}{[m]\\ [r]} = [1].\\)$\nRe-arranging in the usual algebraic way, we have\n\\[\\left[\\text{period}\\right]^2 = \\frac{[r]}{[g]}\\ \\ \\ \\text{or }\\ \\  \\left[period\\right] = \\sqrt{\\frac{[r]}{[g]}}.\\] Now that we know that only the single quantity \\(\\sqrt{[r] / [g]}\\) is involved in making the period, it’s time to abandon the methodological scaffolding we set up for this model. We’ll say simply, “period must be a function of \\(\\sqrt{\\frac{\\strut r}{g}}\\),” a formula you can find in many physics books. Remarkably, according to our dimensional analysis, the mass of the pendulum bob doesn’t make a difference to the period of the pendulum.\nWhat is the function that takes \\(\\sqrt{[r] / [g]}\\) as input and produces period as the output. We don’t know yet. There’s more work to do, along the lines of Chapter 7. Being on Earth, we can’t readily change the value of gravitational acceleration \\(g\\), but we can change the rod length easily. Make some measurements of the period for different rod lengths, plot the period versus \\(\\sqrt{r/g}\\), and we’ll know what the function looks like. SPOILER ALERT: It turns out to be well approximated as a straight line, at least when our measurements are made with pendula swinging through a small angle.\nThere are dozens of named dimensionless quantities, usually called “numbers.” For instance, the Archimedes number, the Biot number, the Bond number. People who work with water, waves, and air—for instance, geophysicists, meteorologists, and people who design ships and airplanes—frequenly run into the Rossby number, the Reynolds number, and the Froude number.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dimensions and units</span>"
    ]
  },
  {
    "objectID": "Modeling/15-dimensions.html#conversion-flavors-of-1",
    "href": "Modeling/15-dimensions.html#conversion-flavors-of-1",
    "title": "15  Dimensions and units",
    "section": "15.5 Conversion: Flavors of 1",
    "text": "15.5 Conversion: Flavors of 1\nNumbers are dimensionless but not necessarily unitless. Failure to accept this distinction is one of the prime reasons people have trouble figuring out how to convert from one unit to another.\nThe number one is a favorite of elementary school students because its multiplication and division tables are completely simple. Anything times one, or anything divided by one, is simply that thing. Addition and subtraction are pretty simple, too, a matter of counting up or down.\nWhen it comes to quantities, there is not just one one but many. And often they look nothing like the numeral 1. Some examples of 1 as a quantity:\n\n\\(\\frac{180}{\\pi} \\frac{\\text{degrees}}{\\text{radians}}\\)\n\\(0.621371 \\frac{\\text{mile}}{\\text{kilometer}}\\)\n\\(3.78541 \\frac{\\text{liter}}{\\text{gallon}}\\)\n\\(\\frac{9}{5} \\frac{^\\circ F}{^\\circ C}\\)\n\\(\\frac{1}{12} \\frac{\\text{dozen}}{\\text{item}}\\)\n\nI like to call these and others different flavors of one.\nIn every one of the above examples, the dimension of the numerator matches the dimension of the denominator. The same is true when comparing feet and meters ([feet / meter] is L/L = [1]), or comparing cups and pints ([cups / pint] is \\(\\text{L}^3/\\text{L}^3 = [1]\\)) or comparing miles per hour and feet per second ([miles/hour / ft per sec] = L/T / L/T = [1]). Each of these quantities has units but it has no dimension.\nIt is helpful to think about conversion between units as a matter of multiplying by the appropriate flavor of 1. Such conversion will not change the dimension of the quantity but will render it in new units.\n\n\n\n\n\n\nImportant 15.1: fps to mph\n\n\n\nExample: Convert 100 feet-per-second into miles-per-hour. First, write the quantity to be converted as a fraction and alongside it, write the desired units after the conversion. In this case that will be \\[100 \\frac{\\text{feet}}{\\text{second}} \\ \\ \\ \\text{into} \\ \\ \\ \\frac{\\text{miles}}{\\text{hour}}\\]\nFirst, we will change feet into miles. This can be accomplished by multiplying by the flavor of one that has units miles-per-foot. Second, we will change seconds into hours. Again, a flavor of 1 is involved.\nWhat number will give a flavor of one? One mile is 5280 feet, so \\[\\frac{1}{5280} \\frac{\\text{miles}}{\\text{foot}}\\] is a flavor of one.\nNext, we need a flavor of one that will turn \\(\\frac{1}{\\text{second}}\\) into \\(\\frac{\\text{1}}{\\text{hour}}\\). We can make use of a minute being 60 seconds, and an hour being 60 minutes. \\[\\underbrace{\\frac{60\\  \\text{s}}{\\text{minute}}}_\\text{flavor of 1}\\  \\underbrace{\\frac{60\\ \\text{minutes}}{\\text{hour}}}_\\text{flavor of 1} = \\underbrace{3600\\frac{\\text{s}}{ \\text{hour}}}_\\text{flavor of 1}\\]\nMultiplying our carefully selected flavors of one by the initial quantity, we get \\[\n\\underbrace{\\frac{1}{5280} \\frac{\\text{mile}}{\\text{foot}}}_\\text{flavor of 1} \\times \\underbrace{3600 \\frac{\\text{s}}{\\text{hour}}}_\\text{flavor of 1} \\times \\underbrace{100 \\frac{\\text{feet}}{\\text{s}}}_\\text{original quantity} = 100 \\frac{3600}{5280} \\frac{\\text{miles}}{\\text{hour}} = 68.18 \\frac{\\text{miles}}{\\text{hour}}\\]",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dimensions and units</span>"
    ]
  },
  {
    "objectID": "Modeling/15-dimensions.html#dimensions-and-linear-combinations",
    "href": "Modeling/15-dimensions.html#dimensions-and-linear-combinations",
    "title": "15  Dimensions and units",
    "section": "15.6 Dimensions and linear combinations",
    "text": "15.6 Dimensions and linear combinations\nLow-order polynomials are a useful way of constructing model functions. For instance, suppose we want a model of the yield of corn in a field per inch of rain over the growing season, will call it corn(rain). The output will have units of bushels (of corn). The input will have units of inches (of rain). A second-order polynomial will be appropriate for reasons to be discussed in Chapter 24.\n\\[\\text{corn(rain)} \\equiv a_0 + a_1\\, \\text{rain} + \\frac{1}{2} a_2\\, \\text{rain}^2\\] Of course, the addition in the linear combination will only make sense if all three terms \\(a_0\\), \\(a_1\\,\\text{rain}\\), and \\(\\frac{1}{2}\\, a_2\\, \\text{rain}^2/2\\) have the same dimension. But \\([\\text{rain}] \\neq [\\text{rain}^2]\\). In order for things to work out, the coefficients must themselves have dimension. We know the output of the function will have dimension \\([\\text{volume}] = \\text{L}^3\\). Thus, \\([a_0] = \\text{L}^3\\).\n\\([a_1]\\) must be different, because it has to combine with the \\([\\text{rain}] = \\text{L}\\) and produce \\(\\text{L}^3\\). Thus, \\([a_1] = \\text{L}^2\\).\nFinally, \\([a_2] = \\text{L}\\). Multiplying that by \\([\\text{rain}]^2\\) will give the required \\(\\text{L}^3\\)\n\n\n\n\n\n\n\nThink in degrees, compute in radians\n\n\n\nIn everyday communication as well as in most domains such as construction, geography, navigation, and astronomy we measure angles in degrees. 90 degrees is a right angle. But in mathematics, the unit of angle is radians where a right angle is 1.5708 radians. (1.5708 is the decimal version of \\(\\pi/2\\).) The conversion function, which we will call raddeg(), is \\[\\text{raddeg}(r) \\equiv \\frac{180}{\\pi} r\\] The function that converts degrees to radians, which we will call degrad() is very similar: \\[\\text{degrad}(d) \\equiv \\frac{\\pi}{180} d\\] (Incidentally, \\(\\frac{180}{\\pi} = 57.296\\) while \\(\\frac{\\pi}{180} = 0.017453\\).)\nIn traditional notation, the trigonometric functions such as \\(\\sin()\\) and \\(\\tan()\\) can be written with an argument either in degrees or radians. For instance, \\(\\sin(90^\\circ) = \\sin\\left(\\frac{\\pi}{2}\\right)\\). Similarly, for the inverse functions like \\(\\arccos()\\) the units of the output are not specified. This works because there is always a human to intervene between the written expression and the eventual computation.\nIn R, as in many other computer languages, there an expression like sin(90 deg) generates an error. In these languages, 90 deg is not a valid expression (although it might be good if it were valid!). In these and many other languages, angles are always given in radians. Such consistency is admirable, but people are not always so consistent. It is a common source of computer bugs that angles in degrees are handed off to functions like \\(\\sin()\\) and that the output of \\(\\arccos()\\) is (wrongly) interpreted as degrees rather than radians.\nFunction composition to the rescue!\nConsider this function given in the Wikipedia article on the position of the sun as seen from Earth.1 \\[\\delta_\\odot(n) \\equiv - 23.44^\\circ \\cdot \\cos \\left [ \\frac{360^\\circ}{365\\, \\text{days}} \\cdot \\left ( n + 10 \\right ) \\right ]\\] Where \\(n\\) is zero at the midnight marking New Years and increases by 1 per day. (The \\(n+10\\) has units of days and translates New Years back 10 days, to the day of the winter solstice.) \\(\\delta_\\odot()\\) gives the declination of the sun: the latitude pieced by an imagined line connecting the centers of the earth and the sun.\nThe Wikipedia formula is well written in that it uses some familiar numbers to help the reader see where the formula comes from. 365 is recognizably the length of the year in days. \\(360^\\circ\\) is the angle traversed when making a full cycle around a circle. \\(23.44^\\circ\\) is less familiar, but the student of geography might recognize it as the latitude of the Tropic of Cancer, the latitude farthest north where the sun is directly overhead at noon (on the day of the summer solstice).\nBut there is a world of trouble for the programmer who implements the formula as\n\ndec_sun &lt;- makeFun(-23.44 * cos((360/365)*(n+10)) ~ n)\n\nFor instance, the equinoxes are around March 21 (n=81) and Sept 21 (n=264). On an equinox, the declination of the sun is zero degrees. But let’s plug \\(n=81\\) and \\(n=264\\) into the formula and see what we get.\n\ndec_sun(81)\n## [1] 5.070321\ndec_sun(264)\n## [1] -23.38324\n\nThe equinoxes aren’t even equal! And they are not close to zero. Does this mean astronomy is wrong?\nThe Wikipedia formula should have been programmed this way, using 2 \\(\\pi\\) radians instead of 360 degrees in the argument to the cosine function:\n\ndec_sun_right &lt;- \n  makeFun(-23.44 * cos(( 2*pi/365)*(n+10)) ~ n)\ndec_sun_right(81)\n## [1] -0.1008749\ndec_sun_right(264)\n## [1] -0.1008749\n\nThe deviation of one-tenth of a degree reflects rounding off the time of the equinox to the nearest day.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dimensions and units</span>"
    ]
  },
  {
    "objectID": "Modeling/15-dimensions.html#footnotes",
    "href": "Modeling/15-dimensions.html#footnotes",
    "title": "15  Dimensions and units",
    "section": "",
    "text": "Article accessed on May 30, 2021↩︎",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dimensions and units</span>"
    ]
  },
  {
    "objectID": "Modeling/16-modeling-scientific-method.html",
    "href": "Modeling/16-modeling-scientific-method.html",
    "title": "16  Modeling and the scientific method",
    "section": "",
    "text": "16.1 Example: Cooling water\nBlock 2 is entitled “Modeling,” and for good reason. This Block has introduced techniques and concepts that provide essential tools for constructing models but do not require an understanding of the operations of Calculus. In the following Blocks, we will study additional techniques and concepts. That study requires us to do two things at once: introduce the concepts and methods of Calculus and show examples of how they apply to modeling.\nBefore continuing with more mathematics, it is helpful to put modeling in context. That context doesn’t come from the mathematics. Instead, it comes from science and, particularly, the scientific method.\nOne description of the scientific method, arising in the mid-20th century, is the hypothetico-deductive model. The process embedded in the hypothetical-deductive model consists of formulating a hypothesis to describe the world, deducing consequences from this hypothesis, and carrying out experiments to look for those consequences. If the consequences are experimentally observed, the experiment corroborates the hypothesis. If not, the experiment refutes the hypothesis. In the progess of science, refuted hypotheses are replaced with alternatives that are compatible with the assembled experimental data. And the cycle of experiment, corroboration or refutation, and hypothesis generation begins again.\nA good analogy for the scientific method comes from detective books and movies. The heroes of the genre—Sherlock Holmes and Hercule Poirot are famously such—are playing the role of the modeler. The detectives’ methods include fingerprints, chemical detection of traces of poison, clever interrogation of suspects and witnesses, etc. These are analogous to the various methods we have and will be studying in this Book. In each detective story, the detective has a goal, often set by the client and identified at the beginning. Goals include identifying the criminal, forestalling a crime, recovering an object of value, and tracking down a missing person. Similarly, good modelers also know what is the objective of their work. Each individual case will have its distinct objective, but in general the objective is to help a person or an organization make a decision: what should be the parameters of a new design, how best to set the operating levels in production, what will be likely outcomes of a new policy, among others.\nOnce the objective has been identified, the scientific method is put to work. The detective collects whatever relevant information is readily available. Based on this information, the detective concocts one or more theories to make sense of the information. Such theories motivate the search for new information. Based on that information or a new development in the case, the theories are refined, discarded, or replaced. The new theories suggest what new data should be collected and how old information should be re-interpreted. This cycle continues. Theories are put to the test.\nEvery detective has access to a rich body of knowledge that provides analogies to the case at hand. In detective fiction, that knowledge often comes from experience with previous crimes, ploys and scams used by criminals, wills revised under duress, heiresses who lose their memory, and so on. The detective puts together a more or less plausible theory from these components.\nSimilarly, in large part, the ability to model comes from knowing about the kinds of techniques that other modelers have found useful. You have met some of these already: functions, low-order polynomials, quantities and their dimensions, ways for dealing with a wide range of magnitudes, setting parameters, etc. In the upcoming Blocks, you’ll learn new techniques that involve understanding relationships between information that comes in different forms. You’ll also learn important ways to extract conclusions from the models you construct.\nThis chapter shows, by example, some widely used modeling techniques. It also shows some automated methods, e.g., model polishing, that can improve a model. And, it’s important to consider potential pitfalls and commonly held modeling misconceptions that mislead modelers down perilous and pointless paths.\nChapter 11 presented data on water cooling from near boiling to room temperature. (See Figure 11.2.) Prof. Stan Wagon of Macalester College collected the data with a purpose in mind: to see whether Newton’s Law of Cooling provides a good description of the actual physical process. In particular, Wagon knew that a simple application of Newton’s Law implies that the water temperature will decay exponentially.\nFigure 16.1 shows an exponential function fitted to the water-cooling data.\nAt first glance, the Model\\(_1()\\) seems a good match to the data. But what do we mean by “good?” Is “good” good enough? And “good enough” for what? The experienced modeler should always have in mind the criterion by which to evaluate “good enough.” These criteria, in turn, are shaped by the purpose of building the model.\nBeware of general-purpose criteria. For instance, a general-purpose criterion is to compare the shape of the function with the data. The exponential model slopes and curves in a similar manner to the data. The deviation of the model output from the data is, at worst, just a few degrees. This is small compared to the 50\\(^\\circ\\)C change in temperature throughout the experiment.\nAnother widely used general-purpose statistical measure of the quality of the match is the R2 (R-squared) value. Mathematically, R2 is always between 0 and 1. (See Chapter 33, where the mathematics of R2 is introduced.) When R2 = 1, the match is perfect. For Model\\(_1()\\) and the cooling water data, R2 = 0.991. Many scientists would interpret this as “almost perfect,” although things are sometimes more complicated … as here.\nWagon’s purpose was not simply to see if an exponential curve resembles the data. Instead, he wanted to know if Newton’s Law of Cooling was consistent with the observed cooling over time. The exponential shape of temperature versus time is just one consequence of Newton’s Law. Another is that the water temperature should eventually reach equilibrium at room temperature.\nThe fitted exponential model fails here, as you can see by zooming in on the right tail of Figure 16.1. In Figure 16.2, we zoom in on the data’s left and right tails.\nIn the scientific method, one takes a theory (Newton’s Law of Cooling), makes predictions from that theory, and compares the predictions to observed data. If the predictions do not match the theory—here, that the water should cool to room temperature—then a creative process is called for, replacing or modifying the theory. The same applies when a mathematical model fails to suit its original purpose.\nThe creative process of constructing or modifying a theory is not primarily a matter of mathematics, it usually involves expert knowledge about the system being modeled. In the case of a cooling mug of water, the “expertise” can be drawn from everyday experience. For instance, everybody knows that it is not entirely a matter of the water cooling off; the mug gets hotter and then cools in its own fashion to room temperature.\nWagon’s initial modification of the theory went like this: Newton’s Law still applies, but with the water in contact with both the room and, more strongly, the mug. To build this model, he needed to draw on the mathematics of dynamical systems (to be introduced in Block 5), producing a new parameterized formula of water temperature versus time.\nWagon went on to check the water-mug-room theory against the data. He found that the improved model did not completely capture the temperature-versus-time data. He applied more expertise, that is, he considered something we all observe: water vapor (“steam”) rises from the water in the mug. This prompted some experimental work where a drop of oil was placed on the water surface to block the vapor. This experiment convinced him that a new revision to the model was called for that would include the cooling due to evaporation.\nWithout being able to anticipate the settings and purposes for your model building, we can’t pretend to teach you the real-world expertise you will need. But we can do something to help you move forward in your work. The remaining sections of this chapter introduce some general-purpose mathematical approaches to refining models. For instance, rather than using a single exponential function, Wagon used a linear combination of exponentials for his modeling. We will also try to warn you of potential pitfalls and ways you can mislead yourself.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling and the scientific method</span>"
    ]
  },
  {
    "objectID": "Modeling/16-modeling-scientific-method.html#example-cooling-water",
    "href": "Modeling/16-modeling-scientific-method.html#example-cooling-water",
    "title": "16  Modeling and the scientific method",
    "section": "",
    "text": "Mod1_fun &lt;- mosaic::fitModel(temp ~ A*exp(-k*time) + C, \n                             data = CoolingWater,\n                             start = list(C = 30, k = 1/20, A = 70))\ngf_point(temp ~ time, data = CoolingWater, alpha = .15 ) |&gt;\n  slice_plot(Mod1_fun(time) ~ time, color = \"blue\") |&gt;\n  gf_labs(x = \"Time (minutes)\", y=\"Temperature (deg. C)\")\n\n\n\n\n\n\n\n\n\n\nFigure 16.1: An exponential model (blue) fitted to the CoolingWater data. \\[\\text{Model}_1 \\equiv T(t) \\equiv 27.0 + 62.1\\ e^{-0.021 t}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEarly in the experiment.\n\n\n\n\n\n\n\n\n\nLate in the experiment\n\n\n\n\n\n\n\nFigure 16.2: The exponential model does not capture the water’s initial almost-boiling temperature. To judge from the \\(A\\) coefficient presented in the caption of Figure 16.1, the room temperature is 27\\(^\\circ\\)C, while the data themselves indicate a room temperature a little less than 25\\(^\\circ\\)C.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling and the scientific method</span>"
    ]
  },
  {
    "objectID": "Modeling/16-modeling-scientific-method.html#identifying-systematic-discrepancies",
    "href": "Modeling/16-modeling-scientific-method.html#identifying-systematic-discrepancies",
    "title": "16  Modeling and the scientific method",
    "section": "16.2 Identifying systematic discrepancies",
    "text": "16.2 Identifying systematic discrepancies\nRefer to Figure 16.1, which shows the measured temperature data along with the initial fitted exponential model. The data systematically differ from the model in four places: the model underestimates the temperature early in the process, then overestimates for an interval, returns later to an underestimation, and finishes up with an overestimation.\nWhen assessing a model, it is a good practice to calculate the difference between the observed data and the model value. Figure 16.3 shows this difference, calculated simply by subtracting from each data point the model value at the corresponding time. In statistical nomenclature, such differences are called the “residuals” from the model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.3: Residuals versus time for Model\\(_1()\\). The comparatively smooth structure indicates systematic deviations between the model and the data.\n\n\n\nThe presence of noise in any measurement is to be expected. You can see such noise in Figure 16.3 in the small, irregular, trembling fluctuations. Conversely, any smooth, slowly changing pattern in the residuals is considered systematic variation. Here, those smooth variations are much larger in amplitude than the irregular noise, indicating that we should investigate the systematic variation more closely.\nThis is a chance for the modeler to speculate on what might be causing the systematic deviations. A good place to start is with the largest residuals. In this case, that’s at the beginning of the temperature recording. Note that early in the recording, the recorded temperature falls much faster than in the model. This is perhaps clearer with reference to Figure 16.1.\nWhat physical process might lead to the initial fast cooling of the water? Answering this question requires both detailed knowledge of how the system works and some creativity. As mentioned above, Wagon speculated that the water might be cooling in two ways: i) heat moving from the water to the air in the room and ii) heat moving from the water into the containing mug. This second process can be fast, making it a candidate for explaining the residuals in the early times of the experiment.\nA quick test of the speculation is to construct a linear combination of two exponential processes, one fast and one slower. This would be difficult to do by eye using the techniques of Chapter 8, but fitModel() can accomplish the task so long as we can give a rough estimate of suitable numerical values for the parameters. Figure 16.4 shows the resulting function and the residuals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.4: The water-cooling model with two exponential processes, one fast and one slow, and the residuals from the observed data.\n\n\n\nFigure 16.4 shows a much-improved match between the model and the data. The residuals are about one-tenth as large as those from the original model.\nDepending on the purpose for which the modeling is being done, this might be the end of the story. The model is within a few tenths of a degree C from the data, good enough for purposes such as prediction. For Prof. Wagon, however, the purpose was to investigate how complete an explanation Newton’s Law of Cooling is for the physical process. He concluded that the residuals in Figure 16.4 still show systematic patterns.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling and the scientific method</span>"
    ]
  },
  {
    "objectID": "Modeling/16-modeling-scientific-method.html#refine-the-data-refine-the-experimental-technique",
    "href": "Modeling/16-modeling-scientific-method.html#refine-the-data-refine-the-experimental-technique",
    "title": "16  Modeling and the scientific method",
    "section": "16.3 Refine the data, refine the experimental technique",
    "text": "16.3 Refine the data, refine the experimental technique\nThe initial model building often suggests how new, informative data might be collected. It’s impossible to generalize this to all situations, but in Prof. Wagon’s work, two possibilities arise:\n\nMeasure the temperature of the mug directly, as well as the temperature of the water.\nStifle other possible physical processes, such as evaporation of water from the top surface of the liquid, and collect new data\n\nProf. Wagon managed (2) by putting a small drop of oil on the water right after it was poured into the mug. This created a thin layer that hindered evaporation. He expected that canceling this non-Newtonian process would make the Newton model a better fit to the new data, providing an estimate of the magnitude of the evaporation effect.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling and the scientific method</span>"
    ]
  },
  {
    "objectID": "Modeling/16-modeling-scientific-method.html#including-new-influences",
    "href": "Modeling/16-modeling-scientific-method.html#including-new-influences",
    "title": "16  Modeling and the scientific method",
    "section": "16.4 Including new influences",
    "text": "16.4 Including new influences\nIn many situations, there is data on more than one factor that might come into play in determining the response variable. The mathematical methods relating to linear combinations covered in Block 3 provide the basic toolkit for considering such new influences. In Block 5, you will see many examples of situations where the system’s behavior depends critically on the interaction between two competing factors.\nThe closely related topic of statistical modeling provides essential concepts and tools for deciding whether to incorporate an additional input to a model function. Part of the importance of a statistical approach comes from a mathematical paradox that will be discussed in Block 4. There will be residuals whenever you have a model of a response variable as a function of explanatory variables. Adding in a new explanatory variable, even if it is utterly unrelated to the system, will make the residuals smaller. Statistical method provides the means to avoid mistakenly including the new explanatory variable by comparing the actual reduction in residuals to what would be expected from a generic random variable.\nAnother important statistical topic is causality: reasoning about what causes what. Often, the modeler’s understanding of how the system under study works can guide the choice of additional variables to include in a model.\nSpace precludes detailed consideration of the statistical phenomena in this book, although the techniques used are well within the capabilities of anyone who has completed this book through Block 3.\nIt’s worth point out a regretable fact of life in the academic world covering the quantitative science. At almost all institutions (as of 2024), calculus is taught in a manner that is totally disconnected from statistics and data science. In part, this is because the professional training of mathematics instructors does not include an emphasis on statistics. This is an artifact of history and is slowly being addressed. MOSAIC Calculus is the result of a successful program to integrate modeling, statistics, calculus, and computing—the M S C of MoSaiC. To fit in with the academic structure at most universities, it was necessarily to deliver the integrated material in a form that could be slotted into existing Calculus curricula and, separately, into existing statistics curricula. The statistics part of the integrated curriculum is packaged into another free, online textbook: Lessons in Statistical Thinking.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling and the scientific method</span>"
    ]
  },
  {
    "objectID": "Modeling/16-modeling-scientific-method.html#interpolation-and-extrapolation",
    "href": "Modeling/16-modeling-scientific-method.html#interpolation-and-extrapolation",
    "title": "16  Modeling and the scientific method",
    "section": "16.5 Interpolation and extrapolation",
    "text": "16.5 Interpolation and extrapolation\nModels are most reliable when the domain of the functions is considered to be the span of the data or observations underlying them. Evaluating a function inside the span of the data is called “interpolation.” Conversely, sometimes functions are evaluated at inputs far outside the span of the data used to construct the function. This is “extrapolation.”\nExtrapolation is unreliable. However, sometimes, the purpose of a model is to help us consider how a system might behave when inputs are outside of the span of already-observed data. To satisfy such a purpose, extrapolation is unavoidable.\nStill, it is possible to adopt mathematical methods that mitigate the impact of extrapolation. It’s key, for instance, to avoid high-order polynomials, as these produce some of the most extreme and misleading behavior. (See Chapter 27.) More reliable alternatives include using only low-order polynomials (as already described in Chapter 12), localized functions such as the sigmoid or gaussian, or splines (Chapter 49).\n\n\n\n\n\n\nExample: Extrapolating gravitation\n\n\n\nIsaac Newton famously developed his universal theory of gravitation by examining the orbits of planets. Planets close to the sun feel a strong tug of the Sun’s gravity, and farther-out planets have receive a weaker tug. Similarly, the motion of satellites around the Earth is determined mainly by the Earth’s gravitational pull. Per Newton’s theory, the acceleration due to gravity goes as the inverse-square of distance from the center of the Earth. Far-away satellites have long orbits, close-in satellites orbit with far lower orbital duration.\nPlanets orbit outside the Sun. Satellites orbit outside the Earth. Application of the inverse-square law to forces outside the Sun or outside the Earth are interpolation. But there are no observations of orbits or forces inside the Sun or Earth. So using the inverse-square law for inside forces is an extrapolation. This extrapolation produces catastrophically misleading models. In reality, the gravitational force on the inside—as you might imagine in a tunnel going through the center of the Earth—increases linearly with distance from the center. (Newton’s theory explains the mechanics of this.)",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling and the scientific method</span>"
    ]
  },
  {
    "objectID": "Modeling/16-modeling-scientific-method.html#sec-mechanism-vs-curve",
    "href": "Modeling/16-modeling-scientific-method.html#sec-mechanism-vs-curve",
    "title": "16  Modeling and the scientific method",
    "section": "16.6 Mechanism versus curve",
    "text": "16.6 Mechanism versus curve\nIt is important to distinguish between two basic types of model:\n\nEmpirical models which are rooted in observation and data.\nMechanistic models such as those created by applying fundamental laws of physics, chemistry, etc.\n\nWe will put off mechanistic models for a while, for two reasons. First, the “fundamental laws of physics, chemistry, and such” are often expressed with the concepts and methods of calculus. We are heading there, but at this point you don’t yet know the core concepts and methods of calculus. Second, most students don’t make a careful study of the “fundamental laws of physics, chemistry, and such” until after they have studied calculus. So examples of mechanistic models will be a bit hollow at this point.\nEmpirical models (sometimes deprecatingly called “curve fitting”) are common in many vital areas such as the social sciences and economics, psychology, clinical medicine, etc. A solid understanding of statistics greatly enhances the reliability of conclusions drawn from empirical models.\nIn addition to physical law, geometry provides many examples of mechanistic connections between elements. An example is the geometric calculation of the forward force on an arrow being pulled back by a bow’s draw.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling and the scientific method</span>"
    ]
  },
  {
    "objectID": "Modeling/16-modeling-scientific-method.html#sec-polishing-parameters",
    "href": "Modeling/16-modeling-scientific-method.html#sec-polishing-parameters",
    "title": "16  Modeling and the scientific method",
    "section": "16.7 Polishing parameters",
    "text": "16.7 Polishing parameters\nSection 11.6 refers to a way to improve parameters in order to better match patterns in data: polishing. Polishing can be carried out by software such as fitModel(). It is usually worthwhile simply because it is a low-cost task: it is easy and mostly automatic.\nPolishing becomes harmful when the modeler misinterprets it as increasing a model’s reliability. Polishing can add a digit or two to a model parameter. Still, those digits may be irrelevant from a statistical perspective and do not provide any warranty that the fitted function is an appropriate approximation to the real-world situation.\nAn analogy might help. Polishing your teeth might make you marginally more attractive, and can be part of good hygiene, but it will not make you a better person.\n\nApplication area 16.1 We look at attemps in the 1970s to predict when oil production and consumption will reach its peak.\n\n\n\n\n\n\n\nApplication area 16.1 Peak Oil?\n\n\n\nNowadays, everybody knows about the problems with oil consumption and climate change; bringing world oil consumption down to zero by 2050 is an often-stated goal of countries throughout the world. The basic theory of climate change is that increased CO\\(_2\\)_ from the consumption of fossil fuels causes warming via the greenhouse effect. (There are other greenhouse gasses as well such as methane.) The CO\\(_2\\)_ greenhouse phenomenon was first modeled by Svante Arrhenius in 1896. His model indicated that if atmospheric CO\\(_2\\)_ doubled, global temperature would rise by 5-6\\(^\\circ\\)C. As of today, atmospheric CO\\(_2\\)_ has increased by 50% from historic levels at the start of the industrial revolution. A simple linear interpolation of Arrhenius’ prediction would put global temperature increase at about 2.5\\(^\\circ\\)C. This is eerily close to the present-day target of an increase by 2 degrees.\nDespite Arrhenius’s prescience, widespread understanding of greenhouse gas effects emerged only around 1990. Up until then, the possibility of concern was that the world might run out of oil and freeze the economy. Policy makers attempted to use past data to predict future oil. These were entirely empirical models: curve fitting. The curve selected was sensible: a sigmoidal function. The exhaustion of oil corresponds to reaching the sigmoid’s upper horizontal asymptote is reach.\nFigure 16.5 shows cumulative oil consumption globally. The value for each year corresponds to all consumption from 1900 up to the given year. It’s revealing to look at a fitted curve using data up to 1973. (In 1973, the world’s oil economy started to change rapidly due to embargoes and oligopolistic behavior by leading oil producers.) The blue curve in Figure 16.5 shows a sigmoidal function as might have been fitted in 1973.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.5: The total cumulative amount of oil produced (units: Terra-watt-hours) globally for each year since 1900. Blue: a sigmoidal model fitted to the data up to 1973. Green: sigmoidal model fitted with data through 2023.\n\n\n\nSuch curve fitting is a fool’s errand for several reasons. Although it might be sensible to imagine cumulative oil production over time as a sigmoid, there is no indication in the data that the curve is leveling off. Nor is there good reason to assume that the mechanisms that shaped the oil economy of 1920 were still in action in 1973, let alone in 2023. Not only is the model an extrapolation, but it depends on the details of polishing of sigmoidal parameters. To illustrate this sensitivity, consider the green curve in Figure 16.5 which is fitted to the data through 2023. The observed data in 2023 is only about one-third the level of the 1973 prediction for 2023.\nNow consider a different purpose for such modeling. Suppose we take at face value international claims of a “net-zero” CO\\(_2\\) economy by 2050 and we want to anticipate the total cumulative effect as of that time. Draw a smooth curve continuing the oil consumption data up through 2100, with a leveling off around 2050. There is no precise way to do this, since there is no reason to think that the growth part of the sigmoidal curve will be the same as the leveling-off part. There is a range of plausible scenarios, but since we are already close or even at the target of a 1.5 degree increase in global temperature, it seems entirely possible that the horizontal asymptote will be above 2 or even 2.5 degrees.\nFortunately, unlike in 1973, there are now detailed mechanistic climate models relating CO\\(_2\\) to global temperatures. We will have to pay careful attention to their predictions as we head toward the goal of net-zero emissions.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling and the scientific method</span>"
    ]
  },
  {
    "objectID": "differentiation-part.html",
    "href": "differentiation-part.html",
    "title": "BLOCK II. Differentiation",
    "section": "",
    "text": "A mathematical function is a relationship between inputs and an output. An important and useful way to work with functions is to examine change in output as the inputs are changed by a small amount. The process of calculating this change-in-output per change-in-input—a rate of change—is called differentation. Often, the rate of change is itself a function. Such rate-of-change functions are given a special label: derivative functions.\nThis Block introduces the concept of a rate-of-change function, ways of computing them, and how the derivative of a function can be inferred from a graph of the function. We will explore the connection between the value of the rate-of-change function and the location of an input that optimizes the output of the original function. We will consider the idea of rate-of-change for a function that has multiple inputs.\nSometimes, your knowledge of a real-world system takes the form of knowing the behavior of the rate-of-change function. This can be an important guide to constructing mathematical models.",
    "crumbs": [
      "BLOCK II. Differentiation"
    ]
  },
  {
    "objectID": "Differentiation/17-continuous-change.html",
    "href": "Differentiation/17-continuous-change.html",
    "title": "17  Continuous change",
    "section": "",
    "text": "17.1 Mathematics in motion\nCalculus is so important that there are many definitions for it. For our purposes, a good definition of calculus is\nIn previous chapters we defined and studied functions. Now it is time to get at the core of calculus, the idea of “continuous change.”\nThe questions that started it all had to do with motion of planets and marbles. In more technical language, “ballistics,” the science of balls. There were words to describe speed: fast and slow. There were words to describe force: strong and weak, heavy and light. And there were words to describe location and distance: far and near, long and short, here and there. But what were the relationships among these things? And how did time fit in, an intangible quantity that had aspects of location (long and short) and speed (quick and slow)?\nGalileo (1564-1642) started the ball rolling.1 As the son of a musician and music theorist, he had a sense of musical time, a steady beat of intervals. When a student of medicine in Pisa, he noted that swinging pendulums kept reliable time, regardless of the amplitude of their swing. After unintentionally attending a geometry lecture, he turned to mathematics and natural philosophy.\nUsing his newly developed apparatus, the telescope, Galileo’s observations put him on a collision course with the accepted classical truth about the nature of the planets. Seeking to understand gravity, he built an apparatus that enabled him accurately to measure the position in time of a ball rolling down a straight ramp. The belled gates he set up to mark the ball’s passage were spaced evenly in musical time: 1, 2, 3, 4, …. To get this even spacing in time, Galileo found he had to position the gates unevenly. Defining as 1 the distance of the first gate from the ball’s release point, the gates were at positions 1, 4, 9, 16, …. You can hear the result in Video 17.1.\nAnyone familiar with the squares of the integers can see the pattern in 1, 4, 9, 16, …. To summarize the pattern as a simple rule, Galileo first took the difference between the successive positions, what we will call the “first increment.”\n\\[\\underbrace{1 - 0}_1 \\ \\ \\ \\ \\ \\underbrace{4 - 1}_3\\ \\ \\ \\ \\ \\underbrace{9 - 4}_{5}\\ \\ \\ \\ \\ \\underbrace{16-9}_7\\ \\ \\ \\underset{{\\Large\\strut}\\text{first increment}}{\\text{}}\\] Next, Galileo repeated the differencing process on the first increment to produce a “second increment.”\n\\[\\underbrace{3 - 1}_2 \\ \\ \\ \\ \\ \\underbrace{5 - 3}_2\\ \\ \\ \\ \\ \\underbrace{7 - 5}_{2}\\ \\ \\ \\underset{{\\Large\\strut}\\text{second increment}}{\\text{}}\\]\nThe rule established by Galileo’s observations for the motion of a ball rolling down the ramp:\nTable 17.1 shows the situation in tabular form. The base measurements are the evenly spaced time and the measured position on the ramp.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Continuous change</span>"
    ]
  },
  {
    "objectID": "Differentiation/17-continuous-change.html#mathematics-in-motion",
    "href": "Differentiation/17-continuous-change.html#mathematics-in-motion",
    "title": "17  Continuous change",
    "section": "",
    "text": "Video 17.1: A re-enactment of Galileo’s rolling-ball experiment. The frets on the ramp are at position numbers 1, 4, 9, 16, 25, … In the video, the demonstrator refers also to the distances from the start: 2 cm, 8 cm, 18 cm, 32 cm, 50 cm, …, that is, 2 cm times the position number. Demonstrator: Prof. Jason Hafner of Rice University. For more about Galileo’s measurements, see Stillman Drake (1986) “Galileo’s physical measurements” American Journal of Physics 54, 302-305 https://doi.org/10.1119/1.14634\n\n\n\n\n\n\n\n\nThe second increment of position is constant.\n\n\n\n\n\nTable 17.1: Galileo’s ball-on-ramp observations and their first & second increments.\n\n\n\n\n\n\\(t\\)\n\\(x(t)\\)\nfirst increment\nsecond increment\n\n\n\n\n0\n0\n1\n2\n\n\n1\n1\n3\n2\n\n\n2\n4\n5\n2\n\n\n3\n9\n7\n\n\n\n4\n16",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Continuous change</span>"
    ]
  },
  {
    "objectID": "Differentiation/17-continuous-change.html#continuous-time",
    "href": "Differentiation/17-continuous-change.html#continuous-time",
    "title": "17  Continuous change",
    "section": "17.2 Continuous time",
    "text": "17.2 Continuous time\nGalileo’s mathematics of first and second increments was suited to the discrete-time measurements he was able to make. It would be for Newton to develop the continuous-time analog of increments.\nTo start, we can imagine a function \\(x(t)\\) that gives the position of the ball at any instant \\(t\\). With this notation, Galileo’s measured positions were \\(x(0), x(1), x(2), x(3), x(4), \\ldots\\), and the first increments were \\(x(1) - x(0)\\), \\(x(2) - x(1)\\), \\(x(3) - x(2)\\), and so on.\nBut just as position \\(x(t)\\) is a continuous function of time \\(t\\), the first increment can also be written as a continous function: \\[y(t) \\equiv x(t+1) - x(t)\\ .\\] Similarly, there is a second increment function: \\[\\begin{eqnarray}z(t) & \\equiv&  y(t+1) - y(t)\\\\ & = & \\left[x(t+2) - x(t+1)\\right] - \\left[x(t+1) - x(t)\\right] \\\\ &=& x(t+2) - 2 x(t+1) + x(t)\\ .\n\\end{eqnarray}\\]\nThe \\(+1\\) and \\(+2\\) in the first and second increment functions correspond to the time elapsed from one belled gate to the next. More generally, rather than using Galileo’s unit of rhythmic time, we can define the increment functions using a time quantity of our own choice; we will call it \\(h\\).\nRe-written using \\(h\\), the first increment becomes \\[y(t) \\equiv x(t+h) - x(t)\\ .\\] The second increment function is \\[\\begin{eqnarray}z(t) & \\equiv&  y(t+h) - y(t)\\\\ & = & \\left[x(t+2h) - x(t+h)\\right] - \\left[x(t+h) - x(t)\\right] \\\\ &=& x(t+2h) - 2 x(t+h) + x(t)\\ .\n\\end{eqnarray}\\]\nEvidently, the numerical values (dimension L) of the first and second increments depend on \\(h\\), which is a choice made by the experimenter, not a fact of nature. If the experimenter selects a large \\(h\\), the first and second increments will be large.\nIt would be nice to frame the ballistics theory so that \\(h\\) does not appear. Newton’s insight amounts to taking two steps:\n\nReplace the simple difference \\(x(t+h) - x(t)\\) with a rate of change, that is:\n\n\\[\\text{Rate of change of } x(t): \\ \\ \\ \\ {\\cal D}_t y(t) \\equiv \\frac{x(t+h) - x(t)}{h} \\tag{17.1}\\]\nPronounce \\({\\cal D}_t y(t)\\) as “the rate of change with respect to \\(t\\).\nLikewise, the second increment will become a “rate of change of a rate of change,” a phrase that is easier to understood when written as a formula. Read \\({\\cal D}_t {\\cal D}_t y(t)\\) as “the rate of change of the rate of change of \\(y(t)\\).”\n\\[\n\\begin{eqnarray}\n{\\cal D}_t {\\cal D}_t y(t)  &\\equiv&   {\\cal D}_t \\left(\\strut \\frac{{\\cal D}_t y(t+h) - {\\cal D}_t y(h)}{h}\\right) \\\\\n&=& \\frac{y(t+h) - y(t)}{h} \\\\\n&=& \\frac{\\frac{x(t+2h) - x(t+h)}{h}- \\frac{x(t+h) - x(t)}{h}}{h}\\\\\n&=& \\frac{x(t+2h) - 2 x(t+h) + x(t)}{h^2}\\ .\n\\end{eqnarray} \\tag{17.2}\\]\nAdmittedly, Math expression 17.2 for the rate-of-change equivalent of Galileo’s second increment hardly looks like an improvement! And it still depends on \\(h\\).\nThis is where the second step of Newton’s insight comes in.\n\nMake \\(h\\) vanishingly small.\n\nIn the next chapters, we will look at how these two steps—use rate of change rather than change and make \\(h\\) vanishingly small—create mathematical entities that allowed Newton to extend Galileo’s work to become a universal theory of motion.\n\n\n\n\n\n\n\\(\\partial\\) is coming up in a bit.\n\n\n\nChapter 19 mades \\(h\\) vanishingly small, so we switch from the big \\({\\cal D}\\) to a smaller one, \\(\\partial\\), to remind us that \\(h\\) has vanished from the picture.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Continuous change</span>"
    ]
  },
  {
    "objectID": "Differentiation/17-continuous-change.html#change-relationships",
    "href": "Differentiation/17-continuous-change.html#change-relationships",
    "title": "17  Continuous change",
    "section": "17.3 Change relationships",
    "text": "17.3 Change relationships\nAs you know, function is a mathematical idea used to represent a relationship between quantities. For instance, the water volume of a reservoir behind a dam varies with the seasons and over the years. As a function, water volume is a relationship between water volume (one quantity) and time (another quantity). Similarly, the flow in a river feeding the reservoir has its own relationship with time. In spring, the river may be rushing with snow-melt, in late summer the river may be dry, but after a summer downpour the river flow again rises briefly. In other words, river flow is a function of time.\nDifferentiation is a way of describing a relationship between relationships. The water volume in the reservoir has a relationship with time. The river flow has a relationship with time. Those two relationships are themselves related: the river flow feeds the reservoir and thereby influences the water volume.\nIt is not easy to keep straight what’s going on in a “relationship between relationships.” Consequently, we need tools such as differentiation to aid our understanding.\n\n\n\n\n\n\nCalculus history—Standing on the shoulders of giants\n\n\n\nJohannes Kepler (1572-1630) spent years analyzing the data collected by astronomer Tycho Brahe (1546-1601). The data showed a relationship between time and the speed of a planet across the sky. Long-standing wisdom claimed that there is also a specific relationship between a planet’s position and time. From antiquity, it had been claimed that planets moved in circular orbits. Kepler worked hard to find the relationship between the two relationships: speed versus time and position versus time. He was unsuccessful until he dropped the assumption that planetary orbits are circular. Testing the hypothesis that orbits are elliptical, Kepler was able to find a simple relationship between speed vs. time and position vs. time.\nBuilding on Kepler’s earlier work, Newton hypothesized that planets might be influenced by the same gravity that pulls an apple to the ground. It was evident from human experience that gravity has the most trivial relationship with time: gravity is constant! But Newton could not find a link between this notion of gravity as a constant and Kepler’s planetary motion as a function of time. Success came when Newton hypothesized—without any direct evidence from experience—that gravity is a function of distance. Newton’s formulation of the relationship between relationships— gravity-as-a-function-of-distance and orbital-position-as-a-function-of time—became the foundation of modern science.\n\n\nNewton’s theories of gravity, force, and motion created an extremely complicated chain or reasoning that is still hard to grasp. Or, more precisely, it is hard to grasp until you have the language for describing relationships between relationships. Newton invented this language: differentiation. As you learn this language, you will find it easier to express and understand relationships between relationships, that is, the mechanisms that account for the ever-changing quantities around us.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Continuous change</span>"
    ]
  },
  {
    "objectID": "Differentiation/17-continuous-change.html#with-respect-to",
    "href": "Differentiation/17-continuous-change.html#with-respect-to",
    "title": "17  Continuous change",
    "section": "17.4 With respect to …",
    "text": "17.4 With respect to …\nWe’ve introduced a bit of new notation in the previous section, \\({\\cal D}_t\\). As mentioned previously, \\({\\cal D}\\) stands for “the rate of change of ____ with respect to .” In use, you put a function in the first slot indicated by _ and the name of an input variable in the second slot. Which function and variable depends on what you want to describe. For instance, the position of a rolling ball is a function of time: \\(x(t)\\).”The rate of change of \\(x(t)\\)” is written \\({\\cal D}_t x(t)\\). This object \\({\\cal D}_t x(t)\\) is itself a function of time.\nAnother example: consider a water reservoir fed by a spring and drained by the water utility to serve its customers. Suppose \\(w(t)\\) is the volume of water in the reservoir, a quantity that changes over time. Then \\({\\cal D}_t w(t)\\) is the rate of change of water volume in the reservoir. Common sense suggests that the rate of change in water volume will be positive during a wet season and negative in a drought.\nThe subscript on \\({\\cal D}_t\\) is the with-respect-to input. To illustrate, suppose that \\(h(v, w)\\) is the volume of the harvest from a field as a function of the amount of irrigation water \\(w\\) and the amount of fertilizer used during the growing season. (Constructing such a function could be done by collecting data over many years of the harvest, along with the amount of water and fertilizer used each year.) Chapter 25 points out that there are two different rate-of-change functions associated with \\(h()\\). One is the rate of change in harvest volume with respect to \\(w\\), the other is the rate change in harvest volume with respect to \\(v\\). In everyday language, \\({\\cal D}_w h(v, w)\\) can be used to predict how much the harvest will change if, next year, the farmer uses less irrigation water. Similarly, \\({\\cal D}_v h(v, w)\\) can inform a farmer’s decision to reduce costs by using less fertilizer.\nStrictly speaking, for functions with just one input the subscript on \\({\\cal D}\\) isn’t needed. Even so, we will always include a subscript, if only for the sake of forming good habits to serve us when we do examine rates of change in functions of multiple inputs.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Continuous change</span>"
    ]
  },
  {
    "objectID": "Differentiation/17-continuous-change.html#footnotes",
    "href": "Differentiation/17-continuous-change.html#footnotes",
    "title": "17  Continuous change",
    "section": "",
    "text": "Galileo was not aware of Kepler’s elliptical theory, even though they lived at the same time.↩︎",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Continuous change</span>"
    ]
  },
  {
    "objectID": "Differentiation/18-rate-of-change.html",
    "href": "Differentiation/18-rate-of-change.html",
    "title": "18  Rate of change",
    "section": "",
    "text": "18.1 Outputs versus rates of change\nImagine a car trip along a scenic road such as that shown in Figure 18.1. As the trip proceeds, the position varies; position is a function of time. The graph’s horizontal axis marks the elapsed time from the start of the trip. The vertical axis denotes the distance from the starting point.\nAt the end of the hour’s journey, the car’s position has changed by 25 miles from the start. The rate of change of the car’s position is a ratio: the change in position divided by the change in time: \\[\\frac{25\\ \\text{miles}}{1\\ \\text{hour}}\\] Writing this as “25 miles per hour” is more compact, with the small word “per” doing the job of reminding that the quantity was produced by dividing change in position by change in time.\nUsing functions to describe the car-trip situation, we can say that position is a function of time. We will call it \\(p(t)\\). The input to the function is time and the output is position.\nA rate of change for the function can be calculated by choosing two different values for time and evaluating the function at those times. The evaluation produces two different values for the output position. Calling the two times \\(t_0\\) and \\(t_1\\), the corresponding outputs are \\(p(t_0)\\) and \\(p(t_1)\\).\nThe average rate of change of \\(p(t)\\) over the interval \\(t_0 \\leq t \\leq t_1\\) is \\[\\frac{p(t_1) - p(t_0)}{t_1 - t_0}\\ .\\] Section 15.2 showed that subtraction is legitimate only when the two quantities involved have the same dimension and units. That is the case here. \\(p(t_1)\\) and \\(p(t_2)\\) both have dimension L and miles as the unit. \\(t_1\\) and \\(t_0\\) both have dimension T and hours as the unit.\nThe division of \\(p(t_1) - p(t_0)\\) (dimension L) by \\(t_1 - t_0\\) (dimension T) is also dimensionally legitimate. The simple reason is that division of one quantity by another is always dimensionally legitimate. The division produces a quantity with dimension L/T.\nA quantity with dimension L/T is utterly different than a quantity of dimension L or a quantity of dimension T. In other words, “25 miles per hour” is neither a position nor a time, it is a velocity.\nOne way to see that velocity is a different kind of quantity than position or time is that you measure the quantities in different ways. You might measure position by noting the passage of a mile marker along the side of the road. You can measure time by reference, say, to your level of boredom or by checking a clock or watch. Divide change in position by change in time to get velocity. But you can also sense velocity directly, by the level of noise in the car or the blurring of nearby objects along the road.\nOn a graph, you also measure in different ways changes in the input to a function and the corresponding changes in output. As always, start by picking the endpoints of an interval in the domain of the function. As an example, Figure 18.2 marks the endpoints of an interval with \\(\\color{magenta}{magenta}\\) dots.\nDraw a rectangle connecting the function values at the start and end of the interval. The change in input is the horizontal extent of the rectangle. The change in output is the vertical extent of the rectangle. If “vertical” and “horizontal” are enough to point out that the two measures are of different kinds of things, you will be reminded by your having to use two different scales for the two measurements.\nOver the interval marked, the average rate of change of the function is still another kind of perceived quantity, the “slope” of the diagonal of the rectangle. Unfortunately, graphs do not typically include a scale for slope, but we have added a scale to Figure 18.3. From the slope scale, you can easily see that the average rate of change is a little less than 30 miles per hour.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Rate of change</span>"
    ]
  },
  {
    "objectID": "Differentiation/18-rate-of-change.html#outputs-versus-rates-of-change",
    "href": "Differentiation/18-rate-of-change.html#outputs-versus-rates-of-change",
    "title": "18  Rate of change",
    "section": "",
    "text": "Figure 18.2: Measuring change in input, change in output, and the rate of change of a function over an interval.\n\n\n\n\n\n\n\n\n## Warning: All aesthetics have length 1, but the data has 101 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 101 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 101 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n\n\n\n\n\n\n\n\n\nFigure 18.3: Adding a scale for slope to the graph.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Rate of change</span>"
    ]
  },
  {
    "objectID": "Differentiation/18-rate-of-change.html#slope-at-a-point",
    "href": "Differentiation/18-rate-of-change.html#slope-at-a-point",
    "title": "18  Rate of change",
    "section": "18.2 Slope at a point",
    "text": "18.2 Slope at a point\nWith a slope scale, you can dispense with the laborious process shown in Figure 18.2: marking an interval, drawing a rectangle, measuring the vertical change, etc. The slope scale lets you read off the rate of change at a glance: pick a point in the domain, look at the slope of the function at that point, and compare it to the slope scale.\nPerhaps you can see that formally defining an interval isn’t an absolute necessity for defining a slope. The function has a slope at every point: the slope of the line tangent to the graph at that point. The easiest way to calculate the slope, at least approximately, is with a graphical aid called a “slope rose” (by analogy to the “compass rose” often printed on geographic maps.\n\n\n\n—A simple bit of graphics makes it easier to provide a quantitative estimate of the function’s slope for any particular input value.\n\n\nTry it! 18.1\n\n\n\n\n\n\n\n\n\nTry it! 18.1 The slope rose\n\n\n\nUsing the slope scale in Figure 18.3, estimate the car’s speed at input \\(t=0.2\\)hr (marked A). How does it compare to the speed at \\(t=0.4\\)hr (marked B)?\nPlace a ruler on the function graph so that the rule is locally tangent to the graph at \\(t=0.2\\). Keeping that point of contact, vary the slope of the ruler until it neatly aligns with the curve. Now, without changing the slope of the ruler, slide it over to the slope scale and read the ruler slope off that scale. The slope is a bit more than 20 miles per hour.\nAt \\(t=0.4\\), the function slope is considerably steeper than at \\(t=0.2\\), about 60 miles per hour.\n\n\nThe function’s slope at a specific input like \\(t=0.2\\) is called the instantaneous slope and corresponds to the instantaneous velocity of the car. , you do not have to measure the car’s velocity by reading the change in position over the interval between two distinct moments in time; you can simply look at the speedometer to get an instantaneous read-out of the velocity. We will translate instantaneous rate of change into the language of functions in Chapter 19.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Rate of change</span>"
    ]
  },
  {
    "objectID": "Differentiation/18-rate-of-change.html#sec-slope-function",
    "href": "Differentiation/18-rate-of-change.html#sec-slope-function",
    "title": "18  Rate of change",
    "section": "18.3 Slope function",
    "text": "18.3 Slope function\nAnother way of showing the car’s journey is to break the full 60-minute trip down into 1-minute intervals. During each such interval, we will graph the car’s distance from the place it was at the beginning of the interval, considering that place to be zero. The overall effect is to construct the car’s journey by 60 piecewise functions, each of which starts a position 0 and ends at the total distance travelled by the car during the 1-minute interval.\nThe distance-versus-time for each of the 60 segments is graphed in Figure 18.4. To orient you, consider a segment when the car’s speed is about 60 mph. During at the end of a one-minute segment, the car will travel 1 mile. When the car is driving more slowly, the distance at the end of the segment will be less.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 18.4: Breaking down the car’s hour-long trip into 60 1-minute segments. The vertical axis is the function distance traveled by the car versus time during that segment.\n\n\n\nNaturally, the slope of each segment in Figure 18.4 gives the speed of the car: distance divided by time. To aid the eye, a slope rose has been added showing a variety of speeds. Each of the red lines in the slope rose has a horizontal extent of 3 minutes. A car speeding at 60 mph for 3 minutes will travel 3 miles. You can confirm that the slope-rose segment for 60 mph runs vertically from -1.5 to 1.5 miles, that is, covering a distance of 3 miles in 3 minutes: 60 mph.\nIf you were to place each segment end-to-end on the previous segment, you would reconstruct the graph in Figure 18.3. Similarly, the sum of the vertical extents of all the segments in Figure 18.4 is the total distance travelled by the car in each hour.\nThe vertical extent of each segment in Figure 18.4 is the distance travelled in 1 minute. So we might as well interpret the vertical scale as the speed in miles per minute. Multiplying the vertical scale by 60 will convert the scale into units of miles per hour. Figure 18.5 is nothing more than this rescaled version of Figure 18.4. We drop the slope rose because Figure 18.5 lets us read the speed (mph) directly from the vertical axis scale.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 18.5: Graphing the slope function \\({\\\\cal D}_t p(t)\\). The value of the slope of \\(p(t)\\) can be read from the vertical axis scale.\n\n\n\nYou can see that the segments in Figure 18.5 vary pretty slowly. It seems fair to connect the ends of the segments with a continuous curve, the blue one shown in Figure 18.5. The blue curve lets us measure the speed of the car continuously, not just at the one-minute intervals.\nIn general, for a function \\(p(t)\\) the rate of change at any instant \\(t\\) over an interval of duration \\(h\\) will be \\[{\\cal D}_t p(t) \\equiv \\frac{p(t+h) - p(t)}{h}\\] where \\(h\\) is the duraction of the interval used to compute the rate of change. We will call this the slope function of \\(p(t)\\). For the car trip, the slope function is essentially the blue curve in Figure 18.5.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Rate of change</span>"
    ]
  },
  {
    "objectID": "Differentiation/18-rate-of-change.html#sec-tree-harvest-example",
    "href": "Differentiation/18-rate-of-change.html#sec-tree-harvest-example",
    "title": "18  Rate of change",
    "section": "18.4 Average rate of change",
    "text": "18.4 Average rate of change\nChapter 19 introduces the instantaneous rate of change of a function. That concept is so important that you will tend to forget there was any such thing as the “average rate of change” over an interval.\nNevertheless, average rate of change can be a useful concept in many circumstances. To illustrate, Figure 18.6 shows a simplified model of the amount of usable wood harvestable from a typical tree in a managed forest of Ponderosa Pine. (You can see some actual forestry research models here.) Such a model, even if simplified, can provide useful insight for forestry planning.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 18.6: A model, somewhat realistic, of the amount of useful wood from a Ponderosa Pine as a function of the number of years from planting to harvest.\n\n\n\nThe overall pattern in Figure 18.6 is that the tree continues to grow until year 50, when it seems to have reached an equilibrium: perhaps growth goes to zero, or rot balances growth.\nIf managing a forest for wood production, it seems sensible to try to get as much wood out of the tree as possible. The maximum volume of wood occurs, in Figure 18.6, at about year 50. Does that mean that harvesting at year 50 is optimal? If not, when is the best time?\n\n\n\n—Do it yourself optimization.\n\n\nTry it! 18.2\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nTry it! 18.2 Spend a moment thinking about Figure 18.6 and draw your own conclusions about when would be the most efficient time to harvest. Right or wrong, putting your own proverbial stake in the ground will help you understand the argument we are about to make.\n\n\nGood forestry practices are “sustainable.” Forests are managed to be continually productive rather than subject to a one-time extraction of value followed by desolation. For sustainability, it is important to consider the life cycle of the forest. After all, continual productivity implies that the forest will continue to produce value into the indefinite future.\nOne implication of managing for sustainability is that the quantity to optimize is not the volume of wood from a one-time harvest. Rather, it is the rate (per year) at which wood can be sustainably extracted from the forest.\nAround year 25, the tree adds usable wood at the fastest instantaneous rate. This might suggest to some that a good time to harvest is near year 25. But, in fact, it makes no sense to harvest at the time of maximum rate of growth; why kill the tree when it is being most productive?\nA better quantity to look at for deciding when to harvest is the average rate of growth in the volume of wood. Remember that “average rate of change” is the rate of change over an extended interval. For wood harvesting, the relevant interval is the time from planting until harvest.\nHarvesting at year 25 will give a total change of 600 board feet over 25 years, corresponding to an average rate of change of \\(600 \\div 25 = 24\\ \\text{board-feet-per-year}\\). But if you wait until year 35, you will have about 900 board feet, giving an average rate of change of \\(900 \\div 35 = 25.7\\) board-feet-per-year (L3 T-1).\nIt is easy to construct a diagram that indicates whether year 35 is best for the harvest. Recall that our fundamental model of change is the straight-line function. So we will model the model of tree growth as a straight-line function. Like the model in Figure 18.6, our straight-line model will start with zero wood when planted. Furthermore, to be faithful to Figure 18.6, we will insist that the straight-line intersect or touch that curve.\nFigure 18.7 reiterates the Figure 18.6 model of the tree annotated with several straight-line models that all give zero harvest-able wood at planting time. Each green line represents a scenario where harvest occurs at \\(t_1\\), \\(t_2\\), etc. From the perspective of representing the rate of growth per year from planting to harvest, the straight-line green models do not need to replicate the actual growth curve. The complexities of the curve are not relevant to the growth rate. Instead, what’s relevant is the slope of a straight-line model connecting the output at planting time to the output at harvest time. In contrast, the magenta curve is not a suitable model because it does not match the situation at any harvest time; it does not touch the curve anywhere after planting!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 18.7: Modeling the tree-growth model with straight lines connecting planting time to various harvest times. The slope of each line is the average rate of growth for that planting time.\n\n\n\nChoose a harvest time that produces the steepest possible green segment to maximize average lumber volume per year. From Figure 18.7, that steepest line glances the growth curve near year 31 (shown as \\(t_3\\) in the diagram).\nIt is best to find the argmax by creating a function that shows explicitly what one is trying to optimize. (Chapter 24, uses the name objective function to identify such a function.) Here, the objective function is \\(\\text{ave.growth(year)} \\equiv \\text{volume(year)} / \\text{year}\\). See Figure 18.8.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 18.8: Graph of the average-growth function ave_growth(year), constructed by dividing volume the time of planting (year 0), by the time in years at harvest.\n\n\n\nThe graph of ave_growth(year) makes clear the maximum average growth from planting to harvest will occur at about year 32.\nThere is no point waiting until after year 50.\nAt year 25, the tree is growing as fast as ever. You will get about 600 board feet of lumber.1 Should you harvest at year 25? No! That the tree is growing so fast means that you will have a lot more wood in years 26, 27, etc. The time to harvest is when the growth is getting smaller so that it is not worth waiting an extra year.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Rate of change</span>"
    ]
  },
  {
    "objectID": "Differentiation/18-rate-of-change.html#sec-dimension-rate-change",
    "href": "Differentiation/18-rate-of-change.html#sec-dimension-rate-change",
    "title": "18  Rate of change",
    "section": "18.5 Dimension of a rate of change",
    "text": "18.5 Dimension of a rate of change\nThe function named \\(\\partial_t f(t)\\) which is the derivative of \\(f(t)\\) takes the same input as \\(f(t)\\); the notation makes that pretty clear. Let’s suppose that \\(t\\) is time and so the dimension of the input is \\([t] = \\text{T}\\).\nThe outputs of the two functions, \\(\\partial_t f(t)\\) and \\(f(t)\\) will not, in general, have the same dimension. Why not? Recall that a derivative is a special case of a slope function, the instantaneous slope function. It is easy to calculate a slope function:\n\\[{\\cal D}_t f(t) \\equiv \\frac{f(t+h) - f(t)}{h}\\] The dimension of the quantity \\(f(t+h) - f(t)\\) must be the same as the dimension of \\(f(t)\\); the subtraction would not be possible otherwise. Likewise, the dimension of \\(h\\) must be the same as the dimension of \\(t\\); the addition \\(t+h\\) wouldn’t make sense otherwise.\n\n\n\n\n\n\nTip\n\n\n\nKeep in mind that the dimension \\([f(t+h) - f(t)]\\) will be the same as \\([f(t)]\\). Why? The result of addition and subtraction will always have the same dimension as the quantities being combined.\n\n\nWhereas the dimension of the output \\(f(t)\\) is simply \\(\\left[f(t)\\right]\\), the dimension of the quotient \\(\\frac{f(t+h) - f(t)}{h}\\) will be different. The output of the derivative function \\(\\partial_t f(t)\\) will be \\[\\left[\\partial_t f(t)\\right] = \\left[f(t)\\right] / \\left[t\\right] .\\]\nSuppose \\(x(t)\\) is the position of a car as a function of time \\(t\\). Position has dimension L. Time has dimension T. The function \\(\\partial_t x(t)\\) will have dimension L/T. Familiar units for L/T are miles-per-hour, which you can recognize as velocity.\nAnother example: Imagine a function pressure() with that takes altitude above sea level (in km) and output pressure (in kPa, “kiloPascal”).2 The derivative function, let’s call it \\(\\partial_\\text{altitude} \\text{pressure}()\\), also takes an input in km, but produces an output in kPA per km: a rate.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Rate of change</span>"
    ]
  },
  {
    "objectID": "Differentiation/18-rate-of-change.html#footnotes",
    "href": "Differentiation/18-rate-of-change.html#footnotes",
    "title": "18  Rate of change",
    "section": "",
    "text": "A “board foot” is a volume, dimension L3. It is a square foot (L^2) times an inch (L).↩︎\nAir pressure at sea level is about 100 kiloPascal.↩︎",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Rate of change</span>"
    ]
  },
  {
    "objectID": "Differentiation/19-evanescent-h.html",
    "href": "Differentiation/19-evanescent-h.html",
    "title": "19  Evanescent h",
    "section": "",
    "text": "19.1 Evanescence algebraically\nOn a radio broadcast, a baseball fanatic described the path of a home run slammed just inside the left-field post: “Coming off the bat, the ball screamed upwards, passing five stories over the head of the first baseman and still gaining altitude. Then, somewhere over mid-left-field, gravity caught up with the ball, forcing it down faster and faster until it crashed into the cheap seats.” A gripping image, perhaps, but wrong about the physics. Gravity does not suddenly catch hold of the ball. Gravity influences the ball in the same way at all phases of flight, whether the ball is climbing or descending. The vertical velocity of the ball is positive while climbing and negative on descent, but that velocity is steadily changing all through the flight: a smooth, almost linear numerical decrease in velocity from the time the ball leaves the bat to when it lands in the bleachers.\nAt each instant, the ball’s vertical velocity has a numerical value in feet-per-second (L T-1). That value changes continuously. If \\(Z(t)\\) is the height of the ball at time \\(t\\), and \\(v_Z(t)\\) is the vertical velocity at time \\(t\\), then the slope function \\[{\\cal D}_t Z(t) \\equiv \\frac{Z(t+h) - Z(t)}{h}\\] tells us the average velocity of the ball over a time interval of \\(h\\).\nThe “average velocity” is a human construction because \\(h\\) is a human choice. The reality pointed to by Newton is that at each instant in time the ball has a continuously changing velocity. Velocity is the physical reality: an instantaneous quantity. The “average velocity” is merely a concession to the way we measure, recording the height at two different times and computing the difference in height divided by the difference in time.\nOur average velocity measurement gets closer to the instantaneous velocity when we make the time interval \\(h\\) smaller. But how small?\nIn the decades after Newton and Leibniz published their work inventing calculus, there was much philosophical concern about \\(h\\). Skeptics pointed out the truth that either \\(h\\) is zero or it is not. If it is zero, then the slope function would be \\[{\\cal D}_t Z(t) \\equiv \\frac{Z(t+0) - Z(t)}{0} = \\frac{Z(t) - Z(t)}{0} = \\frac{0}{0}\\ .\\] This \\(0/0\\) is not a proper arithmetical quantity. On the other hand, if \\(h\\) is not zero, then the slope function is a human construction involving \\(h\\), not a natural one where \\(h\\) plays no role.\nNewton and Leibniz could not address skeptics except by using suggestive but as yet undefined words. These amount to saying “\\(h\\) vanishes,” or “\\(h\\) is an infinitesimal,” or “\\(h\\) is a nascent quantity.”\nAnother good image of \\(h\\) becoming as small as possible comes from University of Oxford mathematician Charles Lutwidge Dodgson (1832-1898). In Alice in Wonderland, Dodgson introduced the character of the Cheshire Cat.\nFor a physical metaphor for evanescent \\(h\\), consider the process of painting. It is evident that a can of paint contains liquid which somehow becomes solid when brushed on a wall or other object. A better way to think about paint is as a “colloid,” small solid particles suspended in a liquid. The purpose of the liquid is to keep the solid particles separate, so that the paint can flow and conform to the surface of the object being painted. Spreading out the liquid on the surface leads to rapid evaporation so that only the solid particles remain. Without the liquid, the particles no longer flow. They remain in place.\nIn the expression \\[{\\cal D}_t p(t) \\equiv \\frac{p(t+h) - p(t)}{h}\\] the quantity \\(h\\) is the liquid and each value of the input is a particle of solid. \\(h\\) separates the solid particles. \\(p(t+h)\\) and \\(p(t)\\) are the values of the function at the slightly separated inputs. Since the inputs are held slightly apart, the function values \\(p(t+h)\\) and \\(p(t)\\) can be distinct and the difference between them, \\(p(t+h) - p(t)\\), can be non-zero. The final result is to remove the \\(h\\) from the difference, that is, divide \\(p(t+h)-p(t)\\) by \\(h\\). By slightly separating the input values, \\(h\\) makes its contribution to the process, but in the end, \\(h\\) evaporates just like the liquid in paint. That is why \\(h\\) is evanescent: eventually it vanishes like vapor.\nLet’s look at a slope function using evanescent \\(h\\). To start, we will analyze \\(f(t) \\equiv t^2\\), one of our pattern-book functions. By definition, \\[{\\cal D}_t f(t) \\equiv \\frac{f(t+h) - f(t)}{h}\\ .\\] We can easily evaluate \\(f(t+h)\\) symbolically: \\[f(t+h) \\equiv (t+h)^2 = t^2 + 2 t h + h^2\\] Similarly, we can find the difference \\(f(t+h) - f(t)\\). It is \\[f(t+h) - f(t) = f(t+h) - t^2 = 2 t h + h^2\\ .\\] Notice that there is still some liquid (that is, \\(h\\)) in the difference. Now we let the difference start to dry, taking out the \\(h\\) by dividing the difference by \\(h\\): \\[\\frac{f(t+h) - f(t)}{h} = \\frac{2 t h + h^2}{h} = 2 t + h\\ .\\] The rate of chaange of \\(f(t)\\) has something solid—\\(2 t\\)—along with a little bit of liquid \\(h\\) that we can leave to evaporate to nothing.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Evanescent h</span>"
    ]
  },
  {
    "objectID": "Differentiation/19-evanescent-h.html#differentiation",
    "href": "Differentiation/19-evanescent-h.html#differentiation",
    "title": "19  Evanescent h",
    "section": "19.2 Differentiation",
    "text": "19.2 Differentiation\nBy this point you should be familiar with the definition of the average rate of change of \\(f(t)\\) over an interval from \\(t\\) to \\(t+h\\):\n\\[{\\cal D}_t f(t) \\equiv \\frac{f(t+h) - f(t)}{h}\\] To indicate that we want a rate of change with evanescent \\(h\\), we add a statement to that effect:\n\\[\\partial_t f(t) \\equiv \\lim_{h\\rightarrow 0} {\\cal D}_t f(t) = \\lim_{h\\rightarrow0}\\frac{f(t+h) - f(t)}{h}\\ .\\] A proper mathematical phrasing of \\(\\lim_{h\\rightarrow 0}\\) is, “the limit as \\(h\\) goes to zero.” Using the paint metaphor, read \\(\\lim_{h\\rightarrow 0}\\) as “once applied to the wall, let the paint dry.”\nTo save space, write \\(\\lim_{h\\rightarrow 0} {\\cal D}_t f(t)\\)a more compact way: \\(\\partial_t f(t)\\). We use the small symbol \\(\\partial\\) as a reminiscence of the role that small \\(h\\) played in the construction of \\(\\partial_t f(t)\\).\nThe function \\(\\partial_t f(t)\\) is called the derivative of the function \\(f(t)\\). The process of constructing the derivative of a function is called differentiation. The roots of these two words are not the same. “Differentiation” comes from “difference,” a nod to subtraction as in “the difference between 4 and 3 is 1.” In contrast, “derivative” comes from “derive,” whose dictionary definition is “obtain something from a specified source,” as in deriving butter from cream. “Derive” is a general term. But “derivative” and “differentiation” always refers to a specific form of related to rates of change.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Evanescent h</span>"
    ]
  },
  {
    "objectID": "Differentiation/19-evanescent-h.html#sec-d-pattern-book",
    "href": "Differentiation/19-evanescent-h.html#sec-d-pattern-book",
    "title": "19  Evanescent h",
    "section": "19.3 Derivatives of the pattern book functions",
    "text": "19.3 Derivatives of the pattern book functions\nThe pattern-book functions are so widely used that it is helpful to memorize facts about their derivatives. Remember that, as always, the derivative of a function is another function. For every pattern-book function, the derivative is itself built from a pattern-book functions. To emphasize this, the list below states the rules using the names of the functions, rather than formulas.\n\n\\(\\partial_x\\) one(x) \\(=\\) zero(x)\n\\(\\partial_x\\) identity(x) \\(=\\) one(x)\n\\(\\partial_x\\) square(x) \\(=\\) 2 identity(x)\n\\(\\partial_x\\) reciprocal(x) \\(=\\) -1/square(x)\n\\(\\partial_x\\) log(x) \\(=\\) reciprocal(x)\n\\(\\partial_x\\) sin(x) \\(=\\) cos(x)\n\\(\\partial_x\\) exp(x) \\(=\\) exp(x)\n\\(\\partial_x\\) sigmoid(x) \\(=\\) gaussian(x)\n\\(\\partial_x\\) gaussian(x) \\(=\\) - x gaussian(x)\n\nIn applications, the pattern-book functions are parameterized, e.g. \\(\\sin\\left(\\frac{2\\pi}{P} t\\right)\\). Chapter 23 introduces the derivatives of the parameterized functions.\nNotice that \\(h\\) does not appear at all in the table of derivatives. Instead, to use the derivatives of the pattern-book functions we need only refer to a list of facts, not the process for discovering those facts.\n\n\n\n\n\n\nFigure 19.3: A diagram showing how differentiation connects the pattern-book functions to one another.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Evanescent h</span>"
    ]
  },
  {
    "objectID": "Differentiation/19-evanescent-h.html#notations-for-differentiation",
    "href": "Differentiation/19-evanescent-h.html#notations-for-differentiation",
    "title": "19  Evanescent h",
    "section": "19.4 Notations for differentiation",
    "text": "19.4 Notations for differentiation\nThere are many notations in wide use for differentiation. In this book, we will denote differentiation in a manner has a close analogy in computer notation.\nWe will write the derivative of \\(f(x)\\) as \\(\\partial_x f(x)\\). If we had a function \\(g(t)\\), with \\(t\\) being the input name, the derivative would be \\(\\partial_t g(t)\\). Since there is nothing special about the name of the input in functions with one input, we could just as well write the one-input function that is the derivative of \\(g()\\) with respect to its input as \\(\\partial_x g(x)\\) or \\(\\partial_z g(z)\\) or even \\(\\partial_{zebra} g(zebra)\\). For functions with just one input, the notation skeptic might argue that there is no need for a subscript on \\(\\partial\\), since it will always match the name of the input to the function being differentiated.\nEarly in the history of calculus, mathematician Joseph-Louis Lagrange (1736-1813) proposed a more compact notation for the derivative of a function with a single input. Rather than \\(\\partial_x f(x)\\), Lagrange wrote \\(f'\\). We pronounce this “f-prime.” This notation is still widely used in calculus textbooks because it is compact. But it is not a viable notation for functions used in modeling since those functions often have more than one input.\nEarlier than Lagrange, Newton used a very compact notation. The historian needs to be careful, because Newton did not use the term “derivative” nor the term “function.” Instead, Newton wrote of “flowing quantities,” that is, quantities that change in time. For Newton, typical names for such flowing quantities were \\(x\\) and \\(y\\). He didn’t use the parentheses that we now associate with functions, just the bare name. Newton used “fluent” to name such flowing quantities. Newton’s fluents were more or less what we call today “functions of time.” What we now call “derivatives,” Newton called “fluxions.” If \\(x\\) is a fluent, then Newton wrote \\(\\dot{x}\\) to stand for the fluxion. This is pronounced “x-dot.” Like Lagrange’s compact prime notation, Newton’s dot notation is still used, particularly in physics.\nThe mathematician Gottfried Wilhelm Leibniz (1646-1716) was a contemporary of Newton. Leibniz developed his own notation for calculus, which was easier to understand than Newton’s. In Leibniz’s notation, the derivative (with respect to \\(x\\)) of \\(f(x)\\) was written \\[\\frac{df}{dx}\\ .\\] The little \\(d\\) stands for “a little bit of” or “a little change in,” so \\(\\frac{df}{dx}\\) makes clear that the derivative is a ratio of two little bits. In the denominator, \\(dx\\) refers to an infinitesimal change in the value of the input \\(x\\). In the numerator, the \\(df\\) names the corresponding change in the output of \\(f()\\) when the input is changed.\nLeibniz’s notation is by far the most widely used in introductory calculus. It has many advantages compared to Newton’s or Lagrange’s notations. For example, it provides an opportunity to name the with-respect-to input. It also provides a nice notation for an operation called “anti-differentiation” which we will meet in Block 4. And many a physics or engineering student has been taught to treat \\(dx\\) as if it were a number when doing algebraic manipulations.\nThe problem with Leibniz’s notation, from the perspective of this book, is that it does not translate well into computer notation. A statement like:\ndf/dx &lt;- x^2 + 3*x\nis a non-starter since the character / is not allowed in a name in most computer languages, including R.\nFor functions with multiple inputs, for instance, \\(h(x,y,z)\\), differentiation can be done with respect to any input. Leibniz’s notation might possibly be used to indicate which is the with-respect-to input; the three derivatives of \\(h()\\) would be written \\(dh/dx\\) and \\(dh/dy\\) and \\(dh/dz\\). However, mathematical notation did not go in this direction. Instead, for functions with multiple inputs, the three derivatives are most usually written \\(\\partial h/\\partial x\\) and \\(\\partial h/partial y\\), and \\(\\partial h/\\partial z\\). In the expression, \\(\\partial h/\\partial y\\), the symbol \\(\\partial\\) is pronounced “partial,” The three different derivatives \\(\\partial h/\\partial x\\), \\(\\partial y /\\partial y\\), and \\(\\partial h/\\partial z\\) are called “partial derivatives” and are the subject of Chapter 25.\nThis book uses \\(\\partial_x h\\), \\(\\partial_y h\\), and \\(\\partial_z h\\) to denote partial derivatives. This adequately identifies the with-respect-to input and has a close analog in computer notation. For instance, if f(x,y,z) has been defined already, the following statements are entirely valid:\ndz_f &lt;- D(f(x,y,z) ~ z)\ndy_f &lt;- D(f(x,y,z) ~ y)\nIt has been more than 300 years since Leibniz’s death. At this point calculus is so we will established that we don’t need the notation \\(df/dx\\) to remind us that a derivative is “a little bit of \\(f\\) divided by a little bit of \\(x\\).”\nThere are several traditional notations for differentiation of a single-input function named \\(f()\\). Here’s a list of some of them, along with the name associated with each:\n\nLeibnitz: \\(\\frac{df}{dx}\\)\nPartial: \\(\\frac{\\partial f}{\\partial x}\\)\nNewton (or “dot”): \\(\\dot{f}\\)\nLagrange (or “prime”): \\(f'\\)\nOne-line (used in this book): \\(\\partial_x f\\)\n\nTo read calculus fluently, you will have to recognize each of these notations. For functions with one input, they all mean the same thing. But when functions have multiple inputs, the choice is between the styles \\(\\partial f / \\partial x\\) and \\(\\partial_x f\\). We use the later because it can easily be incorporated into computer commands.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Evanescent h</span>"
    ]
  },
  {
    "objectID": "Differentiation/19-evanescent-h.html#footnotes",
    "href": "Differentiation/19-evanescent-h.html#footnotes",
    "title": "19  Evanescent h",
    "section": "",
    "text": "https://www.merriam-webster.com/dictionary/evanescent↩︎",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Evanescent h</span>"
    ]
  },
  {
    "objectID": "Differentiation/20-computing.html",
    "href": "Differentiation/20-computing.html",
    "title": "20  Constructing derivatives",
    "section": "",
    "text": "20.1 Why differentiate?\nThis chapter shows to use the computer to construct the derivative of any1 function. This is easy because the task of constructing derivatives is well suited to the computer.\nWe will demonstrate two methods:\nChapter 23 covers the algorithms used by the computer to construct symbolic derivatives. Why teach you to do with paper and pencil the simpler sorts of problems that the computer does perfectly? One reason is that it helps you to anticipate what the results of the computer calculation will be, providing a mechanism to detect error in the way the computation was set up. Another reason is to enable you to follow textbook or classroom demonstrations of formulas which often come from working out a differentiation problem.\nBefore showing the easy computer-based methods for constructing the derivative of a function, it is good to provide some motivation: Why is differentiation so frequently in so many fields of study and application?\nA primary reason lies in the laws of physics. Newton’s Second Law of Motion reads:\nNewton defined used position \\(x(t)\\) as the basis for velocity \\(v(t) = \\partial_t x(t)\\). “Change in motion,” which we call “acceleration,” is in turn the derivative \\(\\partial v(t)\\). Derivatives are also central to the expression of more modern forms of physics such as quantum theory and general relativity.\nMany relationships encountered in the everyday or technical worlds are more understandable if framed as derivatives. For instance,\nOften, we know one member in such function-and-derivative pairs, but to need to calculate the other. Many modeling situations call for putting together different components of change to reveal how some other quantity of interest will change. For example, modeling the financial viability of retirement programs such as the US Social Security involves looking at the changing age structure of the population, the returns on investment, the changing cost of living, and so on. In Block V, we will use derivatives explicitly to construct models of systems, such as an outbreak of disease, with many changing parts.\nDerivatives also play an important role in design. They play an important role in the construction and representation of smooth curves, such as a robot’s track or the body of a car. (See Chapter 49.) Control systems that work to stabilize a airplane’s flight or regulate the speed and spacing of cars are based on derivatives. The notion of “stability” itself is defined in terms of derivatives. (See Chapter 45.) Algorithms for optimizing design choices also often make use of derivatives. (See Chapter 50.)\nEconomics as a field makes considerable use of concepts of calculus—particularly first and second derivatives, the subjects of this Block—although the names used are peculiar to economics, for instance, “elasticity”, “marginal returns” and “diminishing marginal returns.”",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Constructing derivatives</span>"
    ]
  },
  {
    "objectID": "Differentiation/20-computing.html#why-differentiate",
    "href": "Differentiation/20-computing.html#why-differentiate",
    "title": "20  Constructing derivatives",
    "section": "",
    "text": "“The change of motion of an object is proportional to the force impressed; and is made in the direction of the straight line in which the force is impressed.”\n\n\n\n\nElectrical power is the rate of change with respect to time of electrical energy.\nBirth rate is one component of the rate of change with respect to time of population. (The others are the death rate and the rates immigration and emigration.)\nInterest, as in bank interest or credit card interest, is the rate of change with respect to time of assets.\nInflation is the rate of change with respect to time of prices.\nDisease incidence is one component of the rate of change with respect to time of disease prevalence. (The other components are death or recovery from disease.)\nForce is the rate of change with respect to position of energy.\nDeficit (as in spending deficits) is the change with respect to time of debt.\n\n\n\n\n\n\n\n\n\n\nCalculus history—The Wealth of Nations\n\n\n\nThe origins of modern economics, especially the theory of the free market, are attributed to a book published in 1776, The Wealth of Nations. The author, Adam Smith (1723-1790), lays out dozens of relationships between different quantities — wages, labor, stock, interest, prices, profits, and coinage among others. Yet despite the invention of calculus a century before Wealth of Nations, the book uses no calculus.\nConsider this characteristic statement in Wealth of Nations:\n\nThe market price of every particular commodity is regulated by the proportion between the quantity which is brought to market, and the demand of those who are willing to pay the natural price of the commodity.\n\nWithout calculus and the ideas of functions and their derivatives, Smith was not able to think about prices in the modern way where price is shaped by demand and supply. Instead, for Smith, each item has a “natural price”: a fixed quantity that depends on the amount of labor used to produce the item. Nowadays, we understand that productivity changes as new methods of production and new inventions are introduced. But Smith lived near the end of a centuries-long period of static economies. Transportation, agriculture, manufacture, and population size were all much as they had been for the past 500 years or longer. James Watt’s steam engine was introduced only in 1776 and it would be decades before being adapted to the myriad uses of steam power characteristic of the 19th century. The cotton gin (1793), labor-saving agricultural machines such as the McCormick reaper (1831), the assembly line (1901), and the many other innovations of industry all lay in the future when Smith was writing Wealth of Nations.\n\n\n\n\n\n\nFigure 20.1: Demand as a function of price, as first published by Antoine-Augustin Cournot in 1836.\n\n\n\nIt took the industrial revolution and nearly a century of intellectual development before economics had to and could embrace the rapid changes in the production process. In this dynamical view, supply and demand are not mere quantities, but functions of which price is the primary input. The tradition in economics is to use the word “curve” instead of “function,” giving us the phrases “supply curve” and “demand curve.” Making the transition from quantity to function, that is, between a single amount and a relationship between amounts, is a core challenge to those learning economics.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Constructing derivatives</span>"
    ]
  },
  {
    "objectID": "Differentiation/20-computing.html#symbolic-differentiation",
    "href": "Differentiation/20-computing.html#symbolic-differentiation",
    "title": "20  Constructing derivatives",
    "section": "20.2 Symbolic differentiation",
    "text": "20.2 Symbolic differentiation\nThe R/mosaic function D() takes a formula for a function and produces the derivative. It uses the same sort of tilde expression used by makeFun() or contour_plot() or the other R/mosaic tools. For instance, run the code in Active R chunk 20.1 to see the function created by differentiating \\(t \\sin(t)\\) with respect to \\(t\\).\n\n\n\nActive R chunk 20.1\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nIf you prefer, you can use makeFun() to define a function, then hand that function to D() for differentiation as in Active R chunk 20.2:\n\n\n\nActive R chunk 20.2\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nWhen differentiating a function with two (or more) inputs, the function definition will have the names of the inputs on the right-hand side of the tilde expression, as in Active R chunk 20.3 where the two arguments to f() are named x and y. But notice that in the use of D(), the right-hand side of the tilde expression lists only the “with respect to” variable, in this case, x.\n\n\n\nActive R chunk 20.3\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\n\n\nTry it! 20.1\n\n\n\n\n\n\n\n\n\nTry it! 20.1 Derivatives of the pattern book functions\n\n\n\nNeedless to say, D() knows the rules for the derivatives of the pattern-book functions introduced in Section 19.3. For instance,\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nTry the other eight pattern-book functions. For seven of these, D() produces a symbolic derivative. For the eighth, D() resorts to a “finite-difference” derivative (introduced in Section 20.3). Which is this pattern-book function where D() produces a finite-difference derivative?",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Constructing derivatives</span>"
    ]
  },
  {
    "objectID": "Differentiation/20-computing.html#sec-finite-difference-derivatives",
    "href": "Differentiation/20-computing.html#sec-finite-difference-derivatives",
    "title": "20  Constructing derivatives",
    "section": "20.3 Finite-difference derivatives",
    "text": "20.3 Finite-difference derivatives\nWhenever you have a formula amenable to the construction of a symbolic derivative, that is what you should use. Finite-difference derivatives are useful in those situation where you don’t have such a formula. The calculation is simple but has a weakness that points out the advantages of the evanescent-\\(h\\) approach.\nFor a function \\(f(x)\\) and a “small,” non-zero number \\(h\\), the finite-difference approximates the derivative with this formula:\n\\[\\partial_x f(x) \\approx \\frac{f(x+h) - f(x-h)}{2h}\\ .\\] To demonstrate, let’s construct the finite-difference approximation to \\(\\partial_x \\sin(x)\\). Since we already know the symbolic derivative—it is \\(\\partial_x \\sin(x) = \\cos(x)\\)—there is no genuinely practical purpose for this demonstration. Still, it can serve to confirm the symbolic rule.\nWe will call the finite-difference approximation fd_sine() and use makeFun() to construct it:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNotice that fd_sine() has a parameter, h whose default value is being set to 0.01. Whether 0.01 is “small” or not depends on the context. Operationally, we define “small” to be a value that gives practically the same result even if it is made smaller by a factor of 2 or 10.\nAs a demonstration that fd_sine() with \\(h=0.01\\) approximates the genuine \\(\\partial_x \\sin(x)\\), we exploit our knowledge that \\(\\partial_x \\sin(x) = \\cos(x)\\). Figure 20.2 plots out the difference between the \\(h=0.01\\) approximation and the genuine derivative.\n\n\n\n\n\n\n\n\nfd_sine() itself\n\n\n\n\n\n\n\nError from true value.\n\n\n\n\n\n\nFigure 20.2: Subtracting out the (known) symbolic derivative from the finite-difference derivative. Black: \\(h=0.01\\); Magenta: \\(h=0.001\\).\n\n\n\nComparing fd_sine() to the symbolic \\(\\partial_x \\sin(x)\\) for two values of \\(h\\): \\(h=0.01\\) and \\(h=0.001\\). :::\nA glance at Figure 20.2(a) shows that the finite-difference derivative for \\(h=0.01\\) (black) is essentially the same as for \\(h=0.001\\). Thus, \\(h=0.01\\) is good enough. Figure 20.2(b) highlights this, showing the error between the finite-difference derivative and the (known) symbolic derivative \\(\\cos(x)\\). You will need to look carefully at the vertical axis scales in Figure 20.2 to see what’s happening. For \\(h=0.01\\), fd_sine() is not exactly the same as cos(), but it is close, always being within $$0.000017. For many purposes, this would be accurate enough. But not for all purposes. We can make the approximation better by using a smaller \\(h\\). For instance, the \\(h=0.001\\) version of fd_sine() is one-hundred times more accurate: to within $$0.00000017.\nIn practical use, one employs the finite-difference method in those cases where one does not already know the exact derivative function. This would be the case, for example, if the function is a sound wave recorded in the form of an MP3 audio file.\nIn such situations, a practical way to determine what is a small \\(h\\) is to pick one based on your understanding of the situation. For example, much of what we perceive of sound involves mixtures of sinusoids with periods longer than one-two-thousandth of a second, so you might start with \\(h\\) of 0.002 seconds. Use this guess about \\(h\\) to construct a candidate finite-difference approximation. Then, construct another candidate using a smaller h, say, 0.0002 seconds. If the two candidates are a close match to one another, then you have confirmed that your choice of \\(h\\) is adequate.\nIt is tempting to think that the approximation gets better and better as h is made even smaller. But that is not necessarily true for computer calculations. The reason is that quantities on the computer have only a limited precision: about 15 digits. To illustrate, let’s calculate a simple quantity, \\((\\sqrt{3})^2 - 3\\). Mathematically, this quantity is exactly zero. However, on the computer, as in Active R chunk 20.4, it is not quite zero:\n\n\n\nActive R chunk 20.4\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nWe can see this loss of precision at work if we make h very small in the finite-difference approximation to \\(\\partial_x \\sin(x)\\). In Figure 20.3 we are using h = 0.000000000001. The result is unsatisfactory.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 20.3: In computer calculations, using too small an h (\\(h=0.0000000000001\\), magenta compared to (\\(h=0.000000000001\\), black) leads to a loss of accuracy in the finite-difference approximation due to machine round-off error.\n\n\n\nEvanescent \\(h\\) doesn’t produce reliable results in the machine arithmetic for finite-difference derivatives, which involves subtracting two almost-identical values!",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Constructing derivatives</span>"
    ]
  },
  {
    "objectID": "Differentiation/20-computing.html#second-and-higher-order-derivatives",
    "href": "Differentiation/20-computing.html#second-and-higher-order-derivatives",
    "title": "20  Constructing derivatives",
    "section": "20.4 Second and higher-order derivatives",
    "text": "20.4 Second and higher-order derivatives\nMany applications call for differentiating a derivative or even differentiating the derivative of a derivative. In English, such phrases are hard to read. They are much simpler using mathematical notation.\n\n\\(f(x)\\) a function\n\\(\\partial_x f(x)\\) the derivative of \\(f(x)\\)\n\\(\\partial_x \\partial_x f(x)\\), the second derivative of \\(f(x)\\), usually written even more concisely as \\(\\partial_{xx}f f(x)\\).\n\nThere are third-order derivatives, fourth-order, and on up, although they are not often used.\nTo compute a second-order derivative \\(\\partial_{xx} f(x)\\), first differentiate \\(f(x)\\) to produce \\(\\partial_x f(x)\\). Then, still using the techniques described earlier in this chapter, differentiate \\(\\partial_x f(x)\\).\nThere is a shortcut for constructing high-order derivatives using D() in a single step. On the right-hand side of the tilde expression, list the with-respect-to name repeatedly. For instance, look at the functions produced in the following active R chunks:\n\nThe second derivative \\(\\partial_{xx} \\sin(x)\\):\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nThe third derivative \\(\\partial_{xxx} \\ln(x)\\):\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nApplication area 20.1  \n\n\n\n\n\n\n\nApplication area 20.1 Constant acceleration\n\n\n\nPhysics students learn a formula for the position of an object in free fall dropped from a height \\(x_0\\) and at an initial velocity \\(v_0\\): \\[ x(t) \\equiv -\\frac{1}{2} g t^2 + v_0 t + x_0\\ .\\] The acceleration of the object is the second derivative \\(\\partial_{tt} x(t)\\). Use D() to find the object’s acceleration.\nThe second derivative of \\(x(t)\\) with respect to \\(t\\) is calculated in Active R chunk 20.5. Run that code.\n\n\n\nActive R chunk 20.5\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nThe acceleration does not depend on \\(t\\); it is the constant \\(g\\). No wonder \\(g\\) is called “gravitational acceleration.”",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Constructing derivatives</span>"
    ]
  },
  {
    "objectID": "Differentiation/20-computing.html#footnotes",
    "href": "Differentiation/20-computing.html#footnotes",
    "title": "20  Constructing derivatives",
    "section": "",
    "text": "There are functions where the derivative cannot be meaningfully defined. Examples are the absolute-value function or the Heaviside function which we introduced when discussing piecewise functions in Section 9.4. Chapter 22 considers the pathological cases and shows how to spot them at a glance.↩︎",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Constructing derivatives</span>"
    ]
  },
  {
    "objectID": "Differentiation/21-concavity.html",
    "href": "Differentiation/21-concavity.html",
    "title": "21  Concavity and curvature",
    "section": "",
    "text": "21.1 Quantifying concavity and curvature\nIt is an easy visual task to discern the slope of a line segment. A glance shows whether the slope at that point is positive or negative. Comparing the slopes at two locales is also an automatic visual task: most people have little difficulty saying which slope is steeper. One consequence of this visual ability: it is easy to recognize whether a line that touches the graph at a point is tangent to the graph.\nThere are other aspects of functions, introduced in Section 6.2, that are also readily discerned from a glance at the function graph.\nIt often happens in building models that the modeler (you!) knows something about the concavity or the curvature of a function. For example, concavity is essential in classical economics; the curve for supply as a function of price is concave down while the curve for demand as a function of price is concave up. For a train, car, or plane, sideways forces depend on the curvature of the track, road, or trajectory. Road designers need to calculate the curvature to know if the road is safe at the indicated speed.\nIt turns out that quantifying these properties of functions or shapes is naturally done by calculating second derivatives.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Concavity and curvature</span>"
    ]
  },
  {
    "objectID": "Differentiation/21-concavity.html#quantifying-concavity-and-curvature",
    "href": "Differentiation/21-concavity.html#quantifying-concavity-and-curvature",
    "title": "21  Concavity and curvature",
    "section": "",
    "text": "Application area 21.1 —Designing a highway curve\n\n\n\n\n\n\n\nApplication area 21.1 Designing a highway curve\n\n\n\nImagine designing a highway. Due to the terrain, part of the road is oriented east-west and another part north-south. Those two parts need to be connected together for vehicles to use the road! (In math-speak, we might say that the road has to be continuous, but this is just common sense.)\nExperience with highways shows that the connection will be a smooth curve. If the curve is part of a circle, the design needs to specify the radius of curvature. Too tight a radius and the traffic will not be able to handle the centrifugal force; vehicles will drift or skid off the road. A big radius provides safety, but making the radius bigger than required adds road construction costs.\nReal-world highway on- and off-ramps are usually not precisely sections of a circle, so specifying the shape of the ramp is not as simple as setting the radius of the curve. Instead, the radius changes at the entry and exit of the curve. The American Association of State Highway and Transportation Officials Policy on Geometric Design of Highways and Streets (1994) explains why:\nAny motor vehicle follows a transition path as it enters or leaves a circular horizontal curve. The steering change and the consequent gain or loss of centrifugal force cannot be effected instantly. For most curves the average driver can effect a suitable transition path within the limits of normal lane width. However, with combinations of high speed and sharp curvature the resultant longer transition can result in crowding and sometimes actual occupation of adjoining lanes. In such instances transition curves would be appropriate because they make it easier for a driver to confine the vehicle to his or her own lane. The employment of transition curves between tangents and sharp circular curves and between circular curves of substantially different radii warrants consideration.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Concavity and curvature</span>"
    ]
  },
  {
    "objectID": "Differentiation/21-concavity.html#sec-concavity-deriv",
    "href": "Differentiation/21-concavity.html#sec-concavity-deriv",
    "title": "21  Concavity and curvature",
    "section": "21.2 Concavity",
    "text": "21.2 Concavity\nRecall that to find the slope of a function \\(f(x)\\) at any input \\(x\\), you compute the derivative of that function, which we’ve been writing \\(\\partial_x\\,f(x)\\). Plug in some value for the input \\(x\\) and the output of \\(\\partial_x\\, f(x)\\) will be the slope of \\(f(x)\\) at that input. (Chapter 20 introduced some techniques for computing the derivative of any given function.)\nNow we want to show how differentiation can quantify the concavity of a function. First, remember that when we speak of the “derivative” of a function, we mean the first derivative of the function. That full name naturally suggests that there will be a second derivative, a third derivative, and higher-order derivatives.\nAs introduced in Section 6.2, the concavity of a function describes not the slope but the change in the slope. Figure 21.1 depicts the slope of the function in each of the three boxes labelled A, B, and C. In the subdomain marked A, the function slope is positive, while in the subdomain B, the function slope is negative. This transition from the slope at A to the slope at B corresponds to the negative concavity of the function between A and B.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 21.1: Concavity is about how the slope changes from one place in the domain to another.\n\n\n\nSimilarly, the function’s concavity in the interval B to C reflects the transition in the instantaneous slope at B to the different instantaneous slope at C.\nLet’s look at this using symbolic notation. Keep in mind that the function graphed is \\(f(x)\\) while the slope is the function \\(\\partial_x\\,f(x)\\). We’ve seen that the concavity is indicated by the change in slope of \\(f()\\), that is, the change in \\(\\partial_x\\, f(x)\\). We will go back to our standard way of describing the rate of change near an input \\(x\\):\n\\[\\text{concavity.of.f}(x) \\equiv\\ \\text{rate of change in}\\ \\partial_x\\, f(x) = \\] \\[= \\partial_x \\left(\\strut\\partial_x f(x)\\right)\n= \\lim_{h\\rightarrow 0}\\frac{\\partial_x f(x+h) - \\partial_x f(x)}{h}\\] We are defining the concavity of a function \\(f()\\) at any input \\(x\\) to be \\(\\partial_x \\left(\\strut\\partial_x f(x)\\right)\\). We create the concavity_of_f(x) function by applying differentiation twice to the function \\(f()\\).\nSuch a double differentiation of a function \\(f(x)\\) is called the second derivative of \\(f(x)\\). The second derivative is so important in applications that it has its own compact notation: \\[\\text{second derivative of}\\ f()\\ \\text{is written}\\ \\partial_{xx} f(x)\\] Look carefully to see the difference between the first derivative \\(\\partial_x f(x)\\) and the second derivative \\(\\partial_{xx} f(x)\\): it is all in the double subscript \\(_{xx}\\).\nComputing the second derivative is merely a matter of computing the first derivative \\(\\partial_x f(x)\\) and then computing the (first) derivative of \\(\\partial_x f(x)\\). In R this process looks like:\n\ndx_f  &lt;- D(   f(x) ~ x)   # First deriv. of f()\ndxx_f &lt;- D(dx_f(x) ~ x)   # Second deriv. of f()\n\n\n\n\n\n\n\nA shortcut for second derivatives\n\n\n\nA notation shortcut for the two-step process above: double up on the x on the right-hand side of the tilde:\ndxx_f &lt;- D(f(x) ~ x & x)\nRead the ~ x & x as “differentiate by \\(x\\) and then once more by \\(x\\) again.”",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Concavity and curvature</span>"
    ]
  },
  {
    "objectID": "Differentiation/21-concavity.html#sec-curvature-definition",
    "href": "Differentiation/21-concavity.html#sec-curvature-definition",
    "title": "21  Concavity and curvature",
    "section": "21.3 Curvature",
    "text": "21.3 Curvature\nAs you see from Section 21.2, it is easy to quantify the concavity of a function \\(f(x)\\): just evaluate the second derivative \\(\\partial_{xx} f(x)\\). However, it turns out that people cannot do a good job of estimating the quantitative value of concavity by eye.\nTo illustrate, consider the square function, \\(f(x) \\equiv x^2\\). (See Figure 21.2.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 21.2: Does the concavity of the square function vary with \\(x\\)?\n\n\n\nThe square function is concave up. Now a test: Looking at the graph of the square function, where is the concavity the largest? Don’t read on until you’ve pointed where you think the concavity is largest.\n\n \nHave you decided where the concavity is largest?\n \n\nWith your answer to the test question in mind, we can calculate the concavity of the square function using derivatives.\n\\[f(x) \\equiv x^2\\ \\text{      so     }\\\n\\partial_x f(x) = 2 x\\ \\text{     and therefore     }\\ \\partial_{xx} f(x) = 2\\]\nThe second derivative of \\(f(x)\\) is positive, as expected for a function that is concave up. Surprisingly, however, the second derivative is constant.\nThe concavity-related property that the human eye reads from the function graph is not the concavity itself but the curvature of the function. The curvature of \\(f(x)\\) at \\(x_0\\) is defined to be the radius of the circle tangent to the function at \\(x_0\\).\nFigure 21.3 illustrates the changing curvature of \\(f(x) \\equiv x^2\\) by inscribing tangent circles at several points on the function graph, marked with dots. That the function’s thin black line goes right down the middle of the broader lines used to draw the circles shows the tangency of the circle to the function graph.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 21.3: At any point on the graph of a smooth function, there is a circle tangent to the graph. The radius of this circle is \\(1/{\\cal K}\\).\n\n\n\nBlack dots along the graph at the points indicate where the function graph is tangent to the inscribed circle. The visual sign of tangency is that the function graph goes right down the circle’s center.\nThe inscribed circle at \\(x=0\\) is tightest, the circle at \\(x=1\\) larger and the radius of the circle at \\(x=-1.5\\) is the largest of all. Whereas the concavity is the same at all points on the graph, the visual impression that the function is most highly curved near \\(x=0\\) is better captured by the radius of the inscribed circle. The radius of the inscribed circle at any point is the reciprocal of a quantity \\({\\cal K}\\) called the curvature.\nThe curvature \\({\\cal K}\\) of a function \\(f(x)\\) depends on both the first and second derivative. The formula for curvature \\(K\\) is somewhat off-putting; you are not expected to memorize it. But you can see where \\(\\partial x f()\\) and \\(\\partial_{xx}f()\\) come into play.\n\\[{\\cal K}_f(x)  \\equiv \\frac{\\left|\\partial_{xx} f(x)\\right|}{\\ \\ \\ \\ \\left|1 + \\left[\\strut\\partial_x f(x)\\right]^2\\right|^{3/2}}\\]\nMathematically, the curvature function \\(\\cal K()\\) corresponds to the reciprocal of the radius of the tangent circle. When the tangent circle is tight, the output of \\(\\cal K()\\) is large. When radius of the tangent circle is large, that is, when the function is very close to approximating a straight line, the output of \\(\\cal K()\\) is very small.\n\n\n\n\n\n\n\n\nTry it! 21.1\n\n\n\n\n\n\n\n\n\nTry it! 21.1 Calculate the curvature\n\n\n\nCalculate the curvature of the square function: \\(f(x) \\equiv x^2\\). Active R chunk 21.1 will get you started by defining some of the functions needed:\n\n\n\n\nActive R chunk 21.1\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nIs the curvature constant as a function of \\(x\\)?\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe formula for \\(K()\\) is\nabs(dx_f(x))/abs(1 + (dxx_f(x)^2))^(2/3).\n\n\n\n\n\n\nApplication area 21.2 —Centripetal force on the highway\n\n\n\n\n\n\n\nApplication area 21.2 Back to the highway\n\n\n\nReturning to the highway design example in Application area 21.1 … The Policy on geometric design of highways and streets called for the curvature of a road to change gently, giving the driver time to adjust the steering and accommodate the centrifugal force of the car going around the curve.\nChanging curvature implies that \\(\\partial_x {\\cal K}\\) is non-zero. Since \\({\\cal K}\\) depends on the first and second derivatives of \\(f(x)\\), the Policy on gradual change means that the third derivative of \\(f(x)\\) is non-zero.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Concavity and curvature</span>"
    ]
  },
  {
    "objectID": "Differentiation/22-cont-and-smooth.html",
    "href": "Differentiation/22-cont-and-smooth.html",
    "title": "22  Continuity and smoothness",
    "section": "",
    "text": "22.1 Continuity\nAs introduced in Chapter 1, a space is a set of possibilities. For instance, for the function \\(f()\\), the input space—that is, the set of all possible inputs—is the domain of the function. We have focused on spaces that are sets of numbers, which we have denoted as, say, \\(x \\in [-3,3]\\) in mathematical notation or domain(x = -3:3) in R/mosaic notation. Of course, when we write \\(x \\in [-3,3]\\) or domain(x = -3:3) we mean the space of \\(x\\) is the set of all the numbers from -3 to 3.\nYou’ve been learning about sets since early in school. A child learning about numbers starts with the “counting numbers”: \\(1, 2, 3, \\ldots\\). In primary school, the set of numbers is extended to include zero and the negative numbers: \\(-1,-2,-3, \\ldots\\), giving a set called the “integers.” Counting numbers and integers are discrete sets. Between two consecutive members of the counting numbers or the integers, there is not another number of the set.\nThe next step in a child’s mathematical education is the “rational numbers,” that is, numbers that are written as a ratio: \\(\\frac{1}{2}, \\frac{1}{3}, \\frac{2}{3}, \\ldots, \\frac{22}{7}\\), and so on. Rational numbers fit in the spaces between the integers. That is, between any two integers, even consecutive ones, there is a rational number. For instance, the rational number \\(\\frac{1}{2}\\) falls between 0 and 1.\nBetween any two rational numbers, there is another rational number, indeed there is an infinite number of rational numbers. For instance, between \\(\\frac{1}{2}\\) and \\(\\frac{2}{3}\\) is \\(\\frac{6}{11}\\) (and many others, such as \\(\\frac{7}{11}\\) or \\(\\frac{13}{21}\\)). It is useful to think of rational numbers as fitting in the spaces between integers.\nIf you didn’t stumble on the word “spaces” in the previous sentence, you are well on your way to understanding what is meant by “continuous.” For instance, between any two rational numbers there is another rational number. Think of rational numbers as stepping stones that provide a path from any number to any other number.\nIt is a deep question whether the rational numbers are a walkway instead of isolated stepping stones? A walkway is a structure on which you can move any amount, no matter how small, without risk of going off the structure. In contrast, a too-small move along a path of stepping stones will put you in the water.\nA continuous set is like a walkway; however little you move from an element of the set you will still be on the set. The continuous set of numbers is often called the number line, although a more formal name is the real numbers. (“Real” is an unfortunate choice of words, but we are stuck with it.)\nThe underlying metaphor here is space. Between any two points in space, there is another point in space. We will be working with several different kinds of spaces, for instance:\nThe specialty of calculus is describing relationships between continuous sets. Functions such as \\(\\sin()\\) or \\(\\text{line}()\\), which are typical of the functions we study in calculus, take numbers as input and produce a number as output. That is, the functions describe the relationship between an input space and an output space.\nIn this chapter, we will elaborate on continuity, one of the ideas introduced in Chapter 6, and use the concept of continuity to characterize functions in a new way: their smoothness.\nThe intuition behind continuity is simple: If you can draw the graph of a function without lifting the pencil from the paper, the function is continuous.\nContinuity can be an important attribute of a modeling function. Often, we expect that a small change in input produces a small change in output. For instance, if your income changes by one penny, you would expect your lifestyle not to change by much. If the temperature of an oven changes by 1 degree, you don’t expect the quality of the cake you are baking to change in any noticeable way.\nAll of our basic modeling functions are continuous over their entire input domain. (Exception: the domain of the function \\(1/x\\) is the whole number line, except 0, where the positive and negative branches fail to meet up.) To illustrate discontinuity we will consider piecewise functions, as introduced in Section 9.4. The Heaviside function, graphed in Figure 22.2 is discontinuous.\nDrawing the graph of the Heaviside function \\(H(x)\\) involves lifting the pencil at \\(x=0\\).\nIn contrast, the piecewise ramp function (Figure 22.3) is continuous; you don’t need to lift the pencil from the paper to draw the ramp function.\nImagine that you were constructing a model of plant growth as a function of the amount of water (in cc) provided each day. The plant needs about 20 cc of water to thrive. You use the Heaviside function for the model, say \\(H(W-20)\\), where an output of 1 means the plant thrives and a output 0 means the plant does not. The model implies that with 20.001 cc of water, the plant will thrive. But providing only 19.999 cc of water, the plant will die. In other words, a very small change in the input can lead to a large change in the output.\nCommon sense suggests that a change of 0.002 cc in the amount of water—a small fraction of a drop, 2 cubic millimeters of volume—is not going to lead to a qualitative change in output. So you might prefer to use a sigmoid function as your model rather than a Heaviside function.\nOn the other hand, sometimes a very small change in input does lead to a large change in output. For instance, a sensible model of the hardness of water as a function of temperature would include a discontinuity at \\(32^\\circ\\)F, the temperature at which water turns to ice.\nOne of author Charles Dickens’s famous characters described the relationship between income, expenditure, and happiness this way:\nMacawber referred to the common situation in pre-20th century England of putting debtors in prison, regardless of the size of their debt. Macawber’s statement suggests he would model happiness as a Heaviside function \\(H(\\text{income}- \\text{expenditure})\\).\nWhenever the output of a function is a binary (yes-or-no) value, you can anticipate that a model will involve a discontinuous function.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Continuity and smoothness</span>"
    ]
  },
  {
    "objectID": "Differentiation/22-cont-and-smooth.html#continuity",
    "href": "Differentiation/22-cont-and-smooth.html#continuity",
    "title": "22  Continuity and smoothness",
    "section": "",
    "text": "Figure 22.2: The Heaviside function is piecewise constant with a discontiuity at \\(x=0\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 22.3: The ramp function is a continuous piecewise function.\n\n\n\n\n\n\n\n\n“Annual income 20 pounds, annual expenditure 19 [pounds] 19 [shillings] and six [pence], result happiness. Annual income 20 pounds, annual expenditure 20 pounds ought and six, result misery.” — the character Wilkins Micawber in David Copperfield",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Continuity and smoothness</span>"
    ]
  },
  {
    "objectID": "Differentiation/22-cont-and-smooth.html#discontinuity",
    "href": "Differentiation/22-cont-and-smooth.html#discontinuity",
    "title": "22  Continuity and smoothness",
    "section": "22.2 Discontinuity",
    "text": "22.2 Discontinuity\nRecall the logical path that led us to the idea of the derivative of a function. We started with the differencing operator, which takes as input a function and a “small” value of \\(h\\): \\[{\\cal D}_x f(x) \\equiv \\frac{f(x+h) - f(x)}{h}\\] Then, through algebraic manipulation and numerical experiments we found that, once \\(h\\) is small enough, the graph of the slope function \\({\\cal D}_x f(x)\\) does not depend on \\(h\\). And so we defined a function \\(\\partial_x f(x)\\) where \\(h\\) does not play a role, writing \\(\\lim_{h\\rightarrow 0}\\) to remember our care to never divide by zero. \\[\\partial_x f(x) \\equiv \\lim_{h\\rightarrow 0} \\frac{f(x+h) - f(x)}{h}\\ .\\] Conveniently, we found that the derivatives of the pattern-book functions can be written using the pattern-book functions without making any reference to \\(h\\). For instance:\n\n\\(\\partial_x \\ln(x) = 1/x\\) No \\(h\\) appears.\n\\(\\partial_x e^x = e^x\\) No \\(h\\) appears\n\\(\\partial_x x^p = p\\, x^{p-1}\\) No \\(h\\) appears.\nand so on.\n\nWith discontinuous functions, we have no such luck. Figure 22.4 shows what happens if we compute \\({\\cal D}_x H(x)\\), the derivative of the Heaviside function, for smaller and smaller \\(h\\).\n\nCode\nH &lt;- makeFun(ifelse(x &gt;=0, 1, 0) ~ x)\nDH01   &lt;- makeFun((H(x + 0.1) - H(x))/0.1 ~ x)\nDH001  &lt;- makeFun((H(x + 0.01) - H(x))/0.01 ~ x)\nDH0001 &lt;- makeFun((H(x + 0.001) - H(x))/0.001 ~ x)\nslice_plot(DH01(x) ~ x, bounds(x=-0.02:0.02),\n           npts=500, color=\"red\", size=2) %&gt;%\n  slice_plot(DH001(x) ~ x,\n           color=\"darkgreen\", npts=500, size=3, alpha=0.5) %&gt;%\n  slice_plot(DH0001(x) ~ x,\n           color=\"blue\", npts=500, alpha=0.5, size=2) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 22.4: \\({\\cal D}_x H(x)\\), the slope function of the discontinuous Heaviside, function, depends on the value of \\(h\\) used for the slope function. (Red: \\(h=0.1\\); Green: \\(h=0.01\\); Blue \\(h=0.001\\))\n\n\n\nDifferencing the Heaviside function produces very different functions depending on the value of \\(h\\). The bump near \\(x=0\\) gets taller and taller as \\(h\\) gets smaller. Mathematicians would describe this situation as \\[\\lim_{h\\rightarrow0}{\\cal D}_x H(x=0) \\equiv \\lim_{h\\rightarrow 0} \\frac{H(0+h) - H(0)}{h}\\ \\ \\ \\text{does not exist}.\\] Of course, for any given value of \\(h\\), e.g. \\(h=0.000001\\), the function \\({\\cal D}_x H(x)\\) has a definite shape. But that shape keeps changing as \\(h \\rightarrow 0\\), so we cannot point to any specific shape as the “limit as \\(h \\rightarrow 0\\).”\nSince there is no convergence in the shape of \\({\\cal D}_x H(0)\\) as \\(h\\) gets smaller, it is fair to say that the Heaviside function does not have a derivative at \\(x=0\\). But away from \\(x=0\\), the Heaviside function has a perfectly sensible derivative: \\(\\partial_x H(x) = 0\\) for \\(x\\neq 0\\). But there is no derivative at \\(x=0\\).",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Continuity and smoothness</span>"
    ]
  },
  {
    "objectID": "Differentiation/22-cont-and-smooth.html#smoothness",
    "href": "Differentiation/22-cont-and-smooth.html#smoothness",
    "title": "22  Continuity and smoothness",
    "section": "22.3 Smoothness",
    "text": "22.3 Smoothness\nSmoothness is a different concept than continuity, although the two are related. Most simply, any discontinuous function is not smooth at any input where a discontinuity occurs. But even the continuous ramp function is not smooth at the start of the ramp. Intuitively, imagine you were sliding your hand along the ramp function. You would feel the crease at \\(x=0\\).\nA function is not smooth if the derivative of that function is discontinuous. For instance, the derivative of the ramp function is the Heaviside function, so the ramp is not smooth at \\(x=0\\).\nAll of our basic modeling functions are smooth everywhere in their domain. In particular, the derivatives of the basic modeling functions are continuous, as are the second derivative, third derivative, and so on down the line. Such functions are called C-infinity, written \\(C^\\infty\\). The superscript \\(\\infty\\) means that every order of derivative is continuous.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 22.5: A function whose derivative is the ramp function (hence continous) and whose second derivative is the Heaviside function (discontinous). Since the first derivative is continuous, this function has \\(C^1\\) smoothness.\n\n\n\nYou cannot tell from Figure 22.5 that the second derivative is discontinuous. But if you were in a plane flying along that trajectory, you would feel a jerk as you crossed \\(x=0\\).\nMathematicians quantify the “smoothness” of a function by looking at the function’s continuity and the continuity of its derivatives. Smoothness is assessed on a scale \\(C^0, C^1, C^2, \\ldots, C^\\infty\\).\n\n\\(C^0\\): the function \\(f()\\) is continuous. Intuitively, this means that the function’s graph can be drawn without lifting the pencil from the paper.\n\\(C^1\\): the function \\(f()\\) has a derivative over its entire domain and that derivative \\(\\partial_x f(x)\\) is continuous. (See Figure 22.5 for an example.)\n\\(C^2\\): the function \\(\\partial_x f(x)\\) has a derivative over its entire domain and that derivative is continuous. In other words, \\(\\partial_{xx} f(x)\\) exists and is continuous.\n\\(C^n\\): Like \\(C^2\\), but we are talking about the \\(n\\)th-derivative of \\(f(x)\\) existing and being continuous.\n\\(C^\\infty\\): Usually when we denote a sequence with an infinite number of terms, we write down something like \\(C^0, C^1, C^2, \\ldots\\). It would be entirely valid to do this in talking about the \\(C^n\\) sequence. But many of the mathematical functions we work with are infinitely differentiable, that is \\(C^\\infty\\).\n\nExamples of \\(C^\\infty\\) functions:\n\n\\(\\sin(x)\\): the derivatives are \\(\\partial_x \\sin(x) = \\cos(x)\\), \\(\\partial_{xx} \\sin(x) = -\\sin(x)\\), \\(\\partial_{xxx} \\sin(x) =-\\cos(x)\\), \\(\\partial_{xxxx} \\sin(x) =\\sin(x)\\), … You can keep going infinitely.\n\\(e^x\\): the derivatives are \\(\\partial_x e^x = e^x\\), \\(\\partial_{xx} e^x = e^x\\), and so on.\n\\(x^2\\): the derivatives are \\(\\partial_x x^2 = 2 x\\), \\(\\partial_{xx} x^2 = 2\\), \\(\\partial_{xxx} x^2 = 0\\), … Higher order derivatives are all simply 0. Boring, but still existing.\n\nExample of non-\\(C^2\\) functions: We see these often when we take two or more different \\(C^\\infty\\) functions and split their domain, using one function for one subdomain and the other(s) for other subbounds(s).\n\n\\(|x|\\), the absolute value function. \\(|x|\\) is a pasting together of two \\(C^\\infty\\) functions: \\[|x| \\equiv \\left\\{\\begin{array}{rcl}+x & \\text{for} & 0 \\leq x\\\\-x&\\text{for}& \\text{otherwise}\\end{array} \\right.\\ .\\] The domain is split at \\(x=0\\).",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Continuity and smoothness</span>"
    ]
  },
  {
    "objectID": "Differentiation/22-cont-and-smooth.html#practical-smoothness-is-different.",
    "href": "Differentiation/22-cont-and-smooth.html#practical-smoothness-is-different.",
    "title": "22  Continuity and smoothness",
    "section": "22.4 Practical smoothness is different.",
    "text": "22.4 Practical smoothness is different.\nFor engineering and design problems, smoothness means something substantially different than described by the mathematical concepts above. Chapter 49 introduces cubic splines which are continuous functions defined by a finite set of coordinate pairs: two variables of a data frame. Each line of the data frame specifies a “knot point.” The spline consists of ordinary cubic polynomials drawn piecewise between consecutive knot points. At a knot point, the cubics on either side have been arranged to have their first and second derivatives match. Thus, the first two derivatives are continuous. The function is at least \\(C^2\\). The second derivative of a cubic is a straight-line function, so the second derivative of a cubic spline is a series of straight-line functions connected at the knot points. The second derivative does not itself have a derivative at the knot points. So, a cubic spline cannot satisfy the requirements for \\(C^3\\); it is \\(C^2\\).",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Continuity and smoothness</span>"
    ]
  },
  {
    "objectID": "Differentiation/23-rules.html",
    "href": "Differentiation/23-rules.html",
    "title": "23  Derivatives of assembled functions (optional)",
    "section": "",
    "text": "23.1 Using the rules\nChapter 19 used the rules associated with evanescent \\(h\\), that is, \\(\\lim_{h\\rightarrow 0}\\), to confirm our claims about the derivatives of many of the pattern-book functions. We will call these rules h-theory for short. This chapter will use h-theory to find algebraic rules to calculate the derivatives of linear combinations of functions, products of functions, and composition of functions. Remarkably, we can figure out these rules without specifying which functions are being combined. So the rules can be written abstractly using the pronouns \\(f()\\), \\(g()\\), and \\(h()\\). Later, we will apply those rules to specific functions, to show how the rules are used in practical work.\nWhen you encounter a function that you want to differentiate, you first have to examine the function to decide which rule you want to apply. In the following, we will to use the names \\(f()\\) and \\(g()\\), but in practice the functions will often be basic modeling functions, for instance \\(e^{kx}\\) or \\(\\sin\\left(\\frac{2\\pi}{P}t\\right)\\), etc.\nStep 1: Identify f() and g()\nWe will write the rules using two function pronouns, \\(f()\\) and \\(g()\\), which can stand for any functions whatsoever. It is rare to see the product or the composition written explicitly as \\(f(x)g(x)\\) of \\(f(g(x))\\). Instead, you are given something like \\(e^x \\ln(x)\\). The first step in differentiating the product or composition is to identify what are \\(f()\\) and \\(g()\\) individually.\nIn general, \\(f()\\) and \\(g()\\) might be complicated functions, themselves involving linear combinations, products, and composition. But to get started, we will practice with cases where they are simple, pattern-book functions.\nStep 2: Find f’() and g’()\nFor differentiating either products or compositions, you will need to identify both \\(f()\\) and \\(g()\\) (the first step) and then compute the derivatives \\(\\partial_x f()\\) and \\(\\partial_x g()\\). That is, you will write down four functions.\nStep 3: Apply the relevant rule\nRecall from Chapter 9 that will will be working with three important forms for creating new functions out of existing functions:",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Derivatives of assembled functions (optional)</span>"
    ]
  },
  {
    "objectID": "Differentiation/23-rules.html#sec-using-the-rules",
    "href": "Differentiation/23-rules.html#sec-using-the-rules",
    "title": "23  Derivatives of assembled functions (optional)",
    "section": "",
    "text": "Linear combinations, e.g. \\(a f(x) + bg(x)\\)\nProducts of functions, e.g. \\(f(x) g(x)\\)\nCompositions of functions, e.g. \\(f\\left(g(x)\\right)\\)",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Derivatives of assembled functions (optional)</span>"
    ]
  },
  {
    "objectID": "Differentiation/23-rules.html#differentiating-linear-combinations",
    "href": "Differentiation/23-rules.html#differentiating-linear-combinations",
    "title": "23  Derivatives of assembled functions (optional)",
    "section": "23.2 Differentiating linear combinations",
    "text": "23.2 Differentiating linear combinations\nLinear combination is one of the ways in which we make new functions from existing functions. As you recall, linear combination involves scaling functions and then adding the scaled functions as in \\(a f(x) + b g(x)\\): a linear combination of \\(f(x)\\) and \\(g(x)\\). We can easily use \\(h\\)-theory to show what is the result of differentiating a linear combination of functions. First, let’s figure out what is \\(\\partial_x\\, a f(x)\\), Going back to writing \\(\\partial_x\\) as a slope function: \\[\\partial_x\\, a\\,f(x) = \\frac{a\\, f(x + h) - a\\,f(x)}{h}\\\\\n\\ \\\\\n= a \\frac{f(x+h) - f(x)}{h} = a\\, \\partial_x f(x)\\] In other words, if we know the derivative \\(\\partial_x\\, f(x)\\), we can easily find the derivative of \\(a\\, f()\\). Notice that even though \\(h\\) was used in the derivation, it appears nowhere in the result \\(\\partial_x\\, b\\,f(x) = b\\, \\partial_x\\, f(x)\\). The \\(h\\) is solvent to get the paint on the wall and evaporates once its job is done.\nNow consider the derivative of the sum of two functions, \\(f(x)\\) and \\(g(x)\\): \\[\\begin{eqnarray}\n\\partial_x\\, \\left(f(x) + g(x)\\right) & =\\frac{\\left(f(x + h) + g(x + h)\\right) - \\left(f(x) + g(x)\\right)}{h} \\\\\n\\ \\\\\n&= \\frac{\\left(f(x+h) -f(x)\\right) + \\left(g(x+h) - g(x)\\right)}{h}\\\\\n\\ \\\\\n&= \\frac{\\left(f(x+h) -f(x)\\right)}{h} + \\frac{\\left(g(x+h) - g(x)\\right)}{h}\\\\\n\\ \\\\\n&= \\partial_x\\, f(x) + \\partial_x\\, g(x)\n\\end{eqnarray}\\]\nBecause of how \\(\\partial_x\\) can be “passed through” a linear combination, mathematicians say that differentiation is a linear operator. Consider this new fact about differentiation as a down payment on what will eventually become a complete theory telling us how to differentiate a product of two functions or the composition of two functions. We will lay out the \\(h\\)-theory based algebra of this in the next two sections.\nWe can summarize the h-theory result for linear combinations this way:\n\nThe derivative of a linear combination is the linear combination of the derivatives.\n\nThat is:\n\\[\\partial_x \\left({\\large\\strut} \\color{orange}{a} \\color{black}{f(x)} + \\color{orange}{b} \\color{black}{g(x)}\\right) = \\color{orange}{a} {\\color{brown}{f'(x)}} + \\color{orange}{b} \\color{brown}{g'(x)}\\] as well as\n\\[\\partial_x \\left({\\large\\strut} \\color{orange}{a}\\, \\color{black}{f(x)} + \\color{orange}{b}\\, \\color{black}{g(x)}  + \\color{orange}{c}\\, \\color{black}{h(x)} + \\cdots\\right) = \\color{orange}{a}\\, {\\color{brown}{f'(x)}} + \\color{orange}{b}\\, {\\color{brown}{g'(x)}} + \\color{orange}{c}\\, {\\color{brown}{h'(x)}} + \\cdots\\]\n\n\n\n\n\n\nTip 23.1: Derivatives of polynomials are polynomials\n\n\n\nThe derivative of a polynomial is a polynomial of a lower order. We can demonstrate this:\nConsider the polynomial \\[h(x) = \\color{orange}{a}\\color{brown}{x^0}  + \\color{orange}{b} \\color{brown}{x^1} + \\color{orange}{c} \\color{brown}{x^2}\\] The derivative is \\[\\partial_x h(x) = \\color{orange}{a}\\,\\color{brown}{0}\\,   + \\color{orange}{b}\\,\\color{brown}{1}   + \\color{orange}{c}\\, \\color{brown}{2 x} = \\color{orange}{b} +  \\color{brown}{2}\\,\\color{orange}{c}\\,\\color{brown}{x}\\ .\\]",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Derivatives of assembled functions (optional)</span>"
    ]
  },
  {
    "objectID": "Differentiation/23-rules.html#product-rule-for-multiplied-functions",
    "href": "Differentiation/23-rules.html#product-rule-for-multiplied-functions",
    "title": "23  Derivatives of assembled functions (optional)",
    "section": "23.3 Product rule for multiplied functions",
    "text": "23.3 Product rule for multiplied functions\nThe question at hand is how to compute the derivative \\(\\partial_x f(x) g(x)\\). Of course, you can always use numerical differentiation. But let’s look at the problem from the point of view of symbolic differentiation. And since \\(f(x)\\) and \\(g(x)\\) are just pronoun functions, we will assume you are starting out already knowing the derivatives \\(\\partial_x f(x)\\) and \\(\\partial_x g(x)\\).\nThis situation arises particularly when \\(f(x)\\) and \\(g(x)\\) are pattern-book functions for which you already have memorized \\(\\partial_x f(x)\\) and \\(\\partial_x g(x)\\) or are basic modeling functions whose derivatives you will memorize in Section @ref(basic-derivs).\nThe purpose of this section is to derive the formula for \\(\\partial_x f(x) g(x)\\) using \\(f(x)\\), \\(g(x)\\), \\(\\partial_x f(x)\\) and \\(\\partial_x g(x)\\). This formula is called the product rule. The point of showing a derivation of the product rule is to let you see how the logic of evanescent \\(h\\) plays a role. In practice, everyone simply memorizes the rule, which has a beautiful, symmetric form:\n\\[\\text{Product rule:}\\ \\ \\ \\ \\partial_x \\left(\\strut f(x)g(x)\\right) = \\left(\\strut \\partial_x f(x)\\right)\\, g(x) + f(x)\\, \\left(\\strut\\partial_x g(x)\\right)\\] and is even prettier in Lagrange notation (where \\(\\partial_x f(x)\\) is written \\(f'\\)): \\[ \\left(\\strut f g\\right)' = f' g + g' f\\]\nAs with all derivatives, the product rule is based on slope function (Section 18.3). Symbolic derivatives also invoke evanescent \\(h\\) (Chapter 19). \\[F'(x) \\equiv \\lim_{h\\rightarrow 0} \\frac{F(x+h) - F(x)}{h}\\]\nWe also need two other statements about \\(h\\) and functions:\n\nThe derivative \\(F'(x)\\) is the slope of of \\(F()\\) at input \\(x\\). Taking a step of size \\(h\\) from \\(x\\) will induce a change of output of \\(h F'(x)\\), so \\[F(x+h) = F(x) + h F'(x)\\ .\\]\nAny result of the form \\(h F(x)\\), where \\(F(x)\\) is finite, gives 0. More precisely, \\(\\lim_{h\\rightarrow 0} h F(x) = 0\\).\n\nAs before, we will put the standard \\(\\lim_{h\\rightarrow 0}\\) disclaimer against dividing by \\(h\\) until there are no such divisions at all, at which point we can safely use the equality \\(h = 0\\).\nSuppose the function \\(F(x) \\equiv f(x) g(x)\\), a product of the two functions \\(f(x)\\) and \\(g(x)\\).\n\\[F'(x) = \\partial_x \\left(\\strut f(x) g(x) \\right) \\equiv \\lim_{h\\rightarrow 0}\\frac{f(x+h) g(x+h) - f(x) g(x)}{h}\\] We will replace \\(g(x + h)\\) with its equivalent \\(g(x) + h g'(x)\\) giving\n\\[= \\lim_{h\\rightarrow 0} \\frac{f(x+h) \\left(\\strut g(x) + h g'(x) \\right) - f(x) g(x)}{h} \\] \\(g(x)\\) appears in both terms in the numerator, once multiplied by \\(f(x+h)\\) and once by \\(f(x)\\). Collecting those terms give:\n\\[=\\lim_{h\\rightarrow 0}\\frac{\\left(\\strut f(x+ h) - f(x)\\right)  g(x) + \\left(\\strut f(x+h) h\\, g'(x)\\right)}{h}\\] This has two bracketed terms added together over a common denominator. Let’s split them into separate terms:\n\\[=\\lim_{h\\rightarrow 0}\\underbrace{\\left(\\strut \\frac{f(x+h) - f(x)}{h}\\right)}_{f'(x)} g(x) + \\lim_{h\\rightarrow 0}\\frac{\\left(\\strut f(x) + h f'(x)\\right)h\\,g'(x)}{h}\\]\nNotice that the second term has an \\(h\\) both in the numerator and the denominator. \\(h\\)-theory tells us that \\(\\lim_{h\\rightarrow 0} h/h = 1\\).\nThe first term is \\(g(x)\\) multiplied by the familiar form for the derivative of \\(f(x)\\) \\[= f'(x) g(x) + \\lim_{h\\rightarrow 0}\\frac{f(x) h g'(x)}{h} + \\lim_{h\\rightarrow 0}\\frac{h f'(x) h g'(x)}{h}\\] In each of the last two terms there is an \\(h/h\\) involved. This is safely set to 1, since the \\(\\lim_{h\\rightarrow 0}\\) implies that \\(h\\) will not be exactly zero. There remain no divisions by \\(h\\) so we can drop the \\(\\lim_{h\\rightarrow 0}\\) in favor of \\(h=0\\): \\[= f'(x) g(x) + f(x) g'(x) + \\cancel{h f'(x) g'(x)}\\]\n\\[=f'(x) g(x) + g'(x) f(x)\\]\nThe last step relies on statement (2) above.\nSome people find it easier to read the rule in Lagrange shorthand, where \\(f\\) and \\(g\\) stand for \\(f(x)\\) and \\(g(x)\\) respectivly, and \\(f'\\) (“f-prime”) and \\(g'\\) (“g-prime”) stand for \\(\\partial f()\\) and \\(\\partial g()\\).\n\\[\\text{Lagrange shorthand:}\\ \\   \\partial(\\color{magenta}f \\times \\color{brown}g) = (\\color{magenta}f \\times \\color{brown}g)' = \\color{magenta}{f'}\\color{brown}g + \\color{brown}{g'}\\color{magenta}f\\]\n\n\n\n\n\n\nTip 23.2\n\n\n\n\n\n\n\n\n\n\n\n\nTip 23.2 How about multiplying three functions?\n\n\n\nOccasionally, mathematics gives us a situation where being more general produces simplicity.\nIn the case of function products, the generalization is from products of two functions \\(f(x)\\cdot g(x)\\) to products of more than two functions, e.g. \\(u(x) \\cdot v(x) \\cdot w(x)\\).\nThe chain rule here takes a form that makes the overall structure much clearer:\n\\[\\begin{eqnarray}\n\\partial_x \\left(\\strut u(x) \\cdot v(x) \\cdot w(x)\\right) = \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\\\\n\\color{blue}{\\partial_x u(x)} \\cdot v(x) \\cdot w(x)\\ + \\\\\nu(x) \\cdot \\color{blue}{\\partial_x v(x)} \\cdot w(x)\\ + \\\\\nu(x) \\cdot v(x) \\cdot \\color{blue}{\\partial_x w(x)}\\ \\  \\ \\\n\\end{eqnarray}\\]\\end{eqnarray}\nIn the Lagrange shorthand, the pattern is even more evident: \\[\\left( u\\cdot v\\cdot w\\right)' = \\color{blue}{u'}\\cdot v\\cdot w\\ +\\ u\\cdot \\color{blue}{v'}\\cdot w\\ +\\ u\\cdot v\\cdot \\color{blue}{w}'\\]\nAs an example, consider the derivative of \\(x^3\\) with respect to \\(x\\). Obviously, \\(x^3 \\equiv x \\cdot  x \\cdot  x\\), a product of three simple functions:\n\\[\\begin{array}{ccc}\\partial_x x^3 = \\partial_x [x \\cdot x \\cdot x]\\ \\ \\ & = \\\\\n& [\\partial_x x] &x&x& +\\\\\n& x & [\\partial_x x] & x&  +\\\\\n&x & x &[\\partial_x x]\n\\end{array}\\]\nSince \\(\\partial_x x = 1\\) this collapses to \\(\\partial_x x^3 = 3 x^2\\).",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Derivatives of assembled functions (optional)</span>"
    ]
  },
  {
    "objectID": "Differentiation/23-rules.html#chain-rule-for-function-composition",
    "href": "Differentiation/23-rules.html#chain-rule-for-function-composition",
    "title": "23  Derivatives of assembled functions (optional)",
    "section": "23.4 Chain rule for function composition",
    "text": "23.4 Chain rule for function composition\nA function composition, as described in Section 9.2, involves inserting the output of one function (the “interior function”) as the input of the other function (the “exterior function”). As we so often do, we will be using pronouns a lot. A list might help keep things straight:\n\nThere are two functions involved in a composition. Generically, we call them \\(f(y)\\) and \\(g(x)\\). In the composition \\(f(g(x))\\), the exterior function is \\(f()\\) and the interior function is \\(g()\\).\nEach of the two functions \\(f()\\) and \\(g()\\) has an input. In our examples, we use \\(y\\) to stand for the input to the exterior function and \\(x\\) for the input to the interior function.\nAs with all rules for differentiation, we will need to compute the derivatives of the functions involved, each with respect to its own input. So these will be \\(\\partial_y f(y)\\) and \\(\\partial_x g(x)\\).\n\nA reason to use different pronouns for the inputs to \\(f()\\) and \\(g()\\) is to remind us that the output \\(g(x)\\) is in general not the same kind of quantity as the input \\(x\\). In a function composition, the \\(f()\\) function will take the output \\(g(x)\\) as input. But since \\(g(x)\\) is not necessarily the same kind of thing as \\(x\\), why would we want to use the same name for the input to \\(f()\\) as we use for the input to \\(g()\\).\nWith this distinction between the names of the inputs, we can be even more explicit about the composition, writing \\(f(y=g(x))\\) instead of \\(f(g(x))\\). Had we used the pronound \\(x\\) for the input to \\(f()\\) but our explicit statement, although technically correct, would be confusing: \\(f(x = g(x))\\)!\nWith all these pronouns in mind, here is the chain rule for the derivative \\(\\partial_x f(g(x))\\):\n\\[\\partial_x \\left({\\large\\strut} f\\left(\\strut{\\color{magenta}{g(x)}}\\right)\\right) =\\color{brown}{\\partial_y f({\\color{magenta}{g(x)}})} \\times \\color{brown}{\\partial_x g(x)}\\]\nOr, using the Lagrange prime notation, where \\('\\) stands for the derivative of a function with respect to its input, we have \\[\\text{Lagrange shorthand:}\\ \\   \\left(\\color{black}f({\\color{magenta}g)}\\right)' = \\color{brown}{f'} (\\color{magenta}{g}) \\times \\color{blue}{g}'\\]\n\n\n\n\n\n\nTip 23.3: Calculating \\(\\partial_x \\ln(x)\\)\n\n\n\nThe chain rule can be used in a clever way to find a formula for \\(\\partial_x \\ln(x)\\).\nWe’ve already seen that the logarithm is the inverse function to the exponential, and vice versa. That is: \\[e^{\\ln(y)} = y \\ \\ \\ \\text{and}\\ \\ \\ \\ln(e^y) = x\\] Since \\(\\ln(e^y)\\) is the same function as \\(y\\), the derivative \\(\\partial_y \\ln(e^y) = \\partial_y y = 1\\).\nLet’s differentiate the second form using the chain rule: \\[\\partial_y \\ln(e^y) = \\left(\\partial_y \\ln\\right)(e^y)\\, e^x = 1\\] giving \\[\\left(\\partial_y \\ln\\right](e^y) = \\frac{1}{e^y} = \\recip(e^y)\\] Whatever the function \\(\\partial_x \\ln()\\) might be, it takes its input and produces as output the reciprocal of that input. In other words: \\[\\partial_x \\ln(x) = \\frac{1}{x}\\ .\\]\n\n\n\nApplication area 23.1  \n\n\n\n\n\n\n\nApplication area 23.1 Logarithms and Rates per time\n\n\n\nIn news and policy discussions, you will often hear about “inflation rate” or “birth rate” or “interest rate” or “investment rate of return.” In each case, there is a function of time combined with a derivative of that function: with the general form \\[\\frac{\\partial_t g(t)}{g(t)}\\ .\\] In other words, the “rate” is the size of the change with time (\\(\\partial_t f(t)\\)) divided by the size of the whole (\\(f(t)\\) at that time.\n\nInflation rate: The function \\(f(t)\\) is cost_of_living(\\(t\\)).\nPopulation growth rate: The function is population(\\(t\\)).\nInterest rate: The function is account_balance(\\(t\\)).\nInvestment returns: The function is net_worth(\\(t\\)).\n\nIn all these cases, The “rate” is not merely “per time” as would be the case for \\(\\partial_t g(t)\\). Instead the rate is “per unit of the whole per time.” For population growth rate, the “whole” is the population. Such rates involving people are often stated with the phrase is “per capita per year.” (The Latin “per capita” translates to “by head.” Its modern sense is “per unit of population.” Of course, the “unit of population” is a person.)\nNotice the two uses of “per” in the phrase: “births per capita per year.” A proportional rate is two rates in one. Births per capita is a proportion of the population. Births per year is an average rate with respect to time. But “births per capita per year” is a rate in the proportion with respect to time.\nThe rate word “per” also appears as part of “percent,” which literally means “per hundred.” A “percentage change” is the amount of change divided by the base amount. Confusingly, perhaps, “percentage change” is often truncated to the shorter “percent.” This is the case with inflation rates, interest rates, and rates of return on investment. The interest rate on a credit-card debt is stated as a proportion of the current debt; all that is packed into the word “percent.” The interest rate itself is the “proportion of the current debt per year”: two rates in one.\nSimilarly for an inflation rate. “Inflation” is stated as the change in prices divided by the current price: a proportional change. “Inflation rate” is the proportional change per unit of time, where the “whole” is current prices and the rate is change in current prices per year divided by current prices.\nThanks to the chain rule, there is a shortcut way of writing proportional rates per time. Exactly equivalent to the ratio \\(\\frac{\\partial_t f(t)}{f(t)}\\) is \\[\\partial_t \\ln(f(t))\\ .\\]\nDerivatives of logarithms appear often in fields such as economics or finance, where it is common to consider the logarithm of the economic quantity to render changes as percent of the whole.\nFor instance, consider Figure 23.1 which shows the cumulative number of COVID during a period in 2020, early in the pandemic.\n\n\n\n\n\n\nFigure 23.1: Growth in the number of Coronavirus cases in Italy and the US early in the pandemic. Source Note that linear axes are used in the left graph and semi-log axes in the right graph.\n\n\n\nThe two panels in Figure 23.1 show the same data about growing numbers of coronavirus cases, the left graph on linear axes, the right on the now-familiar semi-log axes.\nMost people are excellent at comparing slopes, even if they find it difficult or tedious to quantify a slope with a number and units. For instance, a glance suffices to show that in the left graph, well through mid-March the red curve (Italy) is steeper on any given date than the blue curve (US). Correspondingly, the number of people with coronavirus was growing faster (per day) in Italy.\nThe right graph tells a different story: up until about March 1, the Italian cases were increasing faster than the US cases. Afterwards, the US sees a larger growth rate than Italy until, around March 19, the US growth rate is substantially larger than the Italy growth rate.\nThe previous two paragraphs and their corresponding graphs seem to contradict one another. But they are both accurate, truthful depictions of the same events. What’s different between the two graphs is that the left shows one kind of rate and the right shows another kind of rate. In the left, the slope is new-cases-per-day, the output of the derivative function\nleft graph: \\(\\ \\ \\ \\  \\partial\\_t\\, \\text{daily\\_new\\_cases}(t)\\).\nOn the right, the slope is the proportional increase in cases per day, that is,\nright graph: \\(\\ \\ \\ \\ \\frac{\\partial_t\\, \\text{daily\\_new\\_cases}(t)}{\\text{daily\\_new\\_cases}(t)}\\).\nFrom the chain rule, we know that\n\\[\\partial_t \\left(\\strut\\ln\\left(\\strut(f(t)\\right)\\right) = \\frac{\\partial_t f(t)}{f(t)}\\ .\\]\nSince the right graph is on semi-log axes, the slope we perceive visually is \\(\\partial_t \\left(\\strut\\ln(f(t))\\right)\\). That is an obscure-looking bunch of notation until the chain rule reveals it to be the rate of change in the number of covid cases at time \\(t\\) divided by the number of cases at time \\(t\\).\n\n\n\n\n\n\n\n\nTip 23.4: Proof for the chain rule (optional)\n\n\n\nThe derivation of the chain rule relies on two closely related statements which are expressions of the idea that near any value \\(x\\) a function can be expressed as a linear approximation with the slope equal to the derivative of the function :\n\n\\(g(x + h) = g(x) + h g'(x)\\)\n\\(f(y + \\epsilon) = f(y) + \\epsilon f'(y)\\), which is the same thing as (1) but uses \\(y\\) as the argument name and \\(\\epsilon\\) to stand for the small quantity we usually write with an \\(h\\).\n\nWe will now look at \\(\\partial_x f\\left({\\large\\strut} g(x)\\right)\\) by writing down the fundamental definition of the derivative. This, of course, involves the disclaimer \\(\\lim_{h\\rightarrow 0}\\) until we are sure that there is no division by \\(h\\) involved.\n\\[\\partial_x \\left({\\large\\strut} f\\left(\\strut\\strut g(x)\\right)\\right)  \\equiv \\lim_{h\\rightarrow 0}\\frac{\\color{magenta}{f(g(x+h))} - f(g(x))}{h}\\]\nLet’s examine closely the expression \\(\\color{magenta}{f\\left(\\strut g(x+h)\\right)}\\). Applying rule (1) above turns it into \\[\\lim_{h\\rightarrow 0} f\\left(\\strut g(x) + \\color{blue}{h g'(x)}\\right)\\] Now apply rule (2) but substituting in \\(g(x)\\) for \\(y\\) and \\(\\color{blue}{h g'(x)}\\) for \\(\\epsilon\\), giving\n\\[\\lim_{h\\rightarrow 0} \\color{magenta}{f\\left(\\strut g(x+h)\\right)} = \\lim_{h\\rightarrow 0} \\color{brown}{\\left({\\large\\strut} f\\left(g(x)\\right) + \\color{blue}{h g'(x)}f'\\left(g(x)\\right)\\right)}\\] We will substitute the \\(\\color{blue}{blue}\\) and \\(\\color{brown}{brown}\\) expression for the \\(\\color{magenta}{magenta}\\) expression in \\[\\partial_x f\\left(\\strut g(x)\\right)  \\equiv \\lim_{h\\rightarrow 0}\\frac{\\color{magenta}{f(g(x+h))} - f(g(x))}{h}\\] giving \\[\\partial_x f\\left(\\strut g(x)\\right)  \\equiv \\lim_{h\\rightarrow 0}\\frac{\\color{brown}{f\\left(g(x)\\right) + \\color{blue}{h g'(x)}f'\\left(g(x)\\right)} - f\\left(g(x)\\right)}{h}\\] In the denominator, \\(f\\left(g(x)\\right)\\) appears twice and cancels itself out. That leaves a single term with an \\(h\\) in the numerator and an \\(h\\) in the denominator. Those \\(h\\)’s cancel out, at the same time obviating the need for \\(\\lim_{h\\rightarrow 0}\\) and leaving us with the chain rule: \\[\\partial_x f\\left(\\strut g(x)\\right)  \\equiv \\lim_{h\\rightarrow 0}\\frac{\\color{brown}{ \\color{blue}{h g'(x)} f'\\left(g(x)\\right)}}{h} = f'\\left(g(x)\\right)\\ g'(x)\\]\n\n\n\n\n\n\n\n\nTip 23.5: Derivative of the logarithmic and power-law functions\n\n\n\nKnowing that \\(\\partial_x \\ln(x) = 1/x\\) and the chain rule, we are in a position to demonstrate the power-law rule \\(\\partial_x x^p = p\\, x^{p-1}\\). The key is to use the identity \\(e^{\\ln(x)} = x\\).\n\\[\\partial_x x^p = \\partial_x \\left(e^{\\ln(x)}\\right)^p\\] The rules of exponents allow us to recognize \\[\\left(e^{\\ln(x)}\\right)^p = e^{p \\ln(x)}\\] Thus, \\(x^p\\) can be seen as a composition of the exponential function onto the logarithm function.\nApplying the chain rule to this composition gives \\[\\partial_x e^{p \\ln(x)} = e^{p\\ln(x)}\\partial_x [p \\ln(x)] =\ne^{p\\ln(x)} \\frac{p}{x}\\ .\\] Of course, we already know that \\(e^{p \\ln(x)} = x^p\\), so we have \\[\\partial_x x^p = x^p \\frac{p}{x} = p x^{p-1}\\ .\\]",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Derivatives of assembled functions (optional)</span>"
    ]
  },
  {
    "objectID": "Differentiation/23-rules.html#sec-basic-derivs",
    "href": "Differentiation/23-rules.html#sec-basic-derivs",
    "title": "23  Derivatives of assembled functions (optional)",
    "section": "23.5 Derivatives of the basic modeling functions",
    "text": "23.5 Derivatives of the basic modeling functions\nThe basic modeling functions are the same as the pattern-book functions, but with bare \\(x\\) replaced by \\(\\line(x)\\). In other words, each of the basic modeling functions is a composition of the corresponding pattern-book function with \\(\\line(x)\\). Consequently, the derivatives of the basic modeling functions can be found using the chain rule.\nSuppose \\(f()\\) is one of our pattern-book functions. Then \\[\\partial_x f(\\color{magenta}{ax + b}) = \\color{blue}{a} \\color{brown}{f'(\\color{magenta}{ax + b})}\\] where \\(\\color{blue}{a}\\) is the derivative with respect to \\(x\\) of \\(\\color{magenta}{ax + b}\\).\nHere are the steps for differentiating a basic modeling function \\(\\color{brown}{f}(\\color{magenta}{a x + b})\\) where \\(f()\\) is one of the pattern-book functions:\n\nStep 1: Identify the particular pattern-book function \\(\\color{brown}{f}()\\) and write down its derivative \\(\\color{brown}{f'}\\). For example, if \\(f()\\) is \\(\\sin()\\), then \\(f'()\\) is \\(\\cos()\\).\nStep 2: Find the derivative of the linear interior function. If the function is \\(\\color{magenta}{ax + b}\\), then the derivative is \\(\\color{blue}{a}\\). If the interior function is \\(\\color{magenta}{\\frac{2\\pi}{P}(t-t_0)}\\), the derivative is \\(\\color{blue}{\\frac{2 \\pi}{P}}\\).\nStep 3: Write down the original function \\({f(\\color{magenta}{a x + b})}\\) but replace \\(f()\\) with \\(\\large \\color{brown}{f'()}\\) and pre-multiply by the derivative of the interior function. For instance, \\[\\partial_x f(\\color{magenta}{ax + b}) = {\\large \\color{magenta}{a}}{\\large f'}(\\color{magenta}{ax + b})\\] Another example: \\[\\partial_t \\color{brown}{\\sin}\\left(\\color{magenta}{\\frac{2 \\pi}{P}(t-t_0)} \\right) = {\\color{magenta}{\\frac{2 \\pi}{P}}}\\color{brown}{\\cos}\\left(\\color{magenta}{\\frac{2 \\pi}{P}(t-t_0) }\\right) \\]\n\nThe rule for the derivative of any basic modeling function \\(f(\\line(x))\\) is \\[\\partial_x f(\\line(x)) = \\partial_x \\line(x) \\times \\partial_x f\\left(\\strut\\line(x)\\right)\\]\nTo illustrate, we’ll write the exterior function in black, and its derivative in brown. \\(\\color{magenta}{\\line()}\\) The interior function is drawn in magenta and its derivative in blue. For example:\n\\[f(\\color{magenta}{g(x)})' = \\color{brown}{f'}(\\color{blue}{g(x)})\\ \\color{magenta}{g'(x)}\\]\n\n\n\n\n\n\nTip 23.6\n\n\n\nBy convention, there are different ways of writing \\(\\color{magenta}{\\line()}\\) when constructing the different basic modeling functions from the corresponding pattern-book functions. Regretably, this imposes extra cognitive load on the student. The point here is that all of the basic modeling functions are a patter-book function composed with a \\(\\color{magenta}{\\line()}\\) function, regardless of the form that \\(\\color{magenta}{\\line()}\\) is turned into when obeying convention. In Table 23.1 we have used magenta to help you see the different forms.\n\n\n\nTable 23.1: Various conventions for writing \\(\\color{magenta}{\\line()}\\) in the basic modeling functions.\n\n\n\n\n\n\n\n\n\n\nPattern-book function\n\\(\\longrightarrow\\)\nBasic modeling function\n\n\n\n\n\\(\\sin(x)\\)\n\n\\(\\sin\\left(\\strut {\\color{magenta}{\\frac{2 \\pi}{P} \\left( x-x_0 \\right)}}\\right)\\)\n\n\n\\(\\exp(x)\\)\n\n\\(\\color{magenta}{A}\\color{black}{\\exp\\left(\\strut{\\color{magenta}{k x}}\\right)}\\)\n\n\n\\(x^2\\)\n\n\\(\\color{\\black}{\\left(\\strut {\\color{magenta}{mx + b}}\\right)^2}\\)\n\n\n\\(1/x\\)\n\n\\(\\color{black}{1/\\left(\\strut {\\color{magenta}{mx+ b}}\\right)}\\)\n\n\n\\(\\ln(x)\\)\n\n\\(\\color{black}{\\ln\\left(\\strut {\\color{magenta}{a x + b}}\\right)}\\)\n\n\n\\(\\dnorm(x)\\)\n\n\\(\\color{black}{\\dnorm(\\strut {\\color{magenta}{\\frac{x - \\text{mean}}{\\text{sd}}}})}\\)\n\n\n\\(\\pnorm(x)\\)\n\n\\(\\color{black}{\\pnorm(\\strut {\\color{magenta}{\\frac{x - \\text{mean}}{\\text{sd}}}})}\\)\n\n\n\n\n\n\n\n\nThe consequences of the various forms of \\(\\color{magenta}{\\line()}\\) are a possibly confusing diversity of forms for derivatives of the basic modeling functions. We ill continue with the color convention of writing \\(f()\\) in black, \\(\\color{brown}{f'()}\\) in brown, and \\(\\partial_x \\color{magenta}{\\line()}\\) in blue.\n\n\\(\\partial_x e^{\\color{magenta}{kx}} = {\\color{blue}{k}}\\, \\color{brown}{e^{\\color{magenta}{kx}}}\\)\n\\(\\partial_x \\sin(\\color{magenta}{\\frac{2\\pi}{P} (x-x_0)}) = \\color{blue}{\\frac{2\\pi}{P}}  \\color{brown}{\\cos\\left(\\strut{\\color{magenta}{\\frac{2\\pi}{P}  (x-x_0)}}\\right)}\\)\n\\(\\partial_x (\\color{magenta}{mx + b})^2 = \\color{brown}{2}\\color{blue}{m}\\color{brown}{\\left(\\strut{\\color{magenta}{m x + b}}\\right)}\\).\n\\(\\partial_x \\text{reciprocal}({\\color{magenta}{mx + b}}) = \\partial_x \\frac{1}{{\\color{magenta}{mx + b}}} =  \\color{brown}{\\frac{\\color{blue}{m}}{\\left(\\strut{\\color{magenta}{mx+b}}\\right)^2}}\\)\n\\(\\partial_x \\ln(a x + b) = \\color{brown}{{\\frac{\\color{magenta}{\\color{blue}{a}}}{{\\color{magenta}{ax+b}}}}}\\)\n\\(\\partial_x \\pnorm(\\color{magenta}{\\frac{x-\\text{mean}}{\\text{sd}}}) =\\color{blue}{\\frac{1}{\\text{sd}}}\\color{brown}{\\dnorm\\left(\\strut{\\color{magenta}{\\frac{x-\\text{mean}}{\\text{sd}}}}\\right)}\\).\n\\(\\partial_x \\dnorm\\left(\\color{magenta}{\\frac{x-\\text{mean}}{\\text{sd}}}\\right)=\\color{brown}{-(x - \\text{mean})}\\color{blue}{\\frac{1}{\\text{sd}}}\\color{brown}{\\dnorm\\left(\\strut{\\color{magenta}{\\frac{x-\\text{mean}}{\\text{sd}}}}\\right)}\\)\n\nYou will be using the derivatives of the basic modeling functions so often, that you should practice and practice until you can write the derivative at a glance.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Derivatives of assembled functions (optional)</span>"
    ]
  },
  {
    "objectID": "Differentiation/24-optim.html",
    "href": "Differentiation/24-optim.html",
    "title": "24  Optimization",
    "section": "",
    "text": "24.1 Structure of the problem\nTo “optimize” means to make something as good as possible with the available resources. Optimization problems are common in science, logistics, industry, and any other area where one seeks the best solution to a problem. Some everyday examples:\nIn an optimization problem, there is one or more input quantities whose value you have to choose. The amount of salt; the years to wait from planting to harvesting a tree; the angle of the trail with respect to the slope. We will call this the decision quantity.\nSimilarly, there is one or more output quantity that you value and want to make as good as possible. The taste of the stew; the amount of usable wood harvested; the time it takes to walk up the hill. The output quantities are called the objectives.\nThis chapter deals with optimization problems that involve only a single objective. Problems with multiple objectives are among the most interesting and important in real-world decision making. Single-objective optimization techniques are a component of the more complex decision making, but they are a good place to get started.\nThe model that relates inputs to the objective output is the objective function. Solving an optimization problem—once the modeling phase is complete—amounts to finding a value for the decision quantity (the input to the objective function) that produces the best output from the objective function.\nSometimes the objective is something that you want to minimize, make as small as possible. For instance, in the hiking trail problem, we seek to minimize the time it takes to walk up the trail. Sometimes you want to maximize the objective, as in the wood-harvest problem where the objective is to harvest the most wood per year.\nMathematically, maximization and minimization are the same thing. Every minimization problem can be turned into a maximization problem by putting a negative sign in front of the objective function. To simplify the discussion, in talking about finding the solution to an optimization problem we will imagine that the goal is to maximize. But keep in mind that many circumstances in the real world, “best” can mean minimization. For example, when fitting a function to data, the parameters are selected that minimize the residuals.\nRecall from Section 6.7 that there are two components to the task of maximization or minimization. The argmax is the input to the objective function which produces the largest output. The maximum is the value of that output.1 Argmin and minimum are the words used in a situation where you seek the smallest value of the objective function.\nOnce you have found the argmax you can plug that value into the objective function to find the output value. That value is the maximum.\nTo illustrate the setup of an optimization problem, imagine yourself in the situation of a contest to see who can shoot a tennis ball the farthest into a field with a slingshot. During the contest, you will adjust the vertical angle of launch, place the ball into the slingshot’s cradle, pull back as far as possible, and let go. To win the contest, you need to optimize how you launch the ball.\nThe objective is to maximize the distance traveled by the ball. The objective function models the distance travelled as a function of the quantities you can control, for instance the vertical angle of launch or the amount by which you pull back the slingshot. For simplicity, we will imagine that the slingshot is pulled back by a standard amount, producing a velocity of the ball at release of \\(v_0\\). You will win or lose based on the angle of launch you choose.\nBefore you head out into the field to experiment, let’s prepare by modeling the objective function. Using some principles of physics and mathematics (which you may not yet understand) the distance flown by the ball (horizontally) will be a function of the angle of launch \\(\\theta\\) and the initial velocity \\(v_0\\).\nThe mathematics of such problems involves an area called differential equations, an important part of calculus which we will come to later in the course. Since you don’t have the tools yet, we will just state a simple model of how long the ball stays in the air. \\[\\text{duration}(v_0, \\theta) = 2 \\frac{v_0}{g} \\sin(\\theta)\\] \\(g\\) is the acceleration due to gravity, which is about \\(9.8 \\text{m}\\text{s}^{-2}\\), assuming that the contest is being held on Earth.\nThe horizontal distance travelled by the tennis ball will be \\[\\text{distance}(v_0, \\theta) = \\cos(\\theta) v_0\\times \\text{duration}(v_0, \\theta) = 2 \\frac{v_0^2}{g} \\cos(\\theta)\\sin(\\theta) / g\\] Our objective function is distance(), and we seek to find the argmax. Distance() has two inputs, but let’s assume there is only one decision quantity \\(\\theta\\) and that \\(v_0\\) is fixed. Figure 24.1 is drawn for $v_0 = $ cm/sec.\nFinding the argmax can be accomplished simply by plotting the function \\(\\text{distance}(v_0, \\theta)\\) with 5061 cm/sec.\nIn the simple model of a tennis ball launched at an angle \\(\\theta\\) from the horizontal, the distance travelled is \\[\\text{distance}(\\theta) \\equiv 2 \\frac{v_0^2}{g}\\, \\cos(\\theta)\\sin(\\theta)\\].\nFigure 24.1 shows the objective function \\(\\text{distance}(\\theta)\\). Be careful when interpreting the graph. At first glance, you may think the graph shows the parabolic trajectory of the tennis ball. It does not. Instead, the function tells how far (in cm) the tennis ball flew before hitting the ground. Notice that the distance flown is zero for a launch angle of 90\\(^\\circ\\). If you aim the ball straight up, it will come down in the same place it was launched.\nYou can see that the maximum distance is about 2600 cm and that this occurs at an argmax \\(\\theta^\\star\\) that is a bit less than 50\\(^\\circ\\).\nZooming in on the \\(\\theta\\) axis (Figure 24.1 (a)) let’s you find the argmax with more precision: the argmax is \\(\\theta^\\star = 45^\\circ\\).",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "Differentiation/24-optim.html#structure-of-the-problem",
    "href": "Differentiation/24-optim.html#structure-of-the-problem",
    "title": "24  Optimization",
    "section": "",
    "text": "Definitions…\n\n\n\nThis section introduces several important terms relating to optimization. Become fluent with all of them. This will greatly improve your ability to master the topic.\n\ndecision quantity\nobjectives\nobjective function\nminimize and maximize\nminimum and maximum\nargmax and argmin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nPeople often talk about “finding the maximum.” This is misleading. Instead, the idea is to find the input to the objective function—that is, the argmax—that produces the maximum output.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom 0 to 90 degrees\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Zooming in around the argmax: 40 to 50 degrees\n\n\n\n\n\n\n\n\nFigure 24.1: Zooming in on the argmax of the objective function. It is important to look at the scale of the vertical axis. Any value of \\(\\theta\\) between about 40 and 50 degrees gives a close approximation to the maximum.\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting the argmax\n\n\n\nThe graphical solution given to the slingshot problem is entirely satisfactory. Whether that solution will win the contest depends on whether the model we built for the objective function is correct. We have left out, for instance, air resistance, which is potentially important.\nSolving the optimization problem has prepared us to test the result in the field. Perhaps we will find that the real-world optimum angle is somewhat steeper or shallower than \\(\\theta = 45^\\circ\\).\nBesides the argmax, another important quantity to read from the graph in Figure 24.1 is the precision of the argmax. In strict mathematical terms, the argmax for the tennis-ball problem is exactly 45 degrees at which point \\(\\cos(\\theta)\\sin(\\theta) = 0.5\\). Suppose, however, that the ball were launched at only 40 degrees. Five degrees difference is apparent to the eye, but the result will be essentially the same as for 45 degrees: \\(\\cos(\\theta)\\sin(\\theta) = 0.492\\). The same is true for a launch angle of 50 degrees. For both “sub-optimal” launch angles, the output is within 2 percent of the 45-degree result. It is easy to imagine that a factor outside the scope of the simple model—the wind, for instance—could change the result by as much or more than 2 percent, so a practical report of the argmax should reasonable be “40 to 50 degrees” rather than “exactly 45 degrees.”\nContests are won or lost by margins of less than 1%, so you should not casually deviate from the argmax. On the other hand, \\(45^\\circ\\) is the argmax of the model. Reality may deviate from the model. For instance, suppose that air resistance or wind might might affect distance by 1%. That is. the real-world result might deviate by as much as 1% of the model value. If so, we shouldn’t expect the real-world argmax to be any closer to 45\\(^\\circ\\) than \\(\\pm 5^\\circ\\); anywhere in that domain interval generates an output that is within 1% of the maximum output for the model.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "Differentiation/24-optim.html#derivatives-and-optimization",
    "href": "Differentiation/24-optim.html#derivatives-and-optimization",
    "title": "24  Optimization",
    "section": "24.2 Derivatives and optimization",
    "text": "24.2 Derivatives and optimization\nLet’s now reframe the search for the argmax, stating it in the language of derivatives of the objective function with respect to the decision quantity (\\(\\theta\\) in the slingshot problem). For a function with one input, this will not be an improvement from the look-at-the-graph technique to find the argmax. However, a genuine reason to use derivatives is to set us up in the future to solve problems with more than one input, where it is hard to draw or interpret a graph. Also, describing functions in the language of derivatives can help us think more clearly about aspects of the problem, such as the precision of the argmax.\nWith a graph such as Figure 24.1, it is easy to find the argmax; common sense carries the day. So it won’t be obvious at first why we will take the following approach:\nLet’s denote an argmax of the objective function \\(f(x)\\) by \\(x^\\star\\). Let’s look at the derivative \\(\\partial_x f(x)\\) in the neighborhood of \\(x^\\star\\). Referring to Figure 24.1, where \\(x^\\star = 45^\\circ\\), you may be able to see that \\(\\partial_x f(x^\\star)\\) is zero; the line tangent to the function’s graph at \\(x^\\star\\) is horizontal.\nSeen another way, the slope of \\(f(x)\\) to the left of \\(x^\\star\\) is positive. Moving a tiny bit to the right (that is, increasing \\(x\\) by a very small amount) increases the output \\(f(x)\\). On the other hand, just to the right of \\(x^\\star\\), the slope of \\(f(x)\\) is negative; as you reach the top of a hill and continue on, you will be going downhill. So the derivative function is positive on one side of \\(x^\\star\\) and negative on the other, suggesting that it crosses zero at the argmax.\nCommon sense is correct: Walk uphill to get to the peak, walk downhill to move away from the peak. At the top of a smooth hill, the terrain is level. (Since our modeling functions are smooth, so must be the hills that we visualize the functions with.)\nInputs \\(x^\\star\\) such that \\(\\partial_x f(x^\\star) = 0\\) are called critical points. Why not call them simply argmaxes? Because a the slope will also be zero at an argmin. And it is even possible to have the slope be zero at a point that is neither an argmin or an argmax.\nAt this point, we know that values \\(x^\\star\\) that give \\(\\partial_x f(x^\\star) = 0\\) are “critical points,” but we haven’t said how to figure out whether a given critical point is an argmax, an argmin, or neither. This is where the behavior of \\(\\partial_x f(x)\\) near \\(x=x^\\star\\) is important. If \\(x^\\star\\) is an argmax, then \\(\\partial_x f(x)\\) will be positive to the left of \\(x^\\star\\) and negative to the right of \\(x^\\star\\); walk up the hill to get to \\(x^\\star\\), at the top the hill is flat, and just past the top the hill has a negative slope.\nFor an argmin, changing \\(x\\) from less than \\(x^\\star\\) to greater than \\(x^\\star\\); you will be walking down into the valley, then level at the very bottom \\(x=x^\\star\\), then back up the other side of the valley after you pass \\(x=x^\\star\\). Figure 24.2 shows the situation.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Objective function\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Objective function\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Derivative of objective function\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) Derivative of objective function\n\n\n\n\n\n\n\n\nFigure 24.2: Very close to an argmax or argmin (Panels a and b) objective functions often look like a parabola. Similarly, the derivative of the objective function (Panels c and d) are a straight line. The argmax is located where the derivative function crosses zero.\n\n\n\nThe bottom row of graphs in Figure 24.2 shows the derivative of the objective function \\(f(x)\\), that is, \\(\\partial_x f(x)\\). You can see that for the argmax of \\(f(x)\\), the derivative \\(\\partial_x f(x)\\) is positive to the left and negative to the right. Similarly, near the argmin of \\(f(x)\\), the derivative \\(\\partial_x f(x)\\) is negative to the left and positive to the right.\nStated another way, the derivative \\(\\partial_x f(x)\\) has a negative slope just to the left of an argmin and a positive slope to the left of an argmax.\nThe second derivative of the objective function \\(f(x)\\) at a critical point \\(x^\\star\\) is what tells us whether the critical point is an argmax, an argmin, or neither.\n\n\n\nTable 24.1: Categorizing critical points as argmaxes or argmins using first and second derivatives.\n\n\n\n\n\n\n\n\n\n\nCritical point \\(x^\\star\\)\n\\(\\partial_x f(x^\\star)\\)\n\\(\\partial_{xx} f(x^\\star)\\)\n\n\n\n\nargmax\n0\nnegative\n\n\nargmin\n0\npositive\n\n\nneither\n0\n0\n\n\n\n\n\n\n\nThe slope of a function \\(f(x)\\) at any input \\(x\\) is the value of the derivative function \\(\\partial_x f(x)\\) at that same \\(x\\).\nThe concavity of a function \\(f(x)\\) at any input is the slope of the derivative function, that is, \\(\\partial_{xx} f(x)\\).\nPutting (i) and (ii) together, we get that the concavity of a function \\(f(x)\\) at any input \\(x\\) is the value of the second derivative function, that is, \\(\\partial_{xx} f(x)\\).\nAt an argmax \\(x^\\star\\) of \\(f(x)\\), the value of the derivative function \\(\\partial_x f(x^\\star)\\) is zero and the value of the second derivative function \\(\\partial_{xx} f(x^\\star)\\) is negative. The situation at an argmin is along the same lines, the derivative of the objective function is zero and the second derivative is positive.\n\n\n\n\n\n\n\nImportant 24.1: Where’s the critical point?\n\n\n\nYou’re familiar with the quadratic polynomial: \\[g(x) = a_0 + a_1 x + a_2 x^2\\] The graph of a quadratic polynomial is a parabola, which might be concave up or concave down. A parabola has only one critical point, which might be an argmin or an argmax.\nLet’s find the critical point ….\nWe know that the critical point is \\(x^\\star\\) such that \\(\\partial_x g(x^\\star) = 0\\). Since we know how to differentiate a power law, we can see that \\[\\partial_x g(x) = a_1 + 2 a_2 x\\] and, more specifically, at the critical point \\(x^\\star\\) the derivative will be \\[a_1 + 2 a_2 x^\\star = 0\\] The above is an equation, not a definition. It says that whatever \\(x^\\star\\) happens to be, the quantity \\(a_1 + 2 a_2 x^\\star\\) must be zero. Using simple algebra, we can find the location of the critical point \\[x^\\star = -\\frac{a_1}{2 a_2}\\]\n\n\n\nApplication area 24.1 —In economics, demand for a good is a function of the good’s price (and other things). This function is called the “demand curve.”\n\n\n\n\n\n\n\nApplication area 24.1 The demand “curve”\n\n\n\nIn economics, a monopoly or similar arrangement can set the price for a good or commodity. Monopolists can set the price at a level that generates the most income for themselves.\n\n\n\n\n\n\n\n\nFigure 24.3: Demand as a function of price, as first published by Antoine-Augustin Cournot in 1836. Source)\n\n\n\n\n\nIn 1836, early economist Antoine-Augustin Cournot published a theory of revenue versus demand based on his conception that demand will be a monotonically decreasing function of price. (That is, higher price means lower demand.) we will write as \\(\\text{Demand}(p)\\) demand as a function of price.\nThe revenue generated at price \\(p\\) is \\(R(p) \\equiv p \\text{Demand}(p)\\): price times demand.\nTo find the revenue-maximizing demand, differentiate \\(R(p)\\) with respect to \\(p\\) and find the argmax \\(p^\\star\\) at with \\(\\partial_p R(p^\\star) = 0).\\) This can be done with the product rule.\n\\[\\partial_p R(p) = p \\ \\partial_p \\text{Demand}(p) + \\text{Demand}(p)\\] At the argmax \\(p^\\star\\) we have: \\[p^\\star \\partial_p \\text{Demand}(p^\\star) + \\text{Demand}(p^\\star) = 0 \\ \\ \\stackrel{\\text{solving for}\\ p^\\star}{\\Longrightarrow} \\ \\ p^\\star = - \\frac{\\text{Demand}(p^\\star)}{\\partial_p \\text{Demand}(p^\\star)}\\]\nIf the monopolist knows the demand function \\(D(p)\\), finding the revenue maximizing price is a simple matter. But in general, the monopolist does not know the demand function in advance. Instead, an informed guess is made to set the initial price \\(p_0\\). Measuring sales \\(D(p_0)\\) gives one point on the demand curve. Then, try another price \\(p_1\\). This gives another point on the demand curve as well as an estimate \\[\\partial_p D(p_0) = \\frac{D(p_1) - D(p_0)}{p_1 - p_0}\\] Now the monopolist is set to model the demand curve as a straight-line function and easily to find \\(p^\\star\\) for the model. For instance, if the demand function is modeled as \\(D_1 (p) = a + b p\\), the optimal price will be \\(p^\\star_1 = - \\frac{a + b p^\\star}{b}\\) which can be solved as \\(p^\\star_1 = - a/2b\\).\n\\(p^\\star_1\\) is just an estimate of the optimum price. Still, the monopolist can try out that price, giving a third data point for the demand function. The new data can lead to a better model of the demand function. With the better estimate, find a new a argmax \\(p^\\star_2\\). This sort of iterative process for finding an argmax of a real-world function is very common in practice.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "Differentiation/24-optim.html#sec-flat-on-top",
    "href": "Differentiation/24-optim.html#sec-flat-on-top",
    "title": "24  Optimization",
    "section": "24.3 Be practical!",
    "text": "24.3 Be practical!\nDecision making is about choosing among alternatives. In some engineering or policy contexts, this can mean finding a value for an input that will produce the “best” outcome. For those who have studied calculus, it is natural to believe that calculus-based techniques for optimization are the route to making the decision.\nWe emphasize that the optimization techniques covered in this chapter are only part of a broader set of techniques for real-world decision-making problems. In particular, most policy contexts involve multiple objectives. For example, in designing a car one goal is to make it cheap to manufacture, another to make it attractive, and still another to make it safe. These different objectives are often at odds with one another. In Block 4 of this text, we will discuss some calculus techniques that help policy-makers in multi-objective settings.\nFor now, sticking with the idealized (and often unrealistic) setting of maximizing a single objective, with one or more inputs. Recall the setting for calculus-type maximization. You have a function with one or more inputs, say, \\(f(x)\\) or \\(g(x,y)\\) or, often, \\(h(x, y, z, \\ldots)\\) where \\(\\ldots\\) might be standing for tens or hundreds or thousands of inputs or more.\nIf you can graph the function (feasible for one- or two-input functions), you can often easily scan the graph by eye to find the peak. The basis of the calculus techniques is the observation that, at the argmax of a smooth function, the derivative of the function is 0.\nFor example, consider a style problem that often appears in calculus textbooks. Suppose you have been tasked to design a container for a large volume V of liquid. The design specifications call for the weight of the container to be as little as possible. (This is a minimization problem, then.) In classical textbook fashion, the specifications might also stipulate that the container is to be a cylinder made out of a particular metal of a particular thickness.\nThe above is a lovely geometry/calculus problem. Whether it is relevant to any genuine, real-world problem is another question.\n\n\n\n\n\n\nFigure 24.4: Diagram of a cylinder\n\n\n\nUsing the notation in the diagram, the volume and surface area of the cylinder is \\[V(r, h) \\equiv \\pi r^2 h \\ \\ \\ \\text{and}\\ \\ \\ A(r, h) \\equiv 2 \\pi r^2 + 2 \\pi r h\\]\nMinimizing the weight of the cylinder is our objective (according to the problem statement) and the weight is proportional to the surface area. Since the volume \\(V\\) is given (according to the problem statement), we want to re-write the area function to use volume:\n\\[h(r, V) \\equiv V / \\pi r^2 \\ \\ \\ \\implies\\ \\ \\ A(r, V) = 2 \\pi r^2 + 2 \\pi r V/\\pi r^2 = 2 \\pi r^2 + 2 V / r\\] Suppose \\(V\\) is specified as 1000 liters. A good first step is to choose appropriate units for \\(r\\) to make sure the formula for \\(A(r, V)\\) is dimensionally consistent. Suppose we choose \\(r\\) in cm. Then we want \\(V\\) in cubic centimeters (cc). 1000 liters is 1,000,000 cc. Now we can plot a slice of the area function:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 24.5: Surface area of a cylindrical tank (Figure 24.4) of capacity 1000 liters as a function of radius. Since the volume is specified, the height of the cylinder is a function of radius.\n\n\n\nAs always, the function’s derivative is zero at the optimal \\(r\\). In the graph, the argmin is near \\(r=50\\) cm at which point the minimum is about 50,000 cm\\(^2\\). Since \\(h(r,V) = V/\\pi r^2\\), the required height of cylinder will be near \\(10^6 / \\pi 50^2 = 127\\)cm.\nIn calculus courses, the goal is often to find a formula for the optimal radius as a function of \\(V\\). So we differentiate the objective function—that is, the area function for any \\(V\\) and \\(r\\) with respect to \\(r\\), \\[\\partial_r A(r, V) = 4 \\pi r - 2 V / r^2\\] Setting this to zero (which will be true at the optimal \\(r^\\star\\)) we can solve for \\(r^\\star\\) using \\(V\\): \\[4 \\pi r^\\star - 2 \\frac{V}{\\left[r^\\star\\right]^2} = 0 \\ \\ \\ \\Longrightarrow\\ \\ \\ 4\\pi r^\\star = 2\\frac{V}{\\left[r^\\star\\right]^2} \\Longrightarrow\\ \\ \\ \\left[r^\\star\\right]^3 = \\frac{1}{2\\pi} V \\ \\ \\ \\Longrightarrow\\ \\ \\  r^\\star = \\sqrt[3]{V/2\\pi}\\]\nFor \\(V = 1,000,000\\) cm\\(^3\\), this gives \\(r^\\star = 54.1926\\) cm which in turn implies that the corresponding height \\(h^\\star = V/\\pi (r^\\star)^2 = 108.3852\\) cm.\nWe’ve presented the optimum \\(r^\\star\\) and \\(h^\\star\\) to the nearest micron. (Does that make sense to you? Think about it for a moment before reading on.)\nIn modeling, a good rule of thumb is this: “If you don’t know what a sensible precision is for reporting your result, you don’t yet have a complete grasp of the problem.” Here are two reasonable ways to sort out a suitable precision.\n\nSolve a closely related problem that would have been equivalent for many practical purposes.\nCalculate how much the input can deviate from the argmax while producing a trivial change in the output of the objective function.\n\nApproach (2) is always at hand, since you already know the objective function. Let’s graph the objective function near \\(r = 54.1926\\) …\n\n\n\n\n## Warning: All aesthetics have length 1, but the data has 101 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n\n\n\n\n\n\n\n\n\nFigure 24.6: Zooming in on the argmin for the surface area of the container as a function of radius of the cylinder.\n\n\n\nLook carefully at the axes scales. Deviating from the mathematical optimum by about 5cm (that is, 50,000 microns) produces a change in the output of the objective function by about 400 units out of 55,000. In other words, about 0.7%.\nIt is true that \\(r^\\star = 54.1926\\) cm gives the “best” outcome. And sometimes such precision is warranted. For example, improving the speed of an elite marathon racer by even 0.1% would give her a 7 second advantage: often the difference between silver and gold!\nWhat’s different is that you know exactly the ultimate objective of a marathon: finish faster. But you may not know the ultimate objective of the system your “optimal” tank will be a part of. For instance, your tank may be part of an external fuel pod on an aircraft. Certainly the aircraft designers want the tank to be as light as possible. But they also want to reduce drag as much as possible. A 54 cm diameter tube has about 17% more drag than a 50 cm tube. It is probably well worth increasing weight by 0.7% to save that much drag.\nIn reporting the results from an optimization problem, you ought to give the decision maker all relevant information. That might be as simple as including the above graph in your report.\nWe mentioned another technique for getting a handle on what precision is meaningful: (1) solve a closely related problem. It can requires some insight and creativity to frame the new problem. For instance, large capacity tanks often are shaped like a lozenge: a cylinder with hemi-spherical ends.\n\n\n\n\nDiagram of a cylindrical tank with hemispherical end caps. ```\nUsing \\(h\\) for the length of the cylindrical portion of the tank, and \\(r\\) for the radius, the volume and surface area are: \\[V(r, h) = \\pi r^2 h + \\frac{4}{3} \\pi r^3 \\ \\ \\ \\text{and}\\ \\ \\ A(r,h) = 2 \\pi r h + 4 \\pi r^2\\] Again, \\(V\\) is specified as 1000 liters. As detailed in Exercise 24.18, the surface area of this 1000-liter tank is about 48,400 cm\\(^2\\). This is more than 10% less than for the cylindrical tank.\n\n\n\n\nFigure 24.7",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "Differentiation/24-optim.html#footnotes",
    "href": "Differentiation/24-optim.html#footnotes",
    "title": "24  Optimization",
    "section": "",
    "text": "Another word for an “input” is “argument.” Argmax is the contraction of argument producing the maximum output.↩︎",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "Differentiation/25-partial.html",
    "href": "Differentiation/25-partial.html",
    "title": "25  Partial change and the gradient vector",
    "section": "",
    "text": "25.1 Calculus on two inputs\nFor a function with one input, the derivative function (with respect to that input) outputs a single quantity at each input value. We have often drawn the quantity as a sloping line. This is easy to do on the two-dimensional surface of paper or a screen.\nIt would be silly to restrict our models to using functions of a single variable just so that we can show them on paper or a screen. For functions with multiple inputs, the output of the derivative function will not be a single quantity, but a set of quantities, one for each input. The derivative of a function of multiple inputs with respect to only one of those inputs is called a partial derivative. “Partial” is a reasonable word here, because each partial derivative gives only part of the information about the overall derivative with respect to all of the inputs. The set of all the partial derivatives, called the gradient, is a way of representing the “slope” of a function of multiple inputs.\nAlthough we use contour plots for good practical reasons, the graph of a function \\(g(x,y)\\) with two inputs is a surface, as depicted in Figure 4.7. The derivative of \\(g(x,y)\\) should encode the information needed to approximate the surface at any input \\((x,y)\\). In particular, we want the derivative of \\(g(x,y)\\) to tell us the orientation of the tangent plane to the surface.\nA tangent plane is infinite in extent. Let’s use the word facet to refer to a little patch of the tangent plane centered at the point of contact. Each facet is flat. (it is part of a plane!) Figure 25.1 shows some facets tangent to a familiar curved surface. No two of the facets are oriented the same way.\nBetter than a picture of a summer melon, pick up a hardcover book and place it on a curved surface such as a basketball. The book cover is a flat surface: a facet. The orientation of the cover will match the orientation of the surface at the point of tangency. Change the orientation of the cover and you will find that the point of tangency will change correspondingly.\nIf melons and basketballs are not your style, you can play the same game on an interactive graph of a function with two inputs. The snapshot below is a link to an applet that shows the graph of a function as a blue surface. You can specify a point on the surface by setting the value of the (x, y) input using the sliders. Display the tangent plane (which will be green) at that point by check-marking the “Tangent plane” input. (Acknowledgments to Alfredo Sánchez Alberca who wrote the applet using the GeoGebra math visualization system.)\nFor the purposes of computation by eye, a contour graph of a surface can be easier to deal with. Figure 25.3 shows the contour graph of a smoothly varying function. Three points have been labeled A, B, and C.\nZooming in on each of the marked points presents a simpler picture for each of them, although one that is different for each point. Each zoomed-in plot contains almost parallel, almost evenly spaced contours. If the surface had been exactly planar over the entire zoomed-in domain, the contours would be exactly parallel and exactly evenly spaced. We can approach such exact parallelness by zooming in more closely around the labeled point.\nJust as the function \\(\\line(x) \\equiv a x + b\\) describes a straight line, the function \\(\\text{plane}(x, y) \\equiv a + b x + c y\\) describes a plane whose orientation is specified by the value of the parameters \\(b\\) and \\(c\\). (Parameter \\(a\\) is about the vertical location of the plane, not its orientation.)\nThe linear approximation shown in each panel of Figure 25.4 has the same orientation as the actual function; the contours face in the same way.\nRemember that the point of constructing such facets is to generalize the idea of a derivative from a function of one input \\(f(x)\\) to functions of two or more inputs such as \\(g(x,y)\\). Just as the derivative \\(\\partial_x f(x_0)\\) reflects the slope of the line tangent to the graph of \\(f(x)\\) at \\(x=x_0\\), our plan for the “derivative” of \\(g(x_0,y_0)\\) is to represent the orientation of the facet tangent to the graph of \\(g(x,y)\\) at \\((x=x_0, y=y_0)\\). The question for us now is what information is needed to specify an orientation.\nOne clue comes from the formula for a function whose graph is a plane oriented in a particular direction:\n\\[\\text{plane}(x,y) \\equiv a + b x + cy\\]\nThe point here is to show that two numbers are sufficient to dictate the orientation of a plane. Referring to Figure 25.5 these are 1) the amount that one hand is raised relative to the other and 2) the angle of rotation around the hand-to-hand axis.\nSimilarly, in the formula for a plane, the orientation is set by two numbers, \\(b\\) and \\(c\\) in \\(\\text{plane}(x, y) \\equiv a + b x + c y\\).\nHow do we find the right \\(b\\) and \\(c\\) for the tangent facet to a function \\(g(x,y)\\) at a specific input \\((x_0, y_0)\\)? Taking slices of \\(g(x,y)\\) provides the answer. In particular, these two slices: \\[\\text{slice}_1(x) \\equiv g(x, y_0) = a + b\\, x + c\\, y_0 \\\\ \\text{slice}_2(y) \\equiv g(x_0, y) = a + b x_0 + c\\, y\\]\nLook carefully at the formulas for the slices. In \\(\\text{slice}_1(x)\\), the value of \\(y\\) is being held constant at \\(y=y_0\\). Similarly, in \\(\\text{slice}_2(y)\\) the value of \\(x\\) is held constant at \\(x=x_0\\).\nThe parameters \\(b\\) and \\(c\\) can be read out from the derivatives of the respective slices: \\(b\\) is equal to the derivative of the slice\\(_1\\) function with respect to \\(x\\) evaluated at \\(x=x_0\\), while \\(c\\) is the derivative of the slice\\(_2\\) function with respect to \\(y\\) evaluated at \\(y=y_0\\). Or, in the more compact mathematical notation:\n\\[b = \\partial_x \\text{slice}_1(x)\\left.\\strut\\right|_{x=x_0} \\ \\ \\text{and}\\ \\ c=\\partial_y \\text{slice}_2(y)\\left.\\strut\\right|_{y=y_0}\\] These derivatives of slice functions are called partial derivatives. The word “partial” refers to examining just one input at a time. In the above formulas, the \\({\\large |}_{x=x_0}\\) means to evaluate the derivative at \\(x=x_0\\) and \\({\\large |}_{y=y_0}\\) means something similar.\nYou need not create the slices explicitly to calculate the partial derivatives. Simply differentiate \\(g(x, y)\\) with respect to \\(x\\) to get parameter \\(b\\) and differentiate \\(g(x, y)\\) with respect to \\(y\\) to get parameter \\(c\\). To demonstrate, we will make use of the sum rule: \\[\\partial_x g(x, y) = \\underbrace{\\partial_x a}_{=0} + \\underbrace{\\partial_x b x}_{=b} + \\underbrace{\\partial_x cy}_{=0} = b\\] Similarly, \\[\\partial_y g(x, y) = \\underbrace{\\partial_y a}_{=0} + \\underbrace{\\partial_y b x}_{=0} + \\underbrace{\\partial_y cy}_{=c} = c\\]",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Partial change and the gradient vector</span>"
    ]
  },
  {
    "objectID": "Differentiation/25-partial.html#calculus-on-two-inputs",
    "href": "Differentiation/25-partial.html#calculus-on-two-inputs",
    "title": "25  Partial change and the gradient vector",
    "section": "",
    "text": "Figure 25.1: A melon as a model of a curved surface such as the graph of a function of two inputs. Each tangent facet has its own orientation. (Disregard the slight curvature of the small pieces of paper. Summer humidity has interfered with my attempt to model a flat facet with a piece of Post-It paper!\n\n\n\n\n\n\n\n\n\n\n\nFigure 25.2: An interactive app for visualizing tangent planes. To open, go to https://www.geogebra.org/m/wTh7KKd3.or simply click on the image. You will want to mark the tangent-plane checkbox in the app to show the green tangent plane.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 25.3: A contour plot of a function of 2 inputs with 3 specific inputs marked A, B, and C.\n\n\n\n\n\n\n\n\nNear point ANear point BNear point C\n\n\n\n\n\n\n\n\n\n\n\nActual function\n\n\n\n\n\n\n\nLinear approximation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActual function\n\n\n\n\n\n\n\nLinear approximation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActual function\n\n\n\n\n\n\n\nLinear approximation\n\n\n\n\n\n\n\n\n\n\nFigure 25.4: Zooming in on the neighborhoods of points A, B, and C in Figure 25.3 shows a simple, almost planar, local landscape. The tangent plane for each point is the linear approximation. The brown arrows are gradient vectors, which will be introduced in Section 25.3.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry it! 25.1\n\n\n\n\n\n\n\n\n\nTry it! 25.1 R/mosaic: Orientation of a plane\n\n\n\nTo explore the roles of the parameters \\(b\\) and \\(c\\) in setting the orientation of the plane, use Active R chunk 25.1. The R/mosaic code defines plane(), a function of two inputs whose surface graph is a planar facet. Then this facet is graphed as a contour plot. Change the numerical values of \\(b\\) and \\(c\\) and observe how the orientation of the planar surface changes in the graphs.\n\n\n\nActive R chunk 25.1\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nBy varying \\(a\\) while holding \\(b\\) and \\(c\\) constant, you can that the value of \\(a\\) is irrelevant to the orientation of the plane, just as the intercept of a straight-line graph is irrelevant to the slope of that line. Section 25.3 introduces the gradient vector. As a preview of things to come, we’ve drawn the gradient vector at each of nine positions as brown arrows.\n\n\n\n\n\n\n\n\n\n\nTry it! 25.2\n\n\n\n\n\n\n\n\n\nTry it! 25.2 Plane orientation, analog version\n\n\n\nAn instructive experience is to pick up a rigid, flat object, for instance a smartphone or hardcover book. Hold the object level with pinched fingers at the mid-point of each of the short ends, as shown in Figure 25.5 (left).\n\n\n\n\n\n\n\n\n(a) A level surface\n\n\n\n\n\n\n\n(b) Rotated along the axis running top to bottom\n\n\n\n\n\n\n\n(c) Rotated along the axis running left to right\n\n\n\n\n\n\nFigure 25.5: Two orientations for tipping a planar facet (the black pad). A dog (the paw) is supervising the activity.\n\n\n\nYou can tip the object in one direction by raising or lowering one hand. (panel b) You can tip the object in the other coordinate (panel c) by rotating your thumb and forefinger while keeping hand level constant. By combining these two motions, you can orient the surface of the object in a wide range of directions.\nPilots or sailors know that the orientation of an object—a plane or a boat—is described by three parameter: pitch, roll, and yaw. What’s different for a planar facet? It has no front or back end, unlike a plane or boat, so yaw doesn’t apply. Only pitch and roll are needed to describe the orientation of the facet.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nGet in the habit of noticing the subscript on the differentiation symbol \\(\\partial\\). When taking, for instance, \\(\\partial_y f(x,y,z, \\ldots)\\), all inputs other than \\(y\\) are to be held constant. Some examples:\n\\[\\partial_y 3 x^2 = 0\\ \\ \\text{but}\\ \\ \\\n\\partial_x 3 x^2 = 6x\\\\\n\\ \\\\\n\\partial_y 2 x^2 y = 2x^2\\ \\ \\text{but}\\ \\ \\\n\\partial_x 2 x^2 y = 4 x y\n\\]",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Partial change and the gradient vector</span>"
    ]
  },
  {
    "objectID": "Differentiation/25-partial.html#all-other-things-being-equal",
    "href": "Differentiation/25-partial.html#all-other-things-being-equal",
    "title": "25  Partial change and the gradient vector",
    "section": "25.2 All other things being equal",
    "text": "25.2 All other things being equal\nRecall that the derivative of a function with one input, say, \\(\\partial_x f(x)\\) tells you, at each possible value of the input \\(x\\), how much the output will change proportional to a small change in the value of the input.\nNow that we are in the domain of multiple inputs, writing \\(h\\) to stand for “a small change” is not entirely adequate. Instead, we will write \\(dx\\) for a small change in the \\(x\\) input and \\(dy\\) for a small change in the \\(y\\) input.\nWith this notation, we write the first-order polynomial approximation to a function of a single input \\(x\\) as \\[f(x+dx) = f(x) + \\partial_x f(x) \\times dx\\] Applying this notation to functions of two inputs, we have: \\[g(x + \\color{magenta}{dx}, y) = g(x,y) + \\color{magenta}{\\partial_x} g(x,y) \\times \\color{magenta}{dx}\\] and \\[g(x, y+\\color{brown}{dy}) = g(x,y) + \\color{brown}{\\partial_y} g(x,y) \\times \\color{brown}{dy}\\]\nEach of these statements is about changing one input while holding the other input(s) constant. Or, as the more familiar expression goes, “The effect of changing one input all other things being equal or all other things held constant. (The Latin phrase for this is ceteris paribus, often used in economics.)\nEverything we’ve said about differentiation rules applies not just to functions of one input, \\(f(x)\\), but to functions with two or more inputs, \\(g(x,y)\\), \\(h(x,y,z)\\) and so on.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Partial change and the gradient vector</span>"
    ]
  },
  {
    "objectID": "Differentiation/25-partial.html#sec-gradient-vector",
    "href": "Differentiation/25-partial.html#sec-gradient-vector",
    "title": "25  Partial change and the gradient vector",
    "section": "25.3 Gradient vector",
    "text": "25.3 Gradient vector\nFor functions of two inputs, there are two partial derivatives. For functions of three inputs, there are three partial derivatives. We can, of course, collect the partial derivatives into Cartesian coordinate form. This collection is called the gradient vector.\nJust as our notation for differences (\\(\\cal D\\)) and derivatives (\\(\\partial\\)) involves unusual typography on the letter “D,” the notation for the gradient involves such unusual typography although this time on \\(\\Delta\\), the Greek version of “D.” For the gradient symbol, turn \\(\\Delta\\) on its head: \\(\\nabla\\). That is, \\[\\nabla g(x,y) \\equiv \\left(\\stackrel\\strut\\strut\\partial_x g(x,y), \\ \\ \\partial_y g(x,y)\\right)\\]\nNote that \\(\\nabla g(x,y)\\) is a function of both \\(x\\) and \\(y\\), so in general the gradient vector differs from place to place in the function’s domain.\nThe graphics convention for drawing a gradient vector for a particular input, that is, \\(\\nabla g(x_0, y_0)\\), puts an arrow with its root at \\((x_0, y_0)\\), pointing in direction \\(\\nabla g(x_0, y_0)\\), as in Figure 25.6.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 25.6: The gradient vector \\(\\nabla g(x=1,y=2)\\). The vector points in the steepest uphill direction. Consequently, it is perpendicular to the contour passing through its root.\n\n\n\nA gradient field (see Figure 25.7) is the value of the gradient vector at each point in the function’s domain. Graphically, to prevent over-crowding, the vectors are drawn at discrete points. The lengths of the drawn vectors are set proportional to the numerical length of \\(\\nabla g(x, y)\\), so a short vector means the surface is relatively level, a long vector means the surface is relatively steep.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 25.7: A plot of the gradient field \\(\\nabla g(x,y)\\).",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Partial change and the gradient vector</span>"
    ]
  },
  {
    "objectID": "Differentiation/26-approximation.html",
    "href": "Differentiation/26-approximation.html",
    "title": "26  Local approximations",
    "section": "",
    "text": "26.1 Eight simple, local shapes\nWe have now encountered three concepts in Calculus that will prove a great help in building models.\nTo illustrate how these three concepts simplify modeling, imagine constructing a model of the speed of a bicycle: speed(gear, grade). Consider these three questions that any experienced bicyclist can answer:\nNote that the three questions all have to do with derivatives: local rates of change. An “optimal gear” is a gear at which \\(\\partial_\\text{gear} \\text{speed}(\\text{gear}, \\text{grade}) = 0\\). That you ride slower the higher the numerical value of the slope means that \\(\\partial_\\text{grade} \\text{speed}(\\text{gear}, \\text{grade}) &lt; 0\\). And we know that \\(\\partial_\\text{gear} \\text{speed}(\\text{gear}, \\text{grade})\\) depends on the grade; that is why there is a different optimal gear at each grade.\nIn many modeling situations with a single input, selecting one of eight simple shapes, those shown in Figure 26.1, can get you far.\nTo choose among these shapes, consider your modeling context:\nConsider these historical examples:\nSome other examples:",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Local approximations</span>"
    ]
  },
  {
    "objectID": "Differentiation/26-approximation.html#eight-simple-local-shapes",
    "href": "Differentiation/26-approximation.html#eight-simple-local-shapes",
    "title": "26  Local approximations",
    "section": "",
    "text": "Figure 26.1: The eight simple shapes of local functions with one input.\n\n\n\n\n\nis the relationship positive (slopes up) or negative (slopes down)\nis the relationship monotonic or not\nis the relationship concave up, concave down, or neither\n\n\n\nNewton’s Law of Cooling. Let the input be the difference in temperature between an object and its environment. Let the output be the rate at which the object’s temperature changes. Newton’s Law of Cooling amounts to a choice of shape (B).\nHooke’s Law, describing the force supplied by a compressed or stretched object such as a spring. Let the input be the how much the object is compressed or stretched: negative for compression, positive for stretching. Let the output be the force supplied by the string, with a positive force meaning away from the spring and negative towards the spring. Hooke’s Law is shape (A).\nInverse square law for gravitational force. The input is the distance between the masses, the output is the force, with a negative force corresponding to attraction. The inverse square law corresponds to shape (C).\nChemistry’s Law of Mass Action for an element or molecule reacting with itself. The input is the concentration of the substance, the output is the rate of production of the compound. Shape (D). (For the Law of Mass Action involving two different substances, we need shapes of functions with two inputs. See Section 26.3.)\n\n\n\nThe incidence of an out-of-control epidemic versus time is concave up, but shallow-then-steep. Shape D. As the epidemic is brought under control, the decline is steep-then-shallow and concave up. Shape C. Notice that in each case we are describing only the local behavior of the function.\nHow many minutes can you run as a function of speed? Concave down and shallow-then-steep; you wear out faster if you run at high speed. How far can you walk as a function of time? Steep-then-shallow and concave down; your pace slows as you get tired. Shape (E).\nHow does the stew taste as a function of saltiness. The taste improves as the amount of salt increases … up to a point. Too much salt and the stew is unpalatable. Shape (G).\nHow much fuel is consumed by an aircraft as a function of distance? For long flights the function is concave up and shallow-then-steep; fuel use increases with distance, but the amount of fuel you have to carry also increases with distance and heavy aircraft use more fuel per mile. Shape (E).\nIn micro-economic theory there are production functions that describe how much of a good is produced at any given price, and demand functions that describe how much of the good will be purchased as a function of price.\n\nAs a rule, production increases with price and demand decreases with price. In the short term, production functions tend to be concave down, since it is hard to squeeze increased production out of existing facilities. Shape (F).\nFor demand in the short term, functions will be concave up when there is some group of consumers who have no other choice than to buy the product. An example is the consumption of gasoline versus price: it is hard in the short term to find another way to get to work. Shape (C). In the long term, consumption functions can be concave down as consumers find alternatives to the high-priced good. For example, high prices for gasoline may, in the long term, prompt a switch to more efficient cars, hybrids, or electric vehicles. This will push demand down steeply. Shape (E).",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Local approximations</span>"
    ]
  },
  {
    "objectID": "Differentiation/26-approximation.html#low-order-polynomials",
    "href": "Differentiation/26-approximation.html#low-order-polynomials",
    "title": "26  Local approximations",
    "section": "26.2 Low-order polynomials",
    "text": "26.2 Low-order polynomials\nThere is a simple, familiar functional form that, by selecting parameters appropriately, can take on each of the eight simple shapes: the second-order polynomial. \\[g(x) \\equiv a + b x + c x^2\\] As you know, the graph of \\(g(x)\\) is a parabola.\n\nThe parabola opens upward if \\(0 &lt; c\\). That is the shape of a local minimum.\nThe parabola opens downward if \\(c &lt; 0\\). That is the shape of a local maximum\n\nConsider what happens if \\(c = 0\\). The function becomes simply \\(a + bx\\), the straight-line function.\n\nWhen \\(0 &lt; b\\) the line slopes upward.\nWhen \\(b &lt; 0\\) the line slopes downward.\n\nWith the appropriate choice of parameters, the form \\(a + bx + cx^2\\) is capable of representing four of the eight simple shapes. What about the remaining four? This is where the idea of local becomes important. Those remaining four shapes are the sides of parabolas, as in Figure 26.2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 26.2: Four of the eight simple shapes correspond to the sides of the parabola. The labels refer to the graphs in Figure 26.1.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Local approximations</span>"
    ]
  },
  {
    "objectID": "Differentiation/26-approximation.html#sec-low-order-two",
    "href": "Differentiation/26-approximation.html#sec-low-order-two",
    "title": "26  Local approximations",
    "section": "26.3 The low-order polynomial with two inputs",
    "text": "26.3 The low-order polynomial with two inputs\nFor functions with two inputs, the low-order polynomial approximation looks like this:\n\\[g(x, y) \\equiv a_0 + a_x x + a_y y + a_{xy} x y + a_{yy} y^2 + a_{xx} x^2\\] In reading this form, note the system being used to name the polynomial’s coefficients. First, we’ve used \\(a\\) as the root name of all the coefficients. Sometimes we might want to compare two or more low-order polynomials, so it is convenient to be able to use \\(a\\) for one, \\(b\\) for another, and so on.\nThe subscripts on the coefficients describes exactly which term in the polynomial involves each coefficient. For instance, the \\(a_{yy}\\) coefficient applies to the \\(y^2\\) term, while \\(a_x\\) applies to the \\(x\\) term.\nEach of \\(a_0, a_x,\\) \\(a_y,\\) \\(a_{xy}, a_{yy}\\), and \\(a_{xx}\\) will, in the final model, be a constant quantity. Don’t be confused by the use of \\(x\\) or \\(y\\) in the name of the coefficients. Each coefficient is a constant and not a function of the inputs. Often, your prior knowledge of the system being modeled will tell you something about one or more of the coefficients, for example, whether it is positive or negative. Finding a precise value is often based on quantitative data about the system.\nAs discussed in Section 26.3, there are generic names for the various terms: linear, quadratic, interaction term, and constant term.\n\\[g(x, y) \\equiv a_0 + \\underbrace{a_x x + a_y y}_\\text{linear terms} \\ \\ \\ +\n\\underbrace{a_{xy} x y}_\\text{interaction term} +\\ \\ \\  \\underbrace{a_{yy} y^2 + a_{xx} x^2}_\\text{quadratic terms}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 26.3: A saddle",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Local approximations</span>"
    ]
  },
  {
    "objectID": "Differentiation/26-approximation.html#sec-partial-thought",
    "href": "Differentiation/26-approximation.html#sec-partial-thought",
    "title": "26  Local approximations",
    "section": "26.4 Thinking partially",
    "text": "26.4 Thinking partially\nThe expression for a general low-order polynomial in two inputs can be daunting to think about all at once: \\[g(x, y) \\equiv a_0 + a_x x + a_y y + a_{xy} x y + a_{xx} x^2 + a_{yy} y^2\\] As with many complicated settings, a good approach can be to split things up into simpler pieces. With a low-order polynomial, one such splitting up involves partial derivatives. There are six potentially non-zero partial derivatives for a low-order polynomial, of which two are the same; so only five quantities to consider.\n\n\\(\\partial_x g(x,y) = a_x + a_{xy}y + 2 a_{xx} x\\)\n\\(\\partial_y g(x,y) = a_y + a_{xy}x + 2 a_{yy} y\\)\n\\(\\partial_{xy} g(x,y) = \\partial_{yx} g(x,y) = a_{xy}\\). These are the so-called mixed partial derivatives. It does not matter whether you differentiate by \\(x\\) first or by \\(y\\) first. The result will always be the same for any smooth function.\n\\(\\partial_{xx} g(x,y) = 2 a_{xx}\\)\n\\(\\partial_{yy} g(x,y) = 2 a_{yy}\\)\n\nThe above list states neutral mathematical facts that apply generally to any low-order polynomial whatsoever.1 Those facts, however, shape a way of asking questions of yourself that can help you shape the model of a given phenomenon based on what you already know about how things work.\nTo illustrate, consider the situation of modeling the effect of study \\(S\\) and of tutoring \\(T\\) (a.k.a. office hours, extended instruction) on performance \\(P(S,T)\\) on an exam. In the spirit of partial derivatives, we will assume that all other factors (student aptitude, workload, etc.) are held constant.\nTo start, pick fiducial values for \\(S\\) and \\(T\\) to define the local domain for the model. Since \\(S=0\\) and \\(T=0\\) are easy to envision, we will use those for the fiducial values.\nNext, ask five questions, in this order, about the system being modeled.\n\nDoes performance increase with study time? Don’t over-think this. Remember that the approximation is around a fiducial point. Here, a reasonable answer is, “yes.” we will take\\(\\partial_S P(S, T) &gt; 0\\) to imply that \\(a_S &gt; 0\\). This is appropriate because close to the fiducial point, the other contributors to \\(\\partial_S P(S, T)\\), namely \\(a_{ST}T + 2 a_{SS} S\\) will be vanishingly small.\nDoes performance increase with time spent being tutored? Again, don’t over-think this. Don’t worry (yet) that your social life is collapsing because of the time spent studying and being tutored, and the consequent emotional depression will cause you to fail the exam. We are building a model here and the heuristic being used is to consider factors in isolation. Since (as we expect you will agree) \\(\\partial_T P(S, T) &gt; 0\\), we have that \\(a_T &gt; 0\\).\n\nNow the questions get a little bit harder and will exercise your calculus-intuition since you will have to think about changes in the rates of change.\n\nThis question has to do with the mixed partial derivative, which we’ve written variously as \\(\\partial_{ST} P(S,T)\\) or \\(\\partial_{TS} P(S,T)\\) and which it might be better to think about as \\(\\partial_S \\left[\\partial_T P(S,T) \\right]\\) or \\(\\partial_T \\left[\\partial S P(S,T)\\right]\\). Although these are mathematically equal, often your intuition will favor one form or the other. Recall that we are working on the premise that \\(\\partial_S P(S,T) &gt; 0\\), or, in other words, study will help you do better on the exam. Now for \\(\\partial_T \\left[\\partial S P(S,T)\\right]\\). This is a the matter of whether some tutoring will make your study more effective. Let’s say yes here, since tutoring can help you overcome a misconception that is a roadblock to effective study. So \\(\\partial_{TS} P(S,T) &gt; 0\\) which implies \\(a_{ST} &gt; 0\\).\n\nThe other way round, \\(\\partial_S \\left[\\partial_T P(S,T) \\right]\\) is a matter of whether increasing study will enhance the positive effect of tutoring. We will say yes here again, because a better knowledge of the material from studying will help you follow what the tutor is saying and doing. From pure mathematics, we already know that the two forms of mixed partials are equivalent, but to the human mind they sometimes (and incorrectly) appear to be different in some subtle, ineffable way.\nIn some modeling contexts, there might be no clear answer to the question of \\(\\partial_{xy}\\, g(x,y)\\). That is also a useful result, since it tells us that the \\(a_{xy}\\) term may not be important to understanding that system.\n\nOn to the question of \\(\\partial_{SS} P(S,T)\\), that is, whether \\(a_{SS}\\) is positive, negative, or negligible. We know that \\(a_{SS} S^2\\) will be small whenever \\(S\\) is small, so this is our opportunity to think about bigger \\(S\\). So does the impact of a unit of additional study increase or decrease the more you study? One point of view is that there is some moment when “it all comes together” and you understand the topic well. But after that epiphany, more study might not accomplish as much as before the epiphany. Another bit of experience is that “cramming” is not an effective study strategy. And then there is your social life … So let’s say, provisionally, that there is an argmax to study, beyond which point you’re not helping yourself. This means that \\(a_{SS} &lt; 0\\).\nFinally, consider \\(\\partial_{TT} P(S, T)\\). Reasonable people might disagree here, which is itself a reason to suspect that \\(a_{TT}\\) is negligible.\n\nAnswering these questions does not provide a numerical value for the coefficients on the low-order polynomial, and says nothing at all about \\(a_0\\), since all the questions are about change.\nAnother step forward in extracting what you know about the system you are modeling is to construct the polynomial informed by questions 1 through 5. Since you don’t know the numerical values for the coefficients, this might seem impossible. But there is a another modeler’s trick that might help.\nLet’s imagine that the domain of both \\(S\\) and \\(T\\) or the interval zero to one. This is not to say that we think one hour of study is the most possible but simply to defer the question of what are appropriate units for \\(S\\) and \\(T\\). Very much in this spirit, for the coefficients we will use \\(+0.5\\) when are previous answers indicated that the coefficient should be greater than zero, \\(-0.5\\) when the answers pointed to a negative coefficient, and zero if we don’t know. Using this technique, here is the model, which mainly serves as a basis for checking whether our previous answers are in line with our broader intuition before we move on quantitatively.\n\n\n\n\nP &lt;- makeFun(0.5*S + 0.5*T + 0.5*S*T - 0.5*S^2 ~ S & T)\ncontour_plot(P(S, T) ~ S & T, bounds(S=0:1, T=0:1))\n\n\n\n\n\n\n\n\n\n\nFigure 26.4: The result of our intuitive investigation of the effects of study and tutoring on exam performance. The units are not yet assigned.\n\n\n\nNotice that for small values of \\(T\\), the horizontal spacing between adjacent contours is large. That is, it takes a lot of study to improve performance a little. At large values of \\(T\\) the horizontal spacing between contours is smaller.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Local approximations</span>"
    ]
  },
  {
    "objectID": "Differentiation/26-approximation.html#finding-coefficients-from-data",
    "href": "Differentiation/26-approximation.html#finding-coefficients-from-data",
    "title": "26  Local approximations",
    "section": "26.5 Finding coefficients from data",
    "text": "26.5 Finding coefficients from data\nLow-order polynomials are often used for constructing functions from data. In this section, I’ll demonstrate briefly how this can be done. The full theory will be introduced in Block 5 of this text.\nThe data I’ll use for the demonstration is a set of physical measurements of height, weight, abdominal circumference, etc. on 252 human subjects. These are contained in the Body_fat data frame, shown below.\n\n\n\n\n\n\nOne of the variables records the body-fat percentage, that is, the fraction of the body’s mass that is fat. This is thought to be an indicator of fitness and health, but it is extremely hard to measure and involves weighing the person when they are fully submerged in water. This difficulty motivates the development of a method to approximation body-fat percentage from other, easier to make measurements such as height, weight, and so on.\nFor the purpose of this demonstration, we will build a local polynomial model of body-fat percentage as a function of height (in inches) and weight (in pounds).\nThe polynomial we choose will omit the quadratic terms. It will contain the constant, linear, and interaction terms only. That is \\[\\text{body.fat}(h, w) \\equiv c_0 + c_h h + c_w w + c_{hw} h w\\] The process of finding the best coefficients in the polynomial is called linear regression. Without going into the details, we will use linear regression to build the body-fat model and then display the model function as a contour plot.\n\n\n\n\nmod &lt;- lm(bodyfat ~ height + weight + height*weight,\n          data = Body_fat)\nbody_fat_fun &lt;- makeFun(mod)\ncontour_plot(body_fat_fun(height, weight) ~ height + weight,\n             bounds(weight=c(100, 250), height = c(60, 80))) %&gt;%\n  gf_labs(title = \"Body fat percentage\")\n\n\n\n\n\n\n\n\n\n\nFigure 26.5: A low order polynomial model of body fat percentage as a function of height (inches) and weight (lbs).\n\n\n\nBlock 3 looks at linear regression in more detail.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Local approximations</span>"
    ]
  },
  {
    "objectID": "Differentiation/26-approximation.html#footnotes",
    "href": "Differentiation/26-approximation.html#footnotes",
    "title": "26  Local approximations",
    "section": "",
    "text": "Note that any other derivative you construct, for instance \\(\\partial_{xxy} g(x,y)\\) must always be zero.↩︎",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Local approximations</span>"
    ]
  },
  {
    "objectID": "Differentiation/27-taylor.html",
    "href": "Differentiation/27-taylor.html",
    "title": "27  Polynomials for approximating functions",
    "section": "",
    "text": "27.1 Hundreds of years ago ….\nAlmost all readers of this book will have studied mathematics previously. Those who reached secondary school very likely spent considerable time factoring polynomials, sometimes called “finding the roots.” Many students still remember the “quadratic formula” from high school, which will find applications later in this book.\nStudents and their instructors often struggle to explain what non-textbook problems are solved with factoring polynomials. There are uncountable numbers of posts on the internet addressing the question, many of which refer to the “beauty” of algebra, its benefits in learning logical thinking, and then kick the can down the road by saying that it is crucial to understanding calculus and other mathematical topics at the college level.\nYou have already seen in Chapter 12 how low-order polynomials provide a framework for arranging our intuitive understanding of real-world settings into quick mathematical models of those settings, for instance, in Application area 26.1 on modeling the speed of a bicycle as a function of steepness of the road and the gear selected for pedaling.\nCalculus textbooks have for generations described using polynomials for better theoretical understanding of functions. This author, like many, were so extensively trained in the algebraic use of polynomials that it is practically impossible to ascertain the extent to which they are genuinely useful in developing understanding of the uses of math. It seems wise to follow tradition and include mention of classical textbook problems in this book. That’s one goal of this chapter. But it’s common for polynomials to be used for modeling problems for which they are poorly suited, and even to focus on aspects of functions such as higher-order derivatives, that get in the way of productive work.\nFifteen-hundred years ago, the Indian mathematician Brahmagupta (c. 598 – c. 668 CE) published what is credited as the first clear statement of the quadratic theorem. Clear perhaps to scholars of ancient mathematics and its unfamiliar notation, but not to the general modern reader.\nBrahmagupta’s text also include the earliest known use of zero as a number in its own right, as opposed to being a placeholder in other numbers. He published rules for arithmetic with zero and negative numbers that will be familiar to most (perhaps all) readers of this book. However, his comments on division by zero, for instance that \\(\\frac{0}{0} = 0\\) are not consistent with modern understanding. Today, constructions such as \\(\\frac{0}{0}\\) are called indeterminate forms, even though many beginning students are inclined to prefer Brahmagupta’s opinion on the matter. I mention this because the calculus of indeterminate forms, which resolves many questions previously unresolved since antiquity, are a staple of calculus textbooks.\nMuch earlier records from Babylonian clay tablets show that mathematicians were factoring quadratics as long ago as 2000-1600 BC.\nIn the 15th and 16th century CE, mathematicians such as Scipione del Ferro and Niccolò Tartaglia were stars of royal competitions in factoring polynomials. Such competitions were one way for mathematicians to secure support from wealthy patrons. The competitive environment encouraged mathematicians to keep their findings secret, a attitude which was common up until the late 1600s when Enlightenment scholars came to value the sort of open publication which is a defining element of science today. Young Isaac Newton was elected a fellow of the newly founded Royal Society—full name: the Royal Society of London for Improving Natural Knowledge—in 1672 and was president from 1703 until his death a quarter century later.\nThis is a long and proud history for polynomials, perhaps in itself justifying their placement near the center of the high-school curriculum. It’s easy to see the strong motivation mathematicians would have in the early years of calculus to apply their new tools to understanding polynomials and, later, functions. The importance of polynomials to mathematical culture is signaled by the distinguished name given to the proof that an nth-order polynomial has exactly n roots: the Fundamental theorem of algebra.\nAs for factoring polynomials, in the 1500s mathematicians found formulas for roots of third- and fourth-order polynomials. Today, they are mainly historical artifacts since the arithmetic involved is subject to catastrophic round-off error. By 1824, the formulas-for-factoring road came to a dead end when it was proved that fifth- and higher-order polynomials do not have general solutions written only using square roots or other radicals.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Polynomials for approximating functions</span>"
    ]
  },
  {
    "objectID": "Differentiation/27-taylor.html#a-warning-for-modelers",
    "href": "Differentiation/27-taylor.html#a-warning-for-modelers",
    "title": "27  Polynomials for approximating functions",
    "section": "27.2 A warning for modelers",
    "text": "27.2 A warning for modelers\nLater in this chapter, we’ll return to address classical mathematical problems using polynomials. This section is about the pitfalls of using third- and higher-order polynomials in constructing models of real-world settings. (As mentioned previously, low-order polynomials are a different matter. See Chapter 12.)\nBuilding a reliable model with high-order polynomials requires a deep knowledge of mathematics and introduces serious potential pitfalls. Modern professional modelers learn the alternatives to high-order polynomials (for example, “natural splines” and “radial basis functions”), but newcomers often draw on their high school experience and give unwarranted credence to polynomials.\nThe domain of polynomials, like the power-law functions they are assembled from, is the real numbers, that is, the entire number line \\(-\\infty &lt; x &lt; \\infty\\). (More precisely, the domain is the complex numbers, of which the real numbers are an infinitesimal subset. We’ll look at modeling uses for complex numbers in Chapter 47.)\nTo understand the shape of high-order polynomials, it is helpful to divide the domain into three parts: a wiggly domain at the center and two tail domains, one on the right side and the other on the left.\n\n\n\n\n## Warning: All aesthetics have length 1, but the data has 101 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 101 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 101 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n\n\n\n\n\n\n\n\n\nFigure 27.1: A \\(n\\)th-order polynomial can have up to \\(n-1\\) critical points it wriggles among. This 7th-order polynomial has six local maxima and minima.\n\n\n\nFigure 27.1 shows a 7th order polynomial—that is, the highest-order term is \\(x^7\\). In the wriggly domain in Figure 27.1, there are six argmins or argmaxes. In one of the tail domains the function value heads off to \\(\\infty\\), in the other to \\(-\\infty\\). This is an inescapable feature of all odd-order polynomials: 1, 3, 5, 7, …\nIn contrast, for even-order polynomials (2, 4, 6, …) the function values in the two tail domains go in the same direction, either to \\(\\infty\\) (Hands up!) or to \\(-\\infty\\).\nPolynomials’ runaway behavior does not provide insurance against wild, misleading extrapolations of model formulas. Instead, sigmoid, Gaussian, and sinusoid functions, as well as more modern constructions such as “smoothers,” “natural splines,” and the “wavelets” originating in fractal theory, provide such insurance.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Polynomials for approximating functions</span>"
    ]
  },
  {
    "objectID": "Differentiation/27-taylor.html#indeterminate-forms",
    "href": "Differentiation/27-taylor.html#indeterminate-forms",
    "title": "27  Polynomials for approximating functions",
    "section": "27.3 Indeterminate forms",
    "text": "27.3 Indeterminate forms\nLet’s return to an issue that has bedeviled mathematicians for millennia and misled even the famous Brahmagupta. This is the question of dividing zero by zero or, more generally, dividing any number by zero.\nElementary-school students learn that it is illegal to “divide by zero.” Happily, the punishment for breaking the law is, for most students, a deep red mark on a homework or exam paper. Much more serious consequences, however, can sometimes occur, especially in computer programming. (See Section 27.4.)\nThere is a legal loophole, however, that arises in functions like \\[\\text{sinc}(x)  \\equiv \\frac{\\sin(x)}{x}\\] that involve an input in a position to cause a division by zero, as with the denominator in \\(\\frac{\\sin(x)}{x}\\).\nThe sinc() function (pronounced “sink”) is important today, in part because of its role in converting discrete-time measurements (as in an mp3 recording of sound into continuous signals. So there really are occasions that call for evaluating it at zero input.\nWhat is the value of \\(\\text{sinc}(0)\\)? One answer, favored by arithmetic teachers is that \\(\\text{sinc}(0)\\) is meaningless, because it involves division by zero.\nOn the other hand, \\(\\sin(0) = 0\\) as well, so the sinc function evaluated at zero involves 0/0. This quotient is called an indeterminate form. The logic is this: Suppose \\(0/0 = b\\) for some number \\(b\\). then \\(0 = 0 \\times b = 0\\). So any value of \\(b\\) would do; the value of \\(0/0\\) is “indeterminate.”\nStill another answer is suggested by plotting out sinc(\\(x\\)) near \\(x=0\\) and reading the value off the graph: sinc(0) = 1.\n\n\n\nActive R chunk 27.1\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nTo judge from the output of Active R chunk 27.1, \\(\\sin(0) / 0 = 1\\).\nThe graph of sinc() looks smooth and the shape makes sense. Even if we zoom in very close to \\(x=0\\), the graph continues to look smooth. We call such functions well behaved.\nCompare the well-behaved sinc() to a very closely related function (which does not seem to be so important in applied work): \\(\\frac{\\sin(x)}{x^3}\\).\nBoth \\(\\sin(x)/x\\) and \\(\\sin(x) / x^3\\) involve a divide by zero when evaluated at \\(x=0\\). Both are indeterminate forms 0/0 at \\(x=0\\). But the graph of \\(\\sin(x) / x^3\\) (see Active R chunk 27.2) is not well behaved. \\(\\sin(x) / x^3\\) does not have any particular value at \\(x=0\\); instead, it has an asymptote.\n\n\n\nActive R chunk 27.2\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nActive R chunk 27.2 zooms in around the division by zero. Top: The graph of \\(\\sin(x)/x\\) versus \\(x\\). Bottom: The graph of \\(\\sin(x)/x^2\\). The vertical scales on the two graphs are utterly different.\nSince both \\(\\sin(x)/x\\left.{\\Large\\strut}\\right|_{x=0}\\) and \\(\\sin(x)/x^3\\left. {\\Large\\strut}\\right|_{x=0}\\) involve a divide-by-zero, the answer to the utterly different behavior of the two functions is not to be found at zero. Instead, it is to be found near zero. For any non-zero value of \\(x\\), the arithmetic to evaluate the functions is straight-forward. Note that \\(\\sin(x) / x^3\\) starts its mis-behavior away from zero. The slope of \\(\\sin(x) / x^3\\) is very large near \\(x=0\\), while the slope of \\(\\sin(x) / x\\) smoothly approaches zero.\nSince we are interested in behavior near \\(x=0\\), a useful technique is to approximate the numerator and denominator of both functions by polynomial approximations.\n\n\\(\\sin(x) \\approx x - \\frac{1}{6} x^3\\) near \\(x=0\\)\n\\(x\\) is already a polynomial.\n\\(x^3\\) is already a polynomial.\n\nAs we will see in Section 27.6 , these approximations are exact as \\(x\\) goes to zero. So, when \\(x\\) is sufficiently small, that is, evanescent,\n\\[\\frac{\\sin(x)}{x} = \\frac{x - \\frac{1}{6} x^3}{x} = 1 + \\frac{1}{6} x^2\\] Even at \\(x=0\\), there is nothing indeterminate about \\(1 + x^2/6\\); it is simply 1.\nCompare this to the polynomial approximation to \\(\\sin(x) / x^3\\): \\[\\frac{\\sin(x)}{x^3} = \\frac{x - \\frac{1}{6} x^3}{x^3} = \\frac{1}{x^2} - \\frac{1}{6}\\]\nEvaluating this at \\(x=0\\) involves division by zero. No wonder it is badly behaved.\nThe procedure for checking whether a function involving division by zero behaves well or poorly is described in the first-ever calculus textbook, published in 1697. The title (in English) is: The analysis into the infinitely small for the understanding of curved lines. In honor of the author, the Marquis de l’Hospital, the procedure is called l’Hôpital’s rule.1\nConventionally, the relationship is written \\[\\lim_{x\\rightarrow x_0} \\frac{u(x)}{v(x)} = \\lim_{x\\rightarrow x_0} \\frac{\\partial_x u(x)}{\\partial_x v(x)}\\]\nLet’s try this out with our two example functions around \\(x=0\\):\n\\[\\lim_{x\\rightarrow 0} \\frac{\\sin(x)}{x} = \\frac{\\lim_{x\\rightarrow 0} \\cos(x)}{\\lim_{x \\rightarrow 0} 1} = \\frac{1}{1} = 1\\]\n\\[\\lim_{x\\rightarrow 0} \\frac{\\sin(x)}{x^3} = \\frac{\\lim_{x\\rightarrow 0} \\cos(x)}{\\lim_{x \\rightarrow 0} 3x^2} = \\frac{1}{0} \\ \\ \\ \\ \\text{Indeterminate}!\\] There are other indeterminate forms that involve infinity rather than zero. The mathematical symbol for infinity, \\(\\infty\\), was introduced for this purpose in 1655 but the character has a much longer history as a decorative item. The key to understanding indeterminate forms involving \\(\\infty\\) is to recognize that it is closely related to \\(\\frac{1}{0}\\).\nA careless author who states simply that \\(\\infty \\equiv \\frac{1}{0}\\) will earn the contempt of mathematicians who understand that legitimate statements can only be made when they involve the evanescent, as in \\[\\lim_{h \\rightarrow 0} \\frac{1}{h}\\] which states clearly that \\(h \\neq 0\\).But using the sloppy, non-evanescent notation is convenient for starting to understand why constructions like \\[\\infty \\times 0\\ \\ \\text{or}\\ \\ \\frac{\\infty}{\\infty}\\] are indeterminate forms related to \\(\\frac{0}{0}\\). Using the sloppy notation \\(\\infty \\equiv \\frac{1}{0}\\) provides clarity:\n\\[\\infty \\times 0 = \\frac{1}{0} \\times 0 = \\frac{1 \\times 0}{0} = \\frac{0}{0}\\ \\ \\text{and, similarly, }\\ \\ \\frac{\\infty}{\\infty} = \\frac{1/0}{1/0} = 0/0 .\\]",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Polynomials for approximating functions</span>"
    ]
  },
  {
    "objectID": "Differentiation/27-taylor.html#sec-computing-indeterminate",
    "href": "Differentiation/27-taylor.html#sec-computing-indeterminate",
    "title": "27  Polynomials for approximating functions",
    "section": "27.4 Computing with indeterminate forms",
    "text": "27.4 Computing with indeterminate forms\nIn the early days of electronic computers, division by zero would cause a fault in the computer, often signaled by stopping the calculation and printing an error message to some display. This was inconvenient since programmers did not always foresee and avoid division-by-zero situations.\nAs you’ve seen, modern computers have adopted a convention that simplifies programming considerably. Instead of stopping the calculation, the computer just carries on normally, but produces as a result one of two indeterminate forms: Inf and NaN.\nInf is the output for the simple case of dividing zero into a non-zero number, for instance:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNaN, standing for “not a number,” is the output for more challenging cases: dividing zero into zero, multiplying Inf by zero, or dividing Inf by Inf.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe idea’s brilliance is that any calculation that involves NaN will return a value of NaN. This might seem to get us nowhere. However, most programs are built out of other programs, usually written by people interested in different applications. You can use those programs (mostly) without worrying about the implications of a divide by zero. If it is important to respond in some particular way, you can always check the result for being NaN in your own programs. (Much the same is true for Inf, although dividing a non-Inf number by Inf will return 0.)\nPlotting software will often treat NaN values as “don’t plot this.” that is why it is possible to make a sensible plot of \\(\\sin(x)/x\\) even when the plotting domain includes zero.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Polynomials for approximating functions</span>"
    ]
  },
  {
    "objectID": "Differentiation/27-taylor.html#multiple-inputs",
    "href": "Differentiation/27-taylor.html#multiple-inputs",
    "title": "27  Polynomials for approximating functions",
    "section": "27.5 Multiple inputs?",
    "text": "27.5 Multiple inputs?\nHigh-order polynomials are rarely used with multiple inputs. One reason is the proliferation of coefficients. For instance, here is the third-order polynomial in two inputs, \\(x\\), and \\(y\\). \\[\\underbrace{b_0 + b_x x + b_y y}_\\text{first-order terms} + \\underbrace{b_{xy} x y + b_{xx} x^2 + b_{yy} y^2}_\\text{second-order terms} + \\underbrace{b_{xxy} x^2 y + b_{xyy} x y^2 + b_{xxx} x^3 + b_{yyy} y^3}_\\text{third-order terms}\\]\nThis has 10 coefficients. With so many coefficients it is hard to ascribe meaning to any of them individually. And, insofar as some feature of the function does carry meaning in the modeling situation, that meaning is spread out and hard to quantify.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Polynomials for approximating functions</span>"
    ]
  },
  {
    "objectID": "Differentiation/27-taylor.html#sec-high-order-approx",
    "href": "Differentiation/27-taylor.html#sec-high-order-approx",
    "title": "27  Polynomials for approximating functions",
    "section": "27.6 High-order approximations",
    "text": "27.6 High-order approximations\nDespite the pitfalls of high-order polynomials, they are dear to theoretical mathematicians. In particular, mathematicians find benefits to approximating known functions by high-order polynomials.\nThe applied student might wonder, what is the point of approximating known functions. If we know the function, why approximate? One good reason is to simplify calculations. For instance, suppose you need the value of the pattern-book function \\(\\ln(x)\\) near \\(x=1\\). We know a lot about \\(\\ln(x)\\) at \\(x=1\\) which we can use to construct a simple approximation for the nearby values. For instance, in Chapter 5 we emphasized the fact that \\(\\ln(1) = 0\\).\nAnother important fact about \\(\\ln(x)\\) at zero is its derivative \\(\\partial_x \\ln(x)\\). Using the differentiation rules from Chapter 23 we can perform a useful trick. We know that the exponential function and logarithmic functions are inverses, that is,\n\\[\\ln(\\exp(x)) = x . \\tag{27.1}\\]\nLet’s differentiate both sides. The right-hand side is easy: \\(\\partial_x x = 1\\). The left-hand side is a bit more intricate. Using the chain rule on the left gives\n\\[\\partial_x \\ln(\\exp(x)) = \\partial_x \\ln (\\exp(x)) \\times \\partial_x \\exp(x) = \\exp(x) \\times \\partial_x \\ln(\\exp(x))\\] which may not look like a promising start. But, remembering that the derivative of the right-hand side of #eq-log-exp-x is 1, we have:\n\\[\\partial_x \\ln(\\exp(x)) = \\frac{1}{\\exp(x)}\\] Whatever the input \\(x\\) is, the output of \\(\\exp(x)\\) is some number. Let’s call that number \\(y\\). This gives \\[\\partial_x \\ln(y) = \\frac{1}{y}.\\] At \\(y=1\\), \\(\\partial_y\\ln(1) = 1\\). But \\(y\\) is just the name of the argument and we can replace that name with any other, for instance, \\(x\\). So \\(\\partial_x\\ln(x=1) = 1.\\)\nIt’s easy to differentiate power-law functions like \\(1/x\\). For instance: \\[\\partial_x \\frac{1}{x} = - \\frac{1}{x^2} \\ \\ \\text{and}\\ \\ \\partial_{xx} \\frac{1}{x} = - \\partial_x \\left[\\frac{1}{x^2}\\right] =  \\frac{2}{x^3} .\\] We can keep on going in this manner to find higher derivatives, all of which will have a form like \\(\\pm \\frac{(n-1)!}{x^n}\\). Evaluated at \\(x=1\\), our input of interest, these become \\(\\pm (n-1)!\\) for the nth derivative.\nIn other words, we know a lot about \\(\\ln(x)\\) at \\(x=1\\)—the function value as well as all of its derivatives.\nFor polynomials, the output can be calculated using just multiplication and addition, which makes them attractive for numerical evaluation. As well we can differentiate to any order any polynomial. And, with a bit of practice, we can write down a polynomial whose derivative is a given number at some input value. Once we have learned how to do this, we can, for instance, write down a polynomial that shares the value and derivatives of \\(\\ln(x)\\) at \\(x=1\\). Such a polynomial is called a Taylor Polynomial. If we imagine writing down such a polynomial to infinite order, the result is called a Taylor Series.\nThe significance of this fact is that we can write a polynomial approximation to any function whose value and derivatives can be evaluated, if only at a single input value like \\(x=1\\) we used for \\(\\ln(x)\\).\nTo illustrate, consider another pattern-book function, \\(\\sin(x)\\). We know the value of \\(\\sin(x=0) = 0\\). We can also calculate the derivatives of any order; just walk down the sequence \\[\\cos(x), -\\sin(x), - \\cos(x), \\sin(x), ...\\] and so on in a repeating cycle. Thus, we know the numerical value of \\(\\sin()\\) and its derivatives of any order at an input \\(x=0\\), using the fact that \\(\\cos(0) = 1\\).\nConsider this polynomial: \\[g(x) \\equiv x - \\frac{1}{6} x^3\\] Since the highest-order term is \\(x^3\\) this is a third-order polynomial. (As you will see, we picked these particular coefficients, 0, 1, 0, -1/6, for a reason.) With such simple coefficients the polynomial is easy to handle by mental arithmetic. For instance, for \\(g(x=1)\\) is \\(5/6\\). Similarly, \\(g(x=1/2) = 23/48\\) and \\(g(x=2) = 2/3\\). A person of today’s generation would use an electronic calculator for more complicated inputs, but the mathematicians of Newton’s time were accomplished human calculators. It would have been well within their capabilities to calculate, using paper and pencil, \\(g(\\pi/4) = 0.7046527\\).2\nOur example polynomial, \\(g(x) \\equiv x - \\frac{1}{6}x^3\\), graphed in color in Figure 27.2, does not look exactly like the sinusoid. If we increased the extent of the graphics domain, the disagreement would be even more striking, since the sinusoid’s output is always in \\(-1 \\leq \\sin(x) \\leq 1\\), while the polynomial’s tails are heading off to \\(\\infty\\) and \\(-\\infty\\). But, for a small interval around \\(x=0\\), exactly aligns with the sinusoid.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 27.2: The polynomial \\(g(x) \\equiv x -x^3 / 6\\) is remarkably similar to \\(\\sin(x)\\) near \\(x=0\\).\n\n\n\nIt is clear from the graph that the approximation is excellent near \\(x=0\\) and gets worse as \\(x\\) gets larger. The approximation is poor for \\(x \\approx \\pm 2\\). We know enough about polynomials to say that the approximation will not get better for larger \\(x\\); the sine function has a range of \\(-1\\) to \\(1\\), while the left and right tails of the polynomial are running off to \\(\\infty\\) and \\(-\\infty\\) respectively.\nOne way to measure the quality of the approximation is the error \\({\\cal E}(x)\\) which gives, as a function of \\(x\\), the difference between the actual sinusoid and the approximation: \\[{\\cal E}(x) \\equiv |\\strut\\sin(x) - g(x)|\\] The absolute value used in defining the error reflects our interest in how far the approximation is from the actual function and not so much in whether the approximation is below or above the actual function. Figure 27.3 shows \\({\\cal E}(x)\\) as a function of \\(x\\). Since the error is the same on both sides of \\(x=0\\), only the positive \\(x\\) domain is shown.\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\nFigure 27.3: The error \\({\\cal E}(x)\\) of \\(x - x^3/6\\) as an approximation to \\(\\sin(x)\\). Top panel: linear scale. Bottom panel: on a log-log scale.\n\n\n\nFigure 27.3 shows that for \\(x &lt; 0.3\\), the error in the polynomial approximation to \\(\\sin(x)\\) is in the 5th decimal place. For instance, \\(\\sin(0.3) = 0.2955202\\) while \\(g(0.3) = 0.2955000\\).\nThat the graph of \\({\\cal E}(x)\\) is a straight-line on log-log scales diagnoses \\({\\cal E}(x)\\) as a power law. That is: \\({\\cal E}(x) = A x^p\\). As always for power-law functions, we can estimate the exponent \\(p\\) from the slope of the graph. It is easy to see that the slope is positive, so \\(p\\) must also be positive.\nThe inevitable consequence of \\({\\cal E}(x)\\) being a power-law function with positive \\(p\\) is that \\(\\lim_{x\\rightarrow 0} {\\cal E}(x) = 0\\). That is, the polynomial approximation \\(x - \\frac{1}{6}x^3\\) is exact as \\(x \\rightarrow 0\\).\nThroughout this book, we’ve been using straight-line approximations to functions around an input \\(x_0\\). \\[g(x) = f(x_0) + \\partial_x f(x_0) [x-x_0]\\] One way to look at \\(g(x)\\) is as a straight-line function. Another way is as a first-order polynomial. This raises the question of what a second-order polynomial approximation should be. Rather than the polynomial matching just the slope of \\(f(x)\\) at \\(x_0\\), we can arrange things so that the second-order polynomial will also match the curvature of the \\(f()\\). Since the curvature involves only the first and second derivatives of a function, the polynomial constructed to match both the first and the second derivative will necessarily match the slope and curvature of \\(f()\\). This can be accomplished by setting the polynomial coefficients appropriately.\nStart with a general, second-order polynomial centered around \\(x_0\\): \\[g(x) \\equiv a_0 + a_1 [x-x_0] + a_2 [x - x_0]^2\\] The first- and second-derivatives, evaluated at \\(x=x_0\\) are: \\[\\partial_x g(x)\\left.{\\Large\\strut}\\right|_{x=x_0} = a_1 + 2 a_2 [x  - x_0] \\left.{\\Large\\strut}\\right|_{x=x_0} = a_1\\] \\[\\partial_{xx} g(x)\\left.{\\Large\\strut}\\right|_{x=x_0} =  2 a_2\\] Notice the 2 in the above expression. When we want to express the coefficient \\(a_2\\) using the second derivative of \\(g()\\), we will end up with\n\\[a_2 = \\frac{1}{2} \\partial_{xx} g(x)\\left.{\\Large\\strut}\\right|_{x=x_0}\\]\nTo make \\(g(x)\\) approximate \\(f(x)\\) at \\(x=x_0\\), we need merely set \\[a_1 = \\partial_x f(x)\\left.{\\Large\\strut}\\right|_{x=x_0}\\] and \\[a_2 = \\frac{1}{2} \\partial_{xx} f(x) \\left.{\\Large\\strut}\\right|_{x=x_0}\\] This logic can also be applied to higher-order polynomials. For instance, to match the third derivative of \\(f(x)\\) at \\(x_0\\), set \\[a_3 = \\frac{1}{6} \\partial_{xxx} f(x)  \\left.{\\Large\\strut}\\right|_{x=x_0}\\] Remarkably, each coefficient in the approximating polynomial involves only the corresponding order of derivative. \\(a_1\\) involves only \\(\\partial_x f(x)   \\left.{\\Large\\strut}\\right|_{x=x_0}\\); the \\(a_2\\) coefficient involves only \\(\\partial_{xx} f(x)     \\left.{\\Large\\strut}\\right|_{x=x_0}\\); the \\(a_3\\) coefficient involves only \\(\\partial_{xx} f(x)     \\left.{\\Large\\strut}\\right|_{x=x_0}\\), and so on.\nNow we can explain where the polynomial that started this section, \\(x - \\frac{1}{6} x^3\\) came from and why those coefficients make the polynmomial approximate the sinusoid near \\(x=0\\).\n\n\n\n\n\n\n\n\nOrder\n\\(\\sin(x)\\) derivative\n\\(x - \\frac{1}{6}x^3\\) derivative\n\n\n\n\n0\n\\(\\sin(x) \\left.{\\Large\\strut}\\right|_{x=0} = 0\\)\n\\(\\left( 1 - \\frac{1}{6}x^3\\right)\\left.{\\Large\\strut}\\right|_{x=0} = 0\\)\n\n\n1\n\\(\\cos(x) \\left.{\\Large\\strut}\\right|_{x=0} = 1\\)\n\\(\\left(1 - \\frac{3}{6} x^2\\right) \\left.{\\Large\\strut}\\right|_{x=0}= 1\\)\n\n\n2\n\\(-\\sin(x) \\left.{\\Large\\strut}\\right|_{x=0} = 0\\)\n\\(\\left(- \\frac{6}{6} x\\right) \\left.{\\Large\\strut}\\right|_{x=0} = 0\\)\n\n\n3\n\\(-\\cos(x) \\left.{\\Large\\strut}\\right|_{x=0} = -1\\)\n\\(- 1\\left.{\\Large\\strut}\\right|_{x=0} = -1\\)\n\n\n4\n\\(\\sin(x) \\left.{\\Large\\strut}\\right|_{x=0} = 0\\)\n\\(0\\left.{\\Large\\strut}\\right|_{x=0} = 0\\)\n\n\n\nThe first four derivatives of \\(x - \\frac{1}{6} x^3\\) exactly match, at \\(x=0\\), the first four derivatives of \\(\\sin(x)\\).\nThe polynomial constructed by matching successive derivatives of a function \\(f(x)\\) at some input \\(x_0\\) is called a Taylor polynomial.\n\n\n\n\n\n\nTip 27.1: Practice: a Taylor polynomial for \\(e^x\\).\n\n\n\nLet’s construct a 3rd-order Taylor polynomial approximation to \\(f(x) = e^x\\) around \\(x=0\\).\nWe know it will be a 3rd order polynomial: \\[g_{\\exp}(x) \\equiv a_0 + a_1 x + a_2 x^2 + a_3 x^3\\] The exponential function is particularly nice for examples because the function value and all its derivatives are identical: \\(e^x\\). So\n\\[f(x= 0) = 1\\]\n\\[ \\partial_x f(x=0) = 1\\] \\[\\partial_{xx} f(x=0) = 1\\] \\[\\partial_{xxx} f(x=0) = 1\\] and so on.\nThe function value and derivatives of \\(g_{\\exp}(x)\\) at \\(x=0\\) are: \\[g_{\\exp}(x=0) = a_0\\] \\[\\partial_{x}g_{\\exp}(x=0) = a_1\\] \\[\\partial_{xx}g_{\\exp}(x=0) = 2 a_2\\]\n\\[\\partial_{xxx}g_{\\exp}(x=0) = 2\\cdot3\\cdot a_3 = 6\\, a_3\\] Matching these to the exponential evaluated at \\(x=0\\), we get \\[a_0 = 1\\] \\[a_1 = 1\\] \\[a_2 = \\frac{1}{2}\\] \\[a_3 = \\frac{1}{2 \\cdot 3} = \\frac{1}{6}\\]\nResult: the 3rd-order Taylor polynomial approximation to the exponential at \\(x=0\\) is \\[g_{\\exp}(x) = 1 + x + \\frac{1}{2} x^2 +  \\frac{1}{2\\cdot 3} x^3 +\\frac{1}{2\\cdot 3\\cdot 4} x^4\\]\nFigure 27.4 shows the exponential function \\(e^x\\) and its 3th-order Taylor polynomial approximation near \\(x=0\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 27.4: The 3th-order Taylor polynomial approximation (magenta) to \\(e^x\\) around \\(x=0\\)\n\n\n\nThe polynomial is exact at \\(x=0\\). The error \\({\\cal E}(x)\\) grows with increasing distance from \\(x=0\\):\n\n\n\n\n\n\n\n\n\n\n\n\nlinear axes\n\n\n\n\n\n\n\nlog-log scale\n\n\n\n\n\n\n\nFigure 27.5: The error from a 3rd-order Taylor polynomial approximation to \\(e^x\\) around \\(x=0\\) is a power-law function with exponent \\(4\\).\n\n\n\nThe plot of \\(\\log_{10} {\\cal E}(x)\\) versus \\(\\log_{10} | x |\\) in Figure 27.5 shows that the error grows from zero at \\(x=0\\) as a power-law function. Measuring the exponent of the power-law from the slope of the graph on log-log axes give \\({\\cal E}(x) = a |x-x_0|^5\\). This is typical of Taylor polynomials: for a polynomial of degree \\(n\\), the error will grow as a power-law with exponent \\(n+1\\). This means that the higher is \\(n\\), the faster \\(\\lim_{x\\rightarrow x_0}{\\cal E}(x) \\rightarrow 0\\). On the other hand, since \\({\\cal E}_x\\) is a power law function, as \\(x\\) gets further from \\(x_0\\) the error grows as \\(\\left(x-x_0\\right)^{n+1}\\).\n\n\n\n\n\n\n\n\nCalculus history—Polynomial models of other functions\n\n\n\nBrooke Taylor (1685-1731), a near contemporary of Newton, published his work on approximating polynomials in 1715. Wikipedia reports: “[T]he importance of [this] remained unrecognized until 1772, when Joseph-Louis Lagrange realized its usefulness and termed it ‘the main [theoretical] foundation of differential calculus’.”Source\n\n\n\n\n\n\nFigure 27.6: Brooke Taylor\n\n\n\nDue to the importance of Taylor polynomials in the development of calculus, and their prominence in many calculus textbooks, many students assume their use extends to constructing models from data. They also assume that third- and higher-order monomials are a good basis for modeling data. Both these assumptions are wrong. Least squares is the proper foundation for working with data.\nTaylor’s work preceded by about a century the development of techniques for working with data. One of the pioneers in these new techniques was Carl Friedrich Gauss (1777-1855), after whom the gaussian function is named. Gauss’s techniques are the foundation of an incredibly important statistical method that is ubiquitous today: least squares. Least squares provides an entirely different way to find the coefficients on approximating polynomials (and an infinite variety of other function forms). The R/mosaic fitModel() function for polishing parameter estimates is based on least squares. In Block 5, we will explore least squares and the mathematics underlying the calculations of least-squares estimates of parameters.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Polynomials for approximating functions</span>"
    ]
  },
  {
    "objectID": "Differentiation/27-taylor.html#footnotes",
    "href": "Differentiation/27-taylor.html#footnotes",
    "title": "27  Polynomials for approximating functions",
    "section": "",
    "text": "In many French words, the sequence “os” has been replaced by a single, accented letter, ô.↩︎\nUnfortunately for these human calculators, pencils weren’t invented until 1795. Prior to the introduction of this advanced, graphite-based computing technology, mathematicians had to use quill and ink.↩︎",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Polynomials for approximating functions</span>"
    ]
  },
  {
    "objectID": "Differentiation/Differentiation-projects.html",
    "href": "Differentiation/Differentiation-projects.html",
    "title": "28  Differentiation projects",
    "section": "",
    "text": "28.1 Labor vs capital\nThe Cobb-Douglas production function is a simple mathematical model of how labor \\(L\\) and capital \\(K\\) combine to produce a factory’s output \\(P\\). It is \\[P(L, K) \\equiv A K^\\alpha L^{1-\\alpha}\\ .\\]\nFor simplicity, imagine that capital and labor are both measured in dollars per year—the amount that the labor force is paid in a year and the amount that one could rent a factory for a year.\nWe’ll stick with numbers like \\(K = 10\\) and \\(L = 20\\) to keep things easy to read, but feel free to interpret them as “millions of dollars.”\nCongratuations! Based on your ability to use the Cobb-Douglas model, you’ve been promoted to manager of the factory. One of your jobs is to decide how to balance expenditures on capital and labor to raise productivity.\nOne basic question is what happens when you raise either capital or labor, holding the other one constant. Using ap- propriate partial derivatives evaluated at \\(K = 10\\), \\(L = 20\\), calculate:\nYour economist friend tells you to watch out for “diminishing marginal returns.” This means that, as you increase spending on either labor or capital, the rate of increase in production tends to diminish. You’ll still get increased production as you increase spending, but it won’t increase as fast at high levels of expense as at low levels.\nBut what happens to the value rate of labor when capital spending is increased? You can answer this by comparing the value rate of labor, \\(\\partial_L P\\) , at two different capital spending levels, say \\((K = 10,L = 20)\\) and \\((K = 11,L = 20)\\). Notice that even though you’re looking at the rate with respect to labor, you’re changing the expenditure on capital.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Differentiation projects</span>"
    ]
  },
  {
    "objectID": "Differentiation/Differentiation-projects.html#labor-vs-capital",
    "href": "Differentiation/Differentiation-projects.html#labor-vs-capital",
    "title": "28  Differentiation projects",
    "section": "",
    "text": "If production \\(P(L, K)\\) is also measured in dollars per year (say, the value of the factory output each year), what is the dimension of the constant \\(A\\)?\nAccording to the model, what happens to production if both \\(K\\) and \\(L\\) are increased by a factor constant factor \\(\\beta\\)? (Hint: Substitute in \\(K \\rightarrow \\beta K\\) and \\(L \\rightarrow \\beta L\\) and simplify.)\nConsider a particular factory with \\(A = 2.5\\) and exponent \\(\\alpha = 0.33\\). In a sandbox, implement the function \\(P (K, L)\\). Use your function to compute the production of the factory for \\(K = 10\\) and \\(L=20\\). Confirm that you get \\(P(K=10,L=20)= 39.78\\)\nA factory that rents for $10/yr and where the labor costs $20/yr is silly. Calculate the value \\(P (K, L)\\) when \\(K\\) is $10 million/yr and \\(L\\) is $20 million/yr.\n\n\n\n\n\nThe rate at which an increase in spending on labor will increase productivity.\nThe rate at which an increase in spending on capital will increase productivity.\nBased on the above, if you had to choose between spending on capital or labor, and your goal is to increase productivity as much as possible, which would you spend on, capital or labor?\n\n\n\nCompute the partial derivative of production with respect to labor at a higher level of labor, say \\(L = 21\\), but holding \\(K = 10\\). How does the value of the derivative at \\(L = 21\\) compare to that at \\(L = 20\\)? Is this consistent with the idea of “diminishing marginal returns” for labor?\nDo the same for the partial derivative of production with respect to capital, evaluated at \\(L = 20\\) and \\(K = 11\\). How does the value of the derivative at \\(K = 11\\) compare to that at \\(K = 10\\). Is this consistent with the idea of “diminishing marginal returns” for capital?\nUse an appropriate partial second derivative to find the rate of diminishing partial returns for labor at \\(L = 20\\) and \\(K = 10\\). Show that it is consistent with the difference you got in Part (d).\nUse an appropriate partial second derivative to find the rate of diminishing partial returns for capital at \\(L = 20\\) and \\(K = 10\\). Show that it is consistent with the difference you got in Part (3).\nYou might think of the rate of increase in production with respect to labor as the “value rate” of labor. Similarly, the rate of increase in production with respect to capital is the value rate of capital. Due to diminishing marginal returns, an increase in labor spending, holding capital constant, decreases the value rate of labor. Similarly, an increase in capital spending holding labor spending constant decreases the value rate of capital.\n\n\ni. Compare $\\partial_L P$ at slightly different values of $K$ , holding $L$ constant at 20. Does the value rate of labor increase or decrease with spending on capital?\nii. Similarly, compare $\\partial_K P$ at slightly different values of  $L$, holding $K$ constant at 20. Does the value rate of labor increase or decrease with spending on capital?\niii. Finally, construct and evaluate the mixed partial derivative, $\\partial_L \\partial_K P at $K = 10$, $L = 20$. Compare this to the results you got for the way $\\partial_K P$ changes with increasing $L$ and the way $\\partial_L P$ changes with increasing $K$.\n\n28.1.1 Walking\nIf you’re like many people, you find it harder to walk uphill than down, and find it takes more out of you to walk longer distances than shorter. Let’s build a model of this, using nothing more than your intuition and the method of low-order polynomial approximations.\nLet’s call the map distance walked \\(d\\). (“Map distance” is the horizontal change in position, disregarding vertical changes.) The steepness of the hill will be the “grade” \\(g\\), which is measured as the horizontal distance covered divided by the vertical climb. If you’re going downhill, the grade is negative.\nThe key ingredient in the model: we will measure the “difficulty” or “exertion” to walking as the energy consumed during the walk: \\(E(d, g)\\).\nSome assumptions about walking and energy consumed:\n\nIf you don’t walk, you consume zero energy walking.\nThe energy consumed should be proportional to the length of the walk. This is an assumption, and is probably valid, only for walks of short to medium distances, as opposed to forced marches over tens of miles.\n\nWe will start with the full 2nd-order polynomial in two inputs, and then seek to eliminate terms that aren’t needed.\n\\[E_{big}(d, g) \\equiv a_0 + a_d\\, d + a_g\\, g + a_{dg}\\, d\\, g + a_{dd}\\,d^2 + a_{gg}\\,g^2\\] According to assumption (1), when \\(E(d=0, g) = 0\\). Of course, if you are walking zero distance, it does not matter what the grade is; the energy consumed is still zero.\nConsequently, we know that all terms that don’t include a \\(d\\) should go away. This leaves us with\n\\[E_{medium}(d, g) \\equiv  a_d\\, d + a_{dg}\\, d\\, g + a_{dd}\\,d^2 = d \\left[\\strut a_d + a_{dg}\\, g + a_{dd}\\,d\\right]\\] Assumption (2) says that energy consumed is proportional to \\(d\\). The multiplier on \\(d\\) in \\(E_{medium}()\\) is \\(\\left[\\strut a_d + a_{dg}\\, g + a_{dd}\\,d\\right]\\) which is itself a function of \\(d\\). A proportional relationship implies a multiplier that does not depend on the quantity itself. This means that \\(a_{dd} = 0\\).\nThis leaves us with a very simple model: \\[E(d, g) \\equiv \\left[\\strut a_1 + a_2\\, g\\right]\\, d\\] where we have simplified the labeling on the coefficients since there are only two in the model.\nPerhaps assumption (2) is misplaced and that the energy consumed per unit distance in a walk increases with the length of the walk. If so, we would need to return to the question of \\(a_{dd}\\). This is typical of the modeling cycle. Trying to be economical with model terms highlights the question of which terms are so small they can be ignored.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Differentiation projects</span>"
    ]
  },
  {
    "objectID": "linear-combinations-part.html",
    "href": "linear-combinations-part.html",
    "title": "BLOCK III. Vectors and linear combinations",
    "section": "",
    "text": "Often, quantities are made out of multiple components. For example, the location of an object in space has \\(x\\), \\(y\\), and \\(z\\) components. An important mathematical strategy for working with multiple components relates to the ideas of a vector and combinations of vectors as well as a set of vectors involved in a combination. (The set of vectors is called a matrix.)\nVectors appear naturally in physics: position, velocity, acceleration. They are also a principal building block of algorithms for machine learning, data science, and statistical modeling.\nThis Block introduces the basics of vectors and operations on vectors. There is a broad mathematical subject called “linear algebra” of which vectors and matrices are a part. Here, we focus on a compact set of ideas that are of particular importance in modeling and statistics.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations"
    ]
  },
  {
    "objectID": "Linear-combinations/28-Vectors.html",
    "href": "Linear-combinations/28-Vectors.html",
    "title": "29  Vectors",
    "section": "",
    "text": "29.1 Length & direction\nUntil now, our encounter with functions has been via formulas relating inputs to an output. Now we turn to a different way of representing functions: columns of numbers that we call vectors. The mathematical notation for the name of a vector shows the name surmounted with an arrow, for instance, \\(\\vec{u}\\).\nThe columns-of-numbers framework is central to technical work in many fields. For instance, an algorithm in this framework was the spark that ignited the modern era of search engines. The name given to it by mathematicians is linear algebra, although only the word “linear” conveys helpful information about the subject. (The physicists who developed the first workable quantum theory called it matrix mechanics. A matrix is a collection of vectors.)\nAlthough the words “algebra” and “quantum” may suggest that conceptual difficulties are in store, human intuition is well suited to establishing an understanding. In this book, we use two intuitive formats to introduce linear algebra: (1) geometric and visual and (2) simple arithmetic.\nA vector is a mathematical idea deeply rooted in everyday physical experience. Geometrically, a vector is simply an object consisting only of length and direction.\nA pencil is a physical metaphor for a vector, but a pencil has other non-vector qualities such as diameter, color, and an eraser. And, being a physical object, a pencil has a position in space.\nA line segment has an orientation but no forward or backward direction. In contrast, a vector has a unique direction: like an arrow, one end is the tip and the other the tail. In the pencil metaphor, the writing end is the tip; the eraser is the tail.\nVectors are always embedded in a vector space. Our physical stand-ins for vectors, the pencils, were photographed on a tabletop: a two-dimensional space. Naturally, pencils are embedded in everyday three-dimensional space. (The tabletop is a kind of two-dimensional subspace of three-dimensional space.)\nVectors embedded in three-dimensional space are central to physics and engineering. Quantities such as force, acceleration, and velocity are not simple numerical quantities but vectors with magnitude (that is, length) and direction. For instance, the statement, “The plane’s velocity is 450 miles per hour to the north-north-west,” is perfectly intelligible to most people, describing magnitude and direction. Note that the plane’s velocity vector does not specify the plane’s location; vectors have only the two qualities of magnitude and direction.\nThe gradients that we studied with partial differentiation (Chapter 25) are vectors. A gradient’s direction points directly uphill; its magnitude tells how steep the hill is.\nVectors often represent a change in position, that is, a step or displacement in the sense of “step to the left” or “step forward.” As we will see, constructing instructions for reaching a target is a standard mathematical task. Such instructions have a form like, “take three and a half steps along the green vector, then turn and take two steps backward along the yellow vector.” An individual vector describes a step of a specific length in a particular direction.\nVectors are a practical tool to keep track of relative motion. For instance, consider the problem of finding an aircraft heading and speed to intercept another plane that is also moving. Figure 29.3, a US Navy training movie from the 1950s, shows how to perform such calculations with paper and pencil.\nNowadays, the computer performs such calculations. On the computer, vectors are represented not by pencils (!) but by columns of numbers. For instance, two numbers will do for a vector embedded in two-dimensional space and three for a vector embedded in three-dimensional space. From these numbers, simple arithmetic can produce the vector magnitude and direction.\nRepresenting a vector as a set of numbers requires the imposition of a framework: a coordinate system. In Figure 29.4, the vector (shown by the green pencil) lies in a two-dimensional coordinate system. The two coordinates assigned to the vector are the difference between the tip and the tail along each coordinate direction. In the figure, there are 20 units horizontally and 16 units vertically, so the vector is \\((20, 16)\\).\nBy convention, when we write a vector as a set of coordinate numbers, we write the numbers in a column. For instance, the vector in Figure 29.4, which we will call \\(\\vec{green}\\), is written numerically as:\n\\[\\vec{green} \\equiv \\left[\\begin{array}{c}20\\\\16\\end{array}\\right]\\] In more advanced linear algebra, the distinction between a column vector (like \\(\\vec{green}\\)) and a row vector (like \\(\\left[20 \\ 16\\right]\\)) is important. For our purposes in this block, we need only column vectors.\nIn physics and engineering, vectors describe positions, velocities, acceleration, forces, momentum, and other functions of time or space. In mathematical notation, a vector-valued function can be written \\(\\vec{v}(t)\\). It is common to perform calculus operations such differentiation, writing it as \\(\\partial_t \\vec{v}(t)\\). It is sometimes easier to grasp a vector-valued function by writing it as a column of scalar-valued functions: \\[\\vec{v}(t) = \\left[\\begin{array}{c}v_x(t)\\\\v_y(t)\\\\v_z(t)\\end{array}\\right]\\] where the \\(x\\), \\(y\\), and \\(z\\) refer to the axes of the coordinate system.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/28-Vectors.html#length-direction",
    "href": "Linear-combinations/28-Vectors.html#length-direction",
    "title": "29  Vectors",
    "section": "",
    "text": "Figure 29.1: Three pencils, but just two vectors. The yellow and blue pencils have the same length and direction, so they are the same vector. A pencil has a position, but vectors do not. The green pencil shares the same direction but has a different length, so it differs from the blue/yellow vector.\n\n\n\n\n\n\n\n\n\n \n \n\n\n\nFigure 29.2: Two different vectors. They have the same length and are parallel but point in opposite directions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 29.3: A 1958 US Navy training film on calculations with relative motion. Available at https://youtu.be/j197C0XuNUA\n\n\n\n\n\n\n\n\n\n\n\nFigure 29.4: Representing a vector as a set of numbers requires reference to a coordinate system, shown here as graph paper.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry it! 29.1\n\n\n\n\n\n\n\n\n\nTry it! 29.1 Creating column vectors\n\n\n\nConstruct column vectors with the rbind() function, as in\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nCommas separate the arguments—the coordinate numbers—in the same way as any other R function.\nLater in this block, we will use data frames to define vectors. We will introduce the R syntax for that when we need it.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/28-Vectors.html#the-nth-dimension",
    "href": "Linear-combinations/28-Vectors.html#the-nth-dimension",
    "title": "29  Vectors",
    "section": "29.2 The nth dimension",
    "text": "29.2 The nth dimension\nIn many applications, especially those involving data, vectors have more than three components. Indeed, you will soon be working with vectors with hundreds of components. Services like Google search rely on vector calculations with millions of vectors, each having millions of components.\nLiving as we do in a palpably three-dimensional space and with senses and brains evolved for use in three dimensions, it is hard and maybe even impossible to grasp high-dimensional spaces.\nA lovely 1884 book, Flatland features the inhabitants of a two-dimensional world. The central character in the story, named Square, receives a visitor, Sphere. Sphere is from the three-dimensional world which embeds Flatland. Only with difficulty can Square assemble a conception of the totality of Sphere from the appearing, growing, and vanishing of Sphere’s intersection with the flat world. (Among other things, Flatland is a parody of humanity’s rigid thinking: Square’s attempt to convince Sphere that his three-dimensional world might be embedded in a four-dimensional one leads to rejection and disgrace. Sphere thinks he knows everything.)\n\n\n\n\n\n\nFigure 29.5: Flatland, a movie based on the 1884 book of the same name\n\n\n\nTo use high-dimensional vectors, represent them as a column of numbers.\n\\[\\left[\\begin{array}{r}6.4\\\\3.0\\\\-2.5\\\\17.3\\end{array}\\right]\\ \\ \\ \\left[\\begin{array}{r}-14.2\\\\-6.9\\\\18.0\\\\1.5\\\\-0.3\\end{array}\\right]\\ \\ \\ \\left[\\begin{array}{r}5.3\\\\-9.6\\\\84.1\\\\5.7\\\\-11.3\\\\4.8\\end{array}\\right]\\ \\ \\ \\cdots\\ \\ \\ \\left.\\left[\\begin{array}{r}7.2\\\\-4.4\\\\0.6\\\\-4.1\\\\4.7\\\\\\vdots\\ \\ \\\\-7.3\\\\8.3\\end{array}\\right]\\right\\} n\\]\nSensible people may see mathematical ostentation in promoting simple columns of numbers into “vectors in high-dimensional space.” But doing so lets us draw the analogy between data and familiar geometrical concepts: lengths, angles, alignment, etc. Operations that are mysterious as a long sequence of arithmetic steps become concrete when seen as geometry.\nThere is nothing science-fiction-like about so-called “high-dimensional” spaces; they don’t correspond to a physical place. Nevertheless, many-component vectors often appear in advanced physics. Famously, the Theory of Relativity involves 4-dimensional space-time. The vector representing the state of an ordinary particle contains the position and velocity, \\((x, y, z, v_x, v_y, v_z)\\), as well as angular velocity: nine dimensions. In statistics, engineering, and statistical mechanics, the term degrees of freedom is the preferred alternative to “dimension.” Another example: computer-controlled machine tools have 5 degrees of freedom (or more). There is a cutting tool with an \\(x, y, z\\) position and orientation. (If ever you start to freak out about the idea of a 10-dimensional space, close your eyes and remember that this is only shorthand for the set of arrays with ten elements.)",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/28-Vectors.html#geometry-arithmetic",
    "href": "Linear-combinations/28-Vectors.html#geometry-arithmetic",
    "title": "29  Vectors",
    "section": "29.3 Geometry & arithmetic",
    "text": "29.3 Geometry & arithmetic\nThree mathematical tasks are essential to working with vectors:\n\nMeasure the length of a vector.\nMeasure the angle between two vectors.\nCreate a new vector by scaling a vector. Scaling makes the new vector longer or shorter and may reverse the orientation.\n\nWe have simple geometrical tools for undertaking these tasks: a ruler measures length, and a protractor measures angles. Along with pen and paper, these tools let us draw new vectors of any specified length.\nThe geometrical perspective is helpful for many purposes, but often we need to work with vectors using computers. For this, we use the numerical representation of vectors.\nThis section introduces the arithmetic of vectors. With this arithmetic in hand, we can carry out the above three tasks (and more!) on vectors that consist of a column of numbers. And while we can’t import a ruler, protractor, or paper into high-dimensional space, arithmetic is easy to do, regardless of dimension.\nTo scale a vector \\(\\vec{w}\\) means more or less to change the vector’s length. A good mental image for scaling sees the vector as a step or displacement in the direction of \\(\\vec{w}\\). Scaling means to go on a simple walk, taking one step after the other in the same direction as the \\(\\vec{w}\\). We write a scaled vector by placing a number in front of the name of the vector. \\(3 \\vec{w}\\) is a short walk of three steps; \\(117 \\vec{w}\\) is a considerably longer walk; \\(-5 \\vec{w}\\) means to take five steps backward. You can also take fraction steps: \\(0.5 \\vec{w}\\) is half a step, \\(19.3 \\vec{w}\\) means to take 19 steps followed by a 30% step. Scaling a vector by \\(-1\\) means flipping the vector tip-for-tail; this does not change the length, just the orientation.\nArithmetically, scaling a vector is accomplished simply by multiplying each of the vector’s components by the same number. Two illustrate, consider vectors \\(\\vec{v}\\) and \\(\\vec{w}\\), each with \\(n\\) components. (We use \\(\\vdots\\) to indicate components we haven’t bothered to write out.)\n\\[\\vec{v} \\equiv \\left[\\begin{array}{r}6\\\\2\\\\-4\\\\\\vdots\\\\1\\\\8\\end{array}\\right]\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\vec{w} \\equiv \\left[\\begin{array}{r}-3\\\\1\\\\-5\\\\\\vdots\\\\2\\\\5\\end{array}\\right]\\] To scale a vector by 3 is accomplished by multiplying each component by 3\n\\[3\\, \\vec{v} = 3\\left[\\begin{array}{r}6\\\\2\\\\-4\\\\\\vdots\\\\1\\\\8\\end{array}\\right] = \\left[\\begin{array}{r}18\\\\6\\\\-12\\\\\\vdots\\\\3\\\\24\\end{array}\\right]\\] Vector scaling is perfectly ordinary multiplication applied component by component, that is, *** componentwise ***.\nScaling involves a number (the “scalar”) and a single vector. Other sorts of multiplication involve two or more vectors.\nThe dot product is one sort of multiplication of one vector with another. The dot product between \\(\\vec{v}\\) and \\(\\vec{w}\\) is written \\[\\vec{v} \\bullet \\vec{w}\\].\nThe arithmetic of the dot product involves two steps:\n\nMultiply the two vectors componentwise. For instance: \\[\\underset{\\Large \\vec{v}}{\\left[\\begin{array}{r}6\\\\2\\\\-4\\\\\\vdots\\\\1\\\\8\\end{array}\\right]}\\  \\underset{\\Large \\vec{w}}{\\left[\\begin{array}{r}-3\\\\1\\\\-5\\\\\\vdots\\\\2\\\\5\\end{array}\\right]} = \\left[\\begin{array}{r}-18\\\\2\\\\20\\\\\\vdots\\\\2\\\\40 \\end{array}\\right]\\]\nSum the elements in the componentwise product. For the component-wise product of \\(\\vec{v}\\) and \\(\\vec{w}\\), this will be \\(-18 + 2 + 20 + \\cdots +2 + 40\\). The resulting sum is an ordinary scalar quantity; a dot product takes two vectors as inputs and produces a scalar as an output.\n\n\n\n\n\n\n\n\n\nTry it! 29.2\n\n\n\n\n\n\n\n\n\nTry it! 29.2 Infix notation\n\n\n\nR/mosaic provides a beginner-friendly function for computing a dot product. To mimic the use of the dot, as in \\(\\vec{v} \\bullet \\vec{w}\\), the function will be invoked using infix notation. You have a lot of infix notation experience, even if you have never heard the term. Some examples:\n3 + 2       7 / 4      6 - 2      9 * 3     2 ^ 4\nInfix notation is distinct from the functional notation that you are also familiar with, for instance sin(2) or makeFun(x^2 ~ x).\nYou can, if you like, invoke the +, -, *, /, and ^ operations using functional notation. Nobody does this because the commands are so ugly:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe R language makes it possible to define new infix operators, but there is a catch. The new operators must always have a name that begins and ends with the % symbol, for example, %&gt;% or %*% or %dot%.\nHere is an example of using %dot% to calculate the dot product of two vectors embedded in five-dimensional space:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe vectors combined with %dot% must both have the same number of elements. Otherwise, an error message will result, as here:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe have not yet shown you the use of the dot product in applications. At this point, remember that a dot product is not ordinary multiplication but a two-stage operation of pairwise multiplication followed by summation.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/28-Vectors.html#sec-vector-length",
    "href": "Linear-combinations/28-Vectors.html#sec-vector-length",
    "title": "29  Vectors",
    "section": "29.4 Vector lengths",
    "text": "29.4 Vector lengths\nThe arithmetic used to calculate the length of a vector is based on the Pythagorean theorem. For a vector \\(\\vec{u} = \\left[\\begin{array}{c}4\\\\3\\end{array}\\right]\\) the vector is the hypotenuse of a right triangle with legs of length 4 and 3 respectively. Therefore, \\[\\|\\vec{u}\\| = \\sqrt{4^2 + 3^2} = 5\\ .\\] For vectors with more than two components, follow the same pattern: sum the squares of the components, then take the square root.\nCompute the length of a vector \\(\\vec{u}\\) using the dot product: \\[\\|\\vec{u}\\| = \\sqrt{\\strut\\vec{u} \\bullet \\vec{u}}\\ .\\] Although length has an obvious physical interpretation, in many areas of science, including statistics and quantum physics, the square length is a more fundamental quantity. The square length of \\(\\vec{u}\\) is simply \\(\\|\\vec{u}\\|^2 = \\vec{u}\\bullet \\vec{u}\\).\n\n\n\n\n\n\n\n\nTry it! 29.3\n\n\n\n\n\n\n\n\n\nTry it! 29.3 Calculating vector length\n\n\n\nConsider the two vectors \\[\\vec{u} \\equiv \\left(\\begin{array}{c}3\\\\4\\end{array}\\right) \\  \\  \\ \\mbox{and}  \\ \\ \\ \\vec{w} \\equiv \\left(\\begin{array}{c}1\\\\1\\\\1\\\\1\\end{array}\\right)\n\\]\nThe length of \\(\\vec{u}\\) is \\(|| \\vec{u} || = \\sqrt{\\strut 3^2 + 4^2} = \\sqrt{\\strut 25} = 5\\).\nThe length of \\(\\vec{w}\\) is \\(|| \\vec{w} || = \\sqrt{\\strut 1^2 + 1^2 + 1^2 + 1^2} = \\sqrt{\\strut 4} = 2\\).\n\n\n\nApplication area 29.1 —Linear geometry and statistics\n\n\n\n\n\n\n\nApplication area 29.1 Statistical modeling\n\n\n\nIn statistics, the many applications of linear algebra often involve a simple constant vector, which we will write \\(\\vec{1}\\). It is simply a column vector of 1s, \\[\\vec{1} \\equiv \\left[\\begin{array}{c}1\\\\1\\\\1\\\\\\vdots\\\\1\\\\1\\\\ \\end{array}\\right]\\ .\\] Common statistical calculations can be expressed compactly in vector notation. For example, if \\(\\vec{x}\\) is an \\(n\\)-dimensional vector, then the mean of the components of \\(\\vec{x}\\), which is often written \\(\\bar{x}\\), is \\[\\bar{x} \\equiv \\frac{1}{n}\\  \\vec{x} \\bullet \\vec{1}\\ .\\] The symbol \\(\\bar{}\\) is pronounced “bar”, and \\(\\bar{x}\\) is pronounced “x-bar.”.\nAnother commonly used statistic is the variance of the components of a vector \\(\\vec{x}\\). Calculating the variance is more complicated than the mean: \\[\\text{var}(x) \\equiv \\frac{1}{n-1}\\  (\\vec{x} - \\bar{x}) \\bullet (\\vec{x} - \\bar{x})\\ .\\] The quantity \\(\\vec{x} - \\bar{x}\\) is an example of scalar subtraction, which is done on a component-wise basis. For instance, with \\[\\vec{x} = \\left[\\begin{array}{r}1\\\\2\\\\3\\\\4\\\\\\end{array}\\right]\\] then \\(\\bar{x} = 2.5\\). This being the case, \\[\\vec{x} - \\bar{x} = \\left[\\begin{array}{c}-1.5\\\\-0.5\\\\\\ 0.5\\\\\\ 1.5\\\\\\end{array}\\right]\\ ,\\] with the variance of \\(\\vec{x}\\) being \\[\\frac{1}{4-1} \\left[\\begin{array}{r}-1.5\\\\-0.5\\\\0.5\\\\1.5\\\\\\end{array}\\right] \\bullet \\left[\\begin{array}{r}-1.5\\\\-0.5\\\\0.5\\\\1.5\\\\\\end{array}\\right] = \\frac{5}{3}\\ .\\]",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/28-Vectors.html#sec-angles-dot-product",
    "href": "Linear-combinations/28-Vectors.html#sec-angles-dot-product",
    "title": "29  Vectors",
    "section": "29.5 Angles",
    "text": "29.5 Angles\nAny two vectors of the same dimension have an angle between them. Vectors have only two properties: length and direction. To find the angle between two vectors, pick up one vector and relocate its “tail” to meet the tail of the other vector.\nMeasure the angle between two vectors the short way round: between 0 and 180 degrees. Any larger angle, say 260 degrees, will be identified with its circular complement: 100 degrees is the complement of a 260-degree angle.\nIn 2- and 3-dimensional spaces, we can measure the angle between two vectors using a protractor: arrange the two vectors tail to tail, align the baseline of the protractor with one of the vectors and read off the angle marked by the second vector.\nIt is also possible to measure the angle using arithmetic. Suppose we have vectors \\(\\vec{v}\\) and \\(\\vec{w}\\) embedded in the same dimensional space. That is, \\(\\vec{v}\\) and \\(\\vec{w}\\) have the same number of components:\n\\[\\vec{v} = \\left[\\begin{array}{c}v_1\\\\v_2\\\\\\vdots\\\\v_n\\\\\\end{array}\\right]\\ \\ \\ \\text{and}\\ \\ \\ \\vec{w} = \\left[\\begin{array}{c}w_1\\\\w_2\\\\\\vdots\\\\w_n\\\\\\end{array}\\right]\\ ,\\]\nUsing the dot-product and length notation, we can write the formula for the cosine of the angle between two vectors as \\[\\cos(\\theta) \\equiv \\frac{\\vec{v}\\cdot\\vec{w}}{\\|\\vec{v}\\|\\ \\|\\vec{w}\\|}\\ .\\]\n\n\n\n\n\n\nTip\n\n\n\nRemember that the dot-product-based formula above gives the cosine of the angle between the two vectors. It turns out that in many applications, cosine is what’s needed. If you insist on knowing the angle \\(\\theta\\) rather than \\(\\cos(\\theta)\\), the trigonometric function \\(\\arccos()\\) will do the job. For instance, if \\(\\theta\\) is such that \\(\\cos(\\theta) = 0.6\\), compute the angle in degrees with\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe trigonometric functions in R (and in most other languages) do calculations with angles in units of radians. Multiplication by 180/pi converts radians to degrees. Figure 29.6 shows a graph of converting \\(\\cos(\\theta)\\) to \\(\\theta\\) in degrees.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 29.6: The \\(\\arccos()\\) function (acos() in R) converts \\(\\cos(\\theta)\\) to \\(\\theta\\).\n\n\n\n\n\n\nApplication area 29.2 —Another example of how geometry and statistics have much in common.\n\n\n\n\n\n\n\nApplication area 29.2 “Correlation” is an angle\n\n\n\nWhat does the angle \\(\\theta\\) between two vectors tell us?\nThe angle quantifies the alignment of the vectors. An angle of 0 tells us the vectors point in the same direction, and an angle of 180 degrees means that the vectors point in exactly opposing directions. Either of these—0 or 180 degrees—indicates that the two vectors are perfectly aligned. Such alignment means that appropriate scalar multiplication can make the two vectors equal.\nAngles such as 5 or 175 degrees indicate that the two vectors are mostly aligned but imperfectly. When the angle is 90 degrees—a right angle—the two vectors are perpendicular.\nThe vector alignment has an important meaning in terms of data. Suppose the two vectors are two columns in a data frame: two different variables. In statistics, the correlation coefficient, denoted \\(r\\), is a simple way to describe the relationship between two variables. A non-zero correlation indicates a connection between two variables. For instance, among children, height and age are correlated. Since height increases along with age (for children), the two variables are said to be positively correlated. The largest possible correlation is \\(r=1\\).\nA negative correlation means that one variable decreases as the other increases. Temperature and elevation are negatively correlated; temperature goes down as elevation goes up. The most negative possible correlation is \\(r=-1\\).\nA zero correlation indicates no simple (linear) relationship between the two variables. Zero correlation occurs when the variables are orthogonal, a term described in Section 29.6.\nSeeing columns in a data frame as vectors, the correlation coefficient \\(r\\) is exactly the cosine of the angle between the vectors. However, when Francis Galton invented the correlation coefficient in the 1880s, he did not describe it in these terms. Instead, he used arithmetic directly, producing formulas with which many generations of statistics students have struggled. Those students might have done better in statistics if \\(r\\) had been called alignment and measured in degrees.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/28-Vectors.html#sec-orthogonality",
    "href": "Linear-combinations/28-Vectors.html#sec-orthogonality",
    "title": "29  Vectors",
    "section": "29.6 Orthogonality",
    "text": "29.6 Orthogonality\nTwo vectors are said to be orthogonal when the angle between them is 90 degrees. In everyday speech, we call a 90-degree angle a “right angle.” The word “orthogonal” is a literal translation of “right angle.” (The syllable “gon” indicates an angle, as in the five-angled pentagon or six-angled hexagon. “Ortho” means “right” or “correct,” as in “orthodox” (right beliefs) or “orthodontics” (right teeth) or “orthopedic” (right feet).)\nTwo vectors are at right angles—we prefer “orthogonal” since “right” has many meanings not related to angles—when the dot product between them is zero.\n\n\n\n\n\n\n\n\nTry it! 29.4\n\n\n\n\n\n\n\n\n\nTry it! 29.4 Orthogonality 1\n\n\n\nFind a vector that is orthogonal to \\(\\left[\\strut\\begin{array}{r}1\\\\2\\end{array}\\right]\\).\nThe arithmetic trick is to reverse the order of the components and put a minus sign in front of one of them, so \\(\\left[\\strut\\begin{array}{r}-2\\\\1\\end{array}\\right]\\).\nWe can confirm the orthogonality by calculating the dot product: \\(\\left[\\begin{array}{c}-2\\\\\\ 1\\end{array}\\right] \\cdot \\left[\\strut\\begin{array}{r}1\\\\2\\end{array}\\right] = -2\\times1 + 1 \\times 2 = 0\\).\nIn R, write this as\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\n\nTry it! 29.5\n\n\n\n\n\n\n\n\n\nTry it! 29.5 Orthogonality 2\n\n\n\nFind a vector orthogonal to \\(\\left[\\strut\\begin{array}{r}1\\\\2\\\\3\\end{array}\\right]\\).\nWe have a little more scope here. A simple approach is to insert a zero component in the new vector and then use the two-dimensional trick to fill in the remaining components.\nFor instance, starting with \\(\\left[\\strut\\begin{array}{r}0\\\\\\_\\\\ \\_\\end{array}\\right]\\) the only non-zero components of the dot product will involve the 2 and 3 of the original vector. So \\(\\left[\\strut\\begin{array}{r}0\\\\ -3\\\\ 2\\end{array}\\right]\\) is orthogonal. Or, if we start with \\(\\left[\\strut\\begin{array}{r}\\_\\\\0\\\\\\_\\end{array}\\right]\\) we would construct \\(\\left[\\strut\\begin{array}{r}-3\\\\ 0\\\\ 1\\end{array}\\right]\\).",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/29-linear-combinations.html",
    "href": "Linear-combinations/29-linear-combinations.html",
    "title": "30  Linear combinations of vectors",
    "section": "",
    "text": "30.1 Scaling vectors\nA linear combination is a sum of basic elements, each of which has been scaled. For instance, in Block 1 we looked at linear combinations of functions such as \\[g(t) = A + B e^{kt}\\ .\\] This combines the basic functions \\(\\text{one}(t)\\) and \\(e^{kt}\\) scaled respectively by \\(A\\) and \\(B\\).\nLike functions, vectors can be scaled and added. In one sense, this is just a matter of arithmetic. However, new concepts become accessible using the geometrical interpretation of vectors. This chapter builds gradually to one such concept: “slices” of an embedding space called subspaces. Many questions about constructing approximations—How good can the approximation be? How to make it better?—are clarified by seeing the possibilities for approximation as the elements of a subspace.\nTo scale a vector means to change its length without altering its direction. For instance, scaling by a negative number flips the vector tip-for-tail. Figure 30.1 shows two vectors \\(\\vec{v}\\) and \\(\\vec{w}\\) together with several scaled versions.\nArithmetically, scaling a vector is accomplished by multiplying each component of the vector by the scalar, e.g.\n\\[\\vec{u} = \\left[\\begin{array}{r}1.5\\\\-1\\end{array}\\right]\\ \\ \\ \\ 2\\vec{u} = \\left[\\begin{array}{r}3\\\\-2\\end{array}\\right]\\ \\ \\ \\\n-\\frac{1}{2}\\vec{u} = \\left[\\begin{array}{r}-0.75\\\\0.5\\end{array}\\right]\\ \\ \\ \\ \\]\nGeometrically, however, a vector corresponds to one step in a journey. For example, a vector scaled by 2.5 is a journey of two-and-a-half steps; scaling by -10 means traveling backward ten steps.\nThe subspace associated with a single vector is the set of all possible journeys that scaling a vector can accomplish. Visually, this corresponds to all the points on an infinitely long line defined by two points: the tip and the tail of the vector.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Linear combinations of vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/29-linear-combinations.html#scaling-vectors",
    "href": "Linear-combinations/29-linear-combinations.html#scaling-vectors",
    "title": "30  Linear combinations of vectors",
    "section": "",
    "text": "Figure 30.1: Vectors \\(\\\\vec{v}\\) and \\(\\\\vec{w}\\) and some scaled versions of them.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Linear combinations of vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/29-linear-combinations.html#adding-vectors",
    "href": "Linear-combinations/29-linear-combinations.html#adding-vectors",
    "title": "30  Linear combinations of vectors",
    "section": "30.2 Adding vectors",
    "text": "30.2 Adding vectors\nTo add two vectors, place them tip to tail (without changing the direction). The sum is the new vector running from the tail of the first one to the tip of the second. (Figure 30.2)\n\n\n\n\n\n\nFigure 30.2: Adding two vectors, yellow and green, by placing them tail to tip. The result is the vector going from the tail of yellow to the tip of green. The blue vector shows this result.”\n\n\n\nAdding vectors in this way takes advantage of the rootlessness of a vector. So long as we keep the direction and length the same, we can move a vector to any convenient place. For adding vectors, the convenient arrangement is to place the tail of the second vector at the tip of the first. The result—the blue pencil in Figure 30.2—runs from the first (yellow) pencil’s tail to the second (green) pencil’s tip.\nArithmetically, vector addition is simply a matter of applying addition component-by-component. For instance, consider adding two vectors \\(\\vec{v}\\) and \\(\\vec{w}\\):\n\\[\\underbrace{\\left[\\begin{array}{r}1.5\\\\-1\\\\2\\\\6\\end{array}\\right]}_\\vec{v} + \\underbrace{\\left[\\begin{array}{r}2\\\\4\\\\-2\\\\-3.2\\end{array}\\right]}_\\vec{w} = \\underbrace{\\left[\\begin{array}{r}3.5\\\\3\\\\0\\\\2.8\\end{array}\\right]}_{\\vec{v} + \\vec{w}}\\]\nAdding vectors makes sense only when they inhabit the same embedding space. In other words, the vectors must have the same number of components.\nArithmetic subtraction of one vector from another is a simple componentwise operation. For example:\n\\[\\underbrace{\\left[\\begin{array}{r}1.5\\\\-1\\\\2\\\\6\\end{array}\\right]}_\\vec{v} {\\Large -} \\underbrace{\\left[\\begin{array}{r}2\\\\4\\\\-2\\\\-3.2\\end{array}\\right]}_\\vec{w} = \\underbrace{\\left[\\begin{array}{r}-0.5\\\\-5\\\\4\\\\9.2\\end{array}\\right]}_{\\vec{v} - \\vec{w}}\\ .\\]\nFrom a geometrical point of view, many people like to think of \\(\\vec{v} - \\vec{w}\\) as placing the two vectors tail to tail as in Figure 30.3. Read the result as the vector running from the tip of \\(\\vec{v}\\) to the tip of \\(\\vec{w}\\). In Figure 30.3, the yellow vector is \\(\\vec{v}\\) and the blue vector is \\(\\vec{w}\\). The result of the subtraction is the green vector.\n\n\n\n\n\n\nFigure 30.3: With these vectors, subtracting blue from yellow gives green.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Linear combinations of vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/29-linear-combinations.html#linear-combinations",
    "href": "Linear-combinations/29-linear-combinations.html#linear-combinations",
    "title": "30  Linear combinations of vectors",
    "section": "30.3 Linear combinations",
    "text": "30.3 Linear combinations\nThinking of a scaled vector as a “step” of a given length in a given direction leads us to conceive of a linear combination of vectors as step-by-step instructions for a journey. A central use for the formalism of vectors is to guide our thinking and our algorithms for figuring out how best to get from one “place” to another. We have used quotation marks around “place” because we are not necessarily referring to a physical destination. We will get to what else we might mean by “place” later in this Block.\nAs a fanciful example of getting to a “place,” consider a treasure hunt. Here are the instructions to get there:\n\nOn June 1, go to the flagpole before sunrise.\nAt 6:32, walk 213 paces away from the Sun.\nAt 12:19, walk 126 paces toward the Sun.\n\nThe Sun’s position varies over the day. Consequently, the direction of the Sun on June 1 at 6:32 is different than at 12:19. (Figure 30.4)\n\n\n\n\n\n\nFigure 30.4: For June 1: Sun’s direction at 6:32 and at 12:19 (Location: latitude 38.0091, longitude -104.8871). Source: suncalc.org\n\n\n\nThe treasure-hunt directions are in the form of a linear combination of vectors. So far, we know the direction of each vector. Imagine that the length is one stride or pace. (Admittedly, not a scientific unit of length.) Scaling \\(\\color{magenta}{\\text{the magenta vector}}\\) by -213 and \\(\\color{blue}{\\text{the blue vector}}\\) by 126, then adding the two scaled vectors gives a vector that takes you from the flagpole to the treasure.\nA stickler for details might point out that the “direction of the sun” has an upward component. Common sense dictates that the walk is in the direction of the Sun as projected onto Earth’s surface. Chapter 31 deals with projections of vectors.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Linear combinations of vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/29-linear-combinations.html#functions-as-vectors",
    "href": "Linear-combinations/29-linear-combinations.html#functions-as-vectors",
    "title": "30  Linear combinations of vectors",
    "section": "30.4 Functions as vectors",
    "text": "30.4 Functions as vectors\nSince calculus is about functions, one naturally expects a chapter on vectors in a calculus textbook to make a connection between functions and vectors. Recall from Section 7.4 (entitled “Functions and data”) the idea of representing a function as a table of inputs and the corresponding outputs.\nHere is such a table with some of our pattern-book functions.\n\n\n\n\n\nt\none(t)\nidentity(t)\nexp(t)\nsin(t)\npnorm(t)\n\n\n\n\n0.0\n1\n0.0\n1.000000\n0.0000000\n0.5000000\n\n\n0.1\n1\n0.1\n1.105171\n0.0998334\n0.5398278\n\n\n0.2\n1\n0.2\n1.221403\n0.1986693\n0.5792597\n\n\n0.3\n1\n0.3\n1.349859\n0.2955202\n0.6179114\n\n\n0.4\n1\n0.4\n1.491825\n0.3894183\n0.6554217\n\n\n... 51 rows in total ...\n\n\n\n\n\n\n\n\n\n\n4.6\n1\n4.6\n99.48432\n-0.9936910\n0.9999979\n\n\n4.7\n1\n4.7\n109.94717\n-0.9999233\n0.9999987\n\n\n4.8\n1\n4.8\n121.51042\n-0.9961646\n0.9999992\n\n\n4.9\n1\n4.9\n134.28978\n-0.9824526\n0.9999995\n\n\n5.0\n1\n5.0\n148.41316\n-0.9589243\n0.9999997\n\n\n\n\n\nIn the tabular representation of the pattern-book functions, each function is a column of numbers—a vector.\nFunctions that we construct by linear combination are, in this vector format, just a linear combination of the vectors. For instance, the function \\(g(t) \\equiv 3 - 2 t\\) is \\(3\\cdot \\text{one}(t) - 2 \\cdot \\text{identity}(t)\\)\n\n\n\n\n\nt\none(t)\nidentity(t)\ng(t)\n\n\n\n\n0.0\n1\n0.0\n3.0\n\n\n0.1\n1\n0.1\n2.8\n\n\n0.2\n1\n0.2\n2.6\n\n\n0.3\n1\n0.3\n2.4\n\n\n0.4\n1\n0.4\n2.2\n\n\n... 51 rows in total ...\n\n\n\n\n\n\n\n\n4.6\n1\n4.6\n-6.2\n\n\n4.7\n1\n4.7\n-6.4\n\n\n4.8\n1\n4.8\n-6.6\n\n\n4.9\n1\n4.9\n-6.8\n\n\n5.0\n1\n5.0\n-7.0\n\n\n\n\n\nThe table above is a collection of four vectors: \\(\\vec{\\mathtt t}\\), \\(\\vec{\\mathtt{ one(t)}}\\), \\(\\vec{\\mathtt{identity(t)}}\\), and \\(\\vec{\\mathtt{g(t)}}\\). Each of those vectors has 51 components. In math-speak, we can say that the vectors are “embedded in a 51-dimensional space.”\nIn the table, the function output is tabulated only for select, discrete values of the input. Such discreteness is not a fundamental problem. Interpolation techniques (Section 7.4) enable evaluation of the function for a continuous input.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Linear combinations of vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/29-linear-combinations.html#matrices-and-linear-combinations",
    "href": "Linear-combinations/29-linear-combinations.html#matrices-and-linear-combinations",
    "title": "30  Linear combinations of vectors",
    "section": "30.5 Matrices and linear combinations",
    "text": "30.5 Matrices and linear combinations\nA collection of vectors, such as the one displayed in the previous table, is called a matrix. Each vector in a matrix must have the same number of components.\nAs mathematical notation, we will use bold-faced, capital letters to stand for matrices, for example, \\(\\mathit{M}\\). The symbol \\(\\rightleftharpoons\\) is a reminder that a matrix can contain multiple vectors, just as the symbol \\(\\rightharpoonup\\) in \\(\\vec{v}\\) reminds us that the name “\\(v\\)” refers to a vector. (It is typical in mathematical writing to use single letters to refer to vectors instead of the descriptive, multi-letter names used in data frames.)\nIn the conventions for data, we give a name to each data frame column so that we can refer to it individually. In the conventions used in vector mathematics, single letters refer to the individual vectors.\nAs a case in point, let’s look at a matrix \\(\\mathit{M}\\) containing the two vectors which we’ve previously called \\(\\vec{\\mathtt{one(t)}}\\) and \\(\\vec{\\mathtt{identity(t)}}\\): \\[\\mathit{M} \\equiv \\left[\\begin{array}{rr}1 & 0\\\\\n1 & 0.1\\\\\n1 & 0.2\\\\\n1 & 0.3\\\\\n\\vdots & \\vdots\\\\\n1 & 4.9\\\\\n1 & 5.0\\\\\n\\end{array}\\right]\\ .\\] The linear combination which we might previous have called \\(3\\cdot \\vec{\\mathtt{t}} - 2\\,\\vec{\\mathtt{identity(t)}}\\) can be thought of as\n\\[\\left[\\overbrace{\\begin{array}{r}\n1\\\\\n1 \\\\\n1 \\\\\n1 \\\\\n\\vdots &\\\\\n1 \\\\\n1\n\\end{array}}^{3 \\times}\n\\stackrel{\\begin{array}{r}\n\\\\\n\\\\\n\\\\\n\\\\\n\\\\\n\\\\\n\\\\\n\\\\\n\\end{array}}{\\Large + \\ }\n\\overbrace{\\begin{array}{r}\n0\\\\\n0.1 \\\\\n0.2 \\\\\n0.3 \\\\\n\\vdots\\\\\n4.9 \\\\\n5.0\n\\end{array}}^{-2 \\times}\\right] = \\left[\\begin{array}{r}\n\\\\ \\\\ 3\\\\\n2.8\\\\2.6\\\\2.4\\\\\\vdots\\\\-6.8\\\\-7.0\\\\ \\\\ \\\\\n\\end{array}\\right]\\ ,\\] but this is not conventional notation. Instead, we would write this more concisely as \\[\\stackrel{\\Large\\mathit{M}}{\\left[\\begin{array}{rr}1 & 0\\\\\n1 & 0.1\\\\\n1 & 0.2\\\\\n1 & 0.3\\\\\n\\vdots & \\vdots\\\\\n1 & 4.9\\\\\n1 & 5.0\\\\\n\\end{array}\\right]} \\\n\\stackrel{\\Large\\vec{w}}{\\left[\\begin{array}{r}2\\\\-3\\end{array}\\right]}\\]\nIn symbolic form, the linear combination of the columns of \\(\\mathit{M}\\) using respectively the scalars in \\(\\vec{w}\\) is simply \\(\\mathit{M} \\, \\vec{w}\\). The construction of such linear combinations is called matrix multiplication.\nNaturally, the operation only makes sense if there are as many components to \\(\\vec{w}\\) as columns in \\(\\mathit{M}\\).\n\n\n\n\n\n\nTip\n\n\n\n“Matrix multiplication” might better have been called “\\(\\mathit{M}\\) linearly combined by \\(\\vec{w}\\).” Nevertheless, “matrix multiplication” is the standard term for such linear combinations.\n\n\n\n\n\n\n\n\n\n\nTry it! 30.1\n\n\n\n\n\n\n\n\n\nTry it! 30.1 Making matrices\n\n\n\nIn R, make vectors with the rbind() command, short for “bind rows,” as in\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNote that the vector components appear as successive arguments to the rbind() function.\nCollect multiple vectors into a matrix with the cbind() command, short for “bind columns.” The arguments to cbind() will typically be vectors created by rbind(). For instance, the matrix \\[\\mathit{A} \\equiv \\left[\\vec{u}\\ \\ \\vec{v}\\right]\\ \\ \\text{where}\\ \\ \\vec{u} \\equiv \\left[\\begin{array}{r}2\\\\5\\\\-3\\end{array}\\right]\\ \\ \\text{and}\\ \\ \\vec{v} \\equiv \\left[\\begin{array}{r}1\\\\-4\\\\0\\end{array}\\right]\\] can be constructed in R with these commands.\n\n\n\nActive R chunk 30.1\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nTo compute the linear combination \\(3 \\vec{u} + 1 \\vec{v}\\), that is, \\(\\mathit{A} \\cdot \\left[\\begin{array}{r}3\\\\1\\end{array}\\right]\\) you use the matrix multiplication operator %*%. For instance, the following defines a vector \\[\\vec{x} \\equiv \\left[\\begin{array}{r}3\\\\1\\end{array}\\right]\\] to do the job in a way that is easy to read:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nIt is a mistake to use * instead of %*% for matrix multiplication. Remember that * is for componentwise multiplication, which is different from matrix multiplication. Componentwise multiplication with vectors and matrices will usually give an error message as with:\nNote: Make sure that you have run the code in Active R chunk 30.1 before running this chunk.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe phrase “non-conformable arrays” is R-speak for “I do not know how to do componentwise multiplication with two incompatibly shaped objects.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Linear combinations of vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/29-linear-combinations.html#sub-spaces",
    "href": "Linear-combinations/29-linear-combinations.html#sub-spaces",
    "title": "30  Linear combinations of vectors",
    "section": "30.6 Sub-spaces",
    "text": "30.6 Sub-spaces\nPreviously, we have said that a vector with \\(n\\) components is “embedded” in an \\(n\\)-dimensional space. Think of an embedding space as a kind of club with restricted membership. For instance, a vector with two elements is properly a member of the 2-dimensional club, but a vector with more or fewer than two elements cannot have a place in the two-dimensional club. Similarly, there are clubs for 3-component vectors, 4-component vectors, and so on.\nThe clubhouse itself is a kind of space, the space in which any and all of the vectors that are eligible for membership can be embedded.\nNow imagine the clubhouse arranged into meeting rooms. Each meeting room is just part of the clubhouse space. Which part? That depends on a set of vectors who sponsor the meeting. For instance, in the ten-dimensional clubhouse, a few members, let’s say \\(\\color{blue}{\\vec{u}}\\) and \\(\\color{magenta}{\\vec{v}}\\) decide to sponsor a meeting. That meeting room, part of the whole clubhouse space, is called a subspace.\nA subspace has its own rules for admission. Vectors belong to the subspace only if they are a linear combination of the sponsoring members. The sponsoring members define the subspace, but the subspace itself consists of an infinity of vectors: all possible vectors that amount to a linear combination of the sponsors.\nAs an example, consider the clubhouse that is open to any and all vectors with three components. The diagram in Figure 30.5 shows the clubhouse with just two members present, \\(\\color{blue}{\\vec{u}}\\) and \\(\\color{magenta}{\\vec{v}}\\).\nAny vector can individually sponsor a subspace. In Figure 30.5 the subspace sponsored by \\(\\color{blue}{\\vec{u}}\\) is the extended line through \\(\\color{blue}{\\vec{u}}\\), that is, all the possible scaled versions of \\(\\color{blue}{\\vec{u}}\\). Similarly, the subspace sponsored by \\(\\color{magenta}{\\vec{v}}\\) is the extended line through \\(\\color{magenta}{\\vec{v}}\\). Each of these subspaces is one-dimensional.\n\n\n\n\n\n\n\n\n\nFigure 30.5: Two vectors \\(\\color{blue}{\\vec{u}}\\) and \\(\\color{magenta}{\\vec{v}}\\) embedded in 3-dimensional space. The subspace spanned by an individual vector is shown as a line. You can interact with the display with the cursor: rotating, zooming, etc.\n\n\n\nMultiple vectors can sponsor a subspace. The subspace sponsored by both \\(\\color{blue}{\\vec{u}}\\) and \\(\\color{magenta}{\\vec{v}}\\) contains all the vectors that can be constructed as linear combinations of \\(\\color{blue}{\\vec{u}}\\) and \\(\\color{magenta}{\\vec{v}}\\); the gray subspace in Figure 30.6.\n\n\n\n\n\n\n\n\n\nFigure 30.6: Two vectors \\(\\vec{u}\\) and \\(\\vec{w}\\). The subspace spanned by two vectors is a plane, shown here as a gray surface. You can interact with the display.\n\n\n\nOn the other hand, the subspace sponsored by \\(\\color{magenta}{\\vec{v}}\\) and \\(\\color{blue}{\\vec{u}}\\) is not the entire clubhouse. \\(\\color{magenta}{\\vec{v}}\\) and \\(\\color{blue}{\\vec{u}}\\) lie in a common plane, but not all the vectors in the 3-dimensional clubhouse lied in that plane. In fact, if you rotate Figure 30.6 to “look down the barrel” of either \\(\\color{magenta}{\\vec{v}}\\) or \\(\\color{blue}{\\vec{u}}\\), the plane will entirely disappear from view. A subspace is an infinitesimal slice of the embedding space.\n“Sponsored a subspace” is metaphorical. In technical language, we speak of the subspace spanned by a set of vectors in the same embedding space. Usually, we refer to a “set of vectors” as a matrix. For instance, letting \\[\\mathit{M} \\equiv \\left[{\\Large \\strut}\\color{blue}{\\vec{u}}\\ \\ \\color{magenta}{\\vec{v}}\\right]\\ ,\\] the gray plane in Figure 30.6 is the subspace spanned by \\(\\mathit{M}\\) or, more concisely, \\(span(\\mathit{M})\\).\nFor a more concrete, everyday representation of the subspace spanned by two vectors, a worthwhile experiment is to pick up two pencils pointing in different directions. Place the eraser ends together, pinched between thumb and forefinger. Point the whole rigid assembly in any desired direction; the angle between them will remain the same.\nPlace a card on top of the pencils, slipping it between pressed fingers to hold it tightly in place. The card is another kind of geometrical object: a planar surface. The orientation of two vectors together determines the orientation of the surface. This simple fact will be significant later on.\nOne can replace the pencils with line segments drawn on the card underneath each pencil. Now, the angle is readily measurable in two dimensions. The angle between two vectors in three dimensions is the same as the angle drawn on the two-dimension surface that rests on the vectors.\nNotice that one can also lay a card along a single vector. What is different here is that the card can be rolled around the pencil while still staying in contact; there are many different orientations for such a card even while the vector stays fixed. So a single fixed vector does not uniquely determine the orientation of the planar surface in which the vector can reside. It takes two vectors to determine a unique planar surface.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Linear combinations of vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/30-projection.html",
    "href": "Linear-combinations/30-projection.html",
    "title": "31  Projection & residual",
    "section": "",
    "text": "31.1 Projection terminology\nMany problems in physics and engineering involve the task of decomposing a vector \\(\\vec{b}\\) into two perpendicular component vectors \\(\\hat{b}\\) and \\(\\vec{r}\\), such that \\(\\hat{b} + \\vec{r} = \\vec{b}\\) and \\(\\hat{b} \\cdot \\vec{r} = 0\\). There are infinite ways to accomplish such a decomposition, one for each way or orienting \\(\\hat{b}\\) relative to \\(\\vec{b}\\). Figure 31.1 shows a few examples.\nThe above example shows just one way the task of vector decomposition arises. Our particular interest in this part of the book is in a more general task: finding how to take a linear combination of the columns of a matrix \\(\\mathit{A}\\) to make the best approximation to a given vector \\(\\vec{b}\\). This abstract problem is pertinent to many real-world tasks: finding a linear combination of functions to match a relationship laid out in data, constructing statistical models such as those found in machine learning, and effortlessly solving sets of simultaneous linear equations with any number of equations, and any number of unknowns.\nIn a typical vector decomposition task, the setting determines the relevant direction or subspace. The decomposition is accomplished by projecting the vector onto that direction or subspace. The word “projection” may bring to mind the casting of shadows on a screen in the same manner as an old-fashioned slide projector or movie projector. The light source and focusing lens generate parallel rays that arrive perpendicular to the screen. A movie screen is two-dimensional, a subspace defined by two vectors. Imagining those two vectors to be collected into matrix \\(\\mathit{A}\\), the idea is to decompose \\(\\vec{b}\\) into a component that lies in the subspace defined by \\(\\mathit{A}\\) and another component that is perpendicular to the screen. That perpendicular component is what we have been calling \\(\\vec{r}\\) while the vector \\(\\hat{b}\\) is the projection of \\(\\vec{b}\\) onto the screen. To make it easier to keep track of the various roles played by \\(\\vec{b}\\), \\(\\hat{b}\\), \\(\\vec{r}\\), and \\(\\mathit{A}\\), we will give these vectors English-language names. 1\nProjection is the process of finding, from all the vectors in the model subspace, the particular vector \\(\\hat{b}\\) that is as close as possible to the target vector \\(\\vec{b}\\). To state things another way: projection is the process of finding the model vector that makes the residual vector as short as possible.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Projection & residual</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/30-projection.html#projection-terminology",
    "href": "Linear-combinations/30-projection.html#projection-terminology",
    "title": "31  Projection & residual",
    "section": "",
    "text": "\\(\\vec{b}\\) the target vector\n\\(\\hat{b}\\) the model vector\n\\(\\vec{r}\\) the residual vector\n\\(\\mathit{A}\\) the model space (or “model subspace”)\n\n\n\n\n\n\n\n\n\n\nTry it! 31.1\n\n\n\n\n\n\n\n\n\nTry it! 31.1 Projecting in 3 dimensions\n\n\n\nThe interactive graphic shows a solved projection problem in 3-dimensional space. The figure can be rotated or set spinning, which makes it much easier to interpret the diagram as a three-dimensional object.\n\n\n\n\n\n\n\n\n\nFigure 31.3\n\n\n\nIn addition to the target vector \\(\\vec{b}\\) and the vectors \\(\\vec{u}\\) and \\(\\vec{b}\\) that constitute the matrix \\(\\mathit{A}\\), the diagram includes a translucent plane marking \\(span(\\mathit{A})\\). The goal of projection is, given \\(\\vec{b}\\) and \\(\\mathit{A}\\), to find the model vector (shown in light green). Once the model vector \\(\\vec{x}\\) is known, the residual vector is easy to calculate \\[\\vec{r} \\equiv \\vec{b} - \\hat{b}\\ .\\] Another approach to the problem is to find the residual vector \\(r\\) first, then use that to find the model vector as \\[\\hat{b} \\equiv \\vec{b} - \\vec{r}\\ .\\]\nInterpreting such three-dimensional diagrams on a static printed page can be difficult. Being able to interact with the diagrams helps a lot. For instance, to see that the translucent plane in Figure 31.3 contains \\(\\vec{u}\\) and \\(\\vec{v}\\), rotate the diagram to look edge-on at the plane so that the plane appears as a line on the screen. At such times, vectors \\(\\vec{u}\\) and \\(\\vec{v}\\) disappear. There is no component to \\(\\vec{u}\\) and \\(\\vec{v}\\) that sticks out from the plane; they are entirely embedded in the plane.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Projection & residual</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/30-projection.html#sec-proj-single-vector",
    "href": "Linear-combinations/30-projection.html#sec-proj-single-vector",
    "title": "31  Projection & residual",
    "section": "31.2 Projection onto a single vector",
    "text": "31.2 Projection onto a single vector\nAs we said, projection involves a vector \\(\\vec{b}\\) and a matrix \\(\\mathit{A}\\) that defines the model space. We will start with the simplest case, where \\(\\mathit{A}\\) has only one column. That column is, of course, a vector. We will call that vector \\(\\vec{a}\\), so the projection problem is to project \\(\\vec{b}\\) onto the subspace spanned by \\(\\vec{a}\\).\nFigure 31.4 diagrams the situation of projecting the target vector \\(\\vec{b}\\) onto the model space \\(\\vec{a}\\).\n\n\n\n\n\n\nFigure 31.4: The geometry of projecting \\(\\vec{b}\\) onto \\(\\vec{a}\\) to produce the model vector \\(\\hat{b}\\).\n\n\n\nThe angle between \\(\\vec{a}\\) and \\(\\vec{b}\\) is labelled \\(\\theta\\). Section 29.5 shows how to calculate an angle such as \\(\\theta\\) from \\(\\vec{b}\\) and \\(\\vec{a}\\) with a dot product:\n\\[\\cos(\\theta) = \\frac{\\vec{b} \\bullet \\vec{a}}{\\len{b}\\, \\len{a}}\\ .\\] Knowing \\(\\theta\\) and \\(\\len{b}\\), you can calculate the length of the model vector \\(\\hat{b}\\): \\[\\len{s} = \\len{b} \\cos(\\theta) = \\vec{b} \\bullet \\vec{a} / \\len{a}\\ .\\]\nScaling \\(\\vec{a}\\) by \\(\\len{a}\\) would produce a vector oriented in the model subspace, but it would have the wrong length: length \\(\\len{a} \\len{s}\\). So we need to divide \\(\\vec{a}\\) by \\(\\len{a}\\) to get a unit length vector oriented along \\(\\vec{a}\\):\n\\[\\text{model vector:}\\ \\ \\hat{b} = \\left[\\vec{b} \\bullet \\vec{a}\\right] \\,\\vec{a} / {\\len{a}^2} = \\frac{\\vec{b} \\bullet \\vec{a}}{\\vec{a} \\bullet \\vec{a}}\\  \\vec{a}.\\] . \n\n\n\n\n\n\n\n\nTry it! 31.2\n\n\n\n\n\n\n\n\n\nTry it! 31.2 Projecting one vector onto another\n\n\n\nIn R/mosaic, calculate the projection of \\(\\vec{b}\\) onto \\(\\vec{a}\\) using %onto%. For instance\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nHaving found \\(\\hat{b}\\), the residual vector \\(\\vec{r}\\) can be calculated as \\(\\vec{b}- \\hat{b}\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe two properties that a projection satisfies are:\n\nThe residual vector is perpendicular to each and every vector in \\(\\mathit{A}\\). Since in this example, \\(\\mathit{A}\\) contains only the one vector \\(\\vec{a}\\), we need only look at \\(\\vec{r} \\cdot \\vec{a}\\) and confirm that it is zero.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nThe residual vector plus the model vector exactly equals the target vector. Since we computed r &lt;- b - s, we know this must be true, but still …\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIf the difference between two vectors is zero for every coordinate, the two vectors must be identical.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Projection & residual</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/30-projection.html#projection-onto-a-set-of-vectors",
    "href": "Linear-combinations/30-projection.html#projection-onto-a-set-of-vectors",
    "title": "31  Projection & residual",
    "section": "31.3 Projection onto a set of vectors",
    "text": "31.3 Projection onto a set of vectors\nAs we have just seen, projecting a target \\(\\vec{b}\\) onto a single vector is a matter of arithmetic. Now we will expand the technique to project the target vector \\(\\vec{b}\\) onto multiple vectors collected into a matrix \\(\\mathit{A}\\). Whereas Section 31.2 used trigonometry to find the component of \\(\\vec{b}\\) aligned with the single vector \\(\\vec{a}\\), now we have to deal with multiple vectors at the same time. The result will be the component of \\(\\vec{b}\\) aligned with the subspace sponsored by \\(\\mathit{A}\\).\nIn one situation, projection is easy: when the vectors in \\(\\mathit{A}\\) are mutually orthogonal. In this situation, carry out several one-vector-at-a-time projections: \\[\n\\vec{p_1} \\equiv \\modeledby{\\vec{b}}{\\vec{v_1}}\\ \\ \\ \\ \\\n\\vec{p_2} \\equiv \\modeledby{\\vec{b}}{\\vec{v_2}}\\ \\ \\ \\ \\\n\\vec{p_3} \\equiv \\modeledby{\\vec{b}}{\\vec{v_3}}\\ \\ \\ \\ \\\n\\text{and so on}\\] The projection of \\(\\vec{b}\\) onto \\(\\mathit{A}\\) will be the sum \\(\\vec{p_1} + \\vec{p2} + \\vec{p3}\\).\n\n\n\n\n\n\n\n\nTry it! 31.3\n\n\n\n\n\n\n\n\n\nTry it! 31.3 Projecting a vector onto a subspace\n\n\n\nConsider the following matrix \\(\\mathit{A}\\) made up of mutually orthogonal vectors:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nTo confirm that v1, v2, and v3 are mutually orthogonal, calculate the dot product between any two of them and observe that, in each case, the result is zero.\nNow construct the one-at-a-time projections:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nTo find the projection of \\(\\vec{b}\\) onto the subspace spanned by \\(\\mathit{A}\\), add up the one-at-a-time projections:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow we will confirm that b_on_A is the projection of b onto A. The strategy is to construct the residual from the projection.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nAll that is needed is to confirm that the residual is perpendicular to each and every vector in A:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Projection & residual</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/30-projection.html#a-becomes-q",
    "href": "Linear-combinations/30-projection.html#a-becomes-q",
    "title": "31  Projection & residual",
    "section": "31.4 A becomes Q",
    "text": "31.4 A becomes Q\nNow that we have a satisfactory method for projecting \\(\\vec{b}\\) onto a matrix \\(\\mathit{A}\\) consisting of mutually orthogonal vectors, we need to develop a method for the projection when the vectors in \\(\\mathit{A}\\) are not mutually orthogonal. The big picture here is that we will construct a new matrix \\(\\mathit{Q}\\) that spans the same space as \\(\\mathit{A}\\) but whose vectors are mutually orthogonal. We will construct \\(\\mathit{Q}\\) out of linear combinations of the vectors in \\(\\mathit{A}\\), so we can be sure that \\(span(\\mathit{Q}) = span(\\mathit{A})\\).\nWe introduce the process with an example involving vectors in a 4-dimensional space. \\(\\mathit{A}\\) will be a matrix with two columns, \\(\\vec{v_1}\\) and \\(\\vec{v_2}\\). Here is the setup for the example vectors and model matrix:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe start the construction of the \\(\\mathit{Q}\\) matrix by pulling in the first vector in \\(\\mathit{A}\\). We will call that vector \\(\\vec{q_1}\\)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe next \\(\\mathit{Q}\\) vector will be constructed to be perpendicular to \\(\\vec{q_1}\\) but still in the subspace spanned by \\(\\left[{\\Large\\strut}\\vec{v_1}\\ \\ \\vec{v_2}\\right]\\). We can guarantee this will be the case by making the \\(\\mathit{Q}\\) vector entirely as a linear combination of \\(\\vec{v_1}\\) and \\(\\vec{v_2}\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nSince \\(\\vec{q_1}\\) and \\(\\vec{q_2}\\) are orthogonal and define the same subspace as \\(\\mathit{A}\\), we can construct the projection of \\(\\vec{b}\\) onto \\(\\vec{A}\\) by adding up the projections of \\(\\vec{b}\\) onto the individual vectors in \\(\\mathit{Q}\\), like this:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nTo confirm that this calculation of \\(\\widehat{\\strut b}\\) is correct, construct the residual vector and show that it is perpendicular to every vector in \\(\\mathit{Q}\\) (and therefore in \\(\\mathit{A}\\), which spans the same space).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNote that we defined \\(\\vec{r} = \\vec{b} - \\widehat{\\strut b}\\), so it is guaranteed that \\(\\vec{r} + \\widehat{\\strut b}\\) will equal \\(\\vec{b}\\).\nThis process can be extended to any number of vectors in \\(\\mathit{A}\\). Here is the algorithm for constructing \\(\\mathit{Q}\\):\n\nTake the first vector from \\(\\mathit{A}\\) and call it \\(\\vec{q_1}\\).\nTake the second vector from \\(\\mathit{A}\\) and find the residual from projecting it onto \\(\\vec{q_1}\\). This residual will be \\(\\vec{q_2}\\). At this point, the matrix \\(\\left[\\strut \\vec{q_1}, \\ \\ \\vec{q_2}\\right]\\) consists of mutually orthogonal vectors.\nTake the third vector from \\(\\mathit{A}\\) and project it onto \\(\\left[\\strut \\vec{q_1}, \\ \\ \\vec{q_2}\\right]\\). We can do this because we already have an algorithm for projecting a vector onto a matrix with mutually orthogonal columns. Call the residual from this projection \\(\\mathit{q_3}\\). It will be orthogonal to the vectors in \\(\\left[\\strut \\vec{q_1}, \\ \\ \\vec{q_2}\\right]\\), so all three of the q vectors we’ve created are mutually orthogonal.\nContinue onward, taking the next vector in \\(\\mathit{A}\\), projecting it onto the q-vectors already assembled, and finding the residual from that projection.\nRepeat step (iv) until all the vectors in \\(\\mathit{A}\\) have been handled.\n\n\n\n\n\n\n\n\n\nTry it! 31.4\n\n\n\n\n\n\n\n\n\nTry it! 31.4 Projecting in 10-dimensional space\n\n\n\nProject a \\(\\vec{b}\\) that lives in 10-dimensional space onto the subspace sponsored by five vectors that are not mutually orthogonal:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nConfirm using dot products that the v-vectors are not mutually orthogonal.\nNow to construct the vectors in \\(\\mathit{Q}\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nSince Q consists of mutually orthogonal vectors, the projection of b onto Q can be made one vector at a time.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Projection & residual</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/30-projection.html#footnotes",
    "href": "Linear-combinations/30-projection.html#footnotes",
    "title": "31  Projection & residual",
    "section": "",
    "text": "The motivation for these names will become apparent in later chapters.↩︎",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Projection & residual</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/31-target-problem.html",
    "href": "Linear-combinations/31-target-problem.html",
    "title": "32  The target problem",
    "section": "",
    "text": "32.1 Linear equations\nIn this chapter, we ask you to reconsider a mathematical theory that is universally taught in high-school and to consider augmenting it with newer computational ideas that address the same kind of problems, but which produce useful results even when the mathematical theory insists the “a solution does not exist.”\nThe time-honored theory is that taught in high-school algebra. There is nothing wrong with that theory except that it is incomplete. It does not address the needs of present-day practice, particularly in data science, statistics, and machine learning.\nAlgebra, in its basic sense, is about generalizing arithmetic to handle situations where some quantities are not yet known numerically and so are represented by symbols. The algebra student learns rules for symbolic expressions that allow the expressions to be re-arranged into other forms that would plainly be valid if replaced by numbers. Some examples of these rules:\ni. \\(ax = b\\) is equivalent to \\(x = a/b\\).\nii. \\(a + x = b\\) is equivalent to \\(x=b-a\\).\niii. \\(a x^2 + b x + c = 0\\) is equivalent to \\(x = \\frac{-b\\pm \\sqrt{\\strut b^2 - 4ac}}{2a}\\).\niv. \\(\\ln(ax) = b\\) is equivalent to \\(x = \\frac{1}{a}\\ln(b)\\).\nA major challenge to the algebra student is to use such rules to re-arrange expressions into a form \\(x=\\) that enables \\(x\\) to be calculated from the numerical values of the other symbols. Unfortunately, students are given little or no insight to the historical origins of algebra techniques and why they are not necessarily appropriate for all tasks.\nIn English, the word “algebra” is seen as early as 1551. It comes from a book written by the Persian Muhammad ibn Musa al-Khwarizmi (780-850), The Compendious Book on Calculation by Completion and Balancing. The book introduced the use of rules familiar to every algebra student. In the original Arabic, the title includes the word “al-jabr,” meaning “completion” or “rejoining.” According to some sources, the literal meaning of “al-jabr” was resetting and rejoining broken bones. That literal meaning correctly conveys the importance of the subject, but also the pain endured by many algebra students. (Incidentally, the “algorithm” comes from the name of the book’s author: al Khwarizmi. He is a major figure in the history of mathematics.)\nThis history may not be of immediate interest to every reader, but there is a good point to it. The roots of algebra are ancient and developed in an era very different from our own. Today’s student learns algebra to facilitate the study and practice of physics, chemistry, statistics, engineering, and other fields. None of these fields existed when algebra was being conceived. That is, the theory was developed before the recognition of the problems and calculations that arise in modern practice. Thus, “in practice, theory and practice are different.”\nThis chapter is about re-expressing some basic algebraic theory to align it better to today’s practice.\nThe focus of interest will be the familiar task\n\\[\\ \\ \\ \\ \\ \\ \\text{given}\\ \\ a x = b\\,,\\ \\ \\text{find}\\ \\ x\\ .\\] All algebra students learn that \\(x = b/a\\), with the proviso that if \\(a = 0\\), “there is no solution.”\nA somewhat more advanced algebra task is to work with “simultaneous linear equations,” for example: \\[\\ \\ \\ \\text{given}\\ \\ \\ \\begin{array}{rrrcr}\n3 x & + & 2 y & = & 7\\\\\n-1&+&y&=&4\\end{array}\n\\ \\ \\ \\ \\text{find}\\ \\ x\\,\\&\\,y\\ .\n\\]\nSolving simultaneous linear equations is hard. It involves more arithmetic than \\(ax = b\\) and requires the student to make good choices how to take linear combinations of the two equations to reduce the problem to two equations, one with \\(x\\) as the only unknown and one with \\(y\\). Also, the “there is no solution” proviso is not easy to state, so you cannot know at a glance whether there is indeed a solution.\nThe simultaneous linear equation problem can be more compactly written using matrix and vector notation. \\[\\begin{array}{rrrcr}3x & + &2y & = & 7\\\\-1&+&y&=&4\\end{array} \\ \\ \\text{is the same as}\\ \\ \\left[\\begin{array}{r}3\\\\-1\\end{array}\\right] x + \\left[\\begin{array}{r}2\\\\1\\end{array}\\right] y =\n\\left[\\begin{array}{r}7\\\\4\\end{array}\\right] \\] You can see the vector form as a linear combination of two vectors. Collecting these two vectors into a matrix \\(\\mathit{A}\\), and similarly writing \\(x\\, \\text{and}\\, y\\) as the scalar components of a vector \\(\\vec{x}\\) gives \\[\\left[\\begin{array}{rr}3&2\\\\-1&1\\end{array}\\right]\\ \\vec{x} = \\left[\\begin{array}{r}7\\\\4\\end{array}\\right]\\] Which can be expressed as \\(\\mathit{A} \\vec{x} = \\vec{b}\\).\nA student, recognizing the similarity of \\(\\mathit{A}\\vec{x} = \\vec{b}\\) to \\(a x = b\\) would reasonably suggest the solution \\(\\vec{x} = \\vec{b}/ \\mathit{A}\\). Such a student might be instructed, “No, you cannot do this.” A better response would be, “Good. Now tell me what you mean by \\(\\vec{b}/\\mathit{A}\\)?”\nModern practice often calls for solving \\(\\mathit{A}\\vec{x} = \\vec{b}\\) in settings where a traditional algebra teacher might say, as for \\(0 x = b\\) that “there is no solution.”\nTo illustrate such a setting, recall the problems from Section 11.3 of finding the linear combination of the functions \\(f(\\mathtt{time})=1\\) and \\(g(\\mathtt{time}) = e^{-0.019 \\mathtt{time}}\\) that best matches the CoolingWater data:\nWe seek scalars \\(C\\) and \\(D\\) such that the function \\(C f(\\mathtt{time}) + D g(\\mathtt{time})\\) gives the best possible match to temp.\nWe can compactly write the problem of finding the best linear combination into matrix form by evaluating \\(f()\\) and \\(g()\\) at the values listed in the time column: \\[\\underbrace{\\left[\\begin{array}{rr}1&1.0000\\\\1&0.9812\\\\1&0.9627\\\\\\vdots\\\\1&0.0153\\\\1&0.0150\\end{array}\\right]}_{\\!\\!\\!\\!\\!\\!\\!\\!{\\large\\mathit{A}} = \\left[\\strut f(\\mathtt{time})\\,,\\ \\ \\ g(\\mathtt{time})\\right]} \\underbrace{\\left[\\begin{array}{r}C\\\\D\\end{array}\\right]}_{\\large\\vec{x}} \\ \\text{is the best match to}\\  \\underbrace{\\left[\\begin{array}{r}\\mathtt{98.2}\\\\\\mathtt{94.4}\\\\\\mathtt{91.4}\\\\\\vdots\\\\\\mathtt{25.9}\\\\\\mathtt{25.8}\\end{array}\\right]}_{\\large\\vec{b}}\\]\nRegrettably, the classical algebraicists did not propose a rule for “is the best match to.” Replacing “is the best match to” with \\(=\\) is not literally correct since “there is no solution” that makes the equality literally true.\nWe will use the term target problem to name the task of finding \\(\\vec{x}\\) such that \\(\\mathit{A} \\vec{x}\\) is the best possible match to \\(\\vec{b}\\). This term is motivated by the idea that \\(\\vec{b}\\) is a target, and we seek to use the resources in \\(\\mathit{A}\\) to get as close as possible to the target: choose \\(\\vec{x}\\) such that \\(\\mathit{A} \\vec{x}\\) falls as closely as possible to the target.\nTo address the practical problem in the notation of algebra theory, people write \\[\\mathit{A} \\vec{x} = \\vec{b} - \\vec{r}\\] where \\(\\vec{r}\\) is a vector specially selected to path up \\(\\mathit{A} \\vec{x} = \\vec{b}\\) so that when the best-matching \\(C\\) and \\(D\\) are found, there will be a literal equality solution to \\(\\mathit{A} \\vec{x} = \\vec{b} - \\vec{r}\\).\nAt first glance, \\(\\mathit{A} \\vec{x} = \\vec{b} - \\vec{r}\\) might seem intractable: How are we to find \\(\\vec{r}\\). The answer is that \\(\\vec{r}\\) will be the solution to the projection problem \\(\\vec{b}\\sim\\mathit{A}\\). When \\(\\vec{r}\\) is selected this way, \\(\\vec{r}\\) will be the shortest possible vector that can do the matching up. In other words, by choosing \\(\\vec{r} = \\vec{b} \\sim \\mathit{A}\\) we are implementing the following definition of “is the best match to”: “the best match is the one with the smallest length \\(\\vec{r}\\).”\nIt is remarkable that one can find \\(\\vec{r}\\) even without knowing \\(\\vec{x}\\). That is why we introduced and solved the projection problem before taking on the target problem.\nThe part of the target problem that we have still to figure out is how, given \\(\\vec{r}\\), to find \\(\\vec{x}\\). But even at this point you can see that \\(\\mathit{A}\\vec{x} = \\vec{b} - \\vec{r}\\) must have a solution, since \\(\\vec{b} - \\vec{r}\\) is exactly the model vector \\(\\hat{b}\\) Chapter 31) showed must lie in \\(span{\\mathit{A}}\\).",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>The target problem</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/31-target-problem.html#sec-linear-equations",
    "href": "Linear-combinations/31-target-problem.html#sec-linear-equations",
    "title": "32  The target problem",
    "section": "",
    "text": "Table 32.1: The CoolingWater data.\n\n\n\n\n\n\n\ntime\ntemp\n\n\n\n\n0\n98.2\n\n\n1\n94.4\n\n\n2\n91.4\n\n\n... 222 rows in total ...\n\n\n\n\n\n\n220\n25.9\n\n\n221\n25.8",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>The target problem</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/31-target-problem.html#visualization-in-a-two-dimensional-subspace",
    "href": "Linear-combinations/31-target-problem.html#visualization-in-a-two-dimensional-subspace",
    "title": "32  The target problem",
    "section": "32.2 Visualization in a two-dimensional subspace",
    "text": "32.2 Visualization in a two-dimensional subspace\nTo help you create a mental model of the geometry of the target problem, we will solve it graphically for a two dimensional subspace. That is, we will solve \\(\\left[\\vec{u}, \\vec{v}\\right] \\vec{x} = \\vec{b}\\). For simplicity, the vectors \\(\\vec{u}\\), \\(\\vec{v}\\) and \\(\\vec{b}\\) will have two components. This means that there is no need to project \\(\\vec{b}\\) onto the subspace; it is already there (so long as \\(\\vec{u}\\) and \\(\\vec{v}\\) have different directions. )\nYou may already have encountered the step (ii) technique in your childhood reading. The problem appears in Robert Louis Stevenson’s famous novel, Treasure Island. The story is about the discovery of a treasure map indicating the location of buried treasure on the eponymous Island. There is a red X on the map labelled “bulk of treasure here,” but that is hardly sufficient to guide the dig for treasure. After all, every buried treasure needs some secret to protect it. On the back of the map is written a cryptic clue to the precise location:\n\nTall tree, Spy-glass shoulder, bearing a point to the N. of N.N.E.\nSkeleton Island E.S.E. and by E.\nTen feet.\n\nSkeleton Island is marked on the map, as is Spy-glass Hill. The plateau marked by the red X “was dotted thickly with pine-trees of varying height. Every here and there, one of a different species rose forty or fifty feet clear above its neighbors.” But which of these was the “tall tree” mentioned in the clue?\n\n\n\n\n\n\nFigure 32.1: The map of Treasure Island. The heading ‘E.S.E. and by E.’ is marked with a solid black line starting at Skeleton Island. The heading ‘N. of N.N.E.’ is marked by dotted lines, one of which is positioned to point at the shoulder of Spy-glass Hill. Where the bearing from Skeleton Island meets the bearing to Spy-glass Hill will be the Tall tree.\n\n\n\nWith your new-found background in vectors, you will no doubt recognize that “N. of N.N.E” is the direction of a vector as is “E.S.E. and by E.” Pirate novels seem always to use the length unit of “pace,” which we will use here as well. The target is the shoulder of Spy-glass Hill. Or, in vector terms, \\(\\vec{b}\\) is the vector with Skeleton Island as the tail and the should of Spy-glass Hill as the tip. The vectors are \\(\\vec{u} = \\text{N. of N.N.E.}\\) and \\(\\vec{v} = \\text{E.S.E. and by E.}\\) We need to \\[\\text{solve} \\ \\ \\underbrace{\\left[\\vec{u}, \\vec{v}\\right]}_{\\Large\\strut\\mathit{A}} \\underbrace{\\small\\left[\\begin{array}{c}C\\\\D\\end{array}\\right]}_{\\Large\\vec{x}} = \\vec{b}\\ \\ \\text{for}\\ \\ {\\small\\left[\\begin{array}{c}C\\\\D\\end{array}\\right]}\\ .\\]\nLong John Silver, obviously an accomplished mathematician, starts near Skeleton Island, moving on along the vector that keeps Skeleton Island to the compass bearing one point east of east-south-east. While on the march, he keeps a telescope trained on the shoulder of Spy-glass Hill. When that telescope points one point north of north-north-east, they are in the vicinity of a tall tree. That is the tree matching the clue.\nThe vectors in Treasure Island were perpendicular to one another, that is, mutually orthogonal. The more general situation is that the vectors in \\(\\mathit{A}\\) will be somewhat aligned with one another: not mutually orthogonal. Figure 32.2 illustrates the situation: \\(\\vec{v}\\) is not perpendicular to \\(\\vec{u}\\). The task, still, is to find a linear combination of \\(\\vec{u}\\) and \\(\\vec{v}\\) that will match \\(\\vec{b}\\). The diagram shows the \\(\\vec{u}\\) vector and the subspace aligned with \\(\\vec{u}\\), and similarly for \\(\\vec{v}\\)\n\n\n\n\n\n\nFigure 32.2: The telescope method of solving projection onto two vectors.\n\n\n\nThe algorithm is based in Long John Silver’s technique. Pick either \\(\\vec{u}\\) or \\(\\vec{v}\\), it does not matter which. In the diagram, we’ve picked \\(\\vec{v}\\). Align your telescope with that vector. Now march along the other vector, \\(\\vec{u}\\), carefully keeping the telescope on the bearing aligned with \\(\\vec{v}\\). From the diagram, you can see that when you’ve marched to \\(\\frac{1}{2} \\vec{u}\\), the telescope does not yet have \\(\\vec{b}\\) in view. Similarly, at \\(1 \\vec{u}\\), the target \\(\\vec{b}\\) isn’t yet visible. Marching a little further, to about \\(1.6 \\vec{u}\\) brings you to the point in the \\(\\vec{u}\\)-subspace where the target falls into view. This tells us that the coefficient on \\(\\vec{u}\\) will be 1.6.\nTo find the coefficient on \\(\\vec{v}\\), you will need to march along the line of the telescope, taking steps of size \\(\\|\\vec{v}\\|\\). In the diagram, we’ve marked the march with copies of \\(\\vec{v}\\) to make the counting easier. We will need to march opposite the direction of \\(\\vec{v}\\), so the coefficient will be negative. Taking 2.8 steps of size \\(\\|\\vec{v}\\|\\) brings us to the target. Thus:\n\\[\\vec{b} = 1.6 \\vec{u} - 2.8 \\vec{v}\\ .\\]\nTo handle vectors in spaces where telescopes are not available, we need an arithmetic algorithm. In R, that algorithm is packaged up as qr.solve(). We will pick this up again the next section.\n\n\n\n\n\n\n\n\nTry it! 32.1\n\n\n\n\n\n\n\n\n\nTry it! 32.1 Working in 3 dimensions\n\n\n\nIn 3-dimensional space, visualization of the solution to the target problem is possible, at least for those who have the talent of rotating three-dimensional objects in their head. For the rest of us, a physical model can help; take three pencils labeled \\(\\vec{u}\\), \\(\\vec{v}\\), and \\(\\vec{b}\\) and bury their tails in a small ball of putty. (Chemistry molecular construction kits are a good alternative.)\nIn case putty, pencils, or a molecular model kit are not available, use the interactive diagram in Figure 32.3. This diagram also includes \\(\\hat{b}\\) and \\(\\vec{r}\\) with the hope that this will guide you into orienting the diagram appropriately to see where the solution comes from.\n\n\n\n\n\n\n\n\n\nFigure 32.3: Showing the relative orientation of the three vectors \\(\\vec{u}\\), \\(\\vec{v}\\) and \\(\\vec{b}\\). Drag the image to rotate it.\n\n\n\n\\(\\vec{u}\\) and \\(\\vec{v}\\) are fixed in length. However, their lengths will appear to change as you rotate the space. This might be called the “gun-barrel” effect; a tube looks very short when you look down its longitudinal axis, but looks longer when you look at it from the side. Rotate the space until both \\(\\vec{u}\\) and \\(\\vec{v}\\) reach their maximum apparent length. The viewpoint that accomplishes this is looking downward perpendicularly onto the \\(\\left[\\vec{u},\\vec{v}\\right]\\)-plane. In this orientation, you will be looking down the barrel of the \\(\\vec{r}\\) gun. Vector \\(\\vec{b}\\) is not in that plane, as you can confirm by rotating the plot a bit out of the \\(\\left[\\vec{u},\\vec{v}\\right]\\)-plane. Returning to the perspective looking down perpendicularly on the place, you can see how \\(\\vec{b}\\) corresponds to \\(\\hat{b}\\), the point in the plane where the projection of \\(\\vec{b}\\) will fall.\nTo find the scalar multiplier on \\(\\vec{v}\\), rotate the space until the vector \\(\\vec{u}\\) is pointing straight toward you. You will see only the arrowhead of \\(\\vec{u}\\). Vectors \\(\\vec{v}\\) and \\(\\hat{b}\\) will appear parallel to each other, but that is because you are looking at the plane edge on. In this orientation, \\(\\hat{b}\\) will appear just a little longer than \\(\\vec{v}\\), perhaps 1.2 times longer. So 1.2 is the scalar multiplier on \\(\\vec{v}\\).\nTo figure out the scalar multiplier on \\(\\vec{u}\\), follow the same procedure as in the previous paragraph, but looking down the barrel of \\(\\vec{v}\\). From this perspective, \\(\\vec{u}\\) appears longer than \\(\\hat{b}\\); the scalar multiplier on \\(\\vec{u}\\) will be about 0.9. The solution to \\(\\mathit{A} \\vec{x} = \\hat{b}\\) is \\[\\vec{x} = \\left[\\begin{array}{r}0.9\\\\1.2\\end{array}\\right]\\ .\\]",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>The target problem</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/31-target-problem.html#properties-of-the-solution",
    "href": "Linear-combinations/31-target-problem.html#properties-of-the-solution",
    "title": "32  The target problem",
    "section": "32.3 Properties of the solution",
    "text": "32.3 Properties of the solution\nAs you might expect, there is a known solution to the target problem. We will start by using a computer implementation of this solution to demonstrate some simple properties of the solution. As an example, we will use three vectors \\(\\vec{u}\\), \\(\\vec{v}\\), and \\(\\vec{w}\\) in a 5-dimensional space as the “screen” to be projected onto, and another vector \\(\\vec{b}\\) as the object being projected.\nThe matrix \\(\\mathit{A}\\) is: \\[{\\mathbf A} \\equiv \\left[\\strut \\begin{array}{ccc}|&|&|\\\\\\vec{u} & \\vec{v} & \\vec{w}\\\\|&|&|\\end{array}\\right]\\]\nFor the sake of example, we will make up some vectors. In your own explorations, you can change them to anything you like.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe operator %onto% model vector and from that we can calculate the residual vector.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThose two simple commands constitute a complete solution to the projection problem, where see seek to model vector and the residual vector.\nIn the target problem we want more: How to express \\(\\hat{b}\\) as a linear combination of the columns in \\(\\mathit{A}\\). At the risk of being repetitive, this means finding \\(\\color{magenta}{\\vec{x}}\\) in \\[\\mathit{A}\\ \\color{magenta}{\\large\\vec{x}} = \\vec{b}\\] where \\(\\mathit{A}\\) and \\(\\vec{b}\\) are given.\nThe function qr.solve() finds \\(\\vec{x}\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nHow can we confirm that this really is the solution to the target problem for this set of vectors? Easy! Just multiply \\(\\mathit{A}\\) by the \\(\\vec{x}\\) that we found. The result should be the target vector \\(\\hat{b}\\):\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>The target problem</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/31-target-problem.html#application-of-the-target-problem",
    "href": "Linear-combinations/31-target-problem.html#application-of-the-target-problem",
    "title": "32  The target problem",
    "section": "32.4 Application of the target problem",
    "text": "32.4 Application of the target problem\nSection 32.1 translates into vector/matrix form the problem, originally stated in Block 1, of finding the best linear combination of \\(f(\\mathtt{time}) \\equiv 1\\) and \\(g(\\mathtt{time}) \\equiv e^{-0.019 \\mathtt{time}}\\). Let’s solve that problem now.\nEarlier we introduced rbind() for the purpose of making column vectors, as in\n\nrbind(3,7,-1)\n##      [,1]\n## [1,]    3\n## [2,]    7\n## [3,]   -1\n\nNow we will work with columns of data stored in the CoolingWater data frame. A good way to extract a column from a data frame is using the with() function. For instance,\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNotice that cbind() automatically translated 1 into the vector of all ones.\nWe are all set up to solve the target problem:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nHow good an answer is the x calculated by qr.solve()? Judge for yourself!\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nYou may recall from Block 1 the explanation for the poor match between the model and the data for early times: that the water cooled quickly when poured into the cool mug, but the mug-with-water cooled much slower into the room air.\nLet’s augment the model by adding another vector with a much faster exponential cooling, say, \\(e^{-0.06 \\mathtt{time}}\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n:::",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>The target problem</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/32-stat-modeling.html",
    "href": "Linear-combinations/32-stat-modeling.html",
    "title": "33  Statistical modeling and R2",
    "section": "",
    "text": "33.1 How good a model?\nSo far, we’ve been looking at linear combinations from a distinctively mathematical point of view: vectors, collections of vectors (matrices), projection, angles and orthogonality. We’ve show a few applications of the techniques for working with linear combinations, but have always expressed those techniques using mathematical terminology. In this Chapter, we will take a detour to get a sense of the perspective and terminology of another field: statistics.\nIn the quantitative world, including fields such as biology and genetics, the social sciences, business decision making, etc. there are far more people working with linear combinations with a statistical eye than there are people working with the mathematical form of notation. Statistics is a far wider field than linear combination, so this chapter is not an attempt to replace the need to study statistics and data science. The purpose is merely to show you how a mathematical process can be used as part of a broader framework to provide useful information to decision-makers.\nIt often happens that a model is needed to help organize complex, multivariate data for purposes such as prediction. As a case in point, consider the data available in the Body_fat data frame, which consists of measurements of characteristics such as height, weight, chest circumference, and body fat on 252 men.\nBody fat, the percentage of total body mass consisting of fat, is thought by some to be a good measure of general fitness. To what extent this theory is merely a reflection of general societal attitudes toward body shape is unknown.\nWhatever its actual utility, body fat is hard to measure directly; it involves submerging a person in water to measure total body volume, then calculating the persons mass density and converting this to a reading of body-mass percentage. For those who would like to see body fat used more broadly as a measure of health and fitness, this elaborate procedure stands in the way. And so they seek easier ways to estimate body fat along the lines of the Body Mass Index (BMI), which is a simple arithmetic combination of easily measured height and weight. (Note that BMI is also controversial as anything other than a rough description of body shape. In particular, the label “overweight,” officially \\(25 \\leq \\text{BMI}\\leq 30\\) has at best little connection to actual health.)\nHow can we construct a model, based on the available data, of body fat as a function of the easy-to-measure characteristics such as height and weight? You can anticipate that this will be a matter of applying what we know about the target problem \\[\\text{Given}\\ \\mathit{A}\\  \\text{and}\\ \\vec{b}\\text{, solve } \\mathit{A} \\vec{x} = \\vec{b}\\ \\text{for}\\ \\vec{x}\\]where \\(\\vec{b}\\) is the column of body-mass measurements and \\(\\mathit{A}\\) is the matrix of all the other columns in the data frame.\nIn statistics, the target \\(\\vec{b}\\) is called the response variable and \\(\\mathit{A}\\) is the set of explanatory variables. You can also think of the response variable as the output of the model we will build and the explanatory variables as the inputs to that model.\nAlthough application of the target problem is an essential part of constructing a statistical model, it is far from the only part. For instance, statisticians find is useful to think about “how much” of the response variable is explained by the explanatory variables. Measuring this requires a definition for “how much.” In defining “how much,” statisticians focus not on how much variation there is among the values in the response variable. The standard way to measure this is with the variance, which was introduce in Section 29.4 and can be thought of as the average of pair-wise differences among the elements in \\(\\vec{b}\\).\nto support this focus on the variance of \\(\\vec{b}\\), statisticians typically augment \\(\\vec{A}\\) with a column of ones, which they call the intercept.\nTo move forward, we will extract the response variable from the data and construct \\(\\vec{A}\\), adding in the vector of ones. We will show the vector/matrix commands for doing this, but you don’t have to remember them because statisticians have a more user-friendly interface to the calculations.\nThere are so many ways to construct a linear-combination model from a data frame—all the different combinations of columns plus possibly interaction terms and other transformations—that it is natural to ask, “What’s the best model?”\nAs always, “best” depends on the purpose for your model is to be used. This requires thought on the part of the modeler. One method you will often encounter is called R-squared and written R2.\nThe basic question addressed by R2 is: How much of the variation in the response variable b is accounted for by the columns of the matrix A. The standard way to measure the “amount of variation” in a variable is the variance. In R, you calculate that with\nWe can also look at the variation in the model vector, \\(\\hat{b}\\).\nR2 is simply the ratio of these two variances:\nThis result, 31.7%, is interpreted as the fraction of the variance in the response variable that is accounted for by the model. Near synonyms for “accounted for” are “explained by” or “can be attributed to.”\nIn the same spirit, we can ask how much of the variance in the response variable is unexplained or unaccounted for by the explanatory variables . To answer this, look at the size of the residual:\nNotice that the amount of variance explained, 68.3%, plus the amount remaining unexplained, 31.7%, add up to 100%. This is no accident. The additivity is why statisticians use the variance as a measure of variability.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Statistical modeling and R^2^</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/32-stat-modeling.html#how-good-a-model",
    "href": "Linear-combinations/32-stat-modeling.html#how-good-a-model",
    "title": "33  Statistical modeling and R2",
    "section": "",
    "text": "Please enable JavaScript to experience the dynamic code cell content on this page.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Statistical modeling and R^2^</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/32-stat-modeling.html#machine-learning",
    "href": "Linear-combinations/32-stat-modeling.html#machine-learning",
    "title": "33  Statistical modeling and R2",
    "section": "33.2 Machine learning",
    "text": "33.2 Machine learning\nIf you pay attention to trends, you will know about advances in artificial intelligence and the many claims—some hype, some not—about how it will change everything from animal husbandry to warfare. Services such as Google Translate are based on artificial intelligence, as are many surveillance technologies. (Whether the surveillance is for good or ill is a serious matter.)\nSkills in artificial intelligence are currently a ticket to lucrative employment.\nLike so many things, “artificial intelligence” is a branding term. In fact, what all the excitement is about is not mostly artificial intelligence at all. The advances, by and large, have come over 50 years of development in a field called “statistical learning” or “machine learning,” depending on whether the perspective is from statistics or computer science.\nA major part of the mathematical foundation of statistical (or “machine”) learning is linear algebra. Many workers in “artificial intelligence” are struggling to catch up because they never took linear algebra in college or, if they did, they took a proof-oriented course that didn’t cover the elements of linear algebra that are directly applicable. We are trying to do better in this course.\nSo if you’re diligent, and continue your studies to take actual statistical/machine learning courses, you will find yourself at the top of the heap. Even xkcd, the beloved techno-comic, gets in on the act, as this cartoon reveals:\n\n\n\n\n\n\n\n\n\nLook carefully below the paddle and you will see the Greek letter “lambda”, \\(\\lambda\\). You will meet the linear algebra concept signified by \\(\\lambda\\)—eigenvalues and eigenvectors—in Block 6.\n\n\n\n\n\n\nTip\n\n\n\nWe’ve been using the R/mosaic function df2matrix() to construct the A and b matrices used in linear model from data. This is mainly for convenience: we need a way to carry out the calculations that lets you see the x vector, and calculate the model vector and the residual in the way described in Chapter 32.\nIn practice, statistical modelers use other software. The most famous of these in R is the lm() family of functions. This does all the work of creating the b vector and the A matrix, QR solving, etc. We call it a “family” of functions because the output of lm() is not simply the vector of coefficients x but includes many other features that support statistical inference on the models created.\nIn a statistics course using R, you are very likely to encounter lm(). You will never hear about df2matrix() outside of this book.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Statistical modeling and R^2^</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/32-stat-modeling.html#footnotes",
    "href": "Linear-combinations/32-stat-modeling.html#footnotes",
    "title": "33  Statistical modeling and R2",
    "section": "",
    "text": "Amazingly, this work attracted little attention until after 1900, when Mendel’s laws were rediscovered by the botanists de Vries, Correns, and von Tschermak.↩︎",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Statistical modeling and R^2^</span>"
    ]
  },
  {
    "objectID": "accumulation-part.html",
    "href": "accumulation-part.html",
    "title": "BLOCK IV. Accumulation",
    "section": "",
    "text": "We have already studied the relationship between functions and their derivatives. In this Block, we will examine techniques that exploit that relationship to re-construct a function from knowledge of its derivative by a process called anti-differentiation.\nJust as derivatives tell about rates of change, anti-derivatives tell about the accumulation of change.",
    "crumbs": [
      "BLOCK IV. Accumulation"
    ]
  },
  {
    "objectID": "Accumulation/33-intro.html",
    "href": "Accumulation/33-intro.html",
    "title": "34  Change & accumulation",
    "section": "",
    "text": "34.1 Accumulation\nEvery 10 years, starting in 1790, the US Census Bureau carries out a constitutionally mandated census: a count of the current population. The overall count as a function of year is shown in Figure 34.1. [Source]\nIn the 230 years spanned by the census data, the US population has grown 100-fold, from about 4 million in 1790 to about 330,000,000 in 2020.\nIt is tempting to look for simple patterns in such data. Perhaps the US population has been growing exponentially. A semi-log plot of the same data suggests that the growth is only very roughly exponential. A truly exponential process would present as a curve with a constant derivative, but the derivative of the function in the graph is decreasing over the centuries.\nInsofar as the slope over the semi-log graph is informative, it amounts to this quantity: \\[\\partial_t \\ln(\\text{pop}(t)) = \\frac{\\partial_t\\, \\text{pop}(t)}{\\text{pop}}\\] This is the per-capita rate of growth, that is, the rate of change in the population divided by the population. Conventionally, this fraction is presented as a percentage: percentage growth in the population per year, as in Figure 34.2.\nThe dots in the graph are a direct calculation from the census data. There is a lot of fluctuation, but an overall trend stands out: the population growth rate has been declining since the mid-to late 1800s. The deviations from the trend are telling and correspond to historical events. There is a relatively low growth rate seen from 1860 to 1870: that is the effect of the US Civil War. The Great depression is seen in the very low growth from 1930 to 1940. Baby Boom: look at the growth from 1950-1960. The bump from 1990 to 2000? Not coincidentally, the 1990 Immigration Act substantially increased the yearly rate of immigration.\nIf the trend in the growth rate continues, the US will reach zero net growth about 2070, then continue with negative growth. Of course, negative growth is just decline. A simple prediction from Figure 34.2 is that the argmax of the US population—that is, the year that the growth rate reaches zero—will occur around 2070.\nHow large will the population be when it reaches its maximum?\nIn Block 2, we dealt with situations where we know the function \\(f(t)\\) and want to find the rate of change \\(\\partial_t f(t)\\). Here, we know the rate of change of the population and we need to figure out the population itself, in other words to figure out from a known \\(\\partial_t f(t)\\) what is the unknown function \\(f(t)\\).\nThe process of figuring out \\(f(t) \\longrightarrow \\partial_t f(t)\\) is, of course, called differentiation. The opposite process, \\(\\partial_t f(t) \\longrightarrow f(t)\\) is called anti-differentiation.\nIn this block we will explore the methods for calculating anti-derivatives and some of the settings in which anti-derivative problems arrive.\nImagine a simple setting: water flowing out of a tap into a basin or tank. The amount of water in the basin will be measured in a unit of volume, say liters. Measurement of the flow \\(f(t)\\) of water from the tap into the tank has different units, say liters per second. If volume \\(V(t)\\) is the volume of water in the tank as a function of time, \\(f(t)\\) at any instant is \\(f(t) = \\partial_t V(t)\\).\nThere is a relationship between the two functions \\(f(t)\\) and \\(V(t)\\). Derivatives give a good description of that relationship: \\[f(t) = \\partial_t V(t)\\] This description will be informative if we have measured the volume of water in the basin as a function of time and want to deduce the rate of flow from the tap. Now suppose we have measured the flow \\(f(t)\\) and want to figure out the volume. The volume at any instant is the past flow accumulated to that instant. As a matter of notation, we write this view of the relationship as \\[V(t) = \\int f(t) dt,\\]. Read this as “volume is the accumulated flow.”\nOther examples of accumulation and change:",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Change & accumulation</span>"
    ]
  },
  {
    "objectID": "Accumulation/33-intro.html#accumulation",
    "href": "Accumulation/33-intro.html#accumulation",
    "title": "34  Change & accumulation",
    "section": "",
    "text": "velocity is the rate of change of position with respect to time. Likewise, position is the accumulation of velocity over time.\nforce is the rate of energy with respect to position. Likewise energy is the accumulation of force as position changes.\ndeficit is the rate of change of debt with respect to time. Likewise, debt is the accumulation of deficit over time.",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Change & accumulation</span>"
    ]
  },
  {
    "objectID": "Accumulation/33-intro.html#notation-for-anti-differentiation",
    "href": "Accumulation/33-intro.html#notation-for-anti-differentiation",
    "title": "34  Change & accumulation",
    "section": "34.2 Notation for anti-differentiation",
    "text": "34.2 Notation for anti-differentiation\nFor differentiation we are using the notation \\(\\partial_x\\) as in \\(\\partial_x f(x)\\). Remember that the subscript on \\(\\partial\\) names the with-respect-to input. There are three pieces of information this notation:\n\nThe [\\(\\partial\\)]{style=“color: magenta;} symbol which identifies the operation as partial differentiation.\nThe name of the with-respect-to input \\(\\partial_{\\color{magenta}{x}}\\) written as a subscript to \\(\\partial\\).\nThe function to be differentiated, \\(\\partial_x \\color{magenta}{f(x)}\\).\n\nFor anti-differentiation, our notation must also specify the three pieces of information. It might be tempting to use the same notation as differentiation but replace the \\(\\partial\\) symbol with something else, perhaps \\(\\eth\\) or \\(\\spadesuit\\) or \\(\\forall\\), giving us something like \\(\\spadesuit_x f(x)\\).\nConvention has something different in store. The notation for anti-differentiation is \\[\\large \\int f(x) dx\\] 1. The \\(\\color{magenta}{\\int}\\) is the marker for anti-differentiation. 2. The name of the with-respect-to input is contained in the “dx” at the end of the notation: \\(\\int f(x) d\\color{magenta}{x}\\) 3. The function being anti-differentiated is in the middle \\(\\int \\color{magenta}{f(x)} dx\\).\nFor those starting out with anti-differentiation, the conventional notation can be confusing, especially the \\(dx\\) part. It is easy confuse \\(d\\) for a constant and \\(x\\) for part of the function being anti-differentiated.\nThink of the \\(\\int\\) and the \\(dx\\) as brackets around the function. You need both brackets for correct notation, the \\(\\int\\) and the \\(dx\\) together telling you what operation to perform.\nRemember that just as \\(\\partial_x f(x)\\) is a function, so is \\(\\int f(x) dx\\).",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Change & accumulation</span>"
    ]
  },
  {
    "objectID": "Accumulation/33-intro.html#rmosaic-notation",
    "href": "Accumulation/33-intro.html#rmosaic-notation",
    "title": "34  Change & accumulation",
    "section": "34.3 R/mosaic notation",
    "text": "34.3 R/mosaic notation\nRecall that the notation for differentiation in R/mosaic is D(f(x) ~ x). The R/mosaic notation for anti-differentiation is very similar:\nD(f(x) ~ x)\nThis has the same three pieces of information as \\(\\partial_x f(x)\\)\n\nD() signifies differentiation whereas antiD() signifies anti-differentiation.\n~ x identifies the with-respect-to input.\nf(x) ~ is the function on which the operation is to be performed.\n\nRemember that just as D(f(x) ~ x) creates a new function out of f(x) ~ x, so does antiD(f(x) ~ x).",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Change & accumulation</span>"
    ]
  },
  {
    "objectID": "Accumulation/33-intro.html#dimension-and-anti-differentiation",
    "href": "Accumulation/33-intro.html#dimension-and-anti-differentiation",
    "title": "34  Change & accumulation",
    "section": "34.4 Dimension and anti-differentiation",
    "text": "34.4 Dimension and anti-differentiation\nThis entire block will be about anti-differentiation, its properties and its uses. You already know that anti-differentiation (as the name suggests) is the inverse of differentiation. There is one consequence of this that is helpful to keep in mind as we move on to other chapters. This being calculus, the functions that we construct and operate upon have inputs that are quantities and outputs that are also quantities. Every quantity has a dimension, as discussed in Chapter 15. When you are working with any quantity, you should be sure that you know its dimension and its units.\nThe dimension of the input to a function does not by any means have to be the same as the dimension of the output. For instance, we have been using many functions where the input has dimension time and the output is position (dimension L) or velocity (dimension L/T) or acceleration (dimension L/T\\(^2\\)).\nImagine working with some function \\(f(y)\\) that is relevant to some modeling project of interest to you. Returning to the bracket notation that we used in Chapter 15, the dimension of the input quantity will be [\\(y\\)]. The dimension of the output quantity is [\\(f(y)\\)]. (Remember from 16 that [\\(y\\)] means “the dimension of quantity \\(y\\)” and that [\\(f(y)\\)] means “the dimension of the output from \\(f(y)\\).”)\nThe function \\(\\partial_y f(y)\\) has the same input dimension \\([y]\\) but the output will be \\([f(y)] / [y]\\). For example, suppose \\(f(y)\\) is the mass of fuel in a rocket as a function of time \\(y\\). The output of \\(f(y)\\) has dimension M. The input dimension \\([y]\\) is T.\nThe output of the function \\(\\partial_y f(y)\\) has dimension \\([f(y)] / [y]\\), which in this case will be M / T. (Less abstractly, if the fuel mass is given in kg, and time is measured in seconds, then \\(\\partial_y f(y)\\) will have units of kg-per-second.)\nHow about the dimension of the anti-derivative \\(F(y) = \\int f(y) dy\\)? Since \\(F(y)\\) is the anti-derivative of \\(f(y)\\) (with respect to \\(y\\)), we know that \\(\\partial_y F(y) = f(y)\\). Taking the dimension of both sides \\[[\\partial_y F(y)] = \\frac{[F(y)]}{[y]} = \\frac{[F(y)]}{\\text{T}} = [f(y)] = \\text{M}\\] Consequently, \\([F(y)] = \\text{M}\\).\nTo summarize:\n\nThe dimension of derivative \\(\\partial_y f(y)\\) will be \\([f(y)] / [y]\\).\nThe dimension of the anti-derivative \\(\\int f(y) dy\\) will be \\([f(y)]\\times [y]\\).\n\nOr, more concisely:\n\nDifferentiation is like division, anti-differentiation is like multiplication.\n\nPaying attention to the dimensions (and units!) of input and output can be a boon to the calculus student. Often students have some function \\(f(y)\\) and they are wondering which of the several calculus operations they are supposed to do: differentiation, anti-differentiation, finding a maximum, finding an argmax or a zero. Start by figuring out the dimension of the quantity you want. From that, you can often figure out which operation is appropriate.\nTo illustrate, imagine that you have constructed \\(f(y)\\) for your task and you know, say, \\[[f(y)] = \\text{M       and} \\  \\ \\ \\ \\ [y] = \\text{T}\\ .\\] Look things up in the following table:\n\n\n\nDimension of result\nCalculus operation\n\n\n\n\nM / T\ndifferentiate\n\n\nM T\nanti-differentiate\n\n\nM\nfind max or min\n\n\nT\nfind argmax/argmin or a function zero\n\n\nM T\\(^2\\)\nanti-differentiate twice in succession\n\n\nM / T\\(^2\\)\ndifferentiate twice in succession\n\n\n\nFor example, suppose the output of the accelerometer on your rocket has dimension L / T\\(^2\\). You are trying to figure out from the accelerometer reading what is your altitude. Altitude has dimension L. Look up in the table to see that you want to anti-differentiate acceleration twice in succession.",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Change & accumulation</span>"
    ]
  },
  {
    "objectID": "Accumulation/33-intro.html#sec-preliminary-terrors",
    "href": "Accumulation/33-intro.html#sec-preliminary-terrors",
    "title": "34  Change & accumulation",
    "section": "34.5 From Calculus Made Easy",
    "text": "34.5 From Calculus Made Easy\nCalculus Made Easy, by Silvanus P. Thompson, is a classic, concise, and elegant textbook from 1910. It takes a common-sense approach, sometimes lampooning the traditional approach to teaching calculus.\n\nSome calculus-tricks are quite easy. Some are enormously difficult. The fools who write the textbooks of advanced mathematics—and they are mostly clever fools—seldom take the trouble to show you how easy the easy calculations are. On the contrary, they seem to desire to impress you with their tremendous cleverness by going about it in the most difficult way. — From the preface\n\nThompson’s first chapter starts with the notation of accumulation, which he calls “the preliminary terror.”\n\nThe preliminary terror … can be abolished once for all by simply stating what is the meaning—in common-sense terms—of the two principal symbols that are used in calculating.\nThese dreadful symbols are:\n\n\\(\\Large\\  d\\) which merely means “a little bit of.”\n\nThus \\(dx\\) means a little bit of \\(x\\); or \\(du\\) means a little bit of \\(u\\). Ordinary mathematicians think it more polite to say “an element of,” instead of “a little bit of.” Just as you please. But you will find that these little bits (or elements) may be considered to be indefinitely small.\n\n\\(\\ \\ \\large\\int\\) which is merely a long \\(S\\), and may be called (if you like) “the sum of.”\n\nThus \\(\\ \\int dx\\) means the sum of all the little bits of \\(x\\); or \\(\\ \\int dt\\) means the sum of all the little bits of \\(t\\). Ordinary mathematicians call this symbol “the integral of.” Now any fool can see that if \\(x\\) is considered as made up of a lot of little bits, each of which is called \\(dx\\), if you add them all up together you get the sum of all the \\(dx\\)’s, (which is the same thing as the whole of \\(x\\)). The word “integral” simply means “the whole.” If you think of the duration of time for one hour, you may (if you like) think of it as cut up into \\(3600\\) little bits called seconds. The whole of the \\(3600\\) little bits added up together make one hour.\nWhen you see an expression that begins with this terrifying symbol, you will henceforth know that it is put there merely to give you instructions that you are now to perform the operation (if you can) of totaling up all the little bits that are indicated by the symbols that follow.\n\n\nThe next chapter shows what it means to “total up all the little bits” of a function.",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Change & accumulation</span>"
    ]
  },
  {
    "objectID": "Accumulation/34-visualizing.html",
    "href": "Accumulation/34-visualizing.html",
    "title": "35  Totaling the little bits",
    "section": "",
    "text": "Many students wonder how it is possible to reconstruct a function \\(F(x)\\) from its derivative \\(f(x)\\). The point of this short chapter is to help you develop some intuition about anti-differentiation.\nYou already know the notation meaning “\\(F(x)\\) is the anti-derivative of \\(f(x)\\)”: \\[\\large \\int f(x)\\, dx\\ .\\] In drawing a graph of \\(F(x)\\), we will of course want to use coordinate axes where the quantity \\(x\\) is represented on the horizontal axis and the quantity of the output \\(F(x)\\) is on the vertical axis:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 35.1: The graphics frame in which we want to draw the graph of \\(F(x)\\). A small region of the domain is labeled \\(dx\\). Within that domain, \\(F(x)\\), whatever it is, will be a sloping segment. The gray segment shows what this might look like.\n\n\n\nIt is premature to have drawn a segment of \\(F(x)\\) because we haven’t yet undertaken to compute \\(F(x) = \\int f(x)\\, dx\\). At this point in the process, all we know is \\(f(x)\\), not \\(F(x)\\). Still, since we know \\(f(x)\\), we do know the slope of the little segment of \\(F(x)\\). We just don’t know where that segment should be located vertically in each of the \\(dx\\) regions that make up the whole domain.\nWe cannot draw \\(f(x)\\) in the ordinary way as a curve wending its way across the domain of the graph. Why not? Because the vertical axis of the graphics frame represents \\(F(x)\\) and has a different dimension than the output of \\(f(x)\\).\nBut we can draw \\(f(x)\\) as the slope of a short segment of horizontal extent \\(dx\\), so long as we accept that the vertical position of that segment means nothing: \\(f(x)\\) gives information only about the slope of the segment. At this point, the best we can do is graph \\(f(x)\\) in terms of sloping segments, as in Figure 35.2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 35.2: Graphing \\(f(x)\\) as a series of sloped segments. The slope is the value of \\(f(x)\\). But the vertical position of the segment is not information provided by \\(f(x)\\), so we’ve drawn all of them at the same vertical level.\n\n\n\nEach of the segments in Figure 35.2 has the same horizontal extent, namely \\(dx\\). When we draw a sloping segment over the tiny bit \\(dx\\) of the domain, the vertical extent of the segment will be the product of the width \\(dx\\) and the slope \\(f(x)\\). That is, the vertical extent will be the product \\(f(x) dx\\).\nWhenever we know a function \\(f(x)\\) and have chosen a size for \\(dx\\) we can draw a graph of \\(f(x)\\) in the form shown in Figure 35.2. We are drawing it in this unusual way because we want the graphics frame to be all ready for drawing the graph of \\(F(x)\\) in the normal fashion after we have figured out what \\(F(x)\\) results from accumulating/summing-up all the little \\(f(x) dx\\) segments. When we write \\(\\large\\int\\) in the notation \\[\\large \\int f(x)\\, dx\\] we mean, “sum up all the \\(f(x) dx\\) segments.”\nLet’s now consider how to “sum up all the segments.” we will start in Figure 35.3 with an example where we already know \\(F(x)\\). That way, we can see of our sum of the \\(f(x) dx\\) segments really does reconstruct \\(F(x)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 35.3: Top: A function \\(F(x)\\) for demonstration purposes. Bottom: Slicing \\(F(x)\\) into piecewise domains of width \\(dx=2\\) and anchoring the left-most point of each slice vertically at 0.\n\n\n\nNow imagine that we sliced up \\(F(x)\\) over small sub-domains of \\(x\\), as in Figure 35.3 (bottom). That is, we approximated \\(F()\\) piecewise locally. But we’ve broken the continuity of \\(F(x)\\) by moving each slice up or down so that the left-most point has value 0.\nCan you reconstruct \\(F(x)\\) from the local segments?\nStart by reading off the function value from the last point in the left-most segment. That is been marked in Figure 35.3 with a blue dot. The function value at that dot is 7.072.\nNow take the second segment. The idea is to move that segment upward until it joins the first segment at the blue dot. You can do that by adding 7.072 to the second segment. The result is shown in Figure 35.4(top).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 35.4: Reconstructing the original function \\(F(x)\\) by moving each segment upward to meet its left neighbor. Top: The first two segments are connected. Bottom: The third segment is connected to the first two.\n\n\n\nNow read off the new value at the end of the second segment, it is 4.198. Add this amount to the third segment as in Figure 35.4(bottom).\nContinue this process until you have reconstructed \\(F(x)\\) from the local segments.\nYou may object: “Of course you can reconstruct \\(F(x)\\) from the local segments, but this isn’t the same as reconstructing \\(F(x)\\) from its derivative \\(\\partial_x F(x)\\).” My answer: “That depends on how many segments you use.”\nWhen we make the segment width \\(h\\) smaller and smaller, the individual segments become more and more like straight lines. Figure 35.5 shows the segments for smaller and smaller \\(h\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 35.5: Using small segments gives each segment a simple shape. In Figure 35.4 the segments had width \\(dx=2.0\\) and were discernable curvy. Top of this figure: At width \\(dx=0.25\\) a few of the segments look curved. Middle: This graph zooms in on the subdomain \\(-1 \\\\leq x \\\\leq 1\\) in the top panel, where there is a notably curved segment in the top graph. Setting \\(dx=0.1\\) breaks up such curved segment into components better approximated by a straight line. Bottom: Using \\(dx=0.01\\) further improves the approximation of each segment to a straight line.\n\n\n\nNotice that many of the segments are straight lines. That is understandable, since any function looks like a straight line over a small enough domain.\nEach of those straight-line segments is drawn over a domain \\(x_i &lt; x &lt; x_i+dx\\) that has width \\(dx\\). For \\(dx\\) small enough, the segment is well approximated by a straight line with slope \\(\\partial_x F(x_i)\\). Multiplying slope by width \\(dx\\) gives the segment height: \\(\\left[{\\large\\strut}\\partial_x F(x_i)\\right]\\ dx\\). Of course, remember that \\(\\partial_x F(x) = f(x)\\) helps us see that each of the little segments is \\(f(x_i)\\ dx\\).\nLets review. The standard notation for anti-differentiation can be interpreted as putting together segments, or, in the words of Prof. Thompson in Calculus Made Easy, “totaling up all the little bits.” (See Section 34.5.)\n\nWe start with the function that we already know: \\(\\large f(x)\\). Remember that \\(f(x)\\), at each value of \\(x\\) will be the slope of \\(F(x)\\). Why? Because \\(F(x)\\) is the anti-derivative of \\(f(x)\\), so \\(f(x)\\) is the derivative of \\(F(x)\\).\nNow divide the domain \\(x\\) into many little bits. Each of these sub-domains is \\(\\large dx\\), a little chunk of \\(x\\).\nOn each of the little chunks, draw in \\(f(x)\\). Since \\(f(x)\\) is the slope of \\(F(x)\\), we will draw \\(f(x)\\) for any given chunk as a short line segment of that slope over the chunk, as in Figure 35.2. We will write these little bits, each of which is very nearly a straight-line, as \\(\\large\\color{blue}{f(x) dx}\\).\nAssemble together all the \\(f(x)dx\\) segments from (3) to get \\(F(x)\\). This instruction to assemble is denoted \\[\\Large \\color{blue}\\int\\]\n\nAltogether, we have:\n\\[\\large \\underbrace{\\underbrace{\\Large\\color{magenta}{\\int}}_{\\color{magenta}{\\text{assemble}}} \\underbrace{\\Large \\overbrace{f(x)}^{\\small\\text{slope of F(x)}}\\ \\  \\overbrace{\\strut dx}^{\\small \\text{bits of}\\ x}}_{\\color{blue}{\\text{the slope segments}}}}_{\\text{giving}\\ {\\Large F(x)+C}\\ \\text{altogether.}}\\]\nReturning to the example with which we started the chapter, here are the little segments of the slope function shown in Figure 35.2 assembled together to produce the anti-derivative function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 35.6: \\(\\color{blue}{\\int} f(x)dx\\) means to assemble the straight-line pieces \\(f(x) dx\\) in the manner described in the previous section.",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Totaling the little bits</span>"
    ]
  },
  {
    "objectID": "Accumulation/35-integration.html",
    "href": "Accumulation/35-integration.html",
    "title": "36  Integration",
    "section": "",
    "text": "36.1 Net change\nAnti-derivatives are useful when you know how a quantity is changing but don’t yet know the quantity itself.\nIt is important, of course, to keep track of which is the “quantity itself” and which is the “rate of increase in that quantity.” This always depends on context and your point of view. It is convenient, then, to set some fixed examples to make it easy to keep track of which quantity is which.\nWe will also adopt a convention to make it simpler to recognize which quantity is the “quantity itself” and which is the “rate of increase in that quantity.” We will use CAPITAL LETTERS to name functions that are the quantity itself, and lower-case letters for the rate of increase in that quantity. For example, if talking about motion, an important quantity is momentum and how it changes over time. The momentum itself will be \\({\\mathbf M}(t)\\) while the rate of increase of momentum will be \\(m(t)\\).1 The amount of money a business has on hand at time \\(t\\) is \\({\\mathbf S}(t)\\) measured, say, in dollars. The rate of increase of that money is \\(s(t)\\), in, say, dollars per day.\nNotice that we are using the phrase “rate of increase” rather than “rate of change.” that is because we want to keep straight the meaning of the sign of the lower-case function. If \\(m(t)\\) is positive, the momentum is increasing. If \\(m(t)\\) is negative then it is a “negative rate of increase,” which is, of course, just a “decrease.”\nFor a business, money coming in means that \\(s(t)\\) is positive. Expenditures of money correspond to \\(s(t)\\) being negative. In the fuel example. \\({\\mathbf F}(t)\\) is the amount of fuel in the tank. \\(f(t)\\) is the rate of increase in the amount of fuel in the tank. Of course, engines burn fuel, removing it from the tank. So we would write the rate at which fuel is burned as \\(-f(t)\\): removing fuel is a negative increase in the amount of fuel, an expenditure of fuel.\nThe objective of this chapter is to introduce you to the sorts of calculations, and their notations, that let you figure out how much the CAPITAL LETTER quantity has changed over an interval of \\(t\\) based on what you already know about the value over time of the lower-case function.\nThe first step in any such calculation is to find or construct the lower-case function \\(f(t)\\) or \\(c(t)\\) or \\(m(t)\\) or whatever it might be. This is a modeling phase. In this chapter, we will ignore detailed modeling of the situation and just present you with the lower-case function.\nThe second step in any such calculation is to compute the anti-derivative of the lower-case function, giving as a result the CAPITAL LETTER function. You’ve already seen the notation for this, e.g. \\[{\\Large  F(t) = \\int f(t) dt}\\ \\ \\ \\ \\ \\text{or}\\ \\ \\ \\ \\ {\\Large G(t) = \\int g(t) dt}\\ \\ \\ \\ \\text{and so on.}\\] In this chapter, we will not spend any time on this step; we will assume that you already have at hand the means to compute the anti-derivative. (Indeed, you already have antiD() available which will do the job for you.) Later chapters will look at the issues around and techniques for doing the computations by other means.\nThe remaining steps in such calculations are to work with the CAPITAL LETTER function to compute such things as the amount of that quantity, or the change in that quantity as it is accumulated over an interval of \\(t\\).\nPerhaps it goes without saying, but once you have the CAPITAL LETTER function, e.g. \\(F(t)\\), you can evaluate that function at any input that falls into the domain of \\(F(t)\\). If you have a graph of \\(F(t)\\) versus \\(t\\), just position your finger on the horizontal axis at input \\(t_1\\), then trace up to the function graph, then horizontally to the vertical axis where you can read off the value \\(F(t_1)\\). If you have \\(F()\\) in the form of a computer function, just apply \\(F()\\) to the input \\(t_1\\).\nIn this regard, \\(F(t)\\) is like any other function.\nHowever, in using and interpreting the \\(F(t)\\) that we constructed by anti-differentiating \\(f(t)\\), we have to keep in mind the limitations of the anti-differentiation process. In particular, any function \\(f(t)\\) does not have a unique anti-derivative function. If we have one anti-derivative, we can always construct another by adding some constant: \\(F(t) + C\\) is also an anti-derivative of \\(f(t)\\).\nBut we have a special purpose in mind when calculating \\(F(t_1)\\). We want to figure out from \\(F(t)\\) how much of the quantity \\(f(t)\\) has accumulated up to time \\(t_1\\). For example, if \\(f(t)\\) is the rate of increase in fuel (that is, the negative of fuel consumption), we want \\(F(t_1)\\) to be the amount of fuel in our tank at time \\(t_1\\). That cannot happen. All we can say is that \\(F(t_1)\\) is the amount of fuel in the tank at \\(t_1\\) give or take some unknown constant C.\nInstead, the correct use of \\(F(t)\\) is to say how much the quantity has changed over some interval of time, \\(t_0 \\leq t \\leq t_1\\). This “change in the quantity” is called the net change in \\(F()\\). To calculate the net change in \\(F()\\) from \\(t_0\\) to \\(t_1\\) we apply \\(F()\\) to both \\(t_0\\) and \\(t_1\\), then subtract:\n\\[\\text{Net change in}\\ F(t) \\ \\text{from}\\ t_0 \\ \\text{to}\\ t_1 :\\\\= F(t_1) - F(t_0)\\]",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Integration</span>"
    ]
  },
  {
    "objectID": "Accumulation/35-integration.html#net-change",
    "href": "Accumulation/35-integration.html#net-change",
    "title": "36  Integration",
    "section": "",
    "text": "Try it! 36.1\n\n\n\n\n\n\n\n\n\nTry it! 36.1 Momentum, force, energy\n\n\n\nSuppose you have already constructed the rate-of-change function for momentum \\(m()\\) and implemented it as an R function m(). For instance, \\(m(t)\\) might be the amount of force at any instant \\(t\\) of a car, and \\({\\mathbf M}(t)\\) is the force accumulated over time, better known as momentum. We will assume that the input to m() is in seconds, and the output is in kg-meters-per-second-squared, which has the correct dimension for force.\nYou want to find the amount of force accumulated between time \\(t=2\\) and \\(t=5\\) seconds.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nTo make use of this quantity, you will need to know its dimension and units. For this example, where the dimension [\\(m(t)\\)] is M L T\\(^{-2}\\), and [\\(t\\)] = T, the dimension [\\({\\mathbf M}(t)\\)] will be M L T\\(^{-1}\\). In other words, if the output of \\(m(t)\\) is kg-meters-per-second-squared, then the output of \\(V(t)\\) must be kg- meters-per-second.",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Integration</span>"
    ]
  },
  {
    "objectID": "Accumulation/35-integration.html#the-definite-integral",
    "href": "Accumulation/35-integration.html#the-definite-integral",
    "title": "36  Integration",
    "section": "36.2 The “definite” integral",
    "text": "36.2 The “definite” integral\nWe have described the process of calculating a net change from the lower-case function \\(f(t)\\) using two steps:\n\nConstruct \\(F(t) = \\int f(t) dt\\).\nEvaluate \\(F(t)\\) at two inputs, e.g. \\(F(t_2) - F(t_1)\\), giving a net change, which we will write as \\({\\cal F}(t_1, t_2) = F(t_2) - F(t_1)\\).\n\nAs a matter of notation, the process of going from \\(f(t)\\) to the net change is written as one statement. \\[{\\cal F}(t_1, t_2) = F(t_2) - F(t_1) = \\int_{t_1}^{t_2} f(t) dt\\]\nThe punctuation \\[\\int_{t_1}^{t_2} \\_\\_\\_\\_ dt\\] captures in one construction both the anti-differentiation step (\\(\\int\\_\\_dt\\)) and the evaluation of the anti-derivative at the two bound \\(t_2\\) and \\(t_1\\).\nSeveral names are used to describe the overall process. It is important to become familiar with these.\n\n\\(\\int_a^b f(t) dt\\) is called a definite integral of \\(f(t)\\).\n\\(a\\) and \\(b\\) are called, respectively, the lower bound of integration and the upper bound of integration, although given the way we draw graphs it might be better to call them the “left” and “right” bounds, rather than lower and upper.\nThe pair \\(a, b\\) is called the bounds of integration.\n\nAs always, it pays to know what kind of thing is \\({\\cal F}(t_1, t_2)\\). Assuming that \\(t_1\\) and \\(t_2\\) are fixed quantities, say \\(t_1 = 2\\) seconds and \\(t_2 = 5\\) seconds, then \\({\\cal F}(t_1, t_2)\\) is itself a quantity. The dimension of that quantity is [\\(F(t)\\)] which in turn is [\\(f(t)\\)]\\(\\cdot\\)[\\(t\\)]. So if \\(f(t)\\) is fuel consumption in liters per second, then \\(F(t)\\) will have units of liters, and \\({\\cal F}(t_1, t_2)\\) will also have units of liters.\nRemember also an important distinction:\n\n\\(F(t) = \\int f(t) dt\\) is a function whose output is a quantity.\n\\(F(t_2) - F(t_1) = \\int_{t_1}^{t_2} f(t) dt\\) is a quantity, not a function.\n\nOf course, \\(f(t)\\) is a function whose output is a quantity. In general, the two functions \\(F(t)\\) and \\(f(t)\\) produce outputs that are different kinds of quantities. For instance, the output of \\(F(t)\\) is liters of fuel while the output of \\(f(t)\\) is liters per second: fuel consumption. Similarly, the output of \\(S(t)\\) is dollars, while the output of \\(s(t)\\) is dollars per day.\nThe use of the term definite integral suggests that there might be something called an indefinite integral, and indeed there is. “Indefinite integral” is just a synonym for “anti-derivative.” In this book we favor the use of anti-derivative because it is too easy to leave off the “indefinite” and confuse an indefinite integral with a definite integral. Also, “anti-derivative” makes it completely clear what is the relationship to “derivative.”\nSince 1700, it is common for calculus courses to be organized into two divisions:\n\nDifferential calculus, which is the study of derivatives and their uses.\nIntegral calculus, which is the study of anti-derivatives and their uses.\n\nMathematical notation having been developed for experts rather than for students, very small typographical changes are often used to signal very large changes in meaning. When it comes to anti-differentiation, there are two poles of fixed meaning and then small changes which modify the meaning. The poles are:\n\nAnti-derivative: \\(\\int f(t) dt\\), which is a function whose output is a quantity.\nDefinite integral \\(\\int_a^b f(t) dt\\), which is a quantity, plain and simple.\n\nBut you will also see some intermediate forms:\n\n\\(\\int_a^t f(t) dt\\), which is a function with input \\(t\\).\n\\(\\int_a^x f(t) dt\\), which is the same function as in (a) but with the input name \\(x\\) being used.\n\\(\\int_t^b f(t) dt\\), which is a function with input \\(t\\).\nLess commonly, \\(\\int_x^t f(t) dt\\) which is a function with two inputs, \\(x\\) and \\(t\\). The same is true of \\(\\int_x^y f(t) dt\\) and similar variations.",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Integration</span>"
    ]
  },
  {
    "objectID": "Accumulation/35-integration.html#initial-value-of-the-quantity",
    "href": "Accumulation/35-integration.html#initial-value-of-the-quantity",
    "title": "36  Integration",
    "section": "36.3 Initial value of the quantity",
    "text": "36.3 Initial value of the quantity\nRecall that we are interested in a real quantity \\({\\mathbf F}(t)\\), but we only know \\(f(t)\\) and from that can calculate an anti-derivative \\(F(t)\\). The relationship between them is \\[{\\mathbf F}(t) = F(t) + C\\] where \\(C\\) is some fixed quantity that we cannot determine directly from \\(f(t)\\).\nStill, even if we cannot determine \\(C\\), there is one way we can use \\(F(t)\\) to make definite statements about \\({\\mathbf F}(t)\\). Consider the net change from \\(t_1\\) to \\(t_2\\) in the real quantity \\({\\mathbf F}\\). This is \\[{\\mathbf F}(t_2) - {\\mathbf F}(t_1) =  \\left[F(t_2) + C\\right] - \\left[F(t_1) + C\\right] = F(t_2) - F(t_1)\\] In other words, just knowing \\(F(t)\\), we can make completely accurate statements about net changes in the value of \\({\\mathbf F}(t)\\).\nLet’s develop our understanding of this unknown constant \\(C\\), which is called the constant of integration. To do so, watch the movie in Figure 36.1 showing the process of constructing the anti-derivative \\[F(t) = \\int_2^t f(t) dt\\ .\\]\n\n\n\n\n\n\nFigure 36.1: Constructing the anti-derivative \\(F(t)\\) by reading the slope from \\(f(t)\\) and using that slope to extend the picture of \\(F()\\)\n\n\n\n\nFocus first on the top graph. The function we are integrating, \\(f(t)\\), is known before we carry out the integration, so it is shown in the top graph.\n\n\\(f(t)\\) is the rate of increase in \\(F(t)\\) (or \\({\\mathbf F}(t)\\) for that matter). From the graph, you can read using the vertical axis the value of \\(f(t)\\) for any input \\(t\\). But since \\(f(t)\\) is a rate of increase, we can also depict \\(f(t)\\) as a slope. That slope is being drawn as a magenta arrow. Notice that when \\(f(t)\\) is positive, the arrow slopes upward and when \\(f(t)\\) is negative, the arrow slopes downward. The steepness of the arrow is the value of \\(f(t)\\), so for inputs where the value of \\(f(t)\\) is far from zero the arrow is steeper than for values of \\(f(t)\\) that are near zero.\n\nNow look at both graphs, but concentrate just on the arrows in the two graphs. They are always the same: carbon copies of one another.\nFinally the bottom graph. We are starting the integral at \\(t_1=2\\). Since nothing has yet been accumulated, the value \\(F(t_1 = 2) = 0\\). From (1) and (2), you know the arrow shows the slope of \\(F(t)\\). So as \\(F(t&gt;2)\\) is being constructed the arrow guides the way. When the slope arrow is positive, \\(F(t)\\) is growing. When the slope arrow is negative, \\(F(t)\\) is going down.\n\nIn tallying up the accumulation of \\(f(t)\\), we started at time \\(t=2\\) and with \\(F(t=2) = 0\\). This makes sense, since nothing can be accumulated over the mere instant of time from \\(t=2\\) to \\(t=2\\). On the other hand, it was our choice to start at \\(t=2\\). We might have started at another value of \\(t\\) such as \\(t=0\\) or \\(t=-5\\) or \\(t=-\\infty\\). If so, then the accumulation of \\(f(t)\\) up to \\(t=2\\) would likely have been something other than zero.\nBut what if we knew an actual value for \\({\\mathbf F}(2)\\). This is often the case. For instance, before taking a trip you might have filled up the fuel tank. The accumulation of fuel consumption only tells you how much fuel has been used since the start of the trip. But if you know the starting amount of fuel, by adding that to the accumulation you will know instant by instant how much fuel is in the tank. In other words, \\[{\\mathbf F}(t) = {\\mathbf F}(2) + \\int_2^t f(t) dt\\ .\\] This is why, when we write an anti-derivative, we should always include mention of some constant \\(C\\)—the so-called constant of integration—to remind us that there is a difference between the \\(F(t)\\) we get from anti-differentiation and the \\({\\mathbf F}(t)\\) of the function we are trying to reconstruct. That is, \\[{\\mathbf F}(t) = F(t) + C = \\int f(t) dt + C\\ .\\] We only need to know \\({\\mathbf F}(t)\\) at one point in time, say \\(t=0\\), to be able to figure out the value of \\(C\\): \\[C = {\\mathbf F}(0) - F(0)\\ .\\]\nAnother way to state the relationship between the anti-derivative and \\({\\mathbf F}(t)\\) is by using the anti-derivative to accumulate \\(f(t)\\) from some starting point \\(t_0\\) to time \\(t\\). That is: \\[{\\mathbf F}(t) \\ =\\  {\\mathbf F}(t_0) + \\int_{t_0}^t f(t)\\, dt\\  = \\\n{\\mathbf F}(t_0) + \\left({\\large\\strut}F(t) - F(t_0)\\right)\\]\n\n\n\n\n\n\nCalculus history—Galileo in Pisa\n\n\n\nAn oft-told legend has Galileo at the top of the Tower of Pisa around 1590. The legend illustrates Galileo’s finding that a light object (e.g. a marble) and a heavy object (e.g. a ball) will fall at the same speed. Galileo published his mathematical findings in 1638 in Discorsi e Dimostrazioni Matematiche, intorno a due nuove scienze. (English: Discourses and Mathematical Demonstrations Relating to Two New Sciences)\nIn 1687, Newton published his world-changingPhilosophiae Naturalis Principia Mathematica. (English: Mathematical Principles of Natural Philosophy)\nLet’s imagine the ghost of Galileo returned to Pisa in 1690 after reading Newton’s Principia Mathematica. In this new legend, Galileo holds a ball still in his hand, releases it, and figures out the position of the ball as a function of time.\nAlthough Newton famously demonstrated that gravitational attraction is a function of the distance between to objects, he also knew that at a fixed distance—the surface of the Earth—gravitational acceleration was constant. So Galileo was vindicated by Newton. But, although gravitational acceleration is constant from top to bottom of the Tower of Pisa, Galileo’s ball was part of a more complex system: a hand holding the ball still until release. Acceleration of the ball versus time is therefore approximately a Heaviside function:\n\\(\\text{accel}(t) \\equiv \\left\\{\\begin{array}{rl}0 & \\text{for}\\ t \\leq 3\\\\\n{-9.8}  & \\text{otherwise}\\end{array}\\right.\\)\n\naccel &lt;- makeFun(ifelse(t &lt;= 3, 0, -9.8) ~ t)\n\nAcceleration is the derivative of velocity. We can construct a function \\(V(t)\\) as the anti-derivative of acceleration, but the real-world velocity function will be \\[{\\mathbf V}(t) = {\\mathbf V}(0) + \\int_0^t \\text{accel}(t) dt\\]\n\nV_from_antiD &lt;- antiD(accel(t) ~ t)\nV &lt;- makeFun(V0 + (V_from_antiD(t) - V_from_antiD(0)) ~ t, V0 = 0)\n\nIn the computer expression, the parameter V0 stands for \\({\\mathbf V}(0)\\). We’ve set it equal to zero since, at time \\(t=0\\), Galileo was holding the ball still.\nVelocity is the derivative of position, but the real-world velocity function will be the accumulation of velocity from some starting time to time \\(t\\), plus the position at that starting time: \\[x(t) \\equiv x(0) + \\int_0^t V(t) dt\\] We can calculate \\(\\int V(t) dt\\) easily enough with antiD(), but the function \\(x(t)\\) involves evaluating that anti-derivative at times 0 and \\(t\\):\n\nx_from_antiD &lt;- antiD(V(t) ~ t)\nx &lt;- makeFun(x0 + (x_from_antiD(t) - x_from_antiD(0)) ~ t, x0 = 53)\n\nWe’ve set the parameter x0 to be 53 meters, the height above the ground of the top balcony on which Galileo was standing for the experiment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 36.2: The acceleration, velocity, and position of the ball as a function of time in Galileo’s Tower of Pisa experiment. The ball is released at time \\(t_0\\).\n\n\n\n\n\nIn the (fictional) account of the 1690 experiment, we had Galileo release the ball at time \\(t=0\\). That is a common device in mathematical derivations, but in a physical sense it is entirely arbitrary. Galileo might have let go of the ball at any other time, say, \\(t=3\\) or \\(t=14:32:05\\).\nA remarkable feature of integrals is that it does not matter what we use as the lower bound of integration, so long as we set the initial value to correspond to that bound.\n\n\n\n\n\n\nWhy \\(\\int f(x) dx\\) instead of \\(\\int f(t) dt\\)?\n\n\n\nFor a while you were writing integrals like this: \\(\\int_a^b f(t) dt\\). Then you replaced \\(b\\) with the input name \\(t\\) to get \\(\\int_a^t f(t) dt\\). But then you switched everything up by writing \\(\\int_a^t f(x) dx\\). Is that the same as \\(\\int_a^t f(t) dt\\)? If so, why do you get to replace the \\(t\\) with \\(x\\) in some places but not in others?\nRecall from Chapter 2 that the names used for inputs to a function definition don’t matter so long as they are used consistently on the left and right sides of \\(\\equiv\\). For instance, all these are the same function:\n\n\\(f(x) \\equiv m x + b\\)\n\\(g(t) \\equiv m t + b\\)\n\\(h(\\text{wind}) \\equiv m \\text{wind} + b\\)\n\nNow think about the integral \\(\\int_a^b f(t) dt\\): \\[\\int_a^b f(t) dt = F(b) - F(a)\\ .\\]\nOn the left-hand side, the input name \\(t\\) is prominent, appearing in two places: \\(f(\\color{magenta}{t}) d\\color{magenta}{t}\\). But \\(t\\) is nowhere on the right-hand side. We could have equally well written this as \\(\\int_a^b f(x) dx\\) or \\(\\int_a^b f(\\text{wind}) d\\text{wind}\\). The name we use for the input to \\(f()\\) does not matter so long as it is consistent with the name used in the \\(d\\_\\_\\) part of the notation. Often, the name placed in the blanks in \\(\\int f(\\_\\_) d\\_\\_\\) is called a dummy variable.\nWriting \\(\\int_a^t f(t) dt\\) is perfectly reasonable, but many authors dislike the multiple appearance of \\(t\\). So they write something like \\(\\int_a^t f(x) dx\\) instead.",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Integration</span>"
    ]
  },
  {
    "objectID": "Accumulation/35-integration.html#integrals-from-bottom-to-top",
    "href": "Accumulation/35-integration.html#integrals-from-bottom-to-top",
    "title": "36  Integration",
    "section": "36.4 Integrals from bottom to top",
    "text": "36.4 Integrals from bottom to top\nThe bounds of integration appear in different arrangements. None of these are difficult to derive from the basic forms:\n\nThe relationship between an integral and its corresponding anti-derivative function: \\[\\int_a^b f(x) dx = F(b) - F(a)\\] This relationship has a fancy-sounding name: the second fundamental theorem of calculus.\nThe accumulation from an initial-value \\[{\\mathbf F}(b)\\  =\\  {\\mathbf F}(a) + \\int_a^b f(x) dx\\  = \\ {\\mathbf F}(a) + F(b) - F(a)\\] For many modeling situations, \\(a\\) and \\(b\\) are fixed quantities, so \\(F(a)\\) and \\(F(b)\\) are also quantities; the output of the anti-derivative function at inputs \\(a\\) and \\(b\\). But either the lower-bound or the upper-bound can be input names, as in \\[\\int_0^t f(x) dx = F(t) - F(0)\\]\n\nNote that \\(F(t)\\) is not a quantity but a function of \\(t\\).\nOn occasion, you will see forms like \\(\\int_t^0 f(x)dx\\). You can think of this in either of two ways:\n\nThe accumulation from a time \\(t\\) less than 0 up until 0.\nThe reverse accumulation from 0 until time \\(t\\).\n\nReverse accumulation can be a tricky concept because it violates everyday intuition. Suppose you were harvesting a row of ripe strawberries. You start at the beginning of the row—position zero. Then you move down the row, picking strawberries and placing them in your basket. When you have reached position \\(B\\) your basket holds the accumulation \\(\\int_0^B s(x)\\, dx\\), where \\(s(x)\\) is the lineal density of strawberries—units: berries per meter of row.\nBut suppose you go the other way, starting with an empty basket at position \\(B\\) and working your way back to position 0. Common sense says your basket will fill to the same amount as in the forward direction, and indeed this is the case. But integrals work differently. The integral \\(\\int_B^0 s(x) dx\\) will be the negative of \\(\\int_0^B s(x) dx\\). You can see this from the relationship between the integral and the anti-derivative: \\[\\int_B^0 s(x) dx \\ = \\ S(0) - S(B) \\ =\\ -\\left[{\\large\\strut}S(B) - S(0)\\right]\\ = \\ -\\int_0^B s(x) dx\\]\nThis is not to say that there is such a thing as a negative strawberry. Rather, it means that harvesting strawberries is similar to an integral in some ways (accumulation) but not in other ways. In farming, harvesting from 0 to \\(B\\) is much the same as harvesting from \\(B\\) to 0, but integrals don’t work this way.\nAnother property of integrals is that the interval between bounds of integration can be broken into pieces. For instance:\n\\[\\int_a^c f(x) dx \\ = \\ \\int_a^b f(x) dx + \\int_b^c f(x) dx\\] You can confirm this by noting that \\[\\int_a^b f(x) dx + \\int_b^c f(x) dx \\ = \\ \\left[{\\large\\strut}F(b) - F(a)\\right] + \\left[{\\large\\strut}F(c) - F(b)\\right] = F(c) - F(a) \\ = \\ \\int_a^c f(x) dx\\ .\\]\nFinally, consider this function of \\(t\\): \\[\\partial_t \\int_a^t f(x) dx\\ .\\] First, how do we know it is a function of \\(t\\)? \\(\\int_a^t f(x) dx\\) is a definite integral and has the value \\[\\int_a^t f(x) dx = F(t) - F(a)\\  .\\] Following our convention, \\(a\\) is a parameter and stands for a specific numerical value, so \\(F(a)\\) is the output of \\(F()\\) for a specific input. But according to convention \\(t\\) is the name of an input. So \\(F(t)\\) is a function whose output depends on \\(t\\). Differentiating the function \\(F(t)\\), as with every other function, produces a new function.\nSecond, there is a shortcut for calculating \\(\\partial_t \\int_a^t f(x) dx\\): \\[\\partial_t \\int_a^t f(x) dx\\ =\\ \\partial_t \\left[{\\large\\strut}F(t) - F(a)\\right]\\ .\\] Since \\(F(a)\\) is a quantity and not a function, \\(\\partial_t F(a) = 0\\). That simplies things. Even better, we know that the derivative of \\(F(t)\\) is simply \\(f(t)\\): that is just the nature of the derivative/anti-derivative relationship between \\(f(t)\\) and \\(F(t)\\). Put together, we have: \\[\\partial_t \\int_a^t f(x) dx\\ =\\ f(t)\\ .\\]\nThis complicated-looking identity has a fancy name: the first fundamental theorem of calculus.\n\n\n\n\n\n\nMath out of the World: Backtracking the stars\n\n\n\nIn the 1920s, astronomers and cosmologists questioned the idea that the large-scale universe is static and unchanging. This traditional belief was undermined both by theory (e.g. General Relativity) and observations. The most famous of these were collected and published by Edwin Hubble, starting in 1929 and continuing over the next decade as improved techniques and larger telescopes became available. In recent years, with the availability of the space telescope named in honor of Hubble data has expanded in availability and quality. Figure 36.3 shows a version of Hubble’s 1929 graph based on contemporary data.\n\n\n\n\n\n\nFigure 36.3: The relationship between velocity and distance of stars, using contemporary data in the same format at Edwin Hubble’s 1929 publication.\n\n\n\nEach dot in Figure 36.3 is an exploding star called a supernova. The graph shows the relationship between the distance of the star from our galaxy and the outward velocity of that star. The velocities are large, \\(3 \\times 10^4 = 30,000\\) km/s is about one-tenth the speed of light. Similarly, the distances are big; 600 Mpc is the same as 2 billion light years or \\(1.8 \\times 10^{22} \\text{km}\\). The slope of the line in Figure 36.3 is \\(\\frac{3.75 \\times 10^4\\, \\text{km/s}}{1.8 \\times 10^{22}\\, \\text{km}} = 2.1 \\times 10^{-18}\\, \\text{s}^{-1}\\). For ease of reading, we will call this slope \\(\\alpha\\) and therefore the velocity of a start distance \\(D\\) from Earth is \\[v(D) \\equiv \\alpha D\\ .\\]\nEarlier in the history of the universe each star was a different distance from Earth. We will call this function \\(D(t)\\), distance as a function of time in the universe.\nThe distance travelled by each star from time \\(t\\) (billions of years ago) to the present is \\[\\int_t^\\text{now} v(t) dt  = D_\\text{now} - D(t)\\] which can be re-arranged to give \\[D(t) = D_\\text{now} - \\int_t^\\text{now} v(t) dt .\\] Assuming that \\(v(t)\\) for each star has remained constant at \\(\\alpha D_\\text{now}\\), the distance travelled by each star since time \\(t\\) depends on its current distance like this: \\[\\int_t^\\text{now} v(t) dt = \\int_t^\\text{now} \\left[ \\alpha D_\\text{now}\\right]\\, dt = \\alpha D_\\text{now}\\left[\\text{now} - t\\right]\\] Thus, the position of each star at time \\(t\\) is \\[D(t) = D_\\text{now} - \\alpha D_\\text{now}\\left[\\text{now} - t\\right] = D(t)\\] or, \\[D(t) = D_\\text{now}\\left({\\large\\strut} 1-\\alpha \\left[\\text{now} - t\\right]\\right)\\]\nAccording to this model, there was a common time \\(t_0\\) when when all the stars were at the same place: \\(D(t_0) = 0\\). This happened when \\[\\text{now} - t_0 = \\frac{1}{\\alpha} = \\frac{1}{2.1 \\times 10^{-18}\\, \\text{s}^{-1}} = 4.8 \\times 10^{17} \\text{s}\\ .\\] It seems fair to call such a time, when all the stars where at the same place at the same time, as the origin of the universe. If so, \\(\\text{now} - t_0\\) corresponds to the age of the universe and our estimate of that age is \\(4.8\\times 10^{17}\\text{s}\\). Conventionally, this age is reported in years. To get that, we multiply by the flavor of one that turns seconds into years: \\[\\frac{60\\, \\text{seconds}}{1\\, \\text{minute}} \\cdot \\frac{60\\, \\text{minutes}}{1\\, \\text{hour}} \\cdot \\frac{24\\, \\text{hours}}{1\\, \\text{day}} \\cdot \\frac{365\\, \\text{days}}{1\\, \\text{year}} = 31,500,000 \\frac{\\text{s}}{\\text{year}}\\] The grand (but hypothetical) meeting of the stars therefore occurred \\(4.8 \\times 10^{17} \\text{s} / 3.15 \\times 10^{7} \\text{s/year} = 15,000,000,000\\) years ago. Pretty crowded to have all the mass in the universe in one place at the same time. No wonder they call it the Big Bang!",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Integration</span>"
    ]
  },
  {
    "objectID": "Accumulation/35-integration.html#footnotes",
    "href": "Accumulation/35-integration.html#footnotes",
    "title": "36  Integration",
    "section": "",
    "text": "Momentum is velocity times mass. Newton’s Second Law of Motion stipulates that force equals the rate of change of momentum.↩︎",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Integration</span>"
    ]
  },
  {
    "objectID": "Accumulation/36-functions.html",
    "href": "Accumulation/36-functions.html",
    "title": "37  Functions as vectors",
    "section": "",
    "text": "37.1 Dot product for functions\nNote: This chapter revisits ideas of linear combinations and orthogonality in the context of functions* rather than numerical vectors.\nStarting with Chapter 29, we have been working with the dot product, an operation that combines two vectors to produce a scalar. \\[\\vec{b}\\bullet\\vec{a} \\equiv\n\\left[\\begin{array}{c}b_1\\\\b_2\\\\\\vdots\\\\b_n\\end{array}\\right] \\bullet\n\\left[\\begin{array}{c}a_1\\\\a_2\\\\\\vdots\\\\a_n\\end{array}\\right] \\equiv b_1 a_1 + b_2 a_2 + \\cdots b_n a_n\\] The dot product enables us to use arithmetic to calculate geometric properties of vectors, even in high dimensional spaces that are out of reach of a ruler or protractor. For instance\nWe used such operations to solve the target problem: finding the best approximation of a vector \\(\\vec{b}\\) as a linear combination of a set of vectors in a matrix \\(\\mathit{A}\\).\nAs early as Block 1, we constructed functions as a linear combination of other functions, for example: \\[g(t) \\equiv A + B \\sin\\left(\\frac{2 \\pi}{P} t\\right)\\] where \\(A\\) is the scalar multiplier for the function \\(\\text{one}(t) \\equiv 1\\) and \\(B\\) the scalar multiplier for the sinusoid of period \\(P\\).\nWe will revisit the idea of linear combinations of functions using our new tools of length, included angle, and projection. To do this, we need to have a definition of the dot product suitable for application to functions.\nGiven two functions, \\(f(t)\\) and \\(g(t)\\) defined over some domain \\(D\\), we will compute the dot product of the functions as a sum of the product of the two functions, that is: \\[f(t) \\bullet g(t) \\equiv \\int_{D} f(t)\\,g(t)\\,dt\\ .\\]\nThe left panel of Figure 37.1 shows the functions \\(f(t) \\equiv t^2\\) and \\(\\color{magenta}{\\widehat{f(t)} \\equiv 1/3}\\) on the domain. The center panel shows the residual function, that is \\(f(t) - \\widehat{f(t)}\\). The right panel gives the square of the length of the residual function, which is \\(\\int_{-1}^1 \\left[f(t) - \\widehat{f(t)}\\right]^{1/2}\\, dt\\) as indicated by the area shaded in blue.\nFigure 37.1: Projecting \\(f(t) \\equiv t^2\\) onto \\(g(t) \\equiv \\text{one}(t)\\).",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Functions as vectors</span>"
    ]
  },
  {
    "objectID": "Accumulation/36-functions.html#dot-product-for-functions",
    "href": "Accumulation/36-functions.html#dot-product-for-functions",
    "title": "37  Functions as vectors",
    "section": "",
    "text": "Try it! 37.1\n\n\n\n\n\n\n\n\n\nTry it! 37.1 Function length\n\n\n\nSuppose that our two functions are \\(\\text{one}(t) \\equiv 1\\) and \\(\\text{identity}(t) \\equiv t\\) on the domain \\(0 \\leq t \\leq 1\\). Find the length of each function and the included angle between them.\n\nLength: \\(\\|\\text{one}(t)\\| = \\left[\\int_0^1 1 \\cdot 1\\,dt\\right]^{1/2} = \\left[\\ \\strut t\\left.{\\large\\strut}\\right|_0^1\\ \\right]^{1/2} = 1\\)\nLength: \\(\\|\\text{identity}(t)\\| = \\left[\\int_0^1 t \\cdot t\\,dt\\right]^{1/2} = \\left[\\ \\strut \\frac{1}{2}t^2\\left.{\\large\\strut}\\right|_0^1\\ \\right]^{1/2} = \\frac{1}{\\sqrt{2}}\\)\nIncluded angle: \\[\\cos(\\theta) = \\frac{\\text{one}(t) \\bullet \\text{identity}(t)}{\\|\\strut\\text{one}(t)\\| \\, \\|\\text{identity}(t)\\|}  =\n\\sqrt{2}\\ \\int_0^1 t\\, dt = \\sqrt{\\strut 2} \\left.{\\Large\\strut}\\frac{1}{2} t^2\\right|_0^1 = \\sqrt{\\frac{1}{2}}\\] Since \\(\\cos(\\theta) = \\sqrt{1/2}\\), the angle \\(\\theta\\) is 45 degrees.\n\n\n\n\n\n\n\n\n\n\n\nTry it! 37.2\n\n\n\n\n\n\n\n\n\nTry it! 37.2 Project a function onto another\n\n\n\nProject \\(f(t) \\equiv t^2\\) onto \\(g(t) = \\text{one}(t)\\) over the domain \\(-1 \\leq t \\leq 1\\).\nThe projection of \\(f(t)\\) onto \\(g(t)\\) will be \\[\\widehat{f(t)} = \\frac{f(t) \\bullet g(t)}{g(t) \\bullet g(t)}\\ g(t)\\]\n\n\\(f(t) \\bullet g(t) \\equiv \\int_{-1}^{1} t^2 dt = \\frac{1}{3} \\left.{\\Large \\strut}t^3\\right|_{-1}^{1} = \\frac{2}{3}\\)\n\\(g(t) \\bullet g(t) \\equiv \\int_{-1}^1 \\ dt = 2\\)\n\nThus, \\[\\widehat{f(t)} = \\frac{1}{3} \\text{one(t)} = \\frac{1}{3}\\ .\\]\n\n\n\n\n\nApplication area 37.1 — Seeing sounds as sinusoids.\n\n\n\n\n\n\n\nApplication area 37.1 Sinusoids and sounds\n\n\n\nThe table links to audio files recorded by a human speaker voicing various vowels. Play the sounds to convince yourself that they really are the vowels listed. (It may help to use the controls to slow down the playback.)\nVowel | Player\n------|-------\n\"o\" as in \"stone\" | &lt;audio controls&gt;&lt;source src = \"https://linguistics.ucla.edu/people/hayes/103/Charts/VChart/o.wav\" type = \"audio/wav\"&gt;&lt;/audio&gt;\n\"e\" as in \"eel\" | &lt;audio controls&gt;&lt;source src = \"https://linguistics.ucla.edu/people/hayes/103/Charts/VChart/y.wav\" type = \"audio/wav\"&gt;&lt;/audio&gt;\nAs you may know, the physical stimuli involved in sound are rapid oscillations in air pressure. Our standard model for oscillations is the sinusoid function, which is parameterized by its period and its amplitude. The period of a sound oscillation is short: between 0.3 and 10 milliseconds. The amplitude is small. To get a sense for how small, consider the change in air pressure when you take an elevator up 10 stories in a building. The pressure amplitude of sound at a conversational level of loudness corresponds to taking that elevator upward by 1 to 10 mm.\nThe shapes of the “e” (as in “eel”) and “o” (as in “stone”) sound waves—in short, the waveforms—are drawn in Figure 37.2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 37.2: The waveforms of two vowel sounds. Only about five hundredths of a second is shown.\n\n\n\nThe function resembles none of our small set of pattern-book functions. It is more complicated, more detailed, more irregular than any of the basic modeling functions featured in this book.\nFor many tasks it is helpful to have a modeling approach that is well suited to such detailed and irregular functions. For example, we might want to identify the speaker from a recording, or to play the recording slower or faster without changing the essence of the sound, or to tweak the function to have additional properties such as being exactly on tune while maintaining its individuality as a sound.\nA remarkable aspect of the waveforms in Figure 37.2 is their periodicity. The 0.05 sec graphics domain shown includes roughly seven repetitions of a basic waveform. That is, each cycle lasts about \\(\\frac{0.05 \\text{s}}{7} \\approx 7 \\text{ms}\\). what distinguishes the “e” waveform from the “o” waveform is the shape of the waveform that is being repeated. The individual cycle of the “o” has three peaks of diminishing amplitude. The “e” cycle has two main peaks, high then low. It also has a very fast wiggle superimposed on the two peaks.\nAn important strategy for modeling such complicated oscillations is to decompose (synonym: analyze) them into a linear combination of simpler parts.",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Functions as vectors</span>"
    ]
  },
  {
    "objectID": "Accumulation/36-functions.html#sinusoids-as-vectors",
    "href": "Accumulation/36-functions.html#sinusoids-as-vectors",
    "title": "37  Functions as vectors",
    "section": "37.2 Sinusoids as vectors",
    "text": "37.2 Sinusoids as vectors\nThe sinusoid is our fundamental model of periodic phenomena. To get started with using sinusoids as vectors, we will start with a simple setting: a single sinusoid of a specified frequency.\nFigure 37.3 shows three sinusoids all with the same frequency, but shifted somewhat in time:\n\n\n\n\n\n\n\n\nFigure 37.3: Three sinusoids with a frequency of \\(\\omega=3\\) cycles per second.\n\n\n\n\n\nSince we have a dot product for functions, we can treat each of the three sinusoids as a vector. For instance, consider the length of waveforms A and B and the included angle between them.\n\n\n## vector lengths \nlengthA &lt;- Integrate(waveA(t) * waveA(t) ~ t, bounds(t=0:1)) |&gt; sqrt() \n## Loading required package: cubature\nlengthA\n## [1] 0.7071068\nlengthB &lt;- Integrate(waveB(t) * waveB(t) ~ t, bounds(t=0:1)) |&gt; sqrt()\nlengthB\n## [1] 0.7071068\nlengthC &lt;- Integrate(waveC(t) * waveC(t) ~ t, bounds(t=0:1)) |&gt; sqrt()\nlengthC\n## [1] 0.7071068\n## dot products\ndotAB   &lt;- Integrate(waveA(t) * waveB(t) ~ t, bounds(t=0:1)) \ndotAB\n## [1] -3.984443e-18\ndotAC   &lt;- Integrate(waveA(t) * waveC(t) ~ t, bounds(t=0:1))\ndotAC\n## [1] -0.1545085\ndotBC   &lt;- Integrate(waveB(t) * waveC(t) ~ t, bounds(t=0:1))\ndotBC\n## [1] -0.4755283\n\n\nThe cosine of the included angle \\(\\theta\\) between functions A and B is calculated using the dot product formula: \\[\\cos(\\theta) = \\frac{A\\bullet B}{\\|A\\|\\, \\|B\\|}\\] or, computationally\n\ndotAB / (lengthA * lengthB)\n## [1] -7.968886e-18\n\nSince \\(\\cos(\\theta) = 0\\), wave A and B are orthogonal. Admittedly, there is no right angle to be perceived from the graph, but the mathematics of angles gives this result.\nThe graphical presentation of orthogonality between waveforms A and B is easier to appreciate if we plot out the dot product itself: the integral of waveform A times waveform B. Figure 37.4 shows this integral using colors, blue for positive and orange for negative. The integral is zero, since the positive (blue) areas exactly equal the negative (orange) areas.\n\n\n\n\n\n\n\n\nFigure 37.4: The dot product between waveforms A and B, graphically.\n\n\n\n\n\nIn contrast, waveform A is not orthogonal to waveform C, and similarly for waveform B. Figure 37.5 shows this graphically: the positive and negative areas in the two integrals do not cancel out to zero.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 37.5: The dot products between waveforms A and C (top panel) and between B and C (bottom panel).\n\n\n\nWe can project waveform C onto the 2-dimensional subspace spanned by A and B. Since waveforms A and B are orthogonal, This can be done simply by projecting C onto each of A and B one at a time. Here’s a calculation of the scalar multipliers for A and for B and the model vector (that is, the component of C in the A-B subspace):\n\nA_coef &lt;- dotAC / lengthA^2\nB_coef &lt;- dotBC / lengthB^2\nmod_vec &lt;- makeFun(A_coef*waveA(t) + B_coef*waveB(t) ~ t)\n# length of mod_vec\nIntegrate(mod_vec(t)*mod_vec(t) ~ t, bounds(t=0:1)) |&gt; sqrt()\n## [1] 0.7071068\n\nYou can see that the length of the model vector is the same as the length of the vector being projected. This means that waveform C lies exactly in the subspace spanned by waveforms A and B.\nA time-shifted sinusoid of frequency \\(\\omega\\) can always be written as a linear combination of \\(\\sin(2\\pi\\omega t)\\) and \\(\\cos(2\\pi\\omega t)\\). The coefficients of the linear combination tell us both the amplitude of the time-shifted sinusoid and the time shift.\n\n\n\n\n\n\n\n\nTry it! 37.3\n\n\n\n\n\n\n\n\n\nTry it! 37.3 Adding sinusoids\n\n\n\nConsider the function \\(g(t) \\equiv 17.3 \\sin(2*pi*5*(t-0.02)\\) on the domain \\(0 \\leq t \\leq 1\\) seconds. The amplitude is 17.3. The time shift is 0.02 seconds. Let’s confirm this using the coefficients on the linear combination of sine and cosine of the same frequency.\n\ng &lt;- makeFun(17.3 * sin(2*pi*5*(t-0.02)) ~ t)\nsin5 &lt;- makeFun(sin(2*pi*5*t) ~ t)\ncos5 &lt;- makeFun(cos(2*pi*5*t) ~ t)\nA_coef &lt;- Integrate(g(t) * sin5(t) ~ t, bounds(t=0:1)) /\n  Integrate(sin5(t) * sin5(t) ~ t, bounds(t=0:1))\nA_coef\n## [1] 13.99599\nB_coef &lt;- Integrate(g(t)*cos5(t) ~ t, bounds(t=0:1)) /\n  Integrate(cos5(t) * cos5(t) ~ t, bounds(t=0:1))\nB_coef\n## [1] -10.16868\n\nThe amplitude of \\(g(t)\\) is the Pythagorean sum of the two coefficients:\n\nsqrt(A_coef^2 + B_coef^2)\n## [1] 17.3\n\nThe time delay involves the ratio of the two coefficients:\n\natan2(B_coef, A_coef) / (2*pi*5) \n## [1] -0.02\n\nFor our purposes here, we will need only the Pythagorean sum and will ignore the time delay.\n\n\nFigure 37.6 (top) shows the waveform of a note played on a cello. The note lasts about 1 second. The bottom panel zooms in on the waveform, showing 82 ms (that is, 0.082 s).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 37.6: Waveform recorded from a cello.\n\n\n\nThe whole note starts with a sharp “attack,” followed by a long period called a “sustain,” and ending with a “decay.” Within the sustain and decay, the waveform is remarkably repetitive, seen best in the bottom panel of the figure.\nIf you count carefully in the bottom panel, you will see that the waveform completes 9 cycles in the 0.082 s graphical domain. This means that the period is 0.082 / 9 = 0.0091 s. The frequency \\(\\omega\\) is the reciprocal of this: 1/0.0091 = 109.76 Hz. That is, the cello is vibrating about 110 times per second.\nIn modeling the cello waveform as a linear combination of sinusoids, the frequencies we use ought to respect the period of the cello vibration. Figure 37.7 shows the original waveform as well as the projection of the waveform onto a sinusoid with a frequency of 109.76 Hz. The figure also shows the residual from the projection, which is simply the original waveform minus the projected version.\n\n\n\n\n\n\n\n\nFigure 37.7: Top: The cello waveform and its projection onto a sinusoid with frequency \\(\\omega = 109.76\\) Hz. Bottom: The residual from the projection.\n\n\n\n\n\nThe sinusoid with \\(\\omega = 109.76\\) is not the only one that will repeat every 0.0091 s. So will a sinusoid with frequency \\(2\\omega = 219.52\\), one with frequency \\(3\\omega = 329.28\\) and so on. These multiples of \\(\\omega\\) are called the harmonics of that frequency. In Figure 37.8 (top) the cello waveform is projected onto \\(\\omega\\) and its first harmonic \\(2\\omega\\). In the middle panel, the projection is made onto \\(\\omega\\) and its first three harmonics. In the bottom panel, the projection is onto \\(\\omega\\) and its first eight harmonics.\n\n\n\n\n\n\n\n\nFigure 37.8\n\n\n\n\n\nAs the number of harmonics increases, the approximation gets better and better.\nUntil now, all the plots of the cello waveform have been made in what’s called the time domain. That is, the horizontal axis of the plots has been time, as seems natural for a function of time.\nThe decomposition into sinusoids offers another way of describing the cello waveform: the frequency domain. In the frequency domain, we report the amplitude and phase of the projection onto each frequency, plotting that versus frequency. Figure 37.9 shows the waveform in the frequency domain.\n\n\n\n\n\n\n\n\nFigure 37.9: The frequency domain description of the cello waveform.\n\n\n\n\n\nFrom the amplitude graph in Figure 37.9, you can see that only a handful of frequencies account for almost all of the signal. Thus, the frequency domain representation is in many ways much more simple and compact than the time domain representation.\nThe frequency domain description is an important tool in many fields. As you will see in Block 6, models of many kinds of systems, from the vibrations of buildings during an earthquake, aircraft wings in response to turbulence, and the bounce of a car moving over a rutted road have a very simple form when stated in the frequency domain. Each sinusoid in the input (earthquake shaking, air turbulence, rutted road) gets translated into the same frequency sinusoid in the output (building movement, wing bending, car bound): just the amplitude and phase of the sinusoid is altered.\nThe construction of the frequency domain description from the waveform is called a Fourier Transform, one of the most important techiques in science.\n\nApplication area 37.2 — Molecules as tuning forks!\n\n\n\n\n\n\n\nApplication area 37.2 Molecular spectroscopy\n\n\n\nAn important tool in chemistry is molecular vibrational spectroscopy in which a sample of the material is illuminated by an infrared beam of light. The frequency of infrared light ranges from about \\(300 \\times 10^7\\) Hz to \\(400 \\times 10^{10}\\) Hz, about 30 million to 40 billion times faster than the cello frequency.\nInfrared light is well suited to trigger vibrations in the various bonds of a molecule. By measuring the light absorbed at each frequency, a frequency domain picture can be drawn of the molecules in the sample. This picture can be compared to a library of known molecules to identify the makeup of the sample.\nThe analogous procedure for stringed musical instruments such as the cello or violin would be to rap on the instrument and record the hum of the vibrations induced. The Fourier transform of these vibrations effectively paint a picture of the tonal qualities of the instrument.\n\n\n\n\n\n\n\n\nCalculus history—From Taylor to Lagrange\n\n\n\nChapter 27 describes a method introduced by Brook Taylor (1685–1731) to construct a polynomial of order-\\(n\\) that approximates any smooth function \\(f(x)\\) close enough to some center \\(x_0\\). The method made use of the ability to differentiate \\(f(x)\\) at \\(x_0\\) and produced the general formula: \\[f(x) \\approx f(x_0) + \\frac{f'(x_0)}{1} \\left[x-x_0\\right] + \\frac{f''(x_0)}{2!} \\left[x-x_0\\right]^2 + \\frac{f'''(x_0)}{3!} \\left[x-x_0\\right]^3 + \\cdots + \\frac{f^{(n)}(x_0)}{n!} \\left[x-x_0\\right]^n\\] where \\(f'(x_0) \\equiv \\partial_x f(x)\\left.{\\Large\\strut}\\right|_{x=x_0}\\) and so on.\nUsing polynomials as approximating functions has been an important theme in mathematics history. Brook Taylor was neither the first nor the last to take on the problem.\nIn 1795, Joseph-Louis Lagrange (1736 – 1813) published another method for constructing an approximating polynomial of order \\(n\\). Whereas the Taylor polynomial builds the polynomial that exactly matches the first \\(n\\) derivatives at the center point \\(x_0\\), the Lagrange polynomial has a different objective: to match exactly the values of the target function \\(f(x)\\) at a set of knots (input values) \\(x_0\\), \\(x_1\\), \\(x_2\\), \\(\\ldots, x_n\\). Figure 37.10 shows the situation with the knots shown as orange dots.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 37.10: The Lagrange polynomial of order \\(n\\) is arranged to pass exactly through \\(n+1\\) points on the graph of a function \\(f(x)\\).\n\n\n\nThe Lagrange polynomial is constructed of a linear combinations of functions, one for each of the knots. In the example of Figure @ref(fig:lagrange-sine), there are 6 knots, hence six functions being combined. For knot 2, for instance, has coordinates \\(\\left(\\strut x_2, f(x_2)\\right)\\) and the corresponding function is:\n\\[p_2(x) = \\frac{(x-x_1)}{(x_2 -x_1)}\\left[\\strut\\cdot\\right]\\frac{(x-x_3)(x-x_4)(x-x_5)(x-x_6)}{(x_2 -x_3)(x_2 -x_4)(x_2 -x_5)(x_2 -x_6)}\\] The gap indicated by \\(\\left[\\strut\\cdot\\right]\\) marks where a term being excluded. For \\(p_2(x)\\) that excluded term is \\(\\frac{(x-x_2)}{(x_2 - x_2)}\\). The various functions \\(p_1(x)\\), \\(p_2(x)\\), \\(p_3(x)\\) and so on each leave out an analogous term.\nThree important facts to notice about these ingenious polynomial functions:\n\nThey all have the same polynomial order. For \\(k\\) knots, the order is \\(k-1\\).\nEvaluated at \\(x_i\\), the value of \\(p_i(x_i) = 1\\). For instance, \\(p_2(x_2) = 1\\).\nEvaluated at \\(x_j\\), where \\(j\\neq i\\), the value of \\(p_j(x_i) = 0\\). For example, \\(p_2(x_3) = 0\\).\n\nThe overall polynomial will be the linear combination \\[p(x) = y_1\\, p_1(x) +\ny_2\\, p_2(x) + \\cdots + y_k\\, p_k(x)\\ .\\] Can you see why?",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Functions as vectors</span>"
    ]
  },
  {
    "objectID": "Accumulation/36-functions.html#sec-time-and-tide",
    "href": "Accumulation/36-functions.html#sec-time-and-tide",
    "title": "37  Functions as vectors",
    "section": "37.3 Time and tide",
    "text": "37.3 Time and tide\nFigure 37.11 shows the tides rising and falling over four days. Isaac Newton (1643-1727) was the first to propose that the Moon, orbiting around the Earth, causes the tides. (After all, Newton was the first to realize that the Moon pulls gravitationally on the Earth.)\nPredicting tides is important to mariners, so considerable effort has been put into building models. In Figure 37.11, we have modelled the signal as a linear combination of four sinusoids, although only two of these account for most of the model. The periods and amplitudes of the sinusoids are given in Table 37.1.\n\n\n\nTable 37.1: Sinusoidal components of the tide model shown in magenta. The names are those used in tide research (source).\n\n\n\n\n\nPeriod (hrs)\nAmplitude (feet)\nName\n\n\n\n\n\n\n12.41 & 0.44 & principal lunar semi-diurnal constituent\n12.66 & 0.23 & larger lunar elliptic semi-diurnal constituent\n12.00 & 0.14 & principal solar semi-diurnal constituent\n23.94 & 0.05 & lunar diurnal constituent\n\n\n\n\nLunar and solar in Table 37.1 refer, of course, to the moon and sun. “Diurnal” means “approximately one day long,” and “semi-diurnal” means approximately half-a-day long. Notice that the periods for the semi-diurnal and diurnal lunar constituents are about half a day and a full day, respectively. (The position of the Moon to a viewer at a fixed place on Earth has a period slightly longer than a 24-hour day.) The solar constituent, however, is exactly 12 hours, because the day-length is defined by the position of the sun.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 37.11: A 100-hour record of tide levels (black). A model ([magenta]{style=“color: magenta;}) consisting of a linear combination of four sinusoids. These data are from Bristol Ferry, Rhode Island, USA, but any other sea-shore location would show much the same thing. Source: NOAA\n\n\n\nOur 4-component tide model has merits and demerits. The times of low and high tides are captured almost perfectly. The model doesn’t reproduce so well the high-tide level, but suggests that the day-to-day fluctuations in tide level are not simply random, for instance, caused entirely by the weather.\nWhat to make of the residual between the tide record and the model? As described in Chapter 16, a model residual can be a source for hypotheses about what is missing in the model. Among the factors not in our simple model are the “solar annual constituent” (with a period of 365.24 days), and a “solar semi-annual constituent.” These arise from the elliptical shape of the Earth’s orbit around the Sun. Another missing component is the “shallow water overtides of principal lunar constituent” with a period of 6.2 hours. There are 37 named constituents of tide levels, most of which participate in a very small way and can only be accurately estimated from years of data, not the short 100-hour record we have used.",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Functions as vectors</span>"
    ]
  },
  {
    "objectID": "Accumulation/36-functions.html#fourier-transform",
    "href": "Accumulation/36-functions.html#fourier-transform",
    "title": "37  Functions as vectors",
    "section": "37.4 Fourier transform",
    "text": "37.4 Fourier transform\nBlock 3 introduced the idea of projecting a vector onto a subspace defined by other vectors. Naturally, when defining the subspace it is necessary to say what those other vectors are.\nIn the context of functions, a set of functions that is incredibly useful in many applications was discovered by Jean-Baptiste Joseph Fourier (1768-1830) in his studies of how heat diffuses in, say, a metal bar. These functions are, simply enough, the sinusoids of different periods.\nA “fourier transform” is a projection of a function onto the subspace spanned by Fourier’s sinusoids. That this idea is 200 years old belies its incredible importance today. Figure 37.12 shows the fourier transform of the tide data from Section 37.3. For each sinusoid—\\(A \\sin\\left(\\strut\\frac{2\\pi}{P}(t - t_0\\right)\\)—the graph plots \\(A\\) agains \\(P\\), that is, closely aligned is the sinusoid of period \\(P\\) to the tide signal.\n\n\n\n\n## Joining with `by = join_by(hour)`\n\n\n\n\n\n\n\n\n\n\nFigure 37.12\n\n\n\nReading Figure 37.12, you can see two exceptionally tall spikes, one at a period of 12 hours and a much taller one at a period a little less than 12.5 hours. It’s not a coincidence that these are the periods identified in Section 37.3 as primary components of the tide time series.\nIt suffices for us here to say that a fourier transform is a tool for identifying the periodic components in any signal. But it is also used for many other important tasks in science and engineering. The field of fourier transforms is rich and complex; too much so for us to cover it here.",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Functions as vectors</span>"
    ]
  },
  {
    "objectID": "Accumulation/37-euler.html",
    "href": "Accumulation/37-euler.html",
    "title": "38  Integrals step-by-step",
    "section": "",
    "text": "38.1 Euler method\nThe setting for anti-differentiation (and its close cousin, integration) is that we have a function \\(F(t)\\) which we do not yet know, but we do have access to some information about it: its slope as a function of time \\(f(t) \\equiv \\partial_t F(t)\\) and, perhaps, its value \\(F(t_0)\\) at some definite input value.\nChapter 35 showed some ways to visualize the construction of an \\(F(t)\\) by accumulating short segments of slope. The idea is that we know \\(f(t)\\) which tells us, at any instant, the slope of \\(F(t)\\). So, in drawing a graph of \\(F(t)\\), we put our pen to paper at some input value \\(t_0\\) and then move forward in \\(t\\), setting the instantaneous slope of our curve according to \\(f(t)\\).\nChapter 36 deals with one of the limitations of finding \\(F(t)\\) by anti-differentiation of \\(f(t)\\); the anti-derivative is not unique. This is because to start drawing \\(F(t)\\) we need pick a \\(t_0\\) and an initial value of \\(F(t_0)\\). If we had picked a a different starting point \\(t_1\\) or a different initial value \\(F(t_1)\\), the new curve would be different than the one drawn through \\((t_0, F(t_0))\\), although it would have the same shape, just shifted up or down according to our choice. We summarize this situation algebraically by writing \\[\\int f(t) dt = F(t) + C\\ ,\\] where \\(C\\) is the constant of integration, that is, the vertical shift of the curve.\nThe non-uniqueness of \\(F(t)\\) does not invalidate its usefulness. In particular, the quantity \\(F(b) - F(a)\\), will be the same regardless of which starting point we used to draw \\(F(t)\\). We have two names for \\(F(b) - F(a)\\)\nThese two things, the net change and the definite integral, are really one and the same, a fact we describe by writing \\[\\int_a^b f(t) dt = F(b) - F(a)\\ .\\]\nIn this chapter, we will introduce a simple numerical method for calculating from \\(f()\\) the net change/definite integral. This will be a matter of trivial but tedious arithmetic: adding up lots of little bits of \\(f(t)\\). Later, Chapter 39 shows how to avoid the tedious arithmetic by use of algebraic, symbolic transformations. This symbolic approach has great advantages, and is the dominant method of anti-differentiation found in college-level science textbooks. However, there are many common \\(f(t)\\) for which the symbolic approach is not possible, whereas the numerical method works for any \\(f(t)\\). Even more important, the numerical technique has a simple natural extension to some commonly encountered accumulation problems that look superficially like they can be solved by anti-differentiation but rarely can be in practice. We will meet one such problem and solve it numerically, but a broad approach to the topic, called dynamics or differential equations, will have to wait until Block 6.\nThe starting point for this method is the definition of the derivative of \\(F(t)\\). Reaching back to Chapter 17,\n\\[\\partial_t F(t) \\equiv \\lim_{h\\rightarrow 0} \\frac{F(t+h) - F(t)}{h}\\] To translate this into a numerical method for computing \\(F(t)\\), let’s write things a little differently.\nWith these changes, we have \\[f(t_0) = \\frac{F(t_0+dt) - F(t_0)}{dt}\\ .\\] The one quantity in this relationship that we do not yet know is \\(F(t_0 + dt)\\). So re-arrange the equation so that we can calculate the unknown \\(F(t_0 + dt)\\) from the known \\(F(t_0)\\) and \\(f(t_0)\\). That is, \\[F(t_0 + dt) = F(t_0) + f(t_0)\\, dt\\ .\\]",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Integrals step-by-step</span>"
    ]
  },
  {
    "objectID": "Accumulation/37-euler.html#euler-method",
    "href": "Accumulation/37-euler.html#euler-method",
    "title": "38  Integrals step-by-step",
    "section": "",
    "text": "First, since the problem setting is that we don’t (yet) know \\(F(t)\\), let’s refer to things we do know. In particular, we know \\(f(t) = \\partial_t F(t)\\).\nAgain, recognizing that we don’t yet know \\(F(t)\\), let’s re-write the expression using something that we do know: \\(F(t_0)\\). Stated more precisely, \\(F(t_0)\\) is something we get to make up to suit our convenience. (A common choice is \\(F(t_0)=0\\).)\nLet’s replace the symbol \\(h\\) with the symbol \\(dt\\). Both of them mean “a little bit of” and \\(dt\\) makes explicit that we mean “a little bit of \\(t\\).”\nwe will substitute the limit \\(\\lim_{h\\rightarrow 0}\\) with an understanding that \\(dt\\) will be something “small.” How small? we will deal with that question when we have to tools to answer it.\n\n\n\n\n\n\n\n\nTip\n\n\n\nLet’s consider finding the anti-derivative of \\(\\dnorm()\\), that is, \\(\\int_0^t \\dnorm(x) dx\\). In one sense, you already know the answer, since \\(\\partial_x \\pnorm(x) = \\dnorm(x)\\). But \\(\\pnorm()\\) is just a name. Beneath the name we know \\(\\pnorm()\\) only because it has been numerically constructed by integrating \\(\\dnorm()\\). The \\(\\pnorm()\\) function is so important that the numerically constructed answer has been memorized by software.",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Integrals step-by-step</span>"
    ]
  },
  {
    "objectID": "Accumulation/37-euler.html#area",
    "href": "Accumulation/37-euler.html#area",
    "title": "38  Integrals step-by-step",
    "section": "38.2 Area",
    "text": "38.2 Area\nThe quantity \\[\\Large \\color{magenta}{f(t_0)}\\, \\color{orange}{dt}\\] gives rise to a visualization that has been learned by generations of calculus students. The visualization is so compelling and powerful that many students (and teachers, and textbook authors) mistake the visualization for integration and anti-differentiation themselves.\nWe will start the visualization with a simple graph of \\(f(t)\\), which is called the integrand in the integral \\(\\int_a^b f(t) dt\\). Figure 38.1 shows the graph of \\(f(t)\\). A specific point \\(t_0\\) has been marked on the horizontal axis. Next to it is another mark at \\(t_0 + dt\\). Of course, the distance between these marks is \\(\\color{orange}{dt}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 38.1: Illustrating the interpretation of \\(f(t_0) dt\\) as an “area.”\n\n\n\nPer the usual graphical convention, a position along the vertical axis corresponds to a possible output of \\(f(t)\\). The output for \\(t=t_0\\) is \\(\\color{magenta}{f(t_0)}\\). That same quantity corresponds to the length of the vertical orange segment connecting \\((t_0, 0)\\) to \\((t_0, f(t_0))\\).\nThe \\(\\color{orange}{dt}\\) line segment and the \\(\\color{magenta}{f(t_0)}\\) segment constitute two sides of a rectangle, shown as a shaded zone. The “area” of that rectangle is the product \\(\\color{magenta}{f(t_0)}\\ \\color{orange}{dt}\\).\nIn this sort of visualization, an integral is the accumulation of many of these \\(f(t) dt\\) rectangles. For instance, Figure 38.2 the visualization of the integral \\[\\int_{0}^3 f(t) dt\\ .\\]\n\n\n\n\n## Warning in is.na(x): is.na() applied to non-(list or vector) of type\n## 'expression'\n\n## Warning in is.na(x): is.na() applied to non-(list or vector) of type\n## 'expression'\n\n## Warning in is.na(x): is.na() applied to non-(list or vector) of type\n## 'expression'\n\n\n\n\n\n\n\n\n\n\nFigure 38.2: Visualizing the integral \\(\\\\int_0^3 f(t) dt\\) as the total \"area\" of several \\(f(t) dt\\) bars. The width of each of the bars is \\(dt\\). The height depends on the value of the function \\(f(t)\\) at the bar. For illustration, two of the bars are marked with vertical and horizontal line segments.\n\n\n\nAs always in calculus, we imagine \\(dt\\) as a “small” quantity. In Figure 38.3 you can see that the function output changes substantially over the sub-domain spanned by a single rectangle. Using smaller and smaller \\(dt\\), as in Figure 38.2 brings the visualization closer and closer to the actual meaning of an anti-derivative.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 38.3: Visualizing the integral \\(\\int_0^3 f(t) dt\\) as the total “area” of several \\(f(t) dt\\) bars. The width of each of the bars is \\(dt\\). The height depends on the value of the function \\(f(t)\\) at the bar. For illustration, two of the bars are marked with vertical and horizontal line segments.\n\n\n\n\n\n\n\n\n\nWhy do you keep putting “area” in quotes?\n\n\n\nWhen \\(f(t_i) &lt; 0\\), then \\(f(t_i) dt\\) will be negative. There is no such thing as a negative area, but in constructing an integral the \\(f(t_i)dt\\), being negative, diminishes the accumulated area.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 38.4: The \\(\\int_{-2}^3 g(t) dt\\) covers subdomains where \\(g(t) &gt; 0\\) and where \\(g(t) &lt; 0\\). In those latter subdomains, the “area” is negative, and shown in light orange here.\n\n\n\nAnother problem is that area is a physical quantity, with dimension L\\(^2\\). The quantity produced by integration will have physical dimension \\([f(t)][t]\\), the product of the dimension of the with-respect-to quantity and the output of the function.\n“Area” is an effective metaphor for visualizing integration, but the goal of integration is not to calculate an area but, typically, some other kind of quantity.",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Integrals step-by-step</span>"
    ]
  },
  {
    "objectID": "Accumulation/37-euler.html#the-euler-step",
    "href": "Accumulation/37-euler.html#the-euler-step",
    "title": "38  Integrals step-by-step",
    "section": "38.3 The Euler Step",
    "text": "38.3 The Euler Step\nWithout invoking “areas,” a definite integral \\(\\int_a^b f(t) dt\\) can be computed by constructing the anti-derivative \\(F(t) \\equiv \\int f(t) dt\\) and evaluating it at the upper and lower bounds of integration: \\(F(b) - F(a)\\). In this section, we will look at the numerical process of constructing an anti-derivative function, which uses many of the same concepts as those involved in finding an integral by combining areas of rectangles.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nA definite integral produces a quantity, not a function. The anti-derivative function constructed by using quantities like \\(f(t) dt\\) will be a series of quantities rather than a formula. In particular, it will have the form of a data table, something like this:\n\n\n\n\n\\(t\\)\n\\(F(t)\\)\n\n\n\n\n-2\n10.62\n\n\n-1.5\n6.47\n\n\n-1\n3.51\n\n\n-0.5\n2.02\n\n\n0\n2.4\n\n\n0.5\n3.18\n\n\n1.0\n5.14\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\n\nTo start in creating \\(F()\\) by anti-differentiating \\(f()\\), we will need to create a series of \\(t\\) values. We will do this by specifying a starting value for \\(t\\) and then creating successive values by adding a numerical increment \\(dt\\) to the entries one after the other until we reach a terminating value. For instance, in the above table, the starting value for \\(t\\) is \\(-2\\), the numerical increment is \\(dt=0.5\\), and the terminating value is \\(1\\).\nIn previous chapters of this book we have worked with data tables, but always the data table was given to us, we did not have to construct it.1 Now we need to construct the data frame with the \\(t\\) column containing appropriate values. Computer languages provide many ways to accomplish this task. We will use a simple R/mosaic function Picket(), which constructs a data table like the one shown above. You provide two arguments: the domain for \\(t\\), that is, the desired upper and lower bounds of integration; the interval size \\(dt\\) (which is called h in the argument list). For instance, to construct the \\(t\\) column of the table shown above, you can use Picket() this way:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nAs you can see, the data table produced by Picket() has the \\(t\\) column, as well as a second column named weight. We haven’t explained weight yet, but you can see that it is the same value we specified as h.\nThe name Picket() is motivated by the shape of a picket fence. The pickets are evenly spaced, which keeps things simple but is not a requirement.\nNote that the picket does not say anything at all about the function \\(f(t)\\) being anti-differentiated. The picket can be applied to any function although precision might require a smaller \\(dt\\) for functions that have a lot going on in a small domain.\nThe next step in using the picket to perform anti-differentiation is to apply the function \\(f()\\) to the pickets. That is, we will add a new column, perhaps called vals to the data table.\nAdding a new column is a common task when dealing with data. We will do this with a new function, mutate(), whose specific function is adding new columns (or modifying old ones). Here’s the command to apply \\(f()\\) to t and call the new column vals:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow that we know the value of the function at each of the pickets, the next step is to multiply the value by the spacing between pickets. That spacing, which we set with the argument h = 0.5 in our original call to Picket() is in the column called weight. we will call the result of the multiplication step. Note that the following R command incorporates the previous calculation of vals; we are looking to build up a single command that will do all the work.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe used the name step to identify the product of the height and spacing of the pickets to help you think about the overall calculation as accumulating a series of steps. Each step provides a little more information about the anti-derivative that we will now calculate. In the area metaphor for integration, each step is the area of one vertical bar of the sort presented in the previous section.\nWe will call these Euler steps, a term that will be especially appropriate when, in Block 6, we use integration to calculate the trajectories of systems—such as a ball in flight—that change in time.\nThe final step in constructing the anti-derivative is to add up the steps. This is simple addition. But we will arrange the addition one step at a time. That is, for the second row, the result will be the sum of the first two steps. For the third row, the result will be the sum of the first three steps. And so on. The name for this sort of accumulation of the previous steps is called a cumulative sum. Another name for a cumulative sum is a “running sum”: the sum-so-far as we move down the column of steps. Cumulative sums are computed in R by using cumsum(). Here, we are calling the result of the cumulative sum F to emphasize that it is the result of anti-differentiating \\(f()\\). But keep in mind that the anti-derivative is not just the F column, but the table with both t and F columns. That is, the table has a column for the input as well as the output. That is what it takes to be a function.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can summarize the steps in this Euler approach to numerical integration graphically:\n\n\n\n\n\n\n\n\n1. Create the picket\n\n\n\n\n\n\n\n2. Eval function f(x) at picket locations to set height.\n\n\n\n\n\n\n\n3. Multiply picket height by width\n\n\n\n\n\n\n\n4. Cumulative sum of areas versus t\n\n\n\n\n\n\nFigure 38.5: Steps in a numerical construction of an anti-derivative. (1) Create a set of picket locations over the domain of interest. The locations are spread horizontally by amount dt, so each picket will be dt units wide. (2) evaluate the original function at the picket points to give picket heights. (3) Multiply the picket height by the picket width to create an \"area\". (4) Starting at zero for the left-most picket, add in successive picket areas to construct the points on the anti-derivative function (green). Note that the vertical axis in (4) has a different dimension and units than in steps (1)-(3). In (4) the vertical scale is in the units of the anti-derivative function output.\n\n\n\nFigure 38.6 shows a dynamic version of the process of constructing an anti-derivative by Euler steps. The integrand \\(f(t)\\) is shown in the top panel, the anti-derivative \\(F(t)\\) is shown being built up in the bottom panel. The magenta bar in the top plot is the current Euler step. That step is added to the previously accumulated steps to construct \\(F(t)\\).\n\n\n\n\n\n\nFigure 38.6: A dynamic view of building \\(F(t)\\) from \\(f(t)\\) by accumulating Euler steps.\n\n\n\n\nApplication area 38.1 — What does “daily” tell you?\n\n\n\n\n\n\n\nApplication area 38.1 Russian COVID-19 cases\n\n\n\nThe following graphic from a well-respected news magazine, The Economist, shows the reported number of cases and deaths from Covid-19 during a two-year period in Russia.\n\n\n\n\n\n\nFigure 38.7\n\n\n\nThe figure caption gives information about the units of the quantities being graphed. Notice the word “daily,” which tells us, for example, that in mid-2021 there were about 10,000 new cases of Covid-19 each day and correspondingly about 350 daily deaths.\nHow many total cases and total deaths are reported in the graphic?\nThere are, of course, two distinct ways to present such data which can be easily confused by the casual reader. One important way to present data is as cumulative cases and deaths as a function of date. We will call these \\(C(t)\\) and \\(D(t)\\). Another prefectly legitimate presentation is of the rate of change \\(\\partial_t C(t)\\) and \\(\\partial_t D(t)\\) which, following our informal capital/lower-case-letter convention, we could write \\(c(t)\\) and \\(d(t)\\). Since there is no such thing as a “negative” case or death, we know that \\(C(t)\\) and \\(D(t)\\) are monotonic functions, never decreasing. So the graphs cannot possibly be of \\(C(t)\\) and \\(D(t)\\), since the graphs are far from monotonic. Consequently, the displayed graphs are \\(c(t)\\) and \\(d(t)\\), as confirmed by the word “daily” in the caption.\nTo find \\(C(t)\\) and \\(D(t)\\) requires integrating \\(c(t)\\) and \\(d(t)\\). The value of \\(C(t)\\) and \\(D(t)\\) at the right-most extreme of the graph can be found by calculating the “area” under the \\(c(t)\\) and \\(d(t)\\) curves. But care needs to be taken in reading the horizontal axis. Although the axes are labelled with the year, the tick marks are spaced by one month. (Notice “month” does not appear in the caption.) The far right end of the graph is in early July 2021. The far left end, when the graph moves away from zero cases and deaths, is early April 2020.\nYou can do a reasonable job estimating the “area” by extending the tick marks on the horizontal axis and counting the resulting rectangles that fall under the curve.\n\n\n\n\n\n\nFigure 38.8: Dividing the domain into regions of width \\(dt = 1\\) month.”\n\n\n\nFor the graph of cases, the “area” of each rectangle is \\(\\frac{5000\\, \\text{cases}}{\\text{ day}}\\cdot \\text{1 month}\\). This has the right dimension, “cases,” but the units are screwy. So replace 1 month with 30.5 days (or thereabouts) to get an “area” of each rectangle of 172,500 cases. Similarly, the “area” of the rectangles on the right graph is 3050 deaths.",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Integrals step-by-step</span>"
    ]
  },
  {
    "objectID": "Accumulation/37-euler.html#better-numerics-optional",
    "href": "Accumulation/37-euler.html#better-numerics-optional",
    "title": "38  Integrals step-by-step",
    "section": "38.4 Better numerics (optional)",
    "text": "38.4 Better numerics (optional)\nExcept as a textbook exercise, you will likely never have to compute a numerical anti-derivative from scratch as we did in the previous section. This is a good thing. To understand why, you have to know one of the important features of modern technical work. That feature is: We never work alone in technical matters. There is always a set of people whose previous work we are building on, even if we never know the names of those people. This is because technology is complicated and it is evidently beyond the reach of any human to master all the salient aspects of each piece of technology being incorporated into the work we consider our own.\nOf course this is true for computers, since no individual can build a useful computer from first principles. It is also true for software. One detail in particular is relevant to us here. Computer arithmetic of the sort used in the previous section—particularly addition—is prone to error when adding up lots and lots of small bits. This means that it is not always sensible to choose very small \\(dt\\) to get a very accurate approximation to the anti-derivative.\nFortunately, there are specialists in numerical mathematics who work on ways to improve the accuracy of calculations for mid-sized \\(dt\\). Their work has been incorporated into the results of antiD() and Integrate() and so the details are, for us, unimportant. But they are only unimportant because they have been taken care of.\nTo illustrate how considerably more accuracy can be gained in calculating an anti-derivative, consider that the rectangular bars drawn in the previous sections are intended to approximate the “area” under the function. With this in mind, we can replace the rectangular bars with more suitable shapes that stay closer to the function over the finite extend of each \\(dt\\) domain. The rectangular bars model the function as piecewise constant. A better job can be done with piecewise linear approximations or piecewise quadratic approximations. Often, such refinements can be implemented merely by changing the weight column in the picket data frame used to start off the process.\nOne widely used method, called Gauss-Legendre quadrature can calculate a large segment of an integral accurately (under conditions that are common in practice) with just five evaluations of the integrand \\(f(t)\\).\n\n\n\nTable 38.1: Picket locations and weights For the integral \\(\\int_a^b f(t) dt\\) where \\(c = \\frac{a+b}{2}\\) and \\(w = (b-a)/2\\).\n\n\n\n\n\nlocation\nweight\n\n\n\n\n\\(c - 0.90618 w\\)\n\\(0.236927 \\times w\\)\n\n\n\\(c - 0.53847 w\\)\n\\(0.478629 \\times w\\)\n\n\n\\(c\\)\n\\(0.568889 \\times w\\)\n\n\n\\(c + 0.53847 w\\)\n\\(0.478629 \\times w\\)\n\n\n\\(c + 0.90618 w\\)\n\\(0.236927 \\times w\\)\n\n\n\n\n\n\nThe locations and weights may seem like a wizard parody of mathematics, but those precise values are founded in an advanced formulation of polynomials rooted in the theory of linear combinations to which you will be introduced in Block 5. Needless to say, you can hardly be expected to have any idea where they come from. That is why it is useful to build on the work of experts in specialized areas. It is particularly helpful when such expertise is incorporated into software that faithfully and reliably implements the methods. The lesson to take to heart: Use professional software systems that have been extensively vetted.",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Integrals step-by-step</span>"
    ]
  },
  {
    "objectID": "Accumulation/37-euler.html#footnotes",
    "href": "Accumulation/37-euler.html#footnotes",
    "title": "38  Integrals step-by-step",
    "section": "",
    "text": "The root of the word “data” is the Latin for “given”.↩︎",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Integrals step-by-step</span>"
    ]
  },
  {
    "objectID": "Accumulation/38-symbolic.html",
    "href": "Accumulation/38-symbolic.html",
    "title": "39  Symbolic anti-differentiation",
    "section": "",
    "text": "39.1 The cataloged functions\nYou have already learned how to write down, by sight, the anti-derivative of the many of the pattern-book functions. As a reminder, here is an (almost) complete list of the derivatives and anti-derivatives of the pattern-book functions.\nYou can see that the derivatives and anti-derivatives of the pattern-book functions can be written using the pattern-book functions themselves. The left column contains the symbolic derivatives of the pattern book functions.1 The right column contains the symbolic anti-derivatives. We call them “symbolic,” because they are written down with the same kind of symbols that we use for writing the pattern-book functions themselves.2\nWe are stretching things a bit by including \\(\\dnorm(x)\\) and \\(\\pnorm(x)\\) among the functions that can be integrated symbolically. As you will see later, \\(\\dnorm(x)\\) is special when it comes to integration.\nThink of the above table as the “basic facts” of differentiation and anti-differentiation. It is well worth memorizing the table since it shows many of the relationships among the functions that are central to this book. For the sinusoids, we’ve used the traditional name \\(\\cos(x)\\) to refer to \\(\\sin(x + \\pi/2)\\) and \\(-\\cos(x)\\) instead of \\(\\sin(x - \\pi/2)\\) since generations of calculus students have been taught to name “cosine” as the derivative of “sine,” and don’t always remember the relationship that \\(\\cos(x) =\\sin(x + \\pi/2)\\).\nFor differentiation, any function that can be written using combinations of the pattern-book functions by multiplication, composition, and linear combination has a derivative that can be written using the pattern-book functions. So a complete story of symbolic differentiation is told by the above table and the differentiation rules:\nThis chapter is about the analogous rules for anti-differentiation. The anti-differentiation rule for linear combination is simple: essentially the same as the rule for differentiation.\n\\[\\int \\left[\\strut a\\, f(x) + b\\, g(x)\\right] dx = a\\!\\int\\! f(x) dx + b\\!\\int\\! g(x) dx\\] How about the rules for function products and for composition? The surprising answer is that there are no such rules. There is no template analogous to the product and chain rules for derivatives, that can consistently be used for anti-derivatives. What we have instead are techniques of integration, somewhat vague rules that will sometimes guide a successful search for the anti-derivative, but often will lead you astray.\nIndeed, there are many functions for which a symbolic anti-derivative cannot be constructed from compositions and/or multiplication of pattern-book functions that can be written using pattern-book functions.3\nFortunately, we already know the symbolic anti-derivative form of many functions. We will call those the cataloged functions, but this is not a term in general use in mathematics. For functions not in the catalog, it is non-trivial to find out whether the function has a symbolic anti-derivative or not. This is one reason why the techniques of integration do not always provide a result.\nThe following sections provide an overview of techniques of integration. We start with a description of the cataloged functions and direct you to computer-based systems for looking up the catalog. (These are almost always faster and more reliable than trying to do things by hand.) Then we introduce a new interpretation of the notation for anti-differentiation: differentials. This interpretation makes it somewhat easier to understand the major techniques of integration: substitution and integration by parts. We will finish by returning to a setting where symbolic integration is easy: polynomials.\nRemember that, even if we cannot always find a symbolic anti-derivative, that we can always construct a numerical anti-derivative that will be precise enough for almost any genuine purpose.\nIn a traditional science or mathematics education, students encounter (almost exclusively) basic functions from a mid-sized catalog. For instance: \\(\\sqrt{\\strut\\_\\_\\_\\ }\\), \\(\\sin()\\), \\(\\cos()\\), \\(\\tan()\\), square(), cube(), recip(), \\(\\ln()\\), \\(\\exp()\\), negate(), gaussian(), and so on. This catalog also includes some functions that take two arguments but are traditionally written without using parentheses. For instance, \\(a+b\\) does not look like a function but is entirely equivalent to \\(+(a, b)\\). Others in this class are \\(\\times(\\ ,\\ )\\), \\(\\div(\\ , \\ )\\), \\(-(\\ ,\\ )\\), and ^( , ).\nThe professional applied mathematician’s catalog is much larger. You can see an example published by the US National Institute of Standards and Technology as the Digital Library of Mathematical Functions. (Almost all of the 36 chapters in this catalog, important though they be, are highly specialized and not of general interest across fields.)\nThere is a considerable body of theory for these cataloged functions, which often takes the form of relating them to one another. For instance, \\(\\ln(a \\times b) = \\ln(a) + \\ln(b)\\) demonstrates a relationship among \\(\\ln()\\), \\(+\\) and \\(\\times\\). Along the same lines of relating the cataloged functions to one another is \\(\\partial_x \\sin(x) = \\cos(x)\\) and other statements about derivatives such as those listed in Chapter 20.\nSimply to illustrate what a function catalog looks like, Figure 39.1 shows a page from an 1899 handbook entitled A Short Table of Integrals.\nThe use of cataloged functions is particularly prevalent in textbooks, so the quantitatively sophisticated student will encounter symbolic anti-derivatives of these functions throughout his or her studies.\nThe cataloged functions were assembled with great effort by mathematicians over the decades. The techniques and tricks they used to find symbolic anti-derivatives are not part of the everyday experience of technical workers, although many mathematically minded people find them a good source of recreation.\nCalculus textbooks that include extensive coverage of the techniques and tricks should be understood as telling a story of the historical construction of catalogs, rather than conveying skills that are widely used today. In a practical sense, when the techniques are needed, it is more reliable to access them via computer interface such as WolframAlpha, as depicted in Figure 39.2.\nThe systems can do a good job identifying cases where the techniques will not work. In such systems, they provide the anti-derivative as constructed by numerical integration. The R/mosaic antiD() function works in this same way, although its catalog contains only a tiny fraction of the functions found in professional systems. (But then, only a tiny fraction of the professional cataloged function are widely used in applied work.)",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Symbolic anti-differentiation</span>"
    ]
  },
  {
    "objectID": "Accumulation/38-symbolic.html#sec-cataloged-functions",
    "href": "Accumulation/38-symbolic.html#sec-cataloged-functions",
    "title": "39  Symbolic anti-differentiation",
    "section": "",
    "text": "Figure 39.1: Entries 124-135 from A Short Table of Integrals (1899) by Benjamin Osgood Pierce. The book includes 938 such entries.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 39.2: Pierce’s entry 125 as computed by the WolframAlpha system.",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Symbolic anti-differentiation</span>"
    ]
  },
  {
    "objectID": "Accumulation/38-symbolic.html#differentials",
    "href": "Accumulation/38-symbolic.html#differentials",
    "title": "39  Symbolic anti-differentiation",
    "section": "39.2 Differentials",
    "text": "39.2 Differentials\nBreathing some life into the symbol \\(dx\\) will help in understanding the algebra of techniques for anti-differentiating function compositions and products. We’ve thus far presented \\(dx\\) as a bit of notation: punctuation for identifying the with-respect-to input in anti-derivatives. That is, in interpreting a sequence of symbols like \\(\\int f(x,t) dx\\), we’ve parsed the sequence of symbols into three parts:\n\\[\\underbrace{\\int}_{\\text{integral sign}} \\overbrace{f(x, t)}^{\\text{function to be anti-differentiated}} \\underbrace{dx}_{\\text{'with respect to'}}\\]\nBy analogy, the English sentence\n\\[\\text{We loaded up on snacks.}\\]\nconsists of five parts: the five words in the sentence.\nBut you can also see “We loaded up on snacks” as having three parts:\n\\[\\underbrace{\\text{We}}_{\\text{subject}}\\  \n\\overbrace{\\text{loaded up on}}^{\\text{verb}}\\ \\ \\\n\\underbrace{\\text{snacks}}_{\\text{object}}\\]\nLikewise, the integrate sentence can be seen as consisting of just two parts:\n\\[\\underbrace{\\int}_{\\text{integral sign}} \\overbrace{f(x, t) dx}^{\\text{differential}}\\]\nA differential corresponds to the little sloped segments that we add up when calculating a definite integral numerically using the slope function visualization. That is \\[\\underbrace{\\int}_{\\text{Sum}} \\underbrace{\\overbrace{f(x,t)}^\\text{slope of segment}\\ \\  \\overbrace{dx}^\\text{run}}_\\text{rise}\\]\nA differential is a genuine mathematical object and is used, for example, in analyzing the geometry of curved spaces, as in the Theory of General Relativity. But this is well beyond the scope of this introductory calculus course.\nOur use here for differentials will be to express rules for anti-differentiation of function compositions and products.\nYou should be thinking of differentials when you see a sentence like the following:\n\n“In \\(\\int \\sin(x) \\cos(x) dx\\), make the substitution \\(u = \\sin(x)\\), implying that \\(du = \\cos(x) dx\\) and getting \\(\\int u du\\), which is simple to integrate.”\n\nThe table gives some examples of functions and their differentials. “w.r.t” means “with respect to.”\n\n\n\n\n\n\n\n\n\n\nFunction\nderivative\nw.r.t.\ndifferential\n\n\n\n\n\\(v(x) \\equiv x\\)\n\\(\\partial_x v(x) = 1\\)\nx\n\\(dv = dx\\)\n\n\n\\(u(x) \\equiv x^2\\)\n\\(\\partial_x u(x) = 2x\\)\nx\n\\(du = 2x dx\\)\n\n\n\\(f(x) \\equiv \\sin(x)\\)\n\\(\\partial_x f(x) = \\cos(x)\\)\nx\n\\(df = \\cos(x)dx\\)\n\n\n\\(u(x) \\equiv e^{3 x}\\)\n\\(\\partial_x u(x) = 3 e^{3 x}\\)\nx\n\\(du = 3 e^{3 x} dx\\)\n\n\n\\(g(x) \\equiv t^3\\)\n\\(\\partial_t v(t) = 3 t^2\\)\nt\n\\(dg = 3 t^2 dt\\)\n\n\n\n\nAs you can see, the differential of a function is simply the derivative of that function followed by the little \\(dx\\) or \\(dt\\) or whatever is appropriate for the “with respect to” input.\nNotice that the differential of a function is not written with parentheses: The function \\(u(x)\\) corresponds to the differential \\(du\\).\n\n\n\n\n\n\n\n\nTry it! 39.1\n\n\n\n\n\n\n\n\n\nTry it! 39.1 What is the differential of \\(\\sin(x)\\)?\n\n\n\nAs we’ve seen, \\(\\partial_x \\sin(x) = cos(x)\\). For form the differential of \\(\\sin()\\), take the derivative and suffix it with a \\(dx\\) (since \\(x\\) is the name of the input):\n\\[\\cos(x)\\ dx\\]",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Symbolic anti-differentiation</span>"
    ]
  },
  {
    "objectID": "Accumulation/38-symbolic.html#u-substitution",
    "href": "Accumulation/38-symbolic.html#u-substitution",
    "title": "39  Symbolic anti-differentiation",
    "section": "39.3 U-substitution",
    "text": "39.3 U-substitution\nThere is little reason to use \\(\\partial_t\\) and \\(\\int \\left[\\right]dt\\) to cancel each other out, but it is the basis of an often successful strategy—u-substitution—for finding anti-derivatives symbolically. Here’s the differentiate/integrate algorithm behind u-substitution.\n\nPick a function \\(f()\\) and another function \\(g()\\). Typically \\(f()\\) and \\(g()\\) belong to the family of basic modeling functions, e.g. \\(e^x\\), \\(\\sin(t)\\), \\(x^n\\), \\(\\ln(x)\\), and so on. For the purpose of illustration, we will use \\(f(x) = \\ln(x)\\) and \\(g(t) = \\cos(t)\\).\nCompose \\(f()\\) with \\(g()\\) to produce a new function \\(f(g())\\) which, in our case, will be \\(\\ln(\\cos(t))\\).\nUse the chain rule to find \\(\\partial_t f(g(t))\\). In the example, the derivative of \\(\\ln(x)\\) is \\(1/x\\), the derivative of \\(g(t)\\) is \\(-\\sin(t)\\). By the chain rule, \\[\\partial_t f\\left(\\strut g(t)\\right) = \\partial_t \\underbrace{\\Large\\ln}_{f()}\\left(\\underbrace{\\large\\cos(t)}_{g(t)}\\right) = \\underbrace{\\left[- \\frac{1}{\\cos(t)}\\right]}_{f'(g(t))} \\underbrace{\\left[{\\LARGE\\strut}\\sin(t)\\right]}_{g'(t)} = -  \\frac{\\sin(t)}{\\cos(t)} = - \\tan(t)\\]\n\nIn a sense, we have just watched a function give birth to another through the straightforward process of differentiation. Having witnessed the birth, we know who is the integration parent of \\(\\tan(t)\\), namely \\(\\int \\tan(t) dt = \\ln\\left(\\cos(t)\\right)\\). For future reference, we might write this down in our diary of integrals: \\[\\int \\tan(t) dt = - \\ln(\\cos(t)) + C\\] Saving this fact in your diary is helpful. The next time you need to find \\(\\int \\tan(x) dx\\), you can look up the answer (\\(-\\ln(\\cos(x)) + C\\)) from your diary. If you use \\(\\int \\tan(x) dx\\) a lot, you will probably come to memorize the answer, just as you have already memorized that \\(\\int \\cos(t) dt = \\sin(t)\\) (a fact that you will use a lot in the rest of this course).\nNow for the u-substitution game. The trick is to take a problem of the form \\(\\int h(t) dt\\) and extract from \\(h(t)\\) two functions, an \\(f()\\) and a \\(g()\\). You’re going to do this so that \\(h(t) =  \\partial_t F(g(t))\\), where \\(\\partial_x F(x) = f(x)\\) Once you’ve done this, you have an answer to the original integration question: \\(\\int h(t) dt = F(g(t)) + C\\).\n\n\n\n\n\n\n\n\nTry it! 39.2\n\n\n\n\n\n\n\n\n\nTry it! 39.2 Evaluate \\(\\int \\frac{\\sin(\\ln(x))}{x} dx\\).\n\n\n\nYou don’t know ahead of time that this is an integral amenable to solution by u-substitution. For all you know, it is not. So before you start, look at the function to see if it one of those for which you already know the anti-derivative, for example any of the pattern-book functions or their parameterized cousins the basic modeling functions.\n\nIf so, you’ve already memorized the answer and you are done. If not …\n\nAssume for a moment—without any guarantee that this will work, mind you—that the answer can be built using u-substitution. You will therefore look hard at \\(h()\\) and try to see in it a plausible form that looks like the derivative of some \\(f(g(x))\\).\nIn the problem at hand, we can readily see something of the form \\(f(g(x))\\) in the \\(\\sin(\\ln(x))\\). This immediately gives you a candidate for \\(g(x)\\), namely \\(g(x)\\equiv \\ln(x)\\) We don’t know \\(f()\\) yet, but if \\(g()\\) is the right guess, and if u-substitution is going to work, we know that \\(f()\\) has to be something that produces \\(\\sin()\\) when you differentiate it. That is \\(-\\cos()\\). So now we have a guess \\[h_\\text{guess}(x) = -\\cos(\\ln(x)) \\partial_x \\ln(x) = - \\cos(\\ln(x)) \\frac{dx}{x}\\]\n\nIf this guess matches the actual \\(h()\\) then you win. The answer to \\(\\int h(x) dx\\) will be \\(f(g(x)) = -\\cos(\\ln(x))\\). If not, see if there is any other plausible guess for \\(g(x)\\) to try. If you cannot find one that works, try integration by parts.",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Symbolic anti-differentiation</span>"
    ]
  },
  {
    "objectID": "Accumulation/38-symbolic.html#sec-integration-by-parts1",
    "href": "Accumulation/38-symbolic.html#sec-integration-by-parts1",
    "title": "39  Symbolic anti-differentiation",
    "section": "39.4 Integration by parts (standard presentation)",
    "text": "39.4 Integration by parts (standard presentation)\nIf you do a lot of symbolic anti-differentation, you will often come across functions that you don’t recognize as being the derivative of an already known function. Consider, for instance, \\[\\int x \\cos(x) dx\\ .\\]\nEven though the integrand \\(x \\cos(x)\\) is a simple product of two pattern book functions it is likely not a function that you have previously produced by differentiation. Thus, it is not yet in your diary of anti-derivatives. The purpose of integration by parts is to provide a standard way to re-organize anti-derivatives like \\(\\int x \\cos(x) dx\\), where the integrand is a product of two simple functions, into another form. Being able to do this is no guarantee that the other form will be something you can anti-differentiate, but it is worth rolling the dice to see if you get lucky.\nThe re-organization rule is based on two fundamental properties of differentiation and anti-differentiation.\n\n\\(\\int f'(x) dx = f(x)\\). This is saying nothing more than if \\(f'(x)\\) is the derivative of \\(f(x)\\), then \\(f(x)\\) must be an anti-derivative of \\(f'(x)\\).\n\\(\\partial_x \\left[\\strut u(x)\\cdot v(x) \\right] = u'(x)\\cdot v(x) + v'(x)\\cdot u(x)\\): the product rule of differentiation.\n\nLet’s integrate both sides of the statement of the product rule. For the left side, applying rule (i), we get a simple result:\n\\[\\int\\left[\\strut\\partial_x \\left[\\strut u(x)\\cdot v(x) \\right]\\right] dx = u(x) \\cdot v(x)\\]\nAs for the right side, all we get is two anti-derivatives: \\[\\int\\left[\\strut u'(x)\\cdot v(x) + v'(x)\\cdot u(x)\\right]dx =\n\\int\\left[\\strut u'(x)\\cdot v(x)\\right]dx + \\int\\left[\\strut u(x)\\cdot v'(x)\\right]dx\\] Putting together the previous two expressions and re-arranging gives: \\[\\int u(x)\\cdot v'(x)\\, dx = u(x) \\cdot v(x) - \\int  v(x)\\cdot u'(x)dx\\ \\ \\ \\mathbf{ \\text{parts re-arrangment}}\\] Now, consider a problem like \\(\\int x \\cdot \\cos(x) dx\\) that we don’t yet know how to solve. Let’s associate this problem with the left side of the parts re-arrangement equation. With luck, we will recognize a problem that we will know how to do on the right-hand side.\nTo implement the re-arrangement, we need to split our as yet unknown anti-derivative into two pieces: \\(u(x)\\) and \\(v'(x) dx\\). There are many possible ways to do this but the most obvious is \\[\\int \\underbrace{\\strut x}_{u(x)} \\cdot \\underbrace{\\cos(x) dx}_{v'(x) dx}\\] According to this proposed splitting, we have \\(u(x) = x\\) and \\(v'(x) dx = \\cos(x) dx\\). To plug things into the right side of the parts re-arrangement we will need to find \\(v(x)\\) and \\(u'(x) dx\\). Since we know \\(u(x) = x\\) it is easy to take the differential, \\(du = dx\\). Similarly, we know \\(v'(x) dx = \\cos(x) dx\\) so we can integrate both sides: \\[v(x) = \\underbrace{\\int v'(x) dx = \\int \\cos(x) dx}_{\\text{from }\\ v'(x)\\,dx\\ =\\ \\cos(x)\\,dx} = \\sin(x)\\] Now that we know the \\(v(x)\\) that is consistent with our original splitting of the anti-derivative into \\(\\int u(x) \\cdot v'(x) dx\\) we can plug in our results to the right side of the parts re-arrangement equation:\n\\[\\int x \\cdot \\cos(x)dx = x \\sin(x) - \\int \\underbrace{\\sin(x)}_{v(x)}\\  \\underbrace{\\ 1\\ dx\\ \\strut}_{u'(x) dx}\\] We are in luck! We already know the anti-derivative \\(\\int \\sin(x) dx = -\\cos(x)\\). Substituting this result for the \\(\\int v(x) u'(x) dx\\) term, we arrive at \\[\\int x \\cdot \\cos(x)dx = x \\sin(x) + \\cos(x)\\ .\\]\nThe key creative step in using integration by parts effectively is to choose a helpful split of the original integral into the \\(u(x)\\) and \\(v'(x) dx\\) parts. This is usually based on a solid knowledge of derivatives and anti-derivatives of basic functions as well as insight into the downstream consequences of any choice. In this sense, picking \\(u(x)\\) and \\(v'(x)dx\\) is like making a move in chess. Some players can see two or three moves ahead and so can pick the first move to improve their position. Without such foresight, the best most people can do is to pick a first move that seems attractive and accept that their fate might be either victory or checkmate.\nFor the calculus student learning integration by parts, there is an irony. Gaining enough experience to make good choices of \\(u(x)\\) and \\(v'(x)dx\\) means that you will solve, or read about solving, many anti-differentiation problems. You can, of course, enter the solutions into your diary of anti-derivatives, obviating to that extent the need to perform integration by parts in the future.\n\n\n\n\n\n\n\n\nTry it! 39.3\n\n\n\n\n\n\n\n\n\nTry it! 39.3\n\n\n\nIn demonstrating that \\[\\int x \\cdot \\cos(x)dx = x \\sin(x) + \\cos(x)\\] we followed a number of steps each of which might be subject to error. Best to confirm our solution before accepting it. This can be done by differentiating both sides of our solution: \\[\\partial_x \\int x \\cdot \\cos(x)dx = x \\cos(x) = \\partial_x \\left[\\strut x \\sin(x) + \\cos(x)\\right] = \\underbrace{\\sin(x) + x \\cos(x)}_{\\partial_x \\left[x\\cdot\\sin(x)\\right]}\\  \\underbrace{- \\sin(x)}_{\\partial_x \\cos(x)}= x\\cos(x)\\]\n\n\n\n\n\n\n\n\n\n\nTry it! 39.4\n\n\n\n\n\n\n\n\n\nTry it! 39.4 A unfortunate choice of parts\n\n\n\nWhat would happen in the previous example if we had made a bad choice for \\(u(x)\\) and \\(v'(x) dx\\)? For instance, we might have split \\(x \\cos(x) dx\\) into \\(u(x) = \\cos(x)\\) and \\(v'(x)\\,dx = x\\, dx\\). Working out \\(u'(x)\\,dx\\) and \\(v(x)\\) is easy: \\(u'(x)\\, dx = -\\sin(x)\\, dx\\) and \\(v(x) = \\frac{1}{2} x^2\\). Plugging into the re-arrangement formula gives:\n\\[\\int x \\cdot \\cos(x)\\,dx = \\frac{1}{2} x^2 \\cos(x) - \\int \\frac{1}{2} x^2 \\left[\\strut - \\sin(x)\\right]\\,dx = \\frac{1}{2} x^2 \\cos(x) + \\int \\frac{1}{2} x^2  \\sin(x)\\,dx\\] Unless you know \\(\\int x^2 \\sin(x) dx\\), this re-arrangement leaves you no better off than at the beginning.\nOn the other hand … if you are in the business of compiling diaries of anti-derivatives, you could use this situation to chalk up another entry based on already knowing \\(\\int x \\cdot \\cos(x) dx\\): \\[\\int x^2 \\sin(x) dx = 2 \\int x\\cdot \\cos(x)dx - x^2\\cos(x) = 2x\\sin(x) + 2\\cos(x) - x^2 \\cos(x)\\]\n\n\n\n\n\n\n\n\n\n\nTry it! 39.5\n\n\n\n\n\n\n\n\n\nTry it! 39.5 Find \\(\\int x \\ln(x) dx\\)\n\n\n\nLet \\(u(x) = \\ln(x)\\) and \\(v'(x)dx = x dx\\).\nThen, \\(u'(x)dx = \\frac{1}{x} dx\\) and \\(v(x) = \\frac{1}{2} x^2\\).\nUsing the parts re-arrangement formula …\n\\[\\int x \\ln(x) dx = \\frac{1}{2} x^2 \\cdot \\ln(x) - \\int \\frac{1}{2} x^2\\cdot \\frac{1}{x}\\, dx \\\\\n\\frac{1}{2} x^2 \\cdot \\ln(x) - \\frac{1}{4} x^2\\] And don’t forget, after all this work, to add the constant of integration \\(C\\)!",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Symbolic anti-differentiation</span>"
    ]
  },
  {
    "objectID": "Accumulation/38-symbolic.html#sec-integration-by-parts2",
    "href": "Accumulation/38-symbolic.html#sec-integration-by-parts2",
    "title": "39  Symbolic anti-differentiation",
    "section": "39.5 Integration by parts (optional alternative presentation)",
    "text": "39.5 Integration by parts (optional alternative presentation)\nIntegration by parts applies to integrals that are recognizably of the form \\[\\int f(x) g(x) dx\\] Step 1: Split up the integrand into an \\(f(x)\\) and a \\(g(x)\\) multiplied together. That is, split the integrand into parts that are multiplied together. The way we wrote the integrand, this was trivial.\nStep 2: Pick one of \\(f(x)\\) or \\(g(x)\\). Typically, you pick the one that has a dead-easy anti-derivative. For our general description, let’s suppose this is \\(g(x)\\) which has anti-derivative \\(G(x)\\) (where we know \\(G()\\)).\nStep 3: Construct a helper function \\(h(x) \\equiv f(x) G(x)\\). This requires no work, since we’ve already identified \\(f(x)\\) and \\(G(x)\\) in step (2).\nStep 4: Find \\(\\partial_x h(x)\\). It is always easy to find derivatives, and here we just use the product rule: \\[\\partial_x h(x) = \\partial_x f(x) \\cdot G(x) + f(x)\\cdot\\partial_x G(x)\\] We know from the way we constructed \\(G(x)\\) that \\(\\partial_x G(x) = g(x)\\), so the equation is \\[\\partial_x h(x) = \\partial_x f(x) \\cdot G(x) + f(x)\\cdot g(x)\\]\nStep 5: Anti-differentiate both sides of the previous equation. From the fundamental theorem of calculus, we know how to do the left side of the equation. \\[\\int \\partial_x h(x) = h(x) \\equiv f(x)g(x)\\] The right side of the equation has two parts: \\[\\int \\left[{\\large\\strut}\\partial_x f(x) \\cdot G(x) + f(x)\\cdot g(x)\\right]dx = \\underbrace{\\int \\partial_x f(x) \\cdot G(x) dx}_\\text{Some NEW integral!}\\ \\ \\ \\  + \\underbrace{\\int f(x) g(x) dx}_\\text{The original integral we sought!}\\] Putting together the left and right sides of the equation, and re-arranging gives us a new expression for the original integral we sought to calculate: \\[\\text{Integration by parts re-arrangement}\\\\\\underbrace{\\int f(x) g(x) dx}_\\text{The original integral we sought.} = \\underbrace{f(x) g(x)}_\\text{We know this!}  - \\underbrace{\\int \\partial_x f(x) \\cdot G(x) dx}_\\text{Some NEW integral!}\\] It may seem that we haven’t accomplished much with this re-organization. But we have done something. We took a problem we didn’t otherwise know how to solve (that is \\(\\int f(x) g(x) dx\\)) and broke it down into two parts. One is very simple. The other is an integral. If we are clever in picking \\(g()\\) and lucky, we will be able to figure out the new integral and, thereby, we will have computed the original integral. But everything depends on cleverness and luck!\n\n\n\n\n\n\n\n\nTry it! 39.6\n\n\n\n\n\n\n\n\n\nTry it! 39.6 Find \\(\\int x \\cos(x) dx\\).\n\n\n\nAn obvious choice for the two parts is \\(x\\) and \\(\\cos(x)\\). But which one to call \\(g(x)\\). We will just guess and say \\(g(x)\\equiv \\cos(x)\\) which implies \\(G(x) = \\sin(x)\\). The helper function is \\(h(x) \\equiv f(x) G(x) = x \\sin(x)\\).\nDifferentiating \\(h(x)\\) can be done by the product rule. \\[\\partial_x h(x) = \\sin(x) + x \\cos(x)\\ .\\] Now anti-differentiate both sides of the above, the left side by the fundamental theorem of algebra and the right side by other means: \\[\\int \\partial_x h(x) = h(x) = x \\sin(x)= \\underbrace{\\int\\sin(x)dx}_{-\\cos(x)} + \\underbrace{\\int x \\cos(x) dx}_\\text{The original integral}\\] Re-arranging gives the answer \\[\\underbrace{\\int x \\cos(x) dx}_\\text{The original integral} = x \\sin(x) + \\cos(x) + C\\] The constant of integration \\(C\\) needs to be included to make the equality true.\nTo confirm the result, you can differentiate the right-hand side; differentiation is always easy.\nAlternatively, we can check numerically if \\(\\int x \\cos(x) dx - (x\\sin(x)+cos(x))\\) is a constant. (See @fig-check-constant).)\n\n\n\nActive R chunk 39.1\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nThe numerical check from Active R chunk 39.1 shows \\(\\\\int x \\\\cos(x) dx - (x\\\\sin(x)+cos(x))\\) is a constant.\n\n\n\n\n\n\n\n\n\n\nTry it! 39.7\n\n\n\n\n\n\n\n\n\nTry it! 39.7 Find \\(\\int \\ln(x) dx\\).\n\n\n\nThe easy solution is to recognize that the anti-derivative of \\(\\ln(x)\\) is contained in the table at the top of the chapter. But let’s try doing it by parts as an example (and to show you how it got into the table in the first place).\nIt is hard to see a separate \\(f(x)\\) and \\(g(x)\\) in the integrand \\(\\ln(x)\\). But sometimes you need to be clever. We will set \\(f(x) \\equiv \\ln(x)\\) and \\(g(x) \\equiv 1\\). This means that \\(G(x) = x\\). The helper function is therefore \\(h(x) = x\\ln(x)\\)\nDifferentiating the helper function gives (by the product rule): \\(\\partial_x h(x) = \\ln(x) + x \\frac{1}{x} = \\ln(x) + 1\\)\nIntegrating the differentiated helper function, we find \\[\\int \\partial_x h(x) dx = f(x)g(x) = x \\ln(x) = \\underbrace{\\int \\ln(x) dx}_\\text{The original integral} + \\underbrace{\\int 1 dx}_{x}\\] Re-arranging, we have \\[\\underbrace{\\int \\ln(x) dx}_\\text{The original integral} = x \\ln(x) - x\\ \\  =\\ \\  x\\left[\\strut \\ln(x) - 1\\right]\\]\n\n\n\n\n\n\n\n\n\n\nTry it! 39.8\n\n\n\n\n\n\n\n\n\nTry it! 39.8 Find \\(\\int \\sin(x) e^x dx\\).\n\n\n\nThis isn’t the integral of a pattern book or basic modeling function, and substitution didn’t work, so we try integration by parts.\nThe obvious choice for the two parts is \\(\\sin(x)\\) and \\(e^x\\). Both are really easy to anti-differentiate. Let’s choose \\(g(x) = \\sin(x)\\), giving \\(G(x) = -\\cos(x)\\). The re-arrangement of the original integral will be \\[\\sin(x) e^x + \\int \\cos(x) e^x dx\\] The new integral that we need to compute does not look any friendlier than the original, but who knows? we will do \\(\\int cos(x) e^x dx\\) by parts as well and keep our fingers crossed. That integral turns out to be \\[\\int \\cos(x) e^x dx = \\cos(x) e^x - \\int \\sin(x) e^x dx\\] This may look like we are going in circles, and maybe we are, but let’s put everything together. \\[\\underbrace{\\int \\sin(x) e^x dx}_\\text{The original problem} = \\underbrace{\\sin(x) e^x + \\cos(x) e^x}_\\text{Easy stuff!}\\ \\ \\  - \\underbrace{\\int \\sin(x) e^x dx}_\\text{Also the original problem}\\] Rearranging gives \\[\\int \\sin(x) e^x dx = \\frac{\\sin(x) e^x + \\cos(x) e^x}{2} = \\frac{e^x}{2}\\left[{\\large\\strut} \\sin(x) + \\cos(x)\\right]\\] And don’t forget the constant of integration.\n\n\n[The presentation of integration by parts in this section was formulated by Prof. Michael Brilleslyper.]",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Symbolic anti-differentiation</span>"
    ]
  },
  {
    "objectID": "Accumulation/38-symbolic.html#didnt-work",
    "href": "Accumulation/38-symbolic.html#didnt-work",
    "title": "39  Symbolic anti-differentiation",
    "section": "39.6 Didn’t work?",
    "text": "39.6 Didn’t work?\nIf integration by parts does not work … and it does not always work! … there is a variety of possibilities such as asking a math professor (who has a much larger set of functions at hand than you), looking through a table of integrals (which is to say, the collective calculus diary of generations of math professors), using a computer algebra system, or using numerical integration. One of these will work.\nIf you have difficulty using u-substitution or integration by parts, you will be in the same league as the vast majority of calculus students. Think of your fellow students who master the topic in the way you think of ice dancers. It is beautiful to watch, but you need a special talent and it hardly solves every problem. People who would fall on their face if strapped to a pair of skates have nonetheless made huge contributions in technical fields, even those that involve ice.\nProf. Kaplan once had a heart-to-heart with a 2009 Nobel-prize winner who confessed to always feeling bad and inadequate as a scientist because he had not done well in introductory calculus. It was only when he was nominated for the Nobel that he felt comfortable admitting to his “failure.” Even if you don’t master u-substitution or integration by parts, remember that you can integrate any function using easily accessible resources.",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Symbolic anti-differentiation</span>"
    ]
  },
  {
    "objectID": "Accumulation/38-symbolic.html#integrating-polynomials",
    "href": "Accumulation/38-symbolic.html#integrating-polynomials",
    "title": "39  Symbolic anti-differentiation",
    "section": "39.7 Integrating polynomials",
    "text": "39.7 Integrating polynomials\nOne of the most famous settings for integration comes from the physics of free fall under gravity.\nHere’s the setting. An object—a ball, let’s imagine—is being held at height \\(x_0\\). At \\(t=0\\) the ball is released. Perhaps the ball is released from a standstill in which case it is velocity at release is \\(v_0 = v(t=0) =0\\). Or perhaps the ball has been tossed upward so that \\(v_0 &gt; 0\\), or downward so that \\(v_0 &lt; 0\\). Whichever it is, the initial velocity will be labelled \\(v_0\\).\nOn release, the force that held the ball steady is removed and the object moves under the influence of only one factor: gravity. The effect of gravity near the Earth’s surface is easy to describe: it accelerates the object at a constant rate of about 9.8 m/s\\(^2\\).\nAcceleration is the derivative with respect to time of velocity. Since we know acceleration, to find velocity we find an anti-derivative of acceleration: \\[v(t) = \\int -9.8\\ dt = -9.8\\ t + C\\] The constant of integration \\(C\\) is not just a formality. It has physical meaning. In this case, we see that \\(C=v(0)\\), that is, \\(C = v_0\\).\nVelocity is the derivative of position: height in this case. So height is an anti-derivative of velocity. \\[x(t) = \\int v(t) dt = \\int \\left[\\strut -9.8\\ t + v_0\\right]dt = - \\frac{9.8}{2} t^2 + v_0\\ t + C\\] Why is \\(C\\) back again? it is a convention to use \\(C\\) to denote the constant of integration. Those experienced with this convention know, from context, that the value of \\(C\\) in the integration that produced \\(v(t)\\) has nothing to do with the value of \\(C\\) involved in the production of \\(x(t)\\). The situation is a bit like the presentation of prices in US stores: to the price of the item itself, you must always add “plus taxes.” Nobody with experience would assume that “taxes” is always the same number. It depends on the price and type of the item itself.4 You won’t have to deal with the taxes at the time you pick the item from the shelf, but eventually you will see them when you check out of the store. Think of \\(+\\ C\\) as meaning, “plus some number that we will have to deal with at some point, but not until checkout.”\nLet’s checkout the function \\(x(t)\\) now. For that, we need to figure out the value of \\(C\\). We can do that by noticing that \\(x(0) = C\\). So in the anti-differentiation producing \\(x()\\), \\(C = x_0\\) giving, altogether the formula for free-fall famous from physics classes \\[x(t) =  - \\frac{9.8}{2} t^2 + v_0\\ t + x_0\\] An important thing to notice about \\(x(t)\\): it is a polynomial in \\(t\\). Polynomials can be birthed by successive anti-differentiations of a constant. At each anti-differentiation, each of the previous terms is promoted by one order. That is, the previous constant becomes the first order term. The previous first-order term becomes the second order term, with the division by 2 familiar from anti-differentiating \\(t\\). A previous second-order term will become the new third-order term, again with the division by 3 familiar from anti-differentiating \\(t^2\\).\nStated generally, the anti-derivative of a polynomial is\n\\[{\\Large\\int} \\left[\\strut \\underbrace{a + b t + ct^2 + \\ldots}_\\text{original polynomial}\\right] dt = \\underbrace{C + a\\,t + \\frac{b}{2} t^2 + \\frac{c}{3} t^3 + \\ldots}_\\text{new polynomial}\\] By use of the symbol \\(C\\), it is easy to spot how the constant of integration fits in with the new polynomial. But if we were to anti-differentiate the new polynomial, we had better replace \\(C\\) with some more specific symbol to that we don’t confuse the old \\(C\\) with the one that is going to be produced in the new anti-differentiation.\n\n\n\n\n\n\nTip\n\n\n\nIn exercise 26.16, we introduced a Taylor polynomial approximation to the gaussian function. That might have seemed like a mere exercise in high-order differentiation at the time, but there is something more important at work.\nThe gaussian is one of those functions for which the anti-derivative cannot be written exactly using what the mathematicians call “elementary functions.” (See Section 39.1.) Yet integrals of the gaussian are very commonly used in science, especially in statistics where the gaussian is called the normal PDF.\nThe approach we’ve taken in this book is simply to give a name and a computer implementation of the anti-derivative of the gaussian. This is the function we’ve called \\(\\pnorm()\\) with the R computer implementation pnorm().\nWe never told you the algorithm contained in pnorm(). Nor do we really need to. We all depend on experts and specialists to design and build the computers we use. The same is true of software implementation of functions like pnorm(). And for that matter, for implementations of functions like exp(), log(), sin(), and so on. You don’t have to know about semi-conductors to use a computer productively, and you don’t need to know about numerical algorithms to use those functions.\nOne feasible algorithm for implementing \\(\\pnorm()\\) is to integrate the Taylor polynomial. It is very easy integrate polynomials. To ensure accuracy, different Taylor polynomials can be computed for different centers, say \\(x=0\\), \\(x=1\\), \\(x=2\\), and so on.\nAnother feasible approach integrates \\(\\dnorm()\\) numerically using an advanced algorithm such as Gauss-Hermite quadrature.",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Symbolic anti-differentiation</span>"
    ]
  },
  {
    "objectID": "Accumulation/38-symbolic.html#footnotes",
    "href": "Accumulation/38-symbolic.html#footnotes",
    "title": "39  Symbolic anti-differentiation",
    "section": "",
    "text": "One small deviation from the pattern-book functions is \\(\\int \\frac{dx}{x} = \\ln(|x|)\\). The absolute value \\(|x|\\) in \\(\\ln(|x|)\\) reflects the differing domains of the functions \\(\\ln(x)\\) and \\(1/x\\). Logarithms are defined only the positive half of the number line, while the reciprocal function \\(1/x\\) is defined for all non-zero \\(x\\). Including the absolute value in the argument to log covers situations such as \\(\\int_{-2}^{-1} \\frac{dx}{x}\\) which has the value \\(\\ln(2)\\).↩︎\nMathematicians have a list that is a bit longer than our pattern-book functions—they call them elementary functions and include the tangent and other trig functions and their inverses, as well as what are called “hyperbolic functions” and their inverses.↩︎\nAgain, mathematicians prefer to refer to the “elementary functions” rather than the pattern-book functions. \\(\\dnorm()\\) and \\(\\pnorm()\\) are not elementary functions, and there are several elementary function that we don’t include in the pattern-book list.↩︎\nMany jurisdictions tax food and clothing, etc. at a different rate than other items.↩︎",
    "crumbs": [
      "BLOCK IV. Accumulation",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Symbolic anti-differentiation</span>"
    ]
  },
  {
    "objectID": "dynamics-part.html",
    "href": "dynamics-part.html",
    "title": "BLOCK V. Dynamics",
    "section": "",
    "text": "Often we are interested in real-world systems that change in time, for instance the weather, or the population of a species, or the motion of an object through space. The mathematics relating to changes in time is called dynamics. In this Block, we will explore how to model dynamical systems—systems that change in time—by functions that themselves are static. This paradox of using an unchanging function to describe a changing state can be a boon to the modeler, allowing mathematically simple systems to create rich and complicated phenomena.",
    "crumbs": [
      "BLOCK V. Dynamics"
    ]
  },
  {
    "objectID": "Dynamics/39-diff-eq.html",
    "href": "Dynamics/39-diff-eq.html",
    "title": "40  Differential equations",
    "section": "",
    "text": "40.1 Dynamical systems\nIn this Block, we take on what an important application of derivatives: the representation of dynamical systems.\n“Dynamical systems” (but not under that name) were developed initially in the 1600s to relate planetary motion to the force of gravity. Nowadays, they are used to describe all sorts of physical systems from oscillations in electrical circuits to the ecology of interacting species to the spread of contagious disease.\nAs examples of dynamical systems, consider a ball thrown thrown through the air or a rocket being launched to deploy a satellite. At each instant of time, a ball has a position—a point in \\(x,y,z\\) space—and a velocity \\(\\partial_t x\\), \\(\\partial_t y\\), and \\(\\partial_t z\\). These six quantities, and perhaps others like spin, constitute the instantaneous state of the ball. Rockets have additional components of state, for example the mass of the fuel remaining.\nThe “dynamical” in “dynamical systems” refers to the change in state. For the ball, the state changes under the influence of mechanisms such as gravity and air resistance. The mathematical representation of a dynamical system codifies how the state changes as a function of the instantaneous state. For example, if the instantaneous state is a single quantity called \\(x\\), the instantaneous change in state is the derivative of that quantity: \\(\\partial_t x\\).\nTo say that \\(x\\) changes in time is to say that \\(x\\) is a function of time: \\(x(t)\\). When we write \\(x\\), we mean \\(x()\\) evaluated at an instant. When we write \\(\\partial_t x\\), we mean “the derivative of \\(x(t)\\) with respect to time” evaluated at the same instant as for \\(x\\).\nThe dynamical system describing the motion of \\(x\\) is written in the form of a differential equation, like this:\n\\[\\partial_t x = f(x)\\ .\\] Notice that the function \\(f()\\) is directly a function of \\(x\\), not \\(t\\). This is very different from the situation we studied in Block 3, where we might have written \\(\\partial_t y = \\cos\\left(\\frac{2\\pi}{P} t\\right)\\) and assigned you the challenge of finding the function \\(y(t)\\) by anti-differentiation. (The answer to the anti-differentiation problem, of course, is \\(y(t) = \\frac{P}{2\\pi}\\sin\\left(\\frac{2\\pi}{P} t\\right) + C\\).)\nDynamical systems with multiple state quantities are written mathematically as sets of differential equations, for instance: \\[\\partial_t y = g(y, z)\\\\\n\\partial_t z = h(y, z)\\] We typically use the word system rather than “set,” so a dynamical system is represented by a system of differential equations.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Differential equations</span>"
    ]
  },
  {
    "objectID": "Dynamics/39-diff-eq.html#dynamical-systems",
    "href": "Dynamics/39-diff-eq.html#dynamical-systems",
    "title": "40  Differential equations",
    "section": "",
    "text": "Tip\n\n\n\nIt is essential that you train yourself to distinguish two very different statements\n\nanti-differentiation problems like \\(\\partial_{\\color{blue}{t}} y = g(\\color{blue}{t})\\), which has \\(t\\) as both the with-respect-to variable and as the argument to the function \\(g()\\).\n\nand\n\ndynamical systems like \\[\\partial_{\\color{blue}{t}} \\color{magenta}{y} = g(\\color{magenta}{y})\\ .\\]\n\nThis is one place where Leibniz’s notation for derivatives can be useful: \\[\\underbrace{\\frac{d\\color{magenta}{y}}{d\\color{blue}{t}} = g(\\color{blue}{t})}_{\\text{as in antidifferentiation}}\\ \\ \\ \\text{versus}\\ \\ \\ \\underbrace{\\frac{d\\color{magenta}{y}}{d\\color{blue}{t}} = g(\\color{magenta}{y})}_{\\text{dynamical system}}\\]\n\n\n\n\nApplication area 40.1  \n\n\n\n\n\n\n\nApplication area 40.1 Dynamics and board games\n\n\n\nLet’s illustrate the idea of a dynamical system with a children’s game: “Chutes and Ladders”. Since hardly any children have studied calculus, the game isn’t presented as differential equations, but as a simple board and the rules for the movement along the board.\n\n\n\n\n\n\nFigure 40.1: The game of Chutes and Ladders\n\n\n\nA player’s state in this game is shown by the position of a token, but we will define the state to be the number of the square that the player’s token is on. In Chutes and Ladders the state is one of the integers from 1 to 100. In contrast, the dynamical systems that we will study with calculus have a state that is a point on the number line, or in the coordinate plane, or higher-dimensional space. Our calculus dynamical system describe the change of state using derivatives with respect to time, whereas in chutes and ladders the state jumps from one value to the next value.\nThe game board displays not only the set of possible states but also the rule for changing state jumping from one state to another.\nIn the real game, players roll a die to determine how many steps to take to the next state. But we will play a simpler game: Just move one step forward on each turn, except … from place to place there are ladders that connect two squares. When the state reaches a square holding the foot of a ladder, the state is swept up to the higher-numbered square at the top of the ladder. Similarly, there are chutes. These work much like the ladders but carry the state from a higher-numbered square to a lower-numbered square.\nThe small drawings on the board are not part of the action of the game. Rather, they represent the idea that good deeds lead the player to progress, while wrong-doing produces regression. Thus, the productive gardener in square 1 is rewarded by being moved upward to the harvest in square 38. In square 64 a brat is pulling on his sister’s braids. This misdeed results in punishment: he is moved back to square 60.\nOur dice-free version of Chutes and Ladders is an example of a discrete-time, discrete-state dynamical system. Since there is no randomness involved, the movement of the state is deterministic. (With dice, the movement would be stochastic.)\nThe differential equations of a dynamical system correspond to a continuous-time, continuous-space system. This continuity is the reason we use derivatives to describe the motion of the state. The movement in the systems we will explore is also deterministic.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Differential equations</span>"
    ]
  },
  {
    "objectID": "Dynamics/39-diff-eq.html#state",
    "href": "Dynamics/39-diff-eq.html#state",
    "title": "40  Differential equations",
    "section": "40.2 State",
    "text": "40.2 State\nThe mathematical language of differential equations and dynamical systems is able to describe a stunning range of systems, for example:\n\nphysics\n\nswing of a pendulum\nbobbing of a mass hanging from a spring.\na rocket shooting up from the launch pad\n\ncommerce\n\ninvestment growth\ngrowth in equity in a house as a mortgage is paid up. (“Equity” is the amount of the value of the house that belongs to you.)\n\nbiology\n\ngrowth of animal populations, including predator and prey.\nspread of an infectious disease\ngrowth of an organism or a crop.\n\n\nAll these systems involve a state that describes the configuration of the system at a given instant in time. For the growth of a crop, the state would be, say, the amount of biomass per unit area. For the spread of infectious disease, the state would be the fraction of people who are infectious and the fraction who are susceptible to infection. “State” in this sense is used in the sense of “the state of affairs,” or “his mental state,” or “the state of their finances.”\nSince we are interested in how the state changes over time, sometimes we refer to it as the dynamical state.\nOne of the things you learn when you study a field such as physics or epidemiology or engineering is what constitutes a useful description of the dynamical state for different situations. In the crop and infectious disease examples above, the state mentioned is a strong simplification of reality: a model. Often, the modeling cycle leads the modeler to include more components to the state. For instance, some models of crop growth include the density of crop-eating insects. For infectious disease, a model might include the fraction of people who are incubating the disease but not yet contagious.\nConsider the relatively simple physical system of a pendulum, swinging back and forth under the influence of gravity. In physics, you learn the essential dynamical elements of the pendulum system: the current angle the pendulum makes to the vertical, and the rate at which that angle changes. There are also fixed elements of the system, for instance the length of the pendulum’s rod and the local gravitational acceleration. Although such fixed characteristics may be important in describing the system, they are not elements of the dynamical state. Instead, they might appear as parameters in the functions on the right-hand side of the differential equations.\nTo be complete, the dynamical state of a system has to include all those changing aspects of the system that allow you to calculate from the state at this instant what the state will be at the next instant. For example, the angle of the pendulum at an instant tells you a lot about what the angle will be at the next instant, but not everything. You also need to know which way the pendulum is swinging and how fast.\nFiguring out what constitutes the dynamical state requires knowledge of the mechanics of the system, e.g. the action of gravity, the constraint imposed by the pivot of the pendulum. You get that knowledge by studying the relevant field: electrical engineering, economics, epidemiology, etc. You also learn what aspects of the system are fixed or change slowly enough that they can be considered fixed. (Sometimes you find out that something your intuition tells you is important to the dynamics is, in fact, not. An example is the mass of the pendulum.)",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Differential equations</span>"
    ]
  },
  {
    "objectID": "Dynamics/39-diff-eq.html#state-space",
    "href": "Dynamics/39-diff-eq.html#state-space",
    "title": "40  Differential equations",
    "section": "40.3 State space",
    "text": "40.3 State space\nThe state of a dynamical system tells you the configuration of the system at any instant in time. It is appropriate to think about the instantaneous state as a single point in a state space, a coordinate system with an axis for each component of state. As the system configuration changes with time—say, the pendulum loses velocity as it swings to the left—the instantaneous state moves along a path in the state space. Such a path is called a trajectory of the dynamical system.\nIn this book, we will work almost exclusively with systems that have a one- or two-dimensional state. Consequently, the state space will be either the number line or the coordinate plane. The methods you learn will be broadly applicable to systems with higher-dimensional state.\nFor the deterministic dynamical systems we will be working with, a basic principle is that a trajectory can never cross itself. This can be demonstrated by contradiction. Suppose a trajectory did cross itself. This would mean that the motion from the crossing point couple possibly go in either of two directions; the state might follow one branch of the cross or the other. Such a system would not be deterministic. Determinism implies that from each point in state space the flow goes only in one direction.\nThe dimension of the state space is the same as the number of components of the state; one axis of state space for every component of the state. has important implications for the type of motion that can exist.\n\nIf the state space is one-dimensional, the state as a function of time must be monotonic. Otherwise, the trajectory would cross itself, which is not permitted.\nA state space that is two- or higher-dimensional can support motion that oscillates back and forth. Such a trajectory does not cross itself, instead it goes round and round in a spiral or a closed loop.\n\nFor many decades, it was assumed that all dynamical systems produce either monotonic behavior or spiral or loop behavior. In the 1960s, scientists working on a highly simplified model of the atmosphere discovered numerically that there is a third type of behavior, the irregular and practically unpredictable behavior called chaos. To display chaos, the state space of the system must have at least three elements.\n\nApplication area 40.2 —Describing mathematically the dynamics of an epidemic.\n\n\n\n\n\n\n\nApplication area 40.2 The state of COVID\n\n\n\nWhat does it take to describe the dynamical state of an epidemic?\nNews reports of the COVID pandemic usually focus on the number of new cases each day and the fraction of the population that has been vaccinated. But this is not adequate, even for a simple description of the dynamics.\nFrom a history of new-case counts over time (e.g. Figure 40.2) you can see that the number of new cases waxes and wanes. Knowing that the number of cases today is, say, 100 thousand does not tell you what the number of cases will be in two weeks: 100 thousand is encountered both on the way up and on the way down.\n\n\n\n\n\nCOVID-19 new-case counts in the US over the first two years of the pandemic. [Source: [*New York Times*]](https://www.nytimes.com/interactive/2021/us/covid-cases.html)]\n:::\n\n\n:::\n\n## Dynamics\n\nThe ***dynamics*** of a system is a description of how the individual components of the *state* change as a *function of the entire set of components of the state.* \n\nAt any instant in time, the state is a set of quantities. We will use $x$, $y$, and $z$ for the purpose of illustration, although most of our work in this introduction will be with systems that have just one or two state variables.\n\nThe differential equations describing the $x, y, z$ system have a particular form:\n\n$$\\partial_t x(t) = f(x(t), y(t), z(t))\\ \\, \\\\\n\\partial_t y(t) = g(x(t), y(t), z(t))\\ \\, \\\\\n\\partial_t z(t) = h(x(t), y(t), z(t))\\ .$$\n\nThe way these equations are written is practically impossible to read: the expression $(t)$ is repeated 12 times! It takes concentration to look beyond the $(t)$ to see the overall structure of the equations. to avoid this problem of not seeing the forest for the $(t)$s, the convention is to omit the $(t)$:\n$$\\partial_t x = f(x, y, z)\\\\\n\\partial_t y = g(x, y, z)\\\\\n\\partial_t z = h(x, y, z)$$\nThis leaves it to the reader to remember that $x$ is really $x(t)$ and so on.\n\nThis more concise way of writing the differential equations makes it easier to describe how to interpret the equations. Formally, $\\partial_t x$ is a function, the derivative of the function $x(t)$ with respect to time. But try to put this highly literal interpretation on a back burner. Think of the expression $\\partial_t x =$ as meaning, \"the way the $x$-component of state changes in time is described by ....\" We need three differential equations because there are three components of state in the $x,y,z$ system, and we need to describe for each component the way that component changes.\n\nOn the right side of each equation is a function that takes the state quantities as inputs. Each individual equation can be interpreted as completing the elliptical sentence (that is, ending in \"...\") in the previous paragraph, so that the whole equation reads like, \"The way the $x$-component of state changes at any instant in time is specified by the function $h()$ evaluated at the instantaneous state.\" These functions are called ***dynamical functions*** since they give the rules for the dynamics.\n\nRemember that $x$, $y$, and $z$ are state variables, so they are all *functions of time*. At any instant in time, the values $x$, $y$, $z$ have a specific value. Thus, at any instant in time, evaluating the functions $f(x, y, z)$, $g(x, y, z)$, and $h(x, y, z)$  at the current state produces a specific, scalar value. If we wanted to make this perfectly explicit, we could write $g_x(x(t), y(t), z(t))$, which makes it clear that the output of $g_x()$ is a function of time.\n\n\n::: {.callout-tip}\nMathematically, a ***dynamical system*** consists of two things:\n\ni. The state variables, which is a set of quantities that vary in time.\nii. The dynamics, which is the set of dynamical functions, one function for each of the state variables.\n\n:::\n\n\nA simple example is the dynamics of retirement-account interest. In a retirement account, you put aside money---this is called \"contributing\"---each month. The value $V(t)$ of the account accumulates over time, both due to new monthly deposits and to the interest $r$ earned on the current account value. If you are setting aside $M$ dollars per month, the dynamics are:\n$$\\partial_t V = r V + M\\ .$$\nThe left-hand side of this equation is boilerplate for \"the way the $V$ component of state changes is described by the dynamical function $rV + M$.\" This is a function of $V$ with parameters $r$ and $M$.  In this example, there is just the one state variable $V$, so the dynamical function has only one argument: $V$. \n\nRemember that the dynamical function is something that the modeler constructs from her knowledge of the system. To model the dynamics of a pendulum requires some knowledge of physics. Without getting involved with the physics, we note that the oscillatory nature of pendulum movement means that there must be at least two state variables. A physicist learns that a good way to describe the motion uses these two quantities: the angle $\\theta(t)$ of the pendulum rod with respect to the vertical and the angular velocity $v(t)$ telling how the velocity changes with time. Since there are two state variables, there must be two dynamical functions. For a pendulum, one of the functions, the one for $\\partial_t v$ comes from applying Newton's Second Law: $F = m a$. (Remember that $\\partial_t v$ is an acceleration.) So one of the differential equations is $$\\partial_t v  =  f(\\theta, v) \\equiv - \\sin(\\theta)$$\n\n\nThe other equation comes from the definition that the derivative of the position $\\theta$ is the velocity.\n$$\\partial_t \\theta  =  g(\\theta, v) \\equiv  v\\\\\n$$\n\n::: {.callout-note icon=false data-latex=\"\"}\n## Why?\n\nWhy define a state variable $v$ when it is, by definition, the same as $\\partial_t \\theta(t)$?\n\nEven though the dynamical equation $\\partial_t \\theta(t) = v$ is a calculus tautology, we need always to be explicit about what are the two quantities in the dynamical state. The $\\partial_t \\theta$ differential equation comes for free from basic calculus concepts. The second equation is about the physics, that is, the relationship between forces and acceleration.\n\nThere is a style of writing dynamics equations that discards such tautologies. For example, the pendulum dynamics are often written $$\\partial_{tt} \\theta(t) = - \\sin(\\theta)\\ .$$ This sort of equation, containing a second-order derivative, is called a ***second-order differential equation***. In contrast, the two equations, one for $\\partial_t \\theta$ and one for $\\partial_t v$ are called  ***first-order differential equations*** because each involves a first-order derivative. We will return to this second-order style in @sec-second-order-de since it is often encountered in physics and engineering. For now, we are avoiding the second-order style because it obscures the fact that there are two state variables: $\\theta(t)$ and $v(t)$.\n:::\n\n::: {#thm-interacting-species style=\"display: none;\"}\n:::\n::: {.callout-note icon=false}\n## @thm-interacting-species Interacting species\nConsider the population of two interacting species, say rabbits and foxes. As you know, the relationship between rabbits and foxes is rather unhappy from the rabbits' point of view even if it is fulfilling for the foxes.\n\nMany people assume that such populations are more or less fixed: that the rabbits are in a steady balance with the foxes. In fact, as any gardener can tell you, some years there are lots of rabbits and others not: an oscillation. Just from this fact, we know that the dynamical state must have at least two components. \n\nIn a simple, but informative, model, the two components of the dynamical state are $r(t)$ and $f(t)$, the population of rabbits and foxes respectively. In the absence of foxes, the dynamics of rabbits are exponential growth; each successive generation is larger than the previous one. This can be described by a dynamical equation $\\partial_t r(t) = \\alpha r(t)$, where $\\alpha$ is a fixed quantity that describes rabbit fecundity.\n\nSimilarly, in the absence of food (rabbits are fox food), the foxes will starve or emigrate, so the dynamical equation for foxes is very similar $\\partial_t f(t) = - \\gamma f(t)$, where $\\gamma$ is a fixed quantity that indicates the rate at which foxes die or emigrate.\n\nOf course, in real ecosystems there are many other quantities that change and that are relevant. For instance, foxes eat not only rabbits, but birds and frogs and earthworms and berries. And the diet of rabbits eat weeds and grass (which is generally in plentiful supply), but also the gardener's flowers and carrots (and other vegetables). Growth in the rabbit population leads to decrease in available flowers and vegetables, which in turn leads to slower growth (or even population decline) for rabbits.\n\nIn the spirit of illustrating dynamics, we will leave out these important complexities and imagine that the state consists of just two numbers: how many rabbits there are and how many foxes. The dynamics therefore involve two equations, one for $\\partial_t r$ and one for $\\partial_t f$. For the rabbit/fox model, we will allow the rabbit population change ($\\partial_t r$) to be affected by fox prediation and similarly let the fox population change ($\\partial_t f$) reflect the consumption of rabbits as food, writing:\n$$\\partial_t r = \\ \\ \\ \\ \\ \\alpha\\, r - \\overbrace{\\beta\\, f r}^{\\text{fox predation}}\\\\\n\\partial_t f = \\underbrace{\\delta\\, r f}_{\\text{rabbits as food}} - \\gamma\\, f$$ \n\nThe quantities $\\alpha$, $\\beta$, $\\gamma$, and $\\delta$ are parameters quantify the biology of the system: the reproduction rate of rabbits, the need of foxes for food (rabbits) to reproduce, the hunting success of foxes, and the death or emigration of foxes in response to a shortage of food.\n\nHow are you supposed to know that $r$ and $f$ are state variables while quantities like $\\beta$ and $\\gamma$ are parameters? Because there is a differential equation involving $\\partial_t r$ and $\\partial_t f$, while no differential equation has been given describing $\\partial_t \\beta$ or $\\partial_t \\alpha$.\n\nComing up with this description of dynamics requires knowing something about rabbits and foxes. The particular forms used, for instance the ***interaction term*** $r f$, come from modeling experience. The interaction term is well named because it is about the literal, biological interaction of foxes and rabbits. \n:::\n\n## State space and flow field\n\nFor the purpose of developing intuition it is helpful to represent the instantaneous state as a point in a graphical frame and the dynamics as a field of vectors showing how, for each possible state, the state changes. For instance, in the Rabbit-Fox dynamics, the state is the pair $(r, f)$ and the ***state space*** is the coordinate plane spanned by $r$ and $f$.\n\nThe present state of the system might be any point in the state space. But once we know the present state, the dynamical functions *evaluated at the present state* tell us how the state changes over a small increment in time. The step over a small increment of time can be represented by a vector.\n\nLet's illustrate with the Rabbit-Fox system, whose dynamical equations are given above. The dynamical functions take a position in state space as input. Each of the functions returns a scalar. \n\nTo make a plot, we need numerical values for all the parameters in those equations. \n\nThe vector field corresponding to the dynamics is called a ***flow***, as if it were a pool of swirling water. @fig-rabbit-fox-vectors shows the flow of the rabbit/fox system.\n\n::: {#fig-rabbit-fox-vectors}\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](39-diff-eq_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=90%}\n:::\n:::\n\nThe dynamics of the rabbit/fox system shown as a vector field over the state space. The parameters have been set, for the purpose of illustration, to $\\\\alpha = 2/3$, $\\\\beta = 4/3$, $\\\\gamma = 1$, and $\\\\delta = 1$.\n:::\n\n\nStaying with the analogy to a pool of swirling water or the currents in a river, you can place a lightweight marker such as a leaf at some point in the flow and follow its path over time. This path---position in state space as a function of time---is called the ***trajectory*** of the flow. There are many possible trajectories, depending on where you place the leaf.\n\n@sec-splines considered the path followed by a robot arm, separating out the $x$- and $y$-components of the arm's position over time and calling them functions $x(t)$ and $y(t)$. Analogously, the decomposition of a trajectory from an initial condition in the flow---this would be $r(t)$ and $f(t)$ for the rabbit/fox system---gives us the ***solution*** to the differential equation. \n\nEach component of the solution is called a ***time series*** and is often plotted as a function of time, for instance $r(t)$ versus $t$.\n\nFrom the flow field, you can approximate the trajectory that will be followed from any initial condition. Starting from the ***initial condition***, just follow the flow. You already have some practice following a flow from your study of the ***gradient ascent*** method of optimization described in @sec-optim-and-shape. At the argmax, the gradient is nil. Thus, the gradient ascent method stops at the argmax. We will see an analogous behavior in dynamical systems: any place where the flow is nil is a potential resting point for the state, called a ***fixed point***. \n\n::: {#thm-pendulum-flow style=\"display: none;\"}\n:::\n::: {.callout-note icon=false}\n## @thm-pendulum-flow Flow field for a pendulum\nLet's return to the pendulum and examine its flow field. We will modify the equations just a little bit to include air resistance in the model. Air resistance is a force, so we know it will appear in the $\\partial_t v_\\theta(t)$ equation. A common model for air resistance has it proportional in size to the square of the velocity and with a direction that is the opposite of the velocity. In a differential equation, the model of air resistance can be written as $- \\alpha\\, L\\, \\text{sign}(v(t))\\ v(t)^2$, where $\\text{sign}()$ is a piecewise function that has the value $+1$ when the argument is positive and $-1$ when the argument is negative. $L$ is the length of the pendulum.\n$$\\partial_t \\theta = v\\\\\n\\partial_t v = - \\sin(\\theta) - \\alpha\\,L^2\\, \\text{sign}(v)\\ v^2$$\n(Keep in mind as always that for dynamical systems a state variable like $\\theta$ is also a function of time $\\theta(t)$.) Whenever you have a state variable, you know that it is a function of time and so the explicit $(t)$ is often omitted for the sake of conciseness.\n\n@fig-pendulum-in-air shows the flow field of the pendulum. Also shown is a trajectory and the two time series corresponding to that trajectory.\n\n::: {#fig-pendulum-in-air}\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n\n40.4 Solution containing functions theta(t), v(t).\n\n\n40.5 Warning: All aesthetics have length 1, but the data has 500 rows.\n\n\n40.6 ℹ Please consider using annotate() or provide this layer with data containing\n\n\n40.7 a single row.\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 40.2\n\n\n\nThe flow field of a pendulum with air resistance. From the initial condition (marked by \\(\\color{red}{\\text{x}}\\)), a trajectory is sketched out for \\(0 \\leq t \\leq 20\\). The individual components of that trajectory are graphed as time series.\n\n\nThe pendulum was started out by lifting it to an angle of \\(45^\\circ\\) and giving it an initial upward velocity. The bob swings up for a bit before being reversed by gravity and swinging toward \\(\\theta = 0\\) and beyond. Due to air resistance, the amplitude of swinging decreases over time.\n:::\nThe flow of a dynamical system tells how different points in state space are connected. Because movement of the state is continuous in time and the state space itself is continuous, the connections cannot be stated in the form “this point goes to that point.” Instead, as has been the case all along in calculus, we describe the movement using a “velocity” vector. Each dynamical function specifies one component of the “velocity” vector, taken together they tell the direction and speed of movement of the state at each instant in time.\nPerhaps it would be better to use the term state velocity instead of “velocity.” In physics and most aspects of everyday life, “velocity” refers to the rate of change of physical position of an object. Similarly, the state velocity tells the rate of change of the position of the state. It is a useful visualization technique to think of the state as an object skating around the state space in a manner directed by the dynamical functions. But the state space almost always includes components other than physical position. For instance, in the rabbit/fox model, the state says nothing about where individual rabbits and foxes are located in their environment; it is all about the density of animals in a region.\nIn physics, often the state space consists of position in physical state as well as the physical velocity in physical space. For instance, the state might consist of the three \\(x, y, z\\) components of physical position as well as the three \\(v_x, v_y, v_z\\) components of physical velocity. Altogether, that is a six-dimensional state space. The state velocity also has six components. Three of those components will be the “velocity of the velocity,” that is, the direction and speed with which the physical velocity is changing.\n\nApplication area 40.3  \n\n\n\n\n\n\n\nApplication area 40.3 Board-game flow\n\n\n\nReturning to the Chutes and Ladders game used as an example near the start of this chapter …\nThe state in chutes and ladders is one of the hundred numbers 1, 2, \\(\\ldots\\), 100. This is a discrete state space. Therefore, we can describe the “flow” in a very concrete way: how each state is directly connected to another. Figure 40.3 shows these connections. There is no velocity involved because there is no infinitesimal movement of state. For instance, state 47 connects directly to state 26.\n\n\n\n\n\n\nFigure 40.3: The “flow” connecting the discrete states in the dice-free Chutes and Ladders game. Source: Maj. Austin Davis\n\n\n\nIn the no-dice game, the state follows the arrows. Looking carefully at Figure 40.3, you can see that each state has a forward connection to at most one state. This is the hallmark of determinism.\nIn the children’s game, the play is not deterministic because a die is used to indicate which state follows from each other state. A die has six faces with the six numbers 1 to 6. So, each state is connected to six other states in the forward direction. Which of the six is to be followed depends on the number that comes up on the die. Multiple forward connections means the dynamics are stochastic (random).\nStraightforward examination of the flow often tells you a lot about the big picture of the system. In dice-free Chutes and Ladders, The 100 states are divided into three isolated islands. State 1 is part of the island in the lower right corner of Figure 40.3. Follow the arrows starting from any place on that island and you will eventually reach state 84. And state 84 is part of a cycle \\(84 \\rightarrow 85 \\rightarrow \\cdots \\rightarrow 28 \\rightarrow 84 \\rightarrow \\cdots\\). Once you are on that cycle, you never get off. We will see such cycles in continuous-time dynamical systems as well.\n\n\n\n\n\n\n\n\nCalculus history—Numerical weather forecasting\n\n\n\nWeather forecasting by numerical process is a highly influential book, from 1922, by Lewis Fry Richardson. He envisioned a calculation for a weather forecast as a kind of function. The domain for the forecast is the latitude and longitude of a point on the globe, rather than the rectilinear organization of corridor.\nOne fantastic illustration of the idea shows a building constructed in the form of an inside-out globe. Source At each of many points on the globe, there is a business. (You can see this most easily in the foreground, which shows several boxes of workers.)\n\n\n\n\n\n\nFigure 40.4: An artist’s depiction of the organization of calculations for weather forecasting by Richardson’s system.\n\n\n\nIn each business there is a person who will report the current air pressure at that point on the globe, another person who reports the temperature, another reporting humidity, and so on. To compute the predicted weather for the next day, the business has a staff assigned to visit the neighboring businesses to find out the pressure, temperature, humidity, etc. Still other staffers take the collected output from the neighbors and carry out the arithmetic to translate those outputs into the forecast for tomorrow. For instance, knowing the pressure at neighboring points enables the direction of wind to be calculated, thus the humidity and temperature of air coming in to and out of the region the business handles. In today’s numerical weather prediction models, the globe is divided very finely by latitude, longitude, and altitude, and software handles both the storage of present conditions and the calculation from that of the future a few minutes later. Repeating the process using the forecast enables a prediction to be made for a few minutes after that, and so on.\nSome of the most important concepts in calculus relate to the process of collecting outputs from neighboring points and combining them: for instance finding the difference or the sum. To illustrate, here is the first set of equations from Richardson’s Weather forecasting … written in the notation of calculus:\n\n\n\n\n\n\n\n\n\nYou can hardly be expected at this point to understand the calculations described by these equations, which involve the physics of air flow, the coriolis force, etc. but it is worth pointing out some of the notation:\n\nThe equations are about the momentum of a column of air at a particular latitude (\\(\\phi\\)) and longitude.\n\\(M_E\\) and \\(M_N\\) are east-west and north-south components of that momentum.\n\\(\\partial M_E /\\partial t\\) is the rate at which the east-west momentum will change in the next small interval of time (\\(\\partial t\\)).\n\\(p_G\\) is the air pressure at ground level from that column of air.\n\\(\\partial p_G / \\partial n\\) is about the difference between air pressure in the column of air and the columns to the north and south.\n\nCalculus provides both the notation for describing the physics of climate and the means to translate this physics into arithmetic calculation.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Differential equations</span>"
    ]
  },
  {
    "objectID": "Dynamics/39-diff-eq.html#solution-containing-functions-thetat-vt.",
    "href": "Dynamics/39-diff-eq.html#solution-containing-functions-thetat-vt.",
    "title": "40  Differential equations",
    "section": "40.4 Solution containing functions theta(t), v(t).",
    "text": "40.4 Solution containing functions theta(t), v(t).",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Differential equations</span>"
    ]
  },
  {
    "objectID": "Dynamics/39-diff-eq.html#warning-all-aesthetics-have-length-1-but-the-data-has-500-rows.",
    "href": "Dynamics/39-diff-eq.html#warning-all-aesthetics-have-length-1-but-the-data-has-500-rows.",
    "title": "40  Differential equations",
    "section": "40.5 Warning: All aesthetics have length 1, but the data has 500 rows.",
    "text": "40.5 Warning: All aesthetics have length 1, but the data has 500 rows.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Differential equations</span>"
    ]
  },
  {
    "objectID": "Dynamics/39-diff-eq.html#ℹ-please-consider-using-annotate-or-provide-this-layer-with-data-containing",
    "href": "Dynamics/39-diff-eq.html#ℹ-please-consider-using-annotate-or-provide-this-layer-with-data-containing",
    "title": "40  Differential equations",
    "section": "40.6 ℹ Please consider using annotate() or provide this layer with data containing",
    "text": "40.6 ℹ Please consider using annotate() or provide this layer with data containing",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Differential equations</span>"
    ]
  },
  {
    "objectID": "Dynamics/39-diff-eq.html#a-single-row.",
    "href": "Dynamics/39-diff-eq.html#a-single-row.",
    "title": "40  Differential equations",
    "section": "40.7 a single row.",
    "text": "40.7 a single row.\n```",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Differential equations</span>"
    ]
  },
  {
    "objectID": "Dynamics/40-solution.html",
    "href": "Dynamics/40-solution.html",
    "title": "41  Finding a “solution”",
    "section": "",
    "text": "41.1 The flow field\nAs you saw in the previous chapter, each differential equation in a dynamical system relates the derivative of a function to the function itself. For instance, in \\[\\partial_t x = x\\,(1-x)\\] left-hand side of the equation involves the function \\(x(t)\\). The equation dictates that whatever \\(x(t)\\) might be, it has to be such that \\(\\partial_t x(t)\\) is exactly equal to the function \\(x(t)\\,\\left(\\strut 1- x(t)\\right)\\) Solving a differential equation is the phrase used for finding such self-consistent functions. This chapter is about techniques for finding solutions.\nGiven what you learned in studying Block 3 of this book, you likely will be tempted to approach the task of “finding a solution” by applying symbolic anti-differentiation techniques to the problem. After all, each differential equation in a dynamical system involves a function \\(\\partial_t x(t)\\). To find \\(x(t)\\) from \\(\\partial_t x(t)\\) seems like a matter of applying the “fundamental theorem of calculus,” namely\n\\[\\int \\partial_t x(t) dt = x(t)\\ .\\] Following this logic, we would translate the equation to \\[x(t) = \\int x(t)\\, (1-x(t))dt\\  .\\] But the problems in Block 3 were all of the form \\(\\frac{d\\color{magenta}{x}}{d\\color{blue}{t}} = g(\\color{blue}{t})\\), whereas the problems we work with in this Block are generally of the entirely different form \\(\\frac{d\\color{magenta}{x}}{d\\color{blue}{t}} = g(\\color{magenta}{x})\\). Thus, we will usually need special techniques suited to the format of dynamical systems.\nAdmittedly, in mathematics it is common to refer to integrating a differential equation, but this should be broadly understood as accumulating the increments \\(\\partial_t x(t)\\) starting at some initial condition \\(x(t_0)\\), even if that accumulation is not carried out by symbolic anti-differentiation.\nIn this chapter we will introduce three different techniques to accumulating a solution to a differential equation or a pair of such equations. First, we will look again at the graphical method of “following the flow” in a plot of the flow field. This technique is mainly of use for developing intuition about the dynamics.\nSecond, we will develop a simple Euler method for accumulating a solution. Third, we will explore how to take a guess about the solution and, when the guess is good enough, refine that into an actual solution. This is called the method of ansätze.\nThird, and briefly, we will look at some of the situations where symbolic anti-differentiation can be used. This includes a very brief introduction to substitution methods.\nWith a pair of differential equations, as with the pendulum or the rabbit-fox model, each equation gives one component of the change in state. To draw the flow at single point in state space, evaluate the dynamical functions at that point. Each dynamical function contributes, as its output, one of the components of the state velocity vector. If the parameters in the model have been assigned numerical values, the result of evaluating the right-hand sides will be two numbers.\nA case in point is the rabbit-fox system. The axes in the rabbit-fox state space are denominated in units of rabbit density \\(r\\) and fox density \\(f\\). The differential equations are \\[\\begin{eqnarray}\n\\partial_t r & = & 0.66 r - 1.33 r f\\\\\n\\partial_t f & = & -f + rf\\\\\n\\end{eqnarray}\\]\nTo to find the state velocity at, say, \\(r=2, f=1/4\\), plug those values into the right-hand side:\n\\(\\partial_t r  = 1.33 - 0.66   = 0.66\\ \\ \\ \\)rabbit density per month\n\\(\\partial_t f  = -0.25 + 0.5 = 0.25\\ \\ \\ \\)fox density per month.\nOnce you know the numerical vector value of the state velocity, you need to convert it to a form suitable for plotting in the state space. The conversion is needed because the state space is denominated in rabbit density and fox density, not in rabbit density per month or fox density per month. The conversion is accomplished by multiplying the state velocity vector by a small \\(dt\\), say, 0.1 months.\nThe conversion produces a vector whose components are denominated in the same way as the state space and thus can be plotted meaningfully in the state space.\nTo illustrate, let’s draw a flow vector for the state space coordinate \\((r=2, f = 1/4)\\). Above, we already calculated the components of the state velocity vector;Given the value \\(\\partial_t f = 0.25\\) and \\(\\partial_t f = 0.66\\). For the sake of illustration, we will set \\(dt = 0.1\\) month. Consequently, the vector to be plotted will be \\((0.25, 0.66) dt = (0.025, 0.066)\\)$ with units of rabbit density and fox density respectively. the right. This flow arrow is drawn in Figure 41.1.\nTo draw the entire flow field, repeat this process at many other points in the state space as in Figure 41.2.\nSome people prefer a visualization of short segments of actual trajectories, as in the right panel in Figure 41.2, rather than the state velocity vector. This is a matter of personal preference.\nWith the flow field depicted in sufficient detail, you can now trace out trajectory.\nTo trace out a trajectory, select a initial condition for the system. Then follow the flow, taking only a small step in state space. The next step should be in the direction of the flow arrow at the end of the previous step.\nThe trajectory you draw will be only a sketch, but it can be effective for developing intuition. Figure 41.3 shows a semi-automated version of the go-with-the-flow method. The computer has been used to draw the arrows. When you click in the plot, the computer also undertakes calculation of the trajectory.\nRegrettably, from such a sketch of the trajectory, you cannot easily construct \\(r(t)\\) and \\(f(t)\\) for time-series plots. Also, you don’t get a sense of how slow or fast the flow is going. Click at different initial conditions in the flow and you will see different trajectories, each of which is a closed loop, the sort of cycles seen in the dice-free Chutes and Ladders game. But the shape of the trajectory does not tell you whether it takes a long time or a short time to complete a loop.\nThe next section will show you how the computer constructed the trajectory and how we can get information on the speed of the flow.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Finding a \"solution\"</span>"
    ]
  },
  {
    "objectID": "Dynamics/40-solution.html#the-flow-field",
    "href": "Dynamics/40-solution.html#the-flow-field",
    "title": "41  Finding a “solution”",
    "section": "",
    "text": "Figure 41.1: The flow arrow for the state value \\((r=2, f=1/4)\\) using \\(dt=0.1\\) month.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe flow field depicted by drawing the state velocity vector (multiplied by \\(dt = 0.1\\) to turn it into a length) for many points in the state space.\n\n\n\n\n\n\n\nInstead of plotting the state velocity vector, small snippets of trajectories of duration 0.1 are shown.\n\n\n\n\n\n\n\nFigure 41.2: The flow in the rabbit-fox system shown in two ways.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 41.3: The flow field for the rabbit/fox dynamics. Click at an initial state to generate the trajectory from that state. You may need to pinch in or out to see the flow arrows clearly.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Finding a \"solution\"</span>"
    ]
  },
  {
    "objectID": "Dynamics/40-solution.html#euler-method",
    "href": "Dynamics/40-solution.html#euler-method",
    "title": "41  Finding a “solution”",
    "section": "41.2 Euler method",
    "text": "41.2 Euler method\nRecall from Block 2 the limit definition of the derivative: \\[\\partial_t x(t) = \\lim_{dt \\rightarrow 0} \\frac{x(t + dt) - x(t)}{dt}\\ .\\] we will use this definition to develop a very general way to solve differential equations: the Euler method.\nThe differential equations specify the values of \\(\\partial_t x(t)\\) as the dynamical function. In Block 2, we paid attention to whether the limit exists. But here, we know it must because the dynamical functions themselves don’t involve limits. In working with the differential equation it suffices to pick some small, finite \\(dt\\). How small? Pick \\(dt\\) to be small enough that the result wouldn’t change in any substantial way if we used an even smaller time increment, say \\(dt/10\\).\nOur starting point for solving each differential equation is to re-write it as a finite difference. To illustrate, we will solve the equation \\(\\partial_t x = x (1 - x)\\), which is often called the logistic equation.\nApplying the finite difference definition, we get \\[\\underbrace{\\frac{f(t + dt)- f(t)}{dt}}_{\\text{finite-difference approx.}} = \\underbrace{x (1-x)}_{\\text{dynamical function}}\\ .\\] Multiplying both sides of the above by \\(dt\\) and re-arranging terms produces \\[\\underbrace{f(t + dt)}_{\\text{future state}} = \\underbrace{f(t)}_{\\text{current state}} +\\ \\ \\  \\underbrace{x (1-x) dt}_{\\text{step}}\\] We call this last equation the Euler formula.\nTo use this, we start at the initial condition, say \\(x(t=0) = 0.2\\). This initial condition gives us the first row of a tabular representation of the function \\(x(t)\\): Table 41.1.\n\n\n\n\n\nTable 41.1: Initial state\n\n\n\n\n\ntime\nstate\n\n\n\n\n0\n0.2\n\n\n\n\n\n\n\n\n\n\nTable 41.2: The next time step.\n\n\n\n\n\ntime\nstate\n\n\n\n\n0.0\n\\(0.2\\)\n\n\n0.1\n\\(0.2 + \\color{brown}{0.016} = \\color{blue}{0.216}\\)\n\n\n\n\n\n\n\n\n\n\nTable 41.3: Still another time step\n\n\n\n\n\ntime\nstate\n\n\n\n\n0.0\n\\(0.2\\)\n\n\n0.1\n\\(0.2 + \\color{brown}{0.016} = \\color{blue}{0.216}\\)\n\n\n0.2\n\\(\\color{blue}{0.216} + \\color{magenta}{0.0169} = 0.2329\\)\n\n\n\n\n\n\n\n\nNext, pick a value for \\(dt\\) that we will use for all the following steps, each of which will add a new row to the table. For the example, we will set \\(dt = 0.1\\). When we have constructed the whole table we can go back and check whether that was small enough.\nTo fill in the next row, as in Table 41.2, we apply the Euler formula. Sine \\(dt = 0.1\\), the next time step will be \\(0.1\\). Plug in the current state—which is 0.2 right now—to calculate the future state. The step will be \\(0.2 (1-0.2)\\, dt = \\color{brown}{0.016}\\). Add this step to the current state to get the future state.\nThe next step is shown in Table 41.3 and will bring us to time \\(0.2\\). Use the Euler formula, pluggin in the value of the present state, \\(\\color{blue}{0.216}\\), to find the step. Here that will be \\(0.216 (1-0.216)\\, dt = \\color{magenta}{0.0169.}\\).\nAdd as many rows to the table as you like; the process will be the same for each step, the new row being calculated from the values in the previous row.\nRecognize that this is an iterative process.\n\n\n\n\n\n\n\n\n\nEuler iteration\n\n\n\nAs is so often the case, it is wise to think about carrying fundamental tasks as they can be accomplished by calculus operations—evaluate, differentiate, anti-differentiate, solve, find argmax, iterate. The obvious choice for integrating differential equations is “anti-differentiate,” but as described previously, the techniques we covered in Block 3 are not sufficient for the task. Instead, we use iteration to solve differential equations.\nThis example uses the software you have already seen, Iterate(), to carry out the task. In practice, however, you will use a special form of Iterate() called integrateODE() that makes use of interpolation techniques to give a more precise answer.\nTo implement the iteration to solve \\(\\partial_t x = x (1-x)\\), we need to create a function that takes the current state as input and produces the next state as output. Our one-step function can be this:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNotice that we wrote next_step() with an input slot for \\(dt\\). This will not be part of the state being iterated, just a parameter that allows us easily to explore different values for \\(dt\\).\nUse Iterate() to carry out the iteration of next_step(). Note that we use the fargs argument to Iterate() to pass our selected value for dt to the function next_step(). We will run the iteration for 100 steps. With \\(dt=0.1\\), those 100 steps will 10 units of time.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nOr, in graphical form ….\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\n\nTry it! 41.1\n\n\n\n\n\n\n\n\n\nTry it! 48.4 Iterate vs integrateODE\n\n\n\nThe previous example used Iterate() to solve a differential equation. The output of the iteration was a data frame containing values for the solution at discrete times: 0, 0.1, 0.2, and so on. A data table is a perfectly good way to represent a function, but it is handier to have a function in a form that operations like slice_plot() and D() can be applied to. Another way to look at things is that, mathematically, the solution to a differential equation should be a continuous-time function. Fortunately, we have at hand the interpolation techniques covered in Chapter 49 to carry out the construction of a continuous-time function from a tabular representation. The R/mosaic function integrateODE() connects together the iteration and interpolation to provide a solution that is in the form of continuous-time function(s).\nUse the R/mosaic function integrateODE() to solve differential equations numerically. It is a specialized function that handles sets of first-order differential equations, but any high-order differential equation can be separated into a set of first-order equations.\nTo illustrate, this command will solve the differential equation \\(\\partial_t x = x (1-x)\\) that we took on in the previous example with Iterate().\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe first argument is a tilde expression, but in a form that is different from from that used in functions such as D() or contour_plot(), etc. To the left of the tilde is a single name composed of the state variable—x here—prefixed by a d. The d is just a reminder that we are describing not x itself, but \\(\\partial_t\\ \\mathtt{x}\\). On the right of the tilde is the function from the differential equation, in this case, \\(x(1-x)\\).\nThe next argument is the initial condition. We are starting the integration at \\(x=0.2\\). The bounds() sets the time interval for the integration and dt sets the time step..\nThe output of integrateODE() is an R structure of a type called a “list” that is new to us. The list contains the function(s) created by integrateODE() which you refer to by name (x) using a special form of R punctuation $ suited to lists.. In other words, Soln2$x will be a function, which you can plot like any other function, for instance:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis use of the R symbol $ is new to us. We won’t emphasize it here. Instead, we’ll use the traj_plot() graphics function (introduced in Chapter 49) which already knows how to access the functions created by integrateODE().\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nAn important feature of integrateODE() is its ability to handle sets of first-order differential equations. For instance, the rabbit/fox system \\[\\partial_t r = 0.66\\, r - 1.33\\, r f\\\\\n\\partial_t f = -f + rf\\] will be integrated by this command:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYou can plot the time series using slice_plot()\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nTo plot the trajectory, simply change the tilde expression used in traj_plot(). which creates a time series plot, traj_plot() shows the trajectory.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Finding a \"solution\"</span>"
    ]
  },
  {
    "objectID": "Dynamics/40-solution.html#sec-symbolic-solutions-ODE",
    "href": "Dynamics/40-solution.html#sec-symbolic-solutions-ODE",
    "title": "41  Finding a “solution”",
    "section": "41.3 Symbolic solutions",
    "text": "41.3 Symbolic solutions\nOccasionally it is possible to integrate a differential equation using symbolic techniques. This is particularly true for differential equations that are linear. The example we will handle here is the first-order linear differential equation \\[\\partial_t x = a\\, x\\ .\\] An advantage of symbolic solutions is that parameters can be handled symbolically.\nA method we will use repeatedly in this block is called the “method of ansätze.” An ansatz (singular of the German “ansätze”) is, in this context, a guess for the solution. Since differential equations have been a central part of science for more than 200 years, you can imagine that a large library of equations and their solutions has been assembled. For the equations that are most frequently used and that can be solved symbolically, the solutions are already known. Thus, the “guess” for the solution can be a very well informed guess.\nLet’s see how this works for \\(\\partial_t x = a\\, x\\). From experience, the ansatz will be an exponential function of time, which we can write \\(x(t) \\equiv A e^{\\omega t}\\). We don’t yet know what is the value of \\(\\omega\\) or \\(A\\), so we plug the ansatz into both the left and right sides of the differential equation to work it out.\nPlugging in the ansatz, translates the differential equation to a new form: \\[\\underbrace{A \\omega e^{\\omega t}}_{\\partial_t x(t)}\\  =\\  \\underbrace{a A e^{\\omega t}}_{a x(t)}\\ .\\] Cancelling out the terms that appear on both sides of the equation gives \\[\\omega = a\\ \\ \\ \\text{which implies}\\ \\ \\ x(t) = A e^{a t}\\ .\\] The ansatz substitution didn’t give any result at all for \\(A\\). That is to say, unlike \\(\\omega\\), the \\(A\\) is not determined by the differential equation itself. This means that \\(A\\) must be related to the initial condition. Setting \\(t=0\\) gives \\(x(0) = A\\), so in this simple differential equation, \\(A\\) is the initial condition.\nA slightly more complex differential equation is \\[\\partial_t x = a\\, (x - b)\\ .\\] This also has an exponential solution. It is easiest to see this by defining a new variable \\(y \\equiv x - b\\). By the rules of differentiation, \\(\\partial_t y = \\partial_t x\\), so the differential equation can be re-written in the form \\(\\partial_t y = a y\\). We already know the solution to this is \\(y(t) = y_0 e^{a t}\\). Translating by to \\(x\\) we get \\[x(t) - b = (x_0 -b) e^{at}\\ \\ \\ \\implies x(t) = (x_0 -b)\\,e^{at} + b)\\ .\\]\nFor nonlinear dynamical function, there is no perfectly general way to find symbolic solutions. But for some dynamical functions, it can be done. we will demonstrate by integrating \\(\\partial_t x = x (1-x)\\). The method is made more plausible by using the Leibnizian notation for derivatives, with which the differential equation has this form: \\[\\frac{dx}{dt} = x(1-x)\\ .\\] The Leibnizian notation can be interpreted as the ratio of two differentials: \\(dx\\) and \\(dt\\) in this case.\nThe idea of separating the differential equation is to algebraically move all the \\(x\\) terms to the left side of the equation and all the \\(t\\) terms to the right and then to integrate each side of the equation. \\[dx = x(1-x) dt \\ \\ \\ \\implies \\ \\ \\ \\frac{1}{x(x-1)}dx = dt\\ \\ \\ \\implies\\ \\ \\ \\int\\frac{1}{x(x-1)}dx = \\int dt .\\]\nThe integral on the right side, \\(\\int dt\\), should be easily recognizable, giving \\(\\int dt = t + F\\), where \\(F\\) is the “constant of integration.”\nThe integral on the left side may not be as familiar, but the person solving this problem for the second time will remember that \\[\\frac{1}{x(1-x)} = \\frac{1}{x} + \\frac{1}{1-x}\\] as you can confirm by putting the right side over a common denominator. Each of \\(1/x\\) and \\(1/(1-x)\\) have integrals that are logs: \\(\\int dx/x = \\ln(x) + D\\) and \\(\\int dx/(1-x) = - \\ln(1-x) + E\\). Putting the equation back together again, produces \\[\\ln(x) + D - \\ln(1-x) + E = t + F\\ .\\] At this point, move all the constants of integration over to the right side and consolate them into a single constant of integration \\(C\\). At the same time, collect together the two logarithmic terms, giving: \\[\\ln\\left(\\frac{x}{1-x}\\right) = t + C\\ .\\] Exponentiate both sides to get: \\[\\frac{x}{1-x} = \\underbrace{e^C}_{A} e^t\\  .\\] Since \\(e^C\\) is just a constant, we will write it more simply as \\(A\\).\nNow we have \\[x = Ae^t - x A e^t \\ \\ \\implies\\ \\ \\ x (1 + Ae^t) = Ae^t\\] which gives our solution \\[x = \\frac{Ae^t}{1 + Ae^t}\\ .\\] To find the initial condition symbolically, plug in \\(t=0\\), giving \\(x_0 = A/(1+A)\\) or, equivalently \\(A = x_0/(1-x_0)\\). Our previous examples used \\(x_0 = 0.2\\), for which \\(A = 0.2/0.8 = 0.25\\). Graphing this solution gives us the familiar sigmoid:\n\nSymb_soln = makeFun(A*exp(t)/(1 + A*exp(t)) ~ t)\nslice_plot(Symb_soln(t, A=0.2) ~ t, bounds(t=-5:10))\n\n\n\n\n\n\n\n\nNot all differential equations can be separated in this way, and even for those that can, the integrals may not be tractable. So this route to a solution is not a general-purpose one, unlike the Euler method. Still, the Euler method gives only an approximate solution, so with Euler we need to take care that the approximation is close enough for the purpose at hand. In this case, we have both an Euler solution (with \\(dt=0.1\\)) and a symbolic solution. Figure 41.4 shows the difference between the two solutions, which ideally should be zero. To show more of the time domain of the solution, we will reset the initial condition to \\(x_0 = 0.01\\). This corresponds to \\(A = 1/99\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 41.4: The difference between the Euler and the symbolic solution to \\(\\partial_t x = x (1-x)\\) as a fraction of the symbolic solution. At the worst, the Euler solution is off by 1.5 parts in one-million.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Finding a \"solution\"</span>"
    ]
  },
  {
    "objectID": "Dynamics/41-flow-on-line.html",
    "href": "Dynamics/41-flow-on-line.html",
    "title": "42  Flows on the line",
    "section": "",
    "text": "42.1 Dynamical function and flow\nThe previous two chapters presented ideas relating to dynamical systems: state, state-space, dynamical function, flow, trajectory, “solution.” Now we turn to the some of the phenomena seen in dynamical systems, starting in the simplest way possible: dynamical systems with a single state variable. We will focus on fixed points and their stability, which can be understood qualitatively (although you need to distinguish between a positive and a negative slope). Then we will look at a technique we have encountered since Block 2: approximation of a function by a straight-line function. Such linear dynamics have a straightforward exponential “solution.”\nFinally, we will look at an important example of how careful observation of fixed points and the way dynamics change when we modify a parameter in the dynamical functions provides an understanding of a ecological stability and instability and the consequences that result.\nIn the previous chapter, we saw how to draw a flow field in a two-dimensional state space, evaluating the dynamical functions and using the results to construct a vector. We cannot practically visualize both the flow and the shapes of the two dynamical functions in a single plot, which makes it harder to understand structures such as fixed points.\nHappily, with a one-dimensional state space, we can easily show both the flow vectors and the single dynamical function at once.\nFor ease of reference, we will name the dynamical function for the rest of this section \\(f(x)\\), so that the differential equation is \\[\\partial_t x = f(x)\\ .\\]\nThe flow itself appears as the example in Figure 46.1. The state space is the number line and the flow vectors are, as usual arrows that point from place to place in the state space.\nBecause the state space can be drawn without using the vertical coordinate of the page, we can use that vertical coordinate to show something else: the dynamical function, as in Figure 46.2.\nThe correspondence between the dynamical function and the flow field is easy to see in such a presentation. Where the output of the dynamical is large and positive (say, near \\(x=0\\)), the flow is in the positive direction and relatively fast, as shown by a long, right-pointing flow vector. When the output of the dynamical function is negative (around \\(x=3\\), for instance) the flow is in the negative direction: a left pointing arrow.\nNear a zero crossing of the dynamical function, the flow arrows are negligibly short: the state velocity is very slow. Indeed, at the zero crossings, the state velocity is exactly zero. Such zero crossings are called fixed points: since the state velocity is zero, the state never moves!\nWe can see the dynamics near fixed points more closely by zooming in, as in Figure 46.3 which shows two of the system’s fixed points.\nNotice in Figure 46.3 that the flow is slower the nearer the state is to the fixed point, but it is only exactly zero at the fixed point.\nA calculus technique you will be familiar with from previous Blocks is zooming in a region that we want to examine in detail.\nThe short pieces of the dynamical function shown in Figure 46.4, are, like short pieces of any continuous function: almost exactly straight lines. For the left fixed point, the dynamical function is \\(f(x) \\approx -2.804 (x + 3.055)\\) while for the right it is \\(f(x) \\approx 5.065 (x + 1.586)\\). In Section @ref(symbolic-solutions-ODE) we found symbolically the solutions for dynamical functions in this form. For \\(x_0\\approx-3.055\\) the solution is \\[x(t) \\approx (x_0 + 3.055)e^{-2.804 t} - 3.055\\ ,\\] while for \\(x_0\\approx -1.586\\) the solution is \\[x(t) \\approx (x_0 +1.586)\\, e^{5.065 t} - 1.586\\ .\\] There is something fundamentally different about these two solutions. One of them is exponential decay toward the fixed point, while the other grows exponentially away from the fixed point. We call the dynamics near the fixed-point with exponential decay stable and the dynamics near fixed-point with exponential growth unstable.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Flows on the line</span>"
    ]
  },
  {
    "objectID": "Dynamics/41-flow-on-line.html#dynamical-function-and-flow",
    "href": "Dynamics/41-flow-on-line.html#dynamical-function-and-flow",
    "title": "42  Flows on the line",
    "section": "",
    "text": "43 raw &lt;- doodle_fun( ~ x, seed=920)\n\n\n44 Znotes::phase_line(raw(x) - 2 ~ x, bounds(x=-4:4),\n\n\n45 nix_dyn=TRUE, narrows=25,\n\n\n46 transform=function(x) x^.75)\n\n\n\n\nFigure 46.1: A one-dimensional state space shown with its flow vectors.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 46.2: A one-dimensional state space shown with its flow vectors.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 46.3: Zooming in on the flow for the system shown in Figure 46.2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 46.4: Zooming in closely on each of the fixed points seen in Figure 46.3.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nGraphics such as Figure 46.2 let you see both the flow and the dynamical functions together in one place.\nHow about also showing trajectories? Unfortunately, the two-dimensional extent of a computer screen or a piece of paper make it hard to include still more information in an intelligible way. It would be nice to have a third dimension for the display.\nMajor Austin Davis developed such a display, using time as the third dimension. In the movie below, the state space is shown as a horizontal line, as before. The vertical axis shows the dynamical function as in Figure 46.2. The dynamical function is shown in another way: as the hue and intensity of color, which lets you focus on the activity in the state space. This activity is shown by the moving gray triangles. Each triangle is placed on the phase line to mark an initial condition, then moves right or left according to the dynamics.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Flows on the line</span>"
    ]
  },
  {
    "objectID": "Dynamics/41-flow-on-line.html#generic-behavior",
    "href": "Dynamics/41-flow-on-line.html#generic-behavior",
    "title": "42  Flows on the line",
    "section": "46.1 Generic behavior",
    "text": "46.1 Generic behavior\nSo long as two dynamical systems have similar fixed points with the same stability, their trajectories will be much the same. For example, our model dynamical function might be different in detail, as in Figure 46.5, and still produce the same behavior.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 46.5: The dynamical function shown in black is a distortion of \\(f(x)\\) from the previous plots. Yet the flow field is practically identical and leads to the same outcomes as \\(f(x)\\) for any initial condition.\n\n\n\nSo long as two flows have similar fixed points with the same stability, their trajectories will be much the same. Consequently, studying the fixed points without worrying about the details of the dynamics gives a huge amount of information about the system.\nFor example, Figure 46.6 shows a score of different time series following the solutions from a score of initial conditions. The long-term behaviors for all the time series is similar: they converge to one or another of the stable fixed points.\n\n\n\n\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\nFigure 46.6: Time series from the differential equation \\(\\partial_t x = f(x)\\) starting at many initial conditions. The locations of the three fixed points are marked with horizontal lines. All the solutions convert to one or the other of the two stable fixed points in the system, and depart from the unstable fixed point.\n\n\n\nIt is worth pointing out a consequence of the mathematics of continuous functions: if a system with a continuous dynamical function has a region of state space with two different fixed points, there must be an unstable fixed point in between them.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Flows on the line</span>"
    ]
  },
  {
    "objectID": "Dynamics/41-flow-on-line.html#linearization",
    "href": "Dynamics/41-flow-on-line.html#linearization",
    "title": "42  Flows on the line",
    "section": "46.2 Linearization",
    "text": "46.2 Linearization\nYou can see in Figure 46.6 that many of the solutions approach their final, equilibrium value in an exponential manner. This is particularly true for the solutions with initial conditions very near the stable fixed points. All these solutions are characterized quantitatively by the parameter \\(a\\) in the exponential solution \\(A e^{a t}\\). (Remember, \\(a &lt; 0\\) when there is exponential decay.)\nQuantitative knowledge of \\(a\\) is helpful to understand the time scale of the exponential approach to stable fixed points. We can find a numerical value for \\(a\\) for each fixed point by constructing a linear approximation to the dynamical function near each of the fixed points.\nThe procedure involves the same principles as introduced in Block 2 for constructing low-order polynomial approximations to functions, but here “low-order” means “first order.”\nThe analysis is done separately for each of the fixed points, so the first step is to find the fixed points, the values \\(x^\\star\\) such that \\(f(x^\\star) = 0\\).\nRecall from Block 2 the Taylor polynomial approximation to a function \\(f(x)\\) centered on a point \\(x^\\star\\): \\[f(x) \\approx f(x^\\star) + \\partial_x f(x^\\star) \\left[x - x^\\star\\right]\\] When \\(x^\\star\\) is a fixed point, \\(f(x^\\star) = 0\\) so the approximation is simply \\(f(x) \\approx \\partial_x f(x^\\star) \\left[x - x^\\star\\right]\\). Keep in mind that \\(\\partial_x f(x^\\star)\\) is the derivative function \\(\\partial_x f\\) evaluated at the input \\(x^\\star\\), so \\(\\partial_x f(x^star)\\) is simply a quantity, not a function. Indeed, \\(\\partial_x f(x^star)\\) is exactly the quantity \\(a\\) in the exponential solution \\(e^{a t}\\).\nThis process of constructing the linear approximation \\(f(x) \\approx a \\left[x - x^\\star\\right]\\) is called linearization.\n\n\n\n\n\n\n\n\nTry it! 46.1\n\n\n\n\n\n\n\n\n\nTry it! 46.1 Exponential dynamics near fixed points\n\n\n\nConsider the first-order differential equation \\[\\partial_t x = f(x) \\equiv r x (x - x/K)\\] where \\(r\\) and \\(K\\) are parameters that are greater than zero. Linearizing the nonlinear function \\(f(x)\\) lets us figure out how fast or slow is the exponential growth or decay of the solutions for initial conditions near the fixed points.\n\nThere are two fixed points, one at \\(x_1^\\star = 0\\) and the other at \\(x_2^\\star = K\\). What is the exponential parameter \\(a\\) for each of the fixed points.\nThe derivative (with respect to \\(x\\)) \\(\\partial_x f(x)\\) can be found with the product rule from Block 2. It is \\[\\partial_x f(x) = r\\, (1 - x/K) - r\\, x\\, (1/K)\\]\nEvaluating \\(\\partial_x f(x)\\) at the two fixed points \\(x_1^\\star = 0\\) and \\(x_2^\\star = K\\) gives\n\n\\[\\partial_x f(x_1^\\star) = r\\ \\ \\ \\text{and}\\ \\ \\ \\partial_x f(x_2^\\star) = -r\\] Solutions near \\(x_1^\\star\\) will grow exponentially as \\(e^{r t}\\), unstable since \\(0 &lt; r\\). Solutions near \\(x_2^\\star\\) will decay toward \\(x_2^\\star\\) in an exponential manner as \\(e^{-r t}\\).\n\n\n\n\n\n\n\n\nTip\n\n\n\nIt is critical to distinguish carefully between \\(x^\\star\\), which is the location of the fixed point being examined, and \\(x_0\\), which is the initial condition of the state, that is, \\(x(t=0)\\).\n\n\n\nApplication area 46.1  \n\n\n\n\n\n\n\nApplication area 46.1 Retirement savings redux\n\n\n\nLet’s return to the model of saving for retirement in Chapter 40: \\[\\partial_t V = r\\, V + M\\ .\\] The state variable here is named \\(V\\). The dynamical function is \\[g(V) = r\\, V + M\\] where \\(r\\) is the interest rate (say, 3% per year which is \\(r=0.03\\) per year) and \\(M\\) is the monthly contribution. To keep the units consistent, we set the units of \\(t\\) to be years, of \\(r\\) to be 1/years, of \\(V\\) to be dollars and of \\(M\\) to be dollars-per-year. So a monthly contribution of \\(1000 would come to\\)M=12000$ dollars-per-year.\nFind the amount \\(V\\) that will result from 30 years of savings with an initial condition \\(V_0 = 0\\).\nStep i) Find the fixed point. This is a value \\(V^\\star\\) such that \\[r\\, V^\\star + M = 0\\ \\ \\ \\implies \\ \\ \\ V^\\star = -M/r\\ .\\] Step ii) Find the derivative of the dynamical function evaluated at the fixed point: Since \\(g(V)\\) happens to be a straight-line function, we know the derivative is a constant. So \\(b = \\partial_x g(V^\\star) = r\\).\nStep iii) Translate the state variable into \\(y = V - V^\\star\\). The dynamics of \\(y\\) are \\(\\partial_t y = b y\\), which has an exponential solution \\(y = A e^{bt}\\).\nStep iv) \\(A\\) is the initial condition of \\(y\\). This will be \\(y_0 = V_0 - V^\\star\\). Since we stated that \\(V_0 = 0\\) (no savings at the start), \\(y_0 = -V^\\star\\) and the solution is \\[y(t) = -V^\\star e^{bt} = \\frac{M}{r} e^{rt}\\ .\\]\nStep v) Translate the solution in step (iv) back into terms of \\(V(t)\\). Since \\(y(t) = V(t) - V^\\star\\), this will be \\(V(t) = y(t) + V^\\star\\) or, \\[V(t) = \\frac{M}{r} e^{r t} + V^\\star = \\frac{M}{r} \\left[ e^{r t} - 1\\right]\\ .\\] To get an idea of this retirement plan, that is, \\(r=3\\%\\) and \\(M=12000\\) dollars-per-year, let’s see how much you will have after 30 years and 40 years.\n\nV &lt;- makeFun((M/r)*(exp(r*t)-1) ~ t, r=0.03, M=12000)\nV(30)\n## [1] 583841.2\nV(40)\n## [1] 928046.8\n\nAfter 40 years of contributions, your retirement account will have almost one-million dollars.\nYou could have accomplished the same calculation using integrateODE(), like this:\n\nSoln &lt;- integrateODE(dV ~ r*V + M, V=0, M=12000, r=0.03,  \n                     domain(t=0:40))\n## Solution containing functions V(t).\nSoln$V(30)\n## [1] 583841.2\nSoln$V(40)\n## [1] 928046.8",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Flows on the line</span>"
    ]
  },
  {
    "objectID": "Dynamics/41-flow-on-line.html#bifurcation",
    "href": "Dynamics/41-flow-on-line.html#bifurcation",
    "title": "42  Flows on the line",
    "section": "46.3 Bifurcation",
    "text": "46.3 Bifurcation\nA broad, pressing, social concern goes under the name sustainability. Is it sustainable to burn fossil fuels at steady historical levels, let alone at the increasing rate seen since over the last century? Climate scientists answer resoundingly with a no. Is it sustainable to increase food production to the levels needed for developing economies to approach the sort of consumption seen in rich economies?\nDynamical systems are highly relevant to the questions surrounding sustainability. If the economy is near a stable fixed point, then it is sustainable; the trajectory will bring the state of the economy toward the fixed point. On the other hand, if the economy is near an unstable fixed point, we can expect exponential change.\nIf such exponential changes are not seen, does that mean we are not near an unstable fixed point? One of the terms used to mark the possibility that a stable system can quickly turn unstable is tipping point, defined as\n\nThe point at which a series of small changes or incidents becomes significant enough to cause a larger, more important change. Source: New Oxford American Dictionary\n\nThe mathematics of tipping points is not at all the same as exponential growth. Certainly, in exponential growth one sees a relatively slow rate of change increase to a large rate of change, a situation described by journalists as “sky-rocketing” or “explosive” or, literally, “exponential.” As you’ve seen, exponential growth is a phenomenon seen in linear dynamical systems; there is no special point at which the dynamics changes.\nThere is an area of mathematical theory called catastrophe theory. We will use a famous example to show how catastrophes or tipping points are modeled mathematically.\nThe example comes from a 1977 article in Nature, one of the world’s most prestigious scientific journals. The article, by Robert May, is entitled “Thresholds and breakpoints in ecosystems with a multiplicity of stable states.” The words “thresholds” and “breakpoints” have not been encountered yet in this book, but “multiplicity of stable states” should bring to mind the sort of dynamics seen in Figure 46.2.\nThe setting for the catastrophe is an otherwise bucolic scene, livestock grazing on a pasture. A pasture is a kind of factory for producing vegetable biomass; the grazing is the consumption of the biomass produced.\nAs a model for the production of biomass, denoted \\(v\\) for “vegetation,” we will use \\[\\partial_t v = r v \\left(1 - \\frac{v}{K}\\right)\\] which, as we’ve seen, has an unstable fixed point at \\(v_1^\\star=0\\) and a stable fixed point at \\(v_2^\\star=K\\). Physically, the fixed point \\(v_1^\\star\\) corresponds to a bare field, without any vegetation. It is unstable because any small disturbance in the form of a stray seed landing in the dirt can lead to germination and the rapid growth of vegetation as seeds from the germinated plant spread across the field. Once the field is covered in vegetation, the growth can be exponentially rapid at first but then runs into limited resources: there is only so much sunlight that falls on the field, and the growing vegetation will eventually consume the soil nutrients and water.\nThis biomass production model corresponds to a sustainable system. Once the biomass level is at \\(v_2^\\star\\) it will stay there.\nBut biomass production is not the only thing going on in the pasture. The grazing animals—let’s imagine they are cows—are consuming the grass. To start very simply, suppose that each cow consumes amount \\(C\\) of biomass each day. If there are \\(H\\) cows, the total consumption is \\(H C\\) per day. This modifies the dynamics to a slightly new form \\[\\partial_t v = r \\,v(1-\\frac{v}{K}) - HC\\]. The original, ungrazed dynamics are compared with the grazed dynamics in Figure 46.7.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 46.7: Comparing the pasture dynamics for different herd sizes.\n\n\n\nWith grazing, the net growth of biomass is reduced due to the removal of the consumed biomass by the cows’ consumption. For a moderate herd size, there is still a stable fixed point, but it is at a lower level of biomass than would be seen in the ungrazed field. But if the herd size is too large, the ecosystem completely collapses.\nThis is an example of a tipping point or catastrophe. For moderate herd sizes, there remains a stable fixed point. A farmer might be tempted to add another cow to the pasture, and that is sustainable: there is still a stable fixed point. Indeed, the movement of the stable fixed point might not even be noticed. But add even one cow too many and the fixed point entirely disappears. Still, herd management can fix the problem; take away the cow that tipped the pasture and the fixed point will return.\n\nApplication area 46.2 —Details of chewing\n\n\n\n\n\n\n\nApplication area 46.2 Cows eat grass\n\n\n\nChapter 16 illustrated the iterative process for building models: using the results of a model to suggest possible improvements in the model. Let’s look again at how cows eat grass.\nMissing from the pasture model is a simple idea of how cows eat. If there is very little biomass, the cows cannot continue to eat their fill. Will the reduction in consumption per cow preserve the stable fixed point?\nIn his Nature article, May modeled the consumption rate by a single cow with the functional form \\[C(v) \\equiv \\frac{\\beta v^2}{v_0 - v^2}\\] which is graphed for \\(\\beta=0.1\\) and \\(v_0 = 3\\) in Figure 46.8\n\n\n\n\nconsumption &lt;- makeFun((beta*v^2/(v0^2 + v^2))~ v, beta=0.1, v0=1)\nslice_plot(consumption(v) ~ v, bounds(v=0:10)) %&gt;%\n  gf_labs(y=\"Consumption (tons/day)\", x=\"v: available biomass (tons)\")\n\n\n\n\n\n\n\n\n\n\nFigure 46.8: Consumption of vegetation by a single cow as a function of the amount available in the pasture.\n\n\n\nYou can recognize this as a form of sigmoid. When the amount of vegetation is very large, a cow will eat her fill. That is the saturation of the sigmoid. For small \\(v\\), the cow needs to hunt around for vegetation tall enough to eat, reducing the consumption steeply.\nFigure 46.9 modifies the pasture dynamics to incorporate this sigmoid model of consumption.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 46.9: Pasture dynamics for a sigmoid consumption function.\n\n\n\nWe can start the story with 3 cows in the pasture. Vegetation growth is more than sufficient to provide for these cows. You can see this from the stable fixed point at about 9 tons of biomass, which is more than enough to reach saturation of the sigmoid consumption function.\nThe farmer decides to increase the herd to 5 cows. Nothing much happens. The stable fixed point is at about 8 tons of biomass, entirely adequate to keep the cows well fed in a sustainable manner.\nCan the pasture be sustainable with 7 cows? The stable fixed point remains, now at about 6.5 tons at biomass. The cows are still sustainably well fed.\nYou can see in the 7-cow dynamics a hint of what of what might go wrong. There is a new, unstable fixed point at about 3 tons of biomass. If the pasture ever happened transiently to fall below 3 tons—say due to a summer frost followed by a return to normal weather—the vegetation biomass will head toward the new stable fixed point at 0.5 tons of biomass. At this level, the cows are eating only about one-quarter their normal amount and we can fairly say that the ecosystem has collapsed. But until such a disaster happens, the farmer will see only a sustainable level of biomass with cows well fed.\nIt is only we, who have a mathematical model of the situation, who can anticipate the potential problems.\nSince things are fine with 7 cows in the field, the farmer lets an eighth cow join the herd. That is the tipping point. The happy herd fixed point at 6.5 tons of biomass has disappeared, and the ecosystem collapses, even without a weather disaster.\nRemoving the eighth cow from the pasture will not fix the situation. With seven or even six cows in the pasture, the system won’t be able to grow out of the stable fixed point with 0.5 tons of biomass. Reducing the herd size to five will remove that 0.5 ton fixed point, but the grass will grow back very slowly; the dynamics give positive growth, but very close to zero.\nAvoiding such catastrophes is a major motivation for mathematical modeling.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Flows on the line</span>"
    ]
  },
  {
    "objectID": "Dynamics/42-flow-on-plane.html",
    "href": "Dynamics/42-flow-on-plane.html",
    "title": "43  Flows on the plane",
    "section": "",
    "text": "43.1 Generic behaviors\nLet’s return the rabbit/fox system as an example of flow. Since there are two state quantities, \\(r\\) and \\(f\\), the state space \\((r, f)\\) is a plane. At each point in the state space, the flow vector gives the direction and speed of motion. Like all vectors, a flow vector has only two properties: the direction and length. The speed of motion is the length of the flow vector.\nThe flow itself is a vector field. This is an assignment of a vector to each point of the state space. Graphically, we depict a flow field by selecting a grid of points in the state space, finding the flow vector for each grid point, and drawing those vectors positioned at their respective grid points.\nRecall from Block 5 that it is conventional to specify a vector by giving a coordinate pair for the tip of the vector with the understanding that the tail is at the origin. For the rabbit/fox system, the tip’s coordinate is \\(\\left({\\Large\\strut} g_r(r, f),\\  g_f(r, f)\\right)\\). This notation is potentially confusing, because the letters \\(r\\) and \\(f\\) appear in so many places. Each each vector in Figure 43.1 is drawn at a particular point, say \\((r=0.96, f=0.48)\\). At that point, evaluate the dynamical functions: \\(g_r(r=0.96, f=0.48) = 0.0207\\) and \\(g_r(r=0.96, f=0.48)= 0.941\\).\nA fixed point of the dynamics is a point in the state space where the dynamical functions both evaluate to zero. It is convenient to mark fixed points as the intersection of zero contours of the dynamical functions. Figure 43.2 shows these zero contours (red for rabbits, blue for foxes) laid on top of the flow field. Such zero contours of dynamical functions are called nullclines. (The word means “zero slope”. “Null” corresponds to zero and “cline” is the root of words like “incline” or “decline.”)\nDue to the nature of fixed points, if the initial condition is at the intersection of the nullclines the state will not change. But is the fixed point stable or unstable.\nAs you will see, in two and higher dimensional dynamical systems, there is more than one kind of stability and more than one kind of instability. These different kinds of stability and instability have a direct correspondence to different kinds of behavior in real-world systems.\nVery near the fixed point, dynamics are approximately linear. We will return to a quantitative analysis of this in Chapter 45. Our objective here is to show that there are several generic types of behavior and that the stability of dynamics near the fixed point has to be one of a handful of different types.\nOn a nullcline of a dynamical variable \\(x\\), the \\(x\\)-component of the flow must be zero. The flow will point to positive \\(x\\) on one side of the nullcline and negative \\(x\\) on the other. This is really nothing more than saying that on one side of a zero contour the function value is positive and on the other side negative. We will indicate this on the following diagrams by shading the positive side of the nullcline with the same color as the nullcline itself. Figure 43.3 shows the nullclines of a linear system on separate plots. Notice that flow in the shaded side of the \\(x\\) (red) nullcline the flow always has a positive component to the right. Similarly, in the shaded side of the \\(y\\) (blue) nullcline, the flow always has a positive component to the right.\nPlacing both nullclines on the same plot divides the region near the fixed point into four parts. This is generic behavior. Unless the two nullclines are the same as each other, the two nullclines split the region into four quadrants.\nWe can identify the quadrants by their color—white, red, blue, purple. In each quadrant, the “compass direction” of all flow vectors point to one quadrant of the compass: white to the south-west, red to the south-east, blue to the north-west, and purple to the north-east.\nThis particular linear flow is unstable. Notice that any initial condition in the purple quadrant will lead to a NE trajectory, away from the fixed point. Similarly, any initial condition in the white quadrant leads to a SW trajectory, again away from the fixed point. For an initial condition in the red or blue quadrants, the flow will take the trajectory into either the white or purple quandrants. The initial part of the trajectory may be towards the fixed point, but as soon as the trajectory crosses into white or purple territory, the trajectory leads away from the fixed point. So, the overall flow is unstable. This particular type of instability, where the initial path might be toward the fixed point but eventually leads away from it, is called a saddle. The flow is analogous to the movement of a marble placed on a horse saddle; it might start to roll toward the center of the saddle, but eventually it will roll off to the side.\nAll linear flows will lead to this quadrant structure. Another feature of the structure is that the white quadrant must always be opposite to the purple, and the red opposite to the blue. This allows us to enumerate the different possible types of stability.\nA very compact summary of the dynamics shows just the four compass directions and the relative positions of the quadrants. For instance, \\[\\begin{array}{c|c}\n\\color{red}{\\searrow} & \\color{purple}{\\nearrow}\\\\\\hline\n\\color{gray}{\\swarrow} & \\color{blue}{\\nwarrow}\n\\end{array}\\ ,\\] corresponds to the saddle flow seen in the previous flow field.\nThere are, altogether, eight possible configurations:\nSaddles are unstable, although the trajectory might approach the fixed point at first. A source is unstable; any trajectory heads away from the fixed point. A sink is stable; any trajectory heads toward the fixed point.\nAs for the orbits, one in a clockwise direction and the other counter-clockwise, we cannot yet say from this simple theory whether they are stable or unstable. The orbit we have already met, the rabbit-fox dynamics, has counter-clockwise trajectories that form closed loops. This is called neutral stability.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Flows on the plane</span>"
    ]
  },
  {
    "objectID": "Dynamics/42-flow-on-plane.html#sec-qualitative-stability",
    "href": "Dynamics/42-flow-on-plane.html#sec-qualitative-stability",
    "title": "43  Flows on the plane",
    "section": "",
    "text": "Figure 43.3: The nullclines of a linear dynamical system near the fixed point. \\(x\\) nullcline is red, \\(y\\) nullcline is blue.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 43.4: Four quadrants of linear dynamics near the fixed point.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 43.5: The rabbit/fox system has orbits that are neutrally stable.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Flows on the plane</span>"
    ]
  },
  {
    "objectID": "Dynamics/42-flow-on-plane.html#linearization",
    "href": "Dynamics/42-flow-on-plane.html#linearization",
    "title": "43  Flows on the plane",
    "section": "43.2 Linearization",
    "text": "43.2 Linearization\nFor dynamical systems with two state variables, constructing a linear approximation to dynamics near a fixed point follows a similar procedure to that with one-state-variable systems.\n\nLocate the fixed point.\nConstruct the first-order polynomial approximation to each of the dynamical functions at the fixed point.\n\nFor instance, the pendulum system has state variables \\(\\theta\\) and \\(v\\) with dynamics \\[\\partial_t \\theta  =  g_\\theta(\\theta, v) =  v\\\\\n\\partial_t v  =  g_{v}(\\theta, v) = - \\sin(\\theta)\n\\] There is a fixed point at \\(\\theta = 0\\), \\(v=0\\): this is just the situation of a pendulum hanging down that has no motion.\nThe dynamical function \\(g_\\theta(\\theta, v) = v\\) is already in first-order polynomial form.\nThe other dynamical function, \\(g_v(\\theta, v) = - \\sin(\\theta)\\) is nonlinear. The first order polynomial approximation centered on the fixed point will be \\[g_v(\\theta, v) \\approx \\underbrace{\\color{magenta}{g_{v}(0, 0)}}_0 + \\underbrace{\\color{magenta}{\\partial_\\theta g_v(0, 0)}}_{-\\cos(0)}\\ \\theta + \\underbrace{\\color{magenta}{\\partial_v g_v(0, 0)}}_0\\, v\\] The term \\(g_v(0, 0) = 0\\) because we are evaluating \\(g_v()\\) at a fixed point. The term \\(\\partial_v g_v(0, 0) = 0\\) because \\(g_v()\\) does not depend on \\(v\\).\nWe will use \\(u\\) and \\(w\\) as the dynamical variables in the linear approximation to avoid confusion with the original, nonlinear equations. The linearized dynamics are therefore: \\[\\partial_t u = \\ \\ w\\\\\n\\partial_t w = - u\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 43.6: Flow field, nullclines, and trajectories of the pendulum (black) and the linearized pendulum ($) from three different initial conditions.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Flows on the plane</span>"
    ]
  },
  {
    "objectID": "Dynamics/43-modeling.html",
    "href": "Dynamics/43-modeling.html",
    "title": "44  Modeling dynamics",
    "section": "",
    "text": "44.1 A single state quantity\nIt is a truism that starting a painting with a blank canvas can be daunting: so much choice and so many possibilities can be disabling. The same is true when it comes to constructing a model of a new phenomenon. You will need to decide what are the essential features of the phenomena, how they are connected to one another and how they connect to the question you seek to answer with the model you will eventually build.\nFor painters, there are numerous ways to confront the blank-canvas function. For instance, there are different types of paintings: landscapes, portraits, abstract, and so on. And there are different styles of painting: impressionist, rococo, cubist, art deco, and so on. I don’t think that painters would use this term, but I think of these aids to decision making as a framework, an organization of what you know to guide decision making.\nThree of the frameworks for modeling that we have worked with in this book might be called:\nWe will start with situations that can be modeled with a single state variable. Throughout our examples, we will call that state variable \\(S\\), so the differential equation describing how \\(S\\) changes in time will always be \\[\\partial_t S = f(S)\\]. The modeler chooses the shape of \\(f()\\) depending on the situation being modeled.\nIn principle, there is an infinite number of shapes for \\(f(S)\\). But many modeling settings involve behavior that is simple. We will work with function shapes where the dynamical function \\(f(S)\\) is continuous and has one or two fixed points.\nFigure 44.1 shows four generic dynamical function shapes where these is one fixed point. The location of the fixed point is, of course, the \\(S_0\\) at which \\(f(S_0)=0\\), that is, the intersection of the function and the blue dashed line.\nThe fundamental distinction is between models with stable and models with unstable fixed points. When the situation being modeled has a steady equilibrium, only the stable shapes of \\(f(S)\\) are relevant. A second distinction is between functions with a bounded shape and those with a linear shape. Use a linear shape when you are concerned only with the behavior near the fixed point. But when your model needs to account for behavior far from the fixed point, you need first to have a way to represent what is meant by “far from.” That is represented by the location of the shoulder of the curve relative to the fixed point.\nA setting for a linear stable model is the cooling or warming of an object to the ambient temperature. \\(S\\) stands for the temperature of the object and the fixed point is the ambient temperature. This model often goes under the name Newton’s Law of Cooling. The prestige of the model name distracts from the fact that this is a very simple model. But in the real world there are likely to be complications that make Newton’s Law imprecise, unlike, say, Newton’s Second Law of Motion which is exact (at non-quantum and non-relativistic scales). For instance, a cup of hot water will cool faster than a cup of cold water. In hot water, evaporation from the surface speeds up the warming. When ice is involved, the rate of temperature change slows when melting or freezing is encountered.\nAnother classic setting for a linear, stable model is radioactive decay. The rate at which the atoms in a mass of a radioactive isotope decay is a constant \\(\\alpha\\) that depends on the nuclear structure. A mass of \\(n\\) atoms will produce \\(\\alpha n\\) individual decay events in a given time interval. But, since each decay event reduces \\(n\\) by 1, the differential equation describing the number of radioactive atoms is \\[\\partial_t n = - \\alpha nt\\] which leads to exponential decay towards zero. Notice that only the \\(0 \\leq n\\) half of the state space is relevant, because the number of atoms cannot be negative.\nAn example of a setting where the bounded stable model applies is one where \\(S\\) stands for the amount of prey or food stock, and there is a constant population of predators who have a limited capacity for eating. Consider, for example, \\(S\\) being the availability of acorns. There might be a fixed population of, say, oaks that produce acorns at a given rate. When predation is low, the acorns accumulate. But when there are a lot of acorns, predators might focus on that bountiful food source. Still, they can only eat so many acorns per day before they are full.\nThe linear unstable model is often used to model population growth. The underlying idea, which might or might not correspond to reality, is that there is a set reproduction rate per member of the population. The differential equation is \\[\\partial_t{S} = a S + c\\] with \\(S\\) being the size of the population. The parameter \\(c\\) captures immigration (\\(0 &lt; c\\)) or emigration (\\(c &lt; 0\\)). To be more detailed, the model can be written as summing a birth process and a death process: \\[\\partial_t S = b S - d S + c\\] where \\(b\\) is the birth rate and \\(d\\) is the death rate. Of course, the detailed model collapses arithmetically to the \\(a S + c\\) model, with \\(a = b - d\\). Still, elaborations on the birth or death processes may include the influence of changing factors. We will return to this idea in a bit.\nTake care to use the parameters \\(a\\) and \\(c\\) correctly. The output of \\(f(S) = a S + c\\) must have dimension \\([S]/[t]\\), for instance organisms per hour might be appropriate for bacteria. Thus the individual terms \\(aS\\) and \\(c\\) must have dimenion \\([S]/[t]\\). Consequently, the parameter \\(a\\) has dimension \\(1/[t]\\) as in per hour. That might seem odd until you remember that \\(a\\) is about the creation of new organisms, \\([S]/t\\), per existing organism. Thus the dimension of \\(a\\) is \\([S] / [S][t]\\) which works out to be simply \\(1/[t]\\).\nFor bacteria, the linear unstable model may be realistic for short periods of time, or, more precisely, for as long as the population is small compared to the carrying capacity. In contrast, human and other animal populations often have an important age structure, which is just to say that neither a 6 nor a 60 year old person has the same reproduction rate as a 26 year old. Such an age structure calls for a dynamical state with multiple components.\nBut if the environment is steady—no food shortages, no disease, economy unchanging, etc.—it can be reasonable to describe even age-structured populations as a percentage growth per unit time, e.g. percent per year. Realize that such a description is not only about the biology of reproduction, but summarizes the whole system of aging, death, and reproduction. This summary description may no longer be relevant when the system as a whole changes. An example of this in the human population is seen in countries where the number of births per woman has fallen substantially—by half or more—over the time of a generation. Such falls typically accompany a growth in economic wealth and the realization that more resources (e.g. education) needs to be provided to each offspring.\nThe bounded unstable model is a way to incorporate factors that interfere with sustained exponential growth. Exponential growth requires that the growth rate \\(\\partial_t S\\) increase as \\(S\\) increases: a kind of positive feedback. In the bounded model, the growth rate becomes constant for large \\(S\\). A constant growth rate means that \\(S\\) will grow steadily, that is \\(\\partial_t S = c\\) which has a solution that grows linear in time, as distinct from the exponential solution that results from \\(\\partial_t S = a S\\).\nAn application of the bounded unstable model is seen in the description of micro-organism growth given by Jacques Monod (1910-1976), a Nobel Prize winning biochemist. His idea was that the organisms are reproducing in a kind of sea that has a limited concentration of an essential nutrient, but very large amounts of the nutrient spread out over space. Even though the nutrient is begin consumed by the organisms, more nutrient diffuses in from far away to keep the concentration steady. At large population sizes, the growth rate is nutrient concentration limited, hence constant.\nChapter 16 introduced the idea of a modeling cycle: taking an initial model, examining the consequence/predictions of that model, and then modifying the model to better correspond to observed reality or new mechanism.\nA case in point is the linear unstable model for population growth. The linear model is always appropriate near a fixed point. This is just a consequence of the calculus idea that any function can be approximated by a linear function over a small enough domain. In defining derivatives, the question was what constitutes “small enough.” So a linear dynamical function is a good starting point for dynamics near a fixed point. But, as we’ve seen, extending the linear model far from the fixed point leads to population explosion: exponential growth. This can be a valid idea for modeling a pathogen growing in a bowl of room-temperature chicken salad: the pathogen need not consume all the salad to become a threat, so in the domain of interest—human health—the linear model can do the job.\nBut we observe generally that exponential growth does not continue indefinitely. The demographer Thomas Malthus (1766-1834) famously propounded a “principle of population” which held that it is in the nature of populations to growth exponentially until linally limited by famine or disease. He wrote, “[G]igantic inevitable famine stalks in the rear, and with one mighty blow levels the population with the [lack of] food of the world.”\nMalthus’s model is exponential growth that runs into a wall of limited food. Malthus saw human reproduction as the engine of the immense poverty and suffering of the lower classes in early industrial Britain. This became the basis of an important political dispute, two poles of which are “there is no point helping the poor, because they create their own poverty,” and “the poverty is due to exploitation, not reproduction.”\nFor us in calculus, there is a middle road: Malthus’s mathematical model, the unstable linear model, is much too abrupt and narrow minded and can easily be made more realistic. Adding that realism removes the “one mighty blow” from the situation. Let’s add that realism now.\nRecall the earlier suggestion that the linear unstable model \\(\\partial_t S = a S + c\\) be broken into components: \\[\\partial_t S = \\underbrace{b S}_{\\text{births}}\n- \\underbrace{d S}_{\\text{deaths}} + \\underbrace{c}_{\\text{immigration}}\\ .\\]\nEven in Malthus’s time, there were calls to alleviate poverty by encouraging people to leave for less crowded lands: emigration. In the dynamics, emigration corresponds to a negative value for \\(c\\).\nMaking \\(c\\) negative does not do the job on its own. Note that whatever the value of \\(c\\), the dynamics are unstable. Emigration at a constant rate changes the location of the fixed point, but since the dynamics are unstable, growth will still be unbounded. Suppose, however, that government policy sets an emigration goal not as a constant number of people per year but as a constant fraction, \\(eS\\) of the population. Now the dynamics become \\[\\partial_t S = b S - d S - e S\\ .\\] These dynamics are stable or unstable depending on the value of \\(b - (d+e)\\). If that value is negative, the dynamics are stable, if positive, the dynamics are unstable. The situation resembles (mathematically) that of a nuclear power reactor: the control parameter \\(e\\) has to be carefully manipulated to set the population at a fixed level.\nBut there are other things that come into play. One of them is that the parameter setting the death rate, \\(d\\), need not be constant. From Malthus’s perspective, \\(d\\) would change in episodes set by disease and starvation. In Malthus’s era, pandemics were common and wiped out a major fraction of the population in “one mighty blow.” Similarly, starvation plays out on a smaller time scale than reproduction and seems to cut through the population.\nBut the death rate can also be a function of population \\(S\\). For instance, \\(d = d_0 + d_1 S\\) corresponds to a death rate that increases gradually with population size. (The parameters \\(d_0\\) and \\(d_1\\) are positive.)\nSimilarly, birth rate can depend on population, that is \\(b = b_0 - b_1 S\\). As the population gets larger, there is less food and less space, and these changes can reduce the reproduction rate. (The parameters \\(b_0\\) and \\(b_1\\) are positive.)\nThese models for \\(d\\) and \\(b\\) are simplistic. Why should they have a linear form? The answer is … calculus. Whatever are the functions \\(b(S)\\) and \\(d(S)\\), they must be approximately linear over a small domain.\nLet’s plug in the refined models for birth and death rates into the population models. We get: \\[\\partial_t S = (b_0 - b_1 S) S - (d_0 + d_1 S) S - e\\ .\\] A little algebraic simplification reduces this to:\n\\[\\partial_t S = (b_0 - d_0) S - (b_1 + d_1) S^2 - e\\] Whatever are the size of the quantities \\(b_0, b_1, d_0, d_1\\), so long as they are positive, the dynamical function \\(f(S)\\) will have two fixed points, one at small \\(S\\) and the other at large \\(S\\). For small \\(S\\), the fixed point is unstable. The population will grow away from this fixed point. The fixed point at large \\(S\\) will be stable, hence no population explosion.\nOne of the major flaws with the Malthusian viewpoint is that it treated all the dynamical functions as linear, whereas in reality the functions can have a quadratic shape. The classical differential equation for limited population growth, \\[\\partial_t S = a S (1-S/K)\\ ,\\] was introduced by Pierre-François Verhulst in 1838, just four years after Malthus’s death.\nAnother important flaw with Malthus’s model is that it failed to account for the transition from purely agricultural economies to economies that produced large amounts of other goods and services. It turns out as populations grow wealthier, their reproduction rates decrease. With wealth available in non-food terms—clothing, public health, education—reproduction rates can go down even without the “one mighty blow” of starvation and disease.\nThe next section examines both of these factors—the introduction of multiple state variables and the ability to “soften” the explosive unstable linear dynamics with nonlinear corrections—in making subtle models of the behavior of systems.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Modeling dynamics</span>"
    ]
  },
  {
    "objectID": "Dynamics/43-modeling.html#a-single-state-quantity",
    "href": "Dynamics/43-modeling.html#a-single-state-quantity",
    "title": "44  Modeling dynamics",
    "section": "",
    "text": "Tip\n\n\n\nIf \\(S\\) is observed to oscillate or to reverse the direction of change, then there must be at least one more state variable, that is, at least one more quantity that changes in time. A single first-order differential equation will not be able to model the situation.\nIf there are no fixed points, then the only possible dynamics are continuous increase in \\(S\\) or continuous decrease in \\(S\\). Often, the point of interest will be whether there is some other factor that changes increase to decrease or vice versa. Again, such situations should be modeled in the context of at least two state variables.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 44.1: Four generic dynamics function shapes with one state variable having one fixed point.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nRemember that in a differential equation \\(\\partial_t S = f(S)\\), the input to \\(f()\\) is has dimension \\([S]\\), while the output of \\(f()\\) has a different dimension: \\([S]/[t]\\). Thus, in the oak/acorn example above, the output of \\(f()\\) has a dimension corresponding to acorns per day, a rate of consumption. \\(S\\) is acorns, the output of \\(f(S)\\) is acorns per day.\n\n\n\n\n\nApplication area 44.2  \n\n\n\n\n\n\n\nApplication area 44.2 Bacterial growth\n\n\n\nUnder ideal reproductive conditions, some bacteria can split in two every 20 minutes. What does this tell us about \\(a\\)?\nIt is easy to get confused. For instance, a single bacterium that splits every 20 minutes will go from an initial population of \\(S=1\\) at time \\(t=0\\) to a population of \\(S=2\\) at \\(t=\\frac{1}{3}\\) hour to \\(S=4\\) at \\(t=\\frac{2}{3}\\) hour to \\(S=8\\) at \\(t=1\\) hour. This might make it seem that the reproduction rate is 8 per hour. But this calculation of the population size at hour 1 is redundant with the accumulation that will be accomplished in solving the differential equation.\nThe correct way to calculate \\(a\\) from the stated information that there is a split every 20 minutes works like this:\n\nThe dynamics are \\(\\partial_t S = a S\\). Therefore the solution is \\(S(t) = S_0 e^{a t}\\).\nAccording to the given information about splitting, when the initial condition is that \\(S(0) = 1\\), \\[S(1/3) = S_0 e^{a/3} = e^{a/3} = 2\\ .\\]\nWorking from \\(e^{a/3} = 2\\) gives the parameter value we see: \\(a = 3 \\ln(2)\\).\n\n\n\n\n\n\nApplication area 44.3 —Unstable atoms are everywhere!\n\n\n\n\n\n\n\nApplication area 44.3 Nuclear decay\n\n\n\nRadioactive decay of nuclei is a situation where the linear unstable model is realistic. Previously, we’ve pointed out that radioactive decay of an isotope is well modeled by the linear stable model. This leads the number of atoms of the radioactive isotope to decay exponentially.\nBut there is an important exception. Some radioactive isotopes are fissile, for instance uranium-233, uranium-235, plutonium-239, and plutonium-241. Fissile materials decay via two different mechanisms. One is each-atom-on-its-own decay that applies generally to radioactive isotopes. The other mechanism involves the decay of one atom triggering the decay of others.\nThe means of triggering involves a sub-atomic particle called a neutron. The decay of a fissile atom releases one or more neutrons, depending on the nuclear structure. If one of these neutrons has the correct energy, and if they collide in the appropriate way with a second fissile atom, that second atom will itself undergo decay, leading to the release of more neutrons. Such a process is called a chain reaction.\nTypically, a fissile chain reaction takes place inside an engineered device called a reactor. A simple differential equation of a chain reaction can be constructed using the number of neutrons in the reactor, and is \\[\\partial_t N = \\alpha N\\ .\\] As you’ve already seen, the solution to this is \\(N(t) = A e^{\\alpha t}\\): exponential growth. The value of \\(\\alpha\\) is determined by the design of the device. If the device is small, then neutrons can escape from the device before triggering a reaction, so alpha is small. If there are non-fissile neutron absorbers in the device, neutrons are taken out of play so, again, alpha is small and can even be negative.\nIn everyday terms, rapid exponential growth is called an explosion. Exponential growth cannot continue forever, and indeed a fissile bomb blows itself up, resulting in negative \\(\\alpha\\) as the fissile material is scattered.\nControl of a fission bomb requires a mechanism that changes \\(\\alpha\\) from negative (stable \\(N\\), so no explosion) to positive. This can be accomplished, for instance, by bringing together pieces of fissile material that by themselves has negative \\(\\alpha\\) into a critical mass where \\(\\alpha\\) positive.\nNon-explosive nuclear reactions, as in power plants, call for a kind of juggling act. The device cannot have negative \\(\\alpha\\) or the reaction would die out exponentially. It cannot have positive \\(\\alpha\\) or the reaction would become explosive.\nA power reactors is designed to keep \\(\\alpha\\) near zero. Near zero not at zero. The reactor design needs to allow \\(0 &lt; \\alpha\\) to start up the reaction, but trim this down to zero when the reactor is at the desired power. Typically, there is some combination of active and passive mechanisms to provide this capability. An active mechanism is the insertion of “control rods,” which absorb neutrons by reactor operators to make \\(\\alpha &lt; 0\\) when the power generation is higher than desired. Passive mechanisms can involve the transformation of reactor water into steam, which reduces the ability of neutrons to induce new fissile decays.\nReactors can be very complicated, however. The explosion of the Soviet Union’s Chernobyl reactor in 1986 was caused by a chain of events that led to control rods being withdrawn beyond the design intentions. Due to the construction of the rods, reinserting the rods to dampen the run-away reaction caused the reaction to accelerate. Avoiding such disasters requires a combination of safety oriented design and proper training of the operators. Neither of these were a feature of the 1980s Soviet environment, where secretiveness interfered with proper training and economic motivations to produce were so strong as to encourage widespread cheating.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Modeling dynamics</span>"
    ]
  },
  {
    "objectID": "Dynamics/43-modeling.html#multiple-state-quantities",
    "href": "Dynamics/43-modeling.html#multiple-state-quantities",
    "title": "44  Modeling dynamics",
    "section": "44.2 Multiple state quantities",
    "text": "44.2 Multiple state quantities\nThe previous section examined dynamics of a single state variable. Now we will consider the possibilities when there is a second state variables. It turns out that adding more state variables above two does not introduce many fundamentally new behaviors, so we will focus on dynamical systems with two variables. We will continue to use capital letters, like \\(S\\), to stand for the state variables. But with two (or more) state variables, we will need to give them different names so we can keep tract of what’s doing what to what. The parameters in the dynamical functions will, as has been our practice, be written as lower-case letters, such a, b, r, c, and so on.\nA starting observation is that for dynamics to be genuinely two-dimensional, the differential equations for the state variables need to be coupled to one another. For example, a dynamical system that nominally has two state variables \\(X\\) and \\(Y\\) is:\n\\[\\partial_t X = f(X)\\\\\n\\partial_t Y = g(Y)\\ .\\]\nThe state variables here are not coupled, since the change in each variable depends only on the value of that variable and not on the other.\nCoupled state variables have dynamics that look like this:\n\\[\\begin{eqnarray}\n\\partial_t X & = f(X, Y)\\\\\n\\partial_t Y & = g(X, Y)\\ .\\\\\n\\end{eqnarray}\\] The time evolution of each state variable depends on the other state variable.\nThe most mathematically simple form of coupled dynamics is this:\n\\[\\begin{eqnarray}\n\\partial_t R & = a B\\\\\n\\partial_t B & = c R\\ .\\\\\n\\end{eqnarray}\\]\nWhat type of real-world setting might such a simple model correspond to? Surprisingly, even this simple model has important things to say about complex phenomena such as love and warfare.\nWe will start with warfare, where the signs of the parameters are easy to determine. The model, called Lanchester’s Law, is \\[\\begin{eqnarray}\n\\partial_t R & = - b B\\\\\n\\partial_t B & = -r R\\\\\n\\end{eqnarray}\\] with both parameters \\(r\\) and \\(b\\) taken to be positive.\nThe state variables \\(R\\) and \\(B\\) stand for the size of the two armies in conflict: the Red army versus the Blue army. As the two armies meet in battle, the Blue army causes casualties in the Red army. These casualties reduce the size of the Red army. Similarly, the Red army causes casualties in the Blue army.\nThe model describes the rate of reduction in the armies as being proportional to the size of the opposing army. But the two armies can be different in their efficiency of causing casualties, reflected by possibly different values of the \\(r\\) and \\(b\\) parameters.\nThe dynamics are not exponential. Exponential decay of the army size would correspond to a model like \\(\\partial_t R = - r R\\), an army fighting itself. But the Lanchester model has \\(\\partial_t R = - b B\\).\nWe will defer for a moment finding the trajectories of the state variables in the course of battle. (Hint: in the model, one army wipes out the other.) Instead, we will focus on a surprisingly rich implication of of such simple dynamics.\nLanchester’s Law has a surprising consequence for measuring the overall strength of a force in a way that combines size (\\(R\\) and \\(B\\)) and effectiveness (\\(r\\) and \\(b\\)) and the implications that has for tactics.\nLanchester proposed that the quantity \\[Q(R, B) \\equiv rR^2 - b B^2\\] is a good way to characterize the dynamics. His reasoning was based on a fundamental idea from physics and chemistry: that quantities are conserved. In physics, examples of conserved quantities are energy, momentum, and angular momentum. In chemical reactions, the number of atoms of each species is conserved.\nIt is hard to say how Lanchester came up with the formula \\(rR^2 - b B^2\\): insight is hard to explain. But we can easily demonstrate that it is conserved, that is, the quantity does not change in time regardless of how the battle proceeds. We will do this by showing \\(\\partial_t Q(R, B) = 0\\).\n\\[\\partial_t Q(R, B) = \\partial_t \\left[\\strut rR^2 - b B^2\\right]\\] Applying the chain rule we find that \\[\\partial_t r R^2 = 2 r R\\, \\partial_t R\\ \\ \\ \\ \\text{and}\\ \\ \\ \\partial_t b B^2 = 2 b B\\, \\partial_t B\\ .\\] Substituting in \\(\\partial_t R = - b B\\) and \\(\\partial_t B = - r R\\) gives \\[\\partial_t Q(R, B) =  - 2 r b R B + 2 b r  B R = 0\\ .\\] The conserved quantity \\(Q(R, B)\\) describes an aspect of the battle that goes unchanged over the course of the battle. At any moment, it describes the match between the overall capability of the two armies. That the difference between the capabilities is conserved does not mean the individual capabilities are unchanged in battle. Those capabilities decrease as the army sizes, \\(R\\) and \\(B\\) are reduced. But at any instant in time, the capability of each army is proportional to the square of the army size. change The consequence, is that the capability of each army is, at all times,\nTo illustrate, consider a battle between two armies of archers. The B-army archers are more skilled: they can fire 12 arrows per minute. The R-army archers can fire at only half the rate—6 arrows per minute. But the R army is twice the size of the B army.\nAre the armies equally matched? It may at first glance seem so, since both armies can fire at the same rate. For instance, if there are 1000 archers in the R army and 500 in the B army, both armies start capable of firing 6000 arrows per minute. But Lanchester’s Law tells us that the R army is twice as capable as the B army: \\(6 \\times 1000^2\\) is twice as big as \\(12 \\times 500^2\\).\nTo see why the R army is superior, remember that each arrow can take out only one of the opposing archers. For each B arrow that is on target, the firing rate of R is reduced by 6 arrows per minute. But for each R arrow that hits, the firing rate of B is reduced by 12 arrows per minute. The initial casualty rate for the two armies is the same, but B sees a twice as large reduction in its firing rate.\n\n\n\n\n\n\nCalculus history—Fighting strength\n\n\n\nFrederick William Lanchester (1868-1946) was a British engineer, considered one of the greats of British automotive engineering. But that hardly does justice to him.\nWhile voyaging across the Atlantic to America, he became captivated by the gliding flight of herring gulls. This led to his development of his circulation theory of flight, a foundation of aerofoil theory. In 1906 he published Aerial Flight containing the first full theory of lift and drag. In Aerodonetics (1908) he developed his phugoid theory of aircraft stability, describing oscillations and stalls.\nIn 1914, Lanchester wrote a book-length series of journal articles that were published in 1916 as Aircraft in Warfare: the Dawn of the Fourth Arm. Imagine trying to theorize about a form of conflict that had never been seen!\n\nThe difficulty … is that to get the future into true perspective, it is necessary to be able to look forward along two parallel lines of development—i.e. to visualize the improvement of aircraft possible in the near future as a matter of engineering development, and simultaneously to form a live conception of what this improvement and evolution will open up in the potentialities of the machine as an instrument of war. (p.3)\n\n\n\nMathematician Steven Strogatz proposed in the 1990s that similar equations might be used to describe how love between two people varies over time. Strogatz’s equations are usually written with state variables R and J, standing for Romeo and Juliet. Positive values represent love, negative values are hate. And best to think of the model as a cartoon, but a cartoon that captures some of the behavior seen in reality.\nTo start, let’s consider a form of love that is really more like warfare:\n\\[\\partial_t R = -j J\\\\\n\\partial_t J = - r R \\ .\\] In this pathological relationship, the more Romeo loves Juliet, the faster Juliet’s love decreases toward hate. And vice versa.\nImagine that, somehow, these two perverse people started out in love: \\(R(t=0) &gt; 0\\) and \\(J(t=0) &gt; 0\\). As with Lanchester’s Law, both lovers fall increasingly out of love. Depending on the initial intensity of their love and whether or not \\(jJ^2\\) is bigger than \\(rR^2\\) (Lanchester’s conserved quantity), one of the parties will become indifferent, that is, a love level of 0 while the other’s love level is still positive. For the purpose of example, let’s suppose that Juliet is the first to reach indifference. But unlike the warring armies, the love quantity can become negative. As Juliet’s love becomes negative—remember, Romeo still has a positive level of love—then the true perversity of Romeo’s personality becomes apparent. Juliet’s increasing hostility causes Romeo’s love to grow. Without bound, because this is a linear dynamical function. The result is that Juliet’s hate increases even while Romeo’s love increases. But don’t blame Juliet. If Romeo had been the first to reach indifference, Juliet would suffer the unrequited love and Romeo would be villainously hateful.\nStrangely, if Romeo and Juliet started out mutually hating each other, their personalities would still lead to one having unbounded love for the other, who hates their partner without limit.\nA mathematically small change in the Romeo-Juliet dynamical system can lead to a profound change in the outcome. For instance, changing one sign, as in \\[\\begin{eqnarray}\n\\partial_t R & = r J\\\\\n\\partial_t J & = - j J\\ ,\\\\\n\\end{eqnarray}\\] produces, as we will see, a never-ending cycle of alternating love/hate.\nThe two dynamical functions in the previous examples, \\(-r R\\) and \\(-j J\\) are low-order polynomials. A sensible human being might suggest that the constant term in the polynomials should be added, but the cold, analytic mind of the mathematician would see that this would only change the location of the fixed point and not its stability. That suggests that the next more complicated model to consider involves includes both variables in the dynamical function. Like this:\n\\[\\begin{eqnarray}\n\\partial_t R & = a R + b J\\\\\n\\partial_t J & = c R + d J\\ ,\\\\\n\\end{eqnarray}\\]\nwhere the coefficients \\(a, b, c, d\\) might be either positive or negative depending on the personality of the lovers. Analysis of this model will have to await the introduction of new mathematical tools in Chapter 46.\nWhether Strogatz’s love model is realistic or not, it does illustrate a basic idea of model building: start with simple dynamical functions, check out their consequences, then modify the dynamical functions. We will do this now, with the modifications being purely mathematical along the lines of including different terms in low-order polynomial approximations and considering positive and negative coefficients. As you will see, the simple models correspond to a surprisingly wide range of behaviors.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Modeling dynamics</span>"
    ]
  },
  {
    "objectID": "Dynamics/43-modeling.html#classical-phase-plane-models",
    "href": "Dynamics/43-modeling.html#classical-phase-plane-models",
    "title": "44  Modeling dynamics",
    "section": "44.3 Classical phase-plane models",
    "text": "44.3 Classical phase-plane models\nWe have been using the term state space to refer to the set of possible values for the state variables. When there is only one state variable, the state space corresponds to the number line, or perhaps just the positive half of the number line. When there are two state variables, the state space corresponds to the coordinate plane; any point in the plane is a legitimate state for the system.\nHistorically, another term is used for the two-variable state space: the phase plane. This is just a matter of terminology, but it is so prevalent that you will occasionally see it mentioned. We don’t use it since dynamical systems can have a state space that is 1, 2, 3, or higher dimensional, but the phrase “phase plane” only works for 2-dimensional state spaces.\nIn this section, we will look at some famous models that involve two state variables. Out of respect for history, we will call these “classical phase plane” models, but this is entirely equivalent to saying “classical dynamical models with two state variables.”\nOur purpose in studying these classical models is two-fold: to show how simple models can make it easier to draw out the consequences of the mechanisms that we think are at work in real-world systems; and to show you how modifications to purely linear models can produce dynamics that are realistic even away from fixed points.\nTo start, let’s return to the Rabbit-Fox dynamics models. Classically this is called the predator-prey model. It is also called the “Lotka-Volterra” model in honor of it is inventors: American biophysicist Alfred Lotka (1880-1949) and Italian mathematical physicist Vito Volterra (1860-1940).\nThe two first-order differential equations in the Lotka-Volterra model are \\[\\begin{eqnarray}\n\\partial_t R & = \\alpha R - \\beta F R\\\\\n\\partial_t F & = \\delta F R - \\gamma F\\ ,\\\\\n\\end{eqnarray}\\] each of which contains a linear term (\\(\\alpha R\\) in the \\(R\\) equation, \\(\\gamma F\\) in the F equation). Each equation also contains an interaction term. Here, the mathematical/statistical name for the product of two quantities corresponds nicely with the physical reality that the terms describe what happens to rabbits when they interact with a fox, and similarly what happens to foxes. The parameters \\(\\alpha, \\beta, \\gamma\\), and \\(\\delta\\) can, mathematically, be either positive or negative, but they make sense as representations of rabbits and foxes only if all of them are positive. So the “interaction” is always negative for the rabbits and positive for the foxes.\nRewriting the model provides a bit of insight: \\[\\begin{eqnarray}\n\\partial_t R &  = \\underbrace{(\\alpha - \\beta F)}_{k_R}\\ R\\\\\n\\partial_t F & = \\ \\underbrace{(-\\gamma + \\delta R)}_{k_F}\\ F\\ . \\\\\n\\end{eqnarray}\\] Think about the \\(k_R\\) and \\(k_F\\) terms as the reproduction rates. If the fox population were constant, then the rabbit dynamics would be exponential growth or decay, depending on the sign of \\(k_R = \\alpha - \\beta F\\). Similarly, if the rabbit population were constant, the fox dynamics would be exponential decay or growth, depending on the sign of \\(k_F = -\\gamma + \\delta R\\).\nThe two equations are coupled so that the rabbit population alternating between growth and decay leads the fox population to so alternate, and vice versa.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Modeling dynamics</span>"
    ]
  },
  {
    "objectID": "Dynamics/43-modeling.html#epidemic",
    "href": "Dynamics/43-modeling.html#epidemic",
    "title": "44  Modeling dynamics",
    "section": "44.4 Epidemic",
    "text": "44.4 Epidemic\nIn a communicable disease, such as COVID-19, the infectious agent is transmitted from an infective person to another person who is susceptible. The time course of an epidemic can be modeled simply with two state variables. We will let \\(S(t)\\) be the number of susceptible people and \\(I(t)\\) the number of infective people at any time \\(t\\).\nThe dynamics of the \\(S\\) variable can be simple: \\(S\\) changes when an infective person meets (interacts with!) a susceptible person. That susceptible person, with some probability, becomes infective. So \\[\\partial_t S = -\\beta S I\\ .\\] The dynamics of the \\(I\\) variable are a just a little more complicated. First, every person who is converted from susceptible to infective becomes a new infective. This is the \\(\\beta S I\\) term in the following differential equation. Second, infectives gradually recover. This is often modeled as a simple \\(-\\alpha I\\) term.\n\\[\\partial_t I = \\beta S I - \\alpha I\\] This is famously called the SIR model, standing for the susceptible, infective, recovered chain of events.\nIn the model, recovery means “no longer able to infect.” Thus, a person who has been isolated is considered “recovered,” whether or not they display symptoms of the disease.\nWe usually think of recovery as a time span, for instance taking a week to recover. But \\(\\alpha I\\) does not work this way. To see why, consider the situation starting on the day that there are no more infective people, that is, \\(S=0\\). The dynamics from this day forward are simplified: \\(\\partial_t I = =\\alpha I\\).\nOf course, the solution to this simplified differential equation is \\(I(t) = I_0 e^{-\\alpha t}\\); the size of the infective group gets smaller exponentially. Figure 44.2 compares what \\(I(t)\\) would look like if it takes, say, one week to recover and what \\(I(t)\\) looks like under the model’s \\(\\partial_t I = =\\alpha I\\) dynamics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 44.2: Comparing \\(I(t)\\) for the “takes a week to recover” model and the \\(\\partial_t I = -\\alpha I\\) model.\n\n\n\nTo show the dynamics of the SIR model, we need to propose numerical values for \\(\\alpha\\) and \\(\\beta\\). This is not a trivial matter if the goal is to match the dynamics to a particular disease and size of population. For our purposes here, we will imagine that \\(S(0) = 0.999\\) and \\(I(0) = 0.001\\), which is to say we are looking at the proportion of the population that is susceptible or infective.\nFigure 44.3 shows the flow field for \\(\\beta = 1/2\\) and \\(\\alpha = 1/7\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 44.3: SIR flow field for \\(\\beta=1/2, \\alpha = 1/7\\), and the trajectory for initial conditions \\(S_0 = 0.999, I_0 = 0.001\\).",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Modeling dynamics</span>"
    ]
  },
  {
    "objectID": "Dynamics/44-equilibria.html",
    "href": "Dynamics/44-equilibria.html",
    "title": "45  Equilibrium and transient",
    "section": "",
    "text": "45.1 One state variable\nMany of the natural and constructed objects and systems that we encounter—buildings, bridges, airplanes, the orbits of satellites, heating systems, birds in flight, and so on—are more fully understood if seen as dynamical systems. That may seem strange; a building does not move (we hope!), an airplane stays in steady flight, the seasons have been steady in their progression for as long as records have been kept. And yet … a building might be shaken and even destroyed by an earthquake, airplanes require pilots and control systems for steady flight. Even satellites in far Earth orbit can drift from their desired positions and attitudes and require control corrections.\nA system is said to be in steady state when it stays put, unchanging. Another term often used to express such constancy is equilibrium which occurs when the various forces or processes acting on the system balance out. In the language of dynamical systems, the equilibrium state of a system is called a fixed point. Mathematically, this is a coordinate in the state space where all the right-hand sides of the differential equations equal zero. For a two-dimensional system with dynamics \\[\\partial_t x = f(x, y)\\\\\\partial_t y = g(x,y)\\ ,\\] a fixed point will be a particular value for the state which we will write as \\((x^\\star, y^\\star)\\) where both \\(f(x^\\star, y^\\star) = 0\\) and \\(g(x^\\star, y^\\star) = 0\\).\nAn important vocabulary word in dynamics is transient. In everyday speech, this means something like “just passing through.” it is the same in dynamics: that part of the trajectory which precedes stable, fixed behavior such as at a fixed point. Transients occur whenever a dynamical system has an initial state not on a stable, fixed state. They are also common in systems that are disrupted by some external force, for example in the recovery of an electrical power distribution grid after a disturbance such as an ice storm. After a sharp bang, the ringing in your ears is a transient. When you stand up too suddenly, the “stars” you see are a transient due to diminished blood flow. Turn on an oven? The transient is the warming up until the oven reaches the temperature setting.\nAlthough transients are … Well, transient, they can be very important. Key to the Wright Brothers success was their recognition that air turbulence elicits transients in attitude and that aircraft need control systems that can work fast enough for the craft to survive the transient. If you have driven a car with a broken suspension, you will know that it can be hard to control after the transient caused by hitting a bump in the road.\nSmall disturbances often elicit transients that decay away exponentially. Such transients, like all exponentially decaying processes, can be characterized by a half life: the time it takes the transient to shrink to half its original value. (Not all transients decay exponentially, but that is a story for another course.)\nIn this chapter, we will study the quantitative response to dynamical systems with a fixed point when the state is perturbed by some outside force. Our focus will be on linear or linearized dynamical functions, which are generally an excellent description of dynamics near a fixed point.\nLinear systems with one state variable have simple dynamics: \\[\\partial_t y = k y\\] which has a fixed point at \\(y^\\star = 0\\). Even dynamics like \\(\\partial_t x = k x + b\\) can be easily transformed into this simple form; the fixed point is \\(x^\\star = - b/a\\) and defining \\(y \\equiv x - x^\\star\\) gives the \\(\\partial_t y = k y\\) form.\nThe solution is also simple: \\(y(t) = y_0\\  e^{kt}\\), where \\(y_0\\) is the initial condition on \\(y\\). If the parameter \\(k &lt; 0\\), the dynamics are exponential decay to the fixed point. If \\(0 &lt; k\\), the dynamics are exponential growth away from the fixed point.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Equilibrium and transient</span>"
    ]
  },
  {
    "objectID": "Dynamics/44-equilibria.html#two-state-variables",
    "href": "Dynamics/44-equilibria.html#two-state-variables",
    "title": "45  Equilibrium and transient",
    "section": "45.2 Two state variables",
    "text": "45.2 Two state variables\nIf the state variables \\(x\\) and \\(y\\) are measured with respect to a fixed point, the differential equation of the linear or linearized system is: \\[\\begin{eqnarray}\n\\partial_t x & = a x + b y\\\\\n\\partial_t y & = c x + d y\\ .\\\\\n\\end{eqnarray}\\]\nExponentials are an important form of ansatz for linear differential equations. To show why, let’s review the solution to \\(\\partial_t x = k x\\), but assume that all we know is that the solution is an exponential function of time: \\(x(t) = A e^{\\lambda t}\\) and that we don’t yet know the parameters \\(A\\) or \\(\\lambda\\). As usual for an ansatz, we plug it into both sides of the differential equation, giving \\[\\partial_t A e^{\\lambda t} = k A e^{\\lambda t}\\ \\ \\implies \\lambda A e^{\\lambda t} = k A e^{\\lambda t}\\ \\ \\implies \\lambda = k\\ .\\] Now we know the value of \\(\\lambda\\).\nWhat about \\(A\\)? Evaluate the solution at \\(t=0\\). This gives \\(x(0) = A e^{\\lambda 0} = A\\). So we know \\(A\\) is the initial condition \\(x(0)\\) (which we usually abbreviate \\(x_0\\)).\nWe will try the same approach with the two-state variable system, but we will start with a special case where some of the parameters \\(a, b, c\\), and \\(d\\) are zero.\n\\[\\text{Simplification:}\\ \\ \\  \\ \\ \\begin{array}{l}\\partial_t x  = \\cancel{ax}  +  b y\\\\\\partial_t y =\\ c x + \\cancel{dy}\\end{array}\\ .\\]\nIn the spirit of exponential ansatze, we might try \\[x(t) \\equiv C e^{\\lambda_1 t} \\ \\ \\text{and}\\ \\ \\ y(t) \\equiv D e^{\\lambda_2 t}\\ .\\]\nBut this is unnecessary complexity. To see why plug the ansatze in to the first differential equation to get \\[\\lambda_1 C e^{\\lambda_1 t} = b D e^{\\lambda_2 t}\\ .\\] This can be true only if \\(\\lambda_1 \\neq \\lambda_2\\) because exponentials with different half-lives cannot be proportional to one another.\nIf \\(x(t)\\) and \\(y(t)\\) are proportional to one another, then we hardly need to keep track of both. In fact, we need just one differential equation in \\(x(t)\\). To turn the system of differential equations into a single differential equation we will take the derivative with respect to time of both sides of the top equation, giving: \\[\\partial_{tt} x = b\\,  \\partial_{t\\ }y\\\\\n\\partial_{t\\ } y  =  c\\, x \\] Substitute in the value for \\(\\partial_t y\\) from the bottom equation to get a single, second-order differential equation: \\[ \\partial_{tt} x = b\\, c\\, x\\ .\\]\nPlug in the usual ansatz, \\(x(t) = A e^{\\lambda t}\\) to get \\[\\lambda^2 A e^{\\lambda t} = b\\,c\\, A e^{\\lambda t}\\ \\ \\ \\implies\\ \\ \\ \\ \\lambda = \\pm \\sqrt{\\strut b\\,c}\\] The \\(\\pm\\) is the interesting part here. If, say, \\(b=1\\) and \\(c=1\\), there are two values for \\(\\lambda\\) that will be consistent with the differential equation: \\(\\lambda_1 = 1\\) and \\(\\lambda_2 = -1\\). Either of these will produce a solution that satisfies the differential equation: \\(x_1(t) = A e^{\\lambda_1 t}\\) or \\(x_2(t) = B e^{\\lambda_2 t}\\). So which of the two possibilities is it?\nSince everything about the differential equation is linear, any linear combination of the two possibilities will also satisfy the equation. So we can conclude that \\[x(t) = A e^{\\lambda_1 t} + B e^{\\lambda_2 t}\\ .\\]\nSince \\(\\lambda_2 = -1\\), we know that the \\(B e^{\\lambda_2 t}\\) component of the linear combination will decay to zero. That is, \\(B e^{\\lambda_2 t}\\) is the transient part of the solution.\nWhat are \\(A\\) and \\(B\\)? That depends on the initial condition. Evaluate both sides of the solution equation at \\(t=0\\) to get \\[x(0) = A e^{\\lambda_1 0}+ B e^{\\lambda_2 t} = A + B\\ .\\] At this point, you need to look back at the original system of equations. There are two state variables \\(x\\) and \\(y\\) and therefore we need to specify two components of the initial condition. If \\(x(0)\\) is interpreted as the initial position, then following the example of the pendulum we can look to the velocity \\(\\partial_t x\\) as the second component of the initial condition. From \\(x(t)\\) we can easily calculate the velocity: \\[\\partial_t x(t) = \\lambda_1 A e^{\\lambda_1 t} + \\lambda_2 B e^{\\lambda_2 t}\\ .\\] Again, evaluate this at \\(t=0\\) to get a second equation for the initial condition the pair \\[\\begin{array}{c}\\partial_t x(0)  =   \\lambda_1 A  +  \\lambda_2 B\\\\x(0)  = \\ \\ A \\ \\  + \\ B\\\\\\end{array}\\ \\ \\ \\ \\implies\\ \\ \\ \\\n\\left[\\begin{array}{c}\\lambda_1 \\ \\ \\ \\ \\ \\lambda_2\\\\1 \\ \\ \\ \\ \\ \\  1\\end{array}\\right] \\left[\\begin{array}{c}A\\\\ B\\end{array}\\right] = \\left[\\begin{array}{c}\\partial_t x(0)\\\\ x(0)\\end{array}\\right] .\\] From Block 5, we know how to solve such matrix equations. So, given the initial values \\(x(0)\\) and \\(\\partial_t x(0)\\)—position and velocity—we can find an exact, quantitative solution to the differential equation.\n\n\n\n\n\n\n\n\nTry it! 45.1\n\n\n\n\n\n\n\n\n\nTry it! 45.1 Trajectories and their time series\n\n\n\nFigure 45.1 shows the flow field, some trajectories and their corresponding time series for the system \\[\\partial_t x = b y\\\\ \\partial_t y = c x\\] for \\(b=1\\) and \\(c=2\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 45.1: Three trajectories from the system \\(\\\\partial_t x = y\\) & \\(\\\\partial_t y = 2 x\\)\n\n\n\nEach of these trajectories starts out by heading toward the fixed point at (0,0). Then, each turns and heads away toward \\(\\pm \\infty\\) from the fixed point,\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 45.2: The time series \\(x(t)\\) for the three different trajectories in Figure 45.1.\n\n\n\nEach of the time series is similar, first showing exponential decay toward 0 then exponential growth toward \\(\\pm \\infty\\).\nThe initial conditions for the black and magenta trajectories are very similar. You can imagine a trajectory starting between those initial conditions that would go down the middle of the “trumpet.” This trajectory would be exponential decay toward 0, but would be hard to see since \\(x=0, y=0\\) is an unstable fixed point (a saddle).\nThe initial condition for the black trajectory is \\(x(0)=0.72\\) and \\(y(0)=\\partial_t x(0) = -1\\), while for the magenta trajectory it is \\(x(0)=0.70\\) and \\(y(0)=\\partial x(0) = -1\\). Let’s find each trajectory as a separate linear combination \\(A e^{\\lambda_1 t} + B e^{\\lambda_2 t}\\). The equations to solve are \\[\\left[\\begin{array}{c}1 \\ \\ \\ \\ 1\\\\\\lambda_1 \\ \\  \\lambda_2\\end{array}\\right] \\left[\\begin{array}{c}A\\\\ B\\end{array}\\right] = \\left[\\begin{array}{c}x(0)\\\\ \\partial_t x(0)\\end{array}\\right] .\\]\nBy plugging in the parameters \\(a=0\\), \\(b=1\\), \\(c=2\\), \\(d=0\\) in the dynamical functions, we find that \\[\\lambda = \\pm \\sqrt{\\strut 2} \\approx \\pm 1.4142 \\ .\\] Therefore, we solve \\[\\left[\\begin{array}{c}1.4142 \\ \\ \\  -1.4142\\\\1 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\   1\\end{array}\\right] \\left[\\begin{array}{c}A\\\\ B\\end{array}\\right] = \\left[\\begin{array}{c}\\partial_t\\,x(0)\\\\x(0)\\end{array}\\right] .\\]\nThe computation is shown in Active R chunk 45.1.\n\n\n\nActive R chunk 45.1\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nThe two trajectories are therefore\n\\(x_\\text{black}(t) = 0.0064 e^{1.4142 t} + 0.7136 e^{-1.4142}\\)\n\\(\\color{magenta}{x_\\text{magenta}(t)} = -0.00356 e^{1.4142 t} + 0.70356 e^{-1.4142 t}\\).\nFor both trajectories, the initial amplitude of the decaying exponential is much larger than for the growing exponential. That is why the time series decay toward zero initially. As \\(t\\) grows, the exponential growth become much more important. For the black trajectory, the exponential growth has a positive coefficient, so the growth is toward \\(\\infty\\). But for the magenta trajectory, the exponential growth has a negative coefficient, thus that trajectory grows toward \\(-\\infty\\).\n\n\nThe method we used to solve the simplified problem also works for the original problem \\[\\begin{array}{c}\\partial_t x  = ax  +  b y \\\\ \\partial_t y = c x +  dy\\end{array}\\ .\\]\nStep 1: Differentiate with respect to \\(t\\) both sides of the top equation, giving\n\\[\\begin{array}{c}\\partial_{tt} x  =   a\\, \\partial_t x  +  b\\, \\partial_t y\\\\ \\  \\ \\ \\partial_t y =  c x \\ \\ \\ \\ \\ \\ \\ +  dy\\ \\ \\ \\ \\ \\ \\ \\ \\end{array}\\ .\\] Step 2: Use the second equation to substitute for \\(\\partial_t\\, y\\) in the top equation, giving \\[\\partial_{tt} x = a \\partial_t x + b\\left(\\strut c x + dy\\right) = a\\, \\partial_t x + b\\, c\\, x + b\\, d\\, y\\] Step 3: One more substitution. From the original top equation, we know \\[y = \\frac{\\partial_t x - a x}{b}\\ .\\] Plug this in for \\(y\\) in the result from Step 2, giving \\[\\partial_{tt} x = a\\, \\partial_t x + b\\, c\\, x + b\\, d\\, \\frac{\\partial_t x - a x}{b} = \\left(\\strut a + d\\right)\\ \\partial_t x + \\left(\\strut b c - a d\\right)\\] Step 4: Use the ansatz \\(x(t) = e^{\\lambda t}\\). This produces \\[\\lambda^2 e^{\\lambda t}= (a + d) \\lambda e^{\\lambda t}+ \\left(\\strut bc - ad\\right)e^{\\lambda t} \\ \\ \\ \\implies\\ \\ \\ \\lambda^2 = (a + d) \\lambda + \\left(\\strut bc - ad\\right)\\] which can be solved for \\(\\lambda\\): \\[\\lambda = \\frac{1}{2}\\left(a + d\\right) \\pm \\frac{1}{2}\\sqrt{\\left(a - d\\right)^2 - 4 b c}\\] Again, the \\(\\pm\\) is the interesting bit here. There are two simple solutions that satisfy the differential equation: \\(x_1(t) = e^{\\lambda_1 t}\\) and \\(x_2(t) = e^{\\lambda_2 t}\\). In addition, any linear combination \\(A e^{\\lambda_ t} + B e^{\\lambda_2 t}\\) of these simple solutions will satisfy the differential equations. Once we know \\(\\lambda_1\\) and \\(\\lambda_2\\), the situation is identical to the simplified version. Again, knowing the initial condition \\(x(0)\\) and \\(\\partial_t x(0)\\) allows us to find the coefficients in the linear combination by solving the matrix equation \\[\\left[\\begin{array}{c}\\lambda_1 \\ \\ \\  \\lambda_2\\\\1 \\ \\ \\ \\ \\ 1\\end{array}\\right] \\left[\\begin{array}{c}A\\\\ B\\end{array}\\right] = \\left[\\begin{array}{c}\\partial_tx(0)\\\\  x(0)\\end{array}\\right] .\\]",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Equilibrium and transient</span>"
    ]
  },
  {
    "objectID": "Dynamics/45-eigen.html",
    "href": "Dynamics/45-eigen.html",
    "title": "46  Eigenvalues, eigenvectors",
    "section": "",
    "text": "46.1 Vector solutions to linear differential equations\nIn the previous chapters, you’ve seen how linear dynamics, when unstable, lead trajectories off to infinity. Chapter 44 looked at some ways that nonlinearity can tame instability, as in the simple models of linear growth.\nIn this chapter, we return to linear dynamics to develop a quantitative theory of stability. Such theory is important in many engineering and design applications. For instance, a building exposed to earthquake risk can be economically designed to be strong specifically against the type of shaking produced by earthquakes. An electronic circuit can be designed to be sensitive to certain kinds of communication signals while still resisting noise or jamming.\nThe form in which we have been writing linear differential equations in two state variables is \\[\\begin{eqnarray}\n\\partial_t x & = a x + b y\\\\\n\\partial_t y & = c x + d y\\ .\n\\end{eqnarray}\\]\nA key part of constructing a theory of stability is finding a set of mathematical ideas that enable us to view dynamics in a simpler way. The idea we will introduce here is thinking about the state and trajectory of a differential equation using vectors. We will work here with systems with a two-variable dynamical state, but the results apply just as well to higher dimensional states. That is important in applied work, where the systems being modeled are complicated with many state components.\nWe can re-write the linear differential equation using vector and matrix notation. Suppose that we collect the \\(x\\) and \\(y\\) components of the state into a vector, \\[\\vec{w(t)} =\\left[\\begin{array}{c}x(t)\\\\y(t)\\end{array}\\right]\\ .\\] The differential equation, in terms of \\(\\vec{w(t)}\\) is \\[\\partial_t \\vec{w(t)} = \\left[\\begin{array}{cc}a&b\\\\c&d\\end{array}\\right] \\vec{w(t)}\\ .\\] Now imagine that we pick two non-colinear vectors, \\(\\vec{u_1}\\) and \\(\\vec{u_2}\\) that span the state space. Since the vectors are assumed to span the state, any initial condition can be written as a linear combination of those two vectors: \\[\\vec{w(0)} =\\left[\\begin{array}{c}x(0)\\\\y(0)\\end{array}\\right] = m_1 \\vec{u_1} + m_2 \\vec{u_2}\\ .\\]\nFor the moment, we won’t worry about how best to choose \\(\\vec{u_1}\\) and \\(\\vec{u_2}\\); any two vectors that are not colinear will do.\nAs a running example, we will work with the pair of first-order differential equations \\[\\begin{eqnarray}\n\\partial_t x &= x + y\\\\\n\\partial_t y &= 2 x \\ ,\\\\\n\\end{eqnarray}\\] which, in vector/matrix form are \\[\\partial_t \\vec{w(t)} = \\left[\\begin{array}{cc}1&1\\\\2&0\\end{array}\\right] \\vec{w(t)}\\ .\\] Imagine that we choose, arbitrarily, \\[\\vec{u_1} = \\color{magenta}{\\left[\\begin{array}{r}1\\\\-3\\end{array}\\right]}\\ \\ \\ \\text{and}\\ \\ \\ \\vec{u_2} = \\color{blue}{\\left[\\begin{array}{r}1.0\\\\0\\end{array}\\right]}\\ .\\]\nFor the example, we will calculate a trajectory starting at the initial condition \\(\\vec{w(0)} = \\left[\\begin{array}{r}-1.1\\\\ 2.1\\end{array}\\right]\\):\ntraj &lt;- integrateODE(dx ~ x + y, dy ~ 2*x, # dynamics\n                     x=-1.1, y=2.1, # initial conditions\n                     domain(t=0:2))\ntraj_plot(y(t) ~ x(t), traj)\nThe initial condition (marked “0” in Figure 46.1 is, like any other point in the state space, a linear combination of \\(\\vec{u_1}\\) and \\(\\vec{u_2}\\). We can find the scalar coefficients of the linear combination using any of the methods presented in Block 5, for instance the telescope method. We will illustrate with qr.solve():\nSo, \\(\\vec{w(0)} = -0.7 \\vec{u_1} - 0.4 \\vec{u_2}\\). Keep these scalar coefficients, \\(-0.7\\) and \\(-0.4\\) in mind for the next example.\nWe can use integrateODE() to find the solution starting at any initial condition. In particular, we can find the solution \\(\\vec{u_1}\\) as the initial condition and, similarly, using \\(\\vec{u_2}\\) as the initial condition.\ntraj_u1 &lt;- integrateODE(\n  dx ~ x + y, dy ~ 2*x, # dynamics\n  x=1, y=-3,  #initial conditions\n  domain(t = 0:2))\ntraj_u2 &lt;- integrateODE(\n  dx ~ x + y, dy ~ 2*x, #dynamics\n  x=1, y= 0,  #initial conditions\n  domain(t = 0:2))\nFigure 46.2 shows these trajectories.\nAt first glance, the two trajectories \\(\\vec{u_1(t)}\\) and \\(\\vec{u_2(t)}\\) in Figure 46.2 that start from \\(\\vec{u_1}\\) and \\(\\vec{u_2}\\) might not look much like the trajectory in Figure 46.1 that starts from \\(\\vec{w(0)} = -0.7 \\vec{u_1} - 0.4 \\vec{u_2}\\). But in fact there is a very simple relationship between the trajectories: \\[\\vec{w(t)} = -0.7 \\vec{u_1(t)} - 0.4 \\vec{u_2(t)}\\ .\\] To state the situation more generally, any solution to the differential equations can be written as a linear combination of the solutions starting at \\(\\vec{u_1}\\) and \\(\\vec{u_2}\\), regardless of how \\(\\vec{u_1}\\) and \\(\\vec{u_2}\\) were chosen.\nWe can see this algebraically. Since \\(\\vec{u_1(t)}\\) and \\(\\vec{u_2(t)}\\) are solutions to the linear differential equation, it must be that \\[\\partial_t \\vec{u_1(t)} = \\left[\\begin{array}{cc}1&1\\\\2&0\\end{array}\\right] \\vec{u_1(t)}\\ \\ \\text{and}\\ \\ \\partial_t \\vec{u_2(t)} = \\left[\\begin{array}{cc}1&1\\\\2&0\\end{array}\\right] \\vec{u_2(t)}\\ .\\] Taking a linear combination of these equations gives\n\\[\\partial_t \\left[m_1\\, \\vec{u_1(t)} + m_2 \\vec{u_2(t)}\\right] = \\left[\\begin{array}{cc}1&1\\\\2&0\\end{array}\\right] \\left[m_1\\, \\vec{u_1(t)} + m_2 \\vec{u_2(t)}\\right]\\] The same will be true in general, that is, for the matrix \\(\\left[\\begin{array}{cc}a&b\\\\c&d\\end{array}\\right]\\).",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Eigenvalues, eigenvectors</span>"
    ]
  },
  {
    "objectID": "Dynamics/45-eigen.html#vector-solutions-to-linear-differential-equations",
    "href": "Dynamics/45-eigen.html#vector-solutions-to-linear-differential-equations",
    "title": "46  Eigenvalues, eigenvectors",
    "section": "",
    "text": "## Warning: All aesthetics have length 1, but the data has 500 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 500 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n\n\n\n\n\n\n\n\n\nFigure 46.1: The trajectory calculated starting at (-1.1, 2.1). The graph is annotated with the vectors \\(\\\\vec{u_1}\\) and \\(\\\\vec{u_2}\\) and the flow field.\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n## Warning: All aesthetics have length 1, but the data has 500 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n## All aesthetics have length 1, but the data has 500 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n\n\n\n\n\n\n\n\n\nFigure 46.2: Trajectories from the initial conditions \\(\\color{magenta}{\\vec{u_1}}\\) and \\(\\color{blue}{\\vec{u_2}}\\).",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Eigenvalues, eigenvectors</span>"
    ]
  },
  {
    "objectID": "Dynamics/45-eigen.html#eigenvectors-and-eigenvalues",
    "href": "Dynamics/45-eigen.html#eigenvectors-and-eigenvalues",
    "title": "46  Eigenvalues, eigenvectors",
    "section": "46.2 Eigenvectors and eigenvalues",
    "text": "46.2 Eigenvectors and eigenvalues\nIn the previous section, we saw that the solution to any linear differential equation starting at any initial condition can be written as a linear combination \\(m_1 \\vec{u_1(t)} + m_2 \\vec{u_2(t)}\\), where \\(\\vec{u_1(t)}\\) is the solution starting at an initial condition \\(\\vec{u_1}\\) and \\(\\vec{u_2(t)}\\) is the solution starting at \\(\\vec{u_2}\\). It does not matter how \\(\\vec{u_1}\\) and \\(\\vec{u_2}\\) are chosen, so long as they are not colinear, that is, so long as they span the state space.\nIn this section, we demonstrate that there is a particular way of selecting \\(\\vec{u_1}\\) and \\(\\vec{u_2}\\) that makes the solutions \\(\\vec{u_1(t)}\\) and \\(\\vec{u_2(t)}\\) have a very simple, purely exponential format. The vectors to be chosen are the eigenvectors of the matrix \\(\\left[\\begin{array}{cc}a&b\\\\c&d\\end{array}\\right]\\). We will call these eigenvectors \\(\\vec{\\Lambda_1}\\) and \\(\\vec{\\Lambda_2}\\). (This use of the Greek letter \\(\\Lambda\\) (capital “lambda”) and it is lower-case version \\(\\lambda\\), is conventional in mathematics, physics, and engineering. So it is worth learning to identify the letters.)\nOur task in this section is to show how to compute the eigenvectors \\(\\vec{\\Lambda_1}\\) and \\(\\vec{\\Lambda_2}\\) and that the solutions \\(\\vec{\\Lambda_1(t)}\\) and \\(\\vec{\\Lambda_2(t)}\\) are in fact simple exponentials. Chapter 47 derives the formula for the eigenvectors. Here, we use the R function eigen() to do the calculations for us.\n\n\n\n\n\n\n\n\nTry it! 46.1\n\n\n\n\n\n\n\n\n\nTry it! 46.1 Calculating eigenvectors\n\n\n\nEigenvectors can be calculated using the R function eigen() applied to the abcd matrix that defines the linear differential equation.\nFor the system of first-order differential equations \\[\\partial_t x = x + y\\\\\\partial_t y = 2x\\ \\ \\ \\ \\ \\] the matrix is, as we’ve seen, \\[\\left[\\begin{array}{cc}1&1\\\\2&0\\end{array}\\right]\\ .\\]\nCarrying out the eigenvector calculation is straightforward:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe eigenvectors are the two columns of the matrix labeled vectors returned by the calculation. Here, that is\n\\[\\vec{\\Lambda_1} = \\left[\\begin{array}{r}0.7071\\\\0.7071\\end{array}\\right]\n\\ \\ \\ \\text{and}\\ \\ \\ \\\n\\vec{\\Lambda_2} = \\left[\\begin{array}{r}-0.4472\\\\0.8944\\end{array}\\right]\\ .\\]\nThe calculation also produces eigenvalues. Here that is \\(\\lambda_1 = 2\\) and \\(\\lambda_2 = -1\\).\n\n\nWe can see what’s special about \\(\\vec{\\Lambda_1}\\) and \\(\\vec{\\Lambda_2}\\) by plotting them along with the flow field, as in Figure 46.3.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 46.3: The eigenvectors for the \\(\\left[\\begin{array}{cc}1&1\\\\2&0\\end{array}\\right]\\) plotted along with the flow.\n\n\n\nThe eigenvectors mark the directions where the flow is either directly toward the fixed point or directly away from it. Here, the flow on the subspace of \\(\\color{magenta}{\\vec{\\Lambda_1}}\\) is away from the fixed point, while the flow along the subspace of \\(\\color{blue}{\\vec{\\Lambda_2}}\\) is inward to the fixed point.\nThe consequence of this alignment of the flow with the eigenvectors is that the trajectory from any initial condition \\(m_1 \\vec{\\Lambda_1}\\) will have the form \\(m_1(t) \\vec{\\Lambda_1}\\) and similarly for an initial condition \\(m_2(t) \\vec{\\Lambda_2}\\).\nAs we did in the previous section, let’s calculate the trajectories \\(\\color{magenta}{\\vec{\\Lambda_1(t)}}\\) and \\(\\color{blue}{\\vec{\\Lambda_2(t)}}\\) starting at the two eigenvectors and plot out the \\(y(t)\\) component of the solution. Since we are anticipating an exponential form for the function, we use semi-log axes, where an exponential looks like a straight line.\n\ntraj_eigen1 &lt;- integrateODE(\n  dx ~ x + y, dy ~ 2*x,  # dynamics\n  x = 0.7071, y = 0.7071, # initial conditions\n  domain(t = 0:1) \n  )\n## Solution containing functions x(t), y(t).\ntraj_eigen2 &lt;- integrateODE(\n  dx ~ x + y, dy ~ 2*x,  # dynamics\n  x=-0.4472, y=0.8944,   # initial conditions\n  domain(t = 0:1))\n## Solution containing functions x(t), y(t).\n\n\n\n\n\ntraj_plot(y(t) ~ t, traj_eigen1, color=\"magenta\") |&gt;\n traj_plot(y(t) ~ t, traj_eigen2, color=\"blue\") |&gt; \n  gf_refine(scale_y_log10(\n    breaks=c(0.3290, 0.7071, 0.8944, 5.2248)))\n\n\n\n\n\n\n\n\n\n\nFigure 46.4: Two time series, one showing stability, the other instability.\n\n\n\nWe have marked the \\(y\\) axis with the starting and ending values of each function, so that you can find the exponential parameter \\(k\\) for each function.\n\\(\\color{magenta}{y_1(t) = 0.7071 e^{k_1 t}}\\)\n\\(\\color{blue}{y_2(t)} = 0.8944 e^{k_2 t}\\).\nTo find \\(k_1\\) and \\(k_2\\), plug in \\(t=1\\) to the solution:\n\\(\\color{magenta}{y_1(1) = 5.2248 = 0.7071 e^{k_1}} \\implies k_1=2\\)\n\\(\\color{blue}{y_2(1) = 0.3290 = 0.8944 e^{k_2}} \\implies k_2 = -1\\)\nLook back at the results from the eigen(M) calculation. These values for \\(k_1\\) and \\(k_2\\) are exactly the eigenvalues that were computed from the matrix M. In standard notation, rather than \\(k_1\\) and \\(k_2\\), the notation \\(\\lambda_1 = k_1\\) and \\(\\lambda_2 = k_2\\) is preferred. (Remember, \\(\\lambda\\) is the Greek letter “lambda” in it is lower-case form.) Every solution to the differential equation has the form \\[m_1\\, e^{\\lambda_1 t} \\vec{\\Lambda_1} + m_2\\, e^{\\lambda_2 t} \\vec{\\Lambda_2}\\ .\\] The scalar coefficients \\(m_1\\) and \\(m_2\\) can be found from the initial condition. The stability of the system depends only on \\(\\lambda_1\\) and \\(\\lambda_2\\). If either one of these is positive, then the system is unstable.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Eigenvalues, eigenvectors</span>"
    ]
  },
  {
    "objectID": "Dynamics/46-second-order.html",
    "href": "Dynamics/46-second-order.html",
    "title": "47  Force-balance equations",
    "section": "",
    "text": "47.1 Ballistics\nUp to now, we have been studying dynamics in the format of one or more first-order differential equations. For instance, \\[\\partial_t x = f(x, y)\\\\\n\\partial_t y = g(x, y)\\] where \\(f(x,y)\\) and \\(g(x,y)\\) are the dynamical functions. This is not the style in which differential equations were introduced in the late 1600s. Instead, Isaac Newton (1642-1727) wrote his differential equations in the format of his Second Law of Motion, which reads (in a 1792 translation):\nIn contemporary language, we would say things differently. Instead of “alteration of motion” we would write \\(\\partial_t v\\), where \\(v\\) is the velocity of the moving object. We call \\(\\partial_t v\\) the acceleration. Instead of “motive force” we would say simply “force,” and instead of “made in the direction of the right line in which that force is impressed”, we would say that velocity, acceleration, and force are vector quantities. Newton’s “ever proportional to” amounts to saying that \\[\\partial_t \\vec{v} = b \\vec{F}\\,\\] that is, change in motion is proportional to force. Newton stipulated that the constant of proportionality, \\(b\\), is the reciprocal of mass, that is \\(1/m\\). Writing acceleration \\(\\vec{a} = \\partial_t \\vec{v}\\), the previous equation amounts to \\[m \\vec{a} = \\vec{F}\\ ,\\]\nthe form in which beginning physics students first hear it.\nNewton, of course, was very interested in gravity. From previous experiments dropping weights and rolling balls down ramps, as was done by Galileo Galilee (1564-1642), Newton knew that the force of gravity on an object (near the surface of Earth) is proportional to the object’s mass, that is \\[\\vec{F} = -g m\\ ,\\] where the direction of \\(\\vec{F}\\) is straight downwards toward the center of the Earth. The negative sign in front of \\(g\\) reflects this downward direction. We are assuming that position and velocity are both defined in a positive, upward direction.1 developed his Theory of General Relativity.]\nThe simple model of an object moving under the force of gravity is \\(\\partial_t v = g\\). Notice that this is not a linear differential equation—\\(g\\) is not a linear function of \\(v\\) but a constant, and there is no fixed point—so the solution is not an exponential. But we can find the solution easily enough by integrating both sides of the equation with respect to \\(t\\).\n\\[\\int \\partial_t v\\, dt = \\int -g\\, dt \\ \\ \\implies v(t) = -g\\, t + C\\] where \\(C\\) captures the constants of integration from both integrals into one number.\nIt is worth noticing how much mathematics needs to be understood before this method of solution makes sense. The fundamental theorem of calculus is what tells us that \\(\\int \\partial_t v\\, dt = v(t) + B\\), and you have to know about how to anti-differentiate a constant function to make sense of \\(\\int g\\, dt = -g\\,t + D\\). You also need to know why constants of integration, such as \\(B\\) and \\(D\\), get included when writing the function that results from an anti-differentiation. (You might need to revisit Block 3 to refresh your memory about such things.)\nThere is also physical context to be considered. By setting \\(t=0\\) in \\(v(t) = -g\\,t + C\\), for instance, we can identify \\(C\\) as the velocity at time zero, which we might write \\(v(0)\\) or \\(v_0\\) for short. And what about the position of the object? The solution \\(v(t) = -g\\,t + v_0\\) has nothing to say directly about the position \\(x(t)\\) of the object as a function of time. We can work position \\(x(t)\\) into things by recognizing that \\(v(t) = \\partial_t x(t)\\), which is the definition of velocity.\nAnti-differentiating both sides of \\(v(t) = -g\\, t + v_0\\) gives us a more complete story that include both initial velocity \\(v_0\\) and initial position \\(x_0\\):\n\\(\\int v(t)\\, dt = \\int \\left(\\strut -g\\, t + v_0\\right)\\ dt \\implies x(t) = -\\frac{1}{2} g\\,t^2 + v_0\\,t + x_0\\ ,\\)\nwhere \\(x_0\\) is the constant of integration from this second stage of anti-differentiation. (Plug in \\(t=0\\) to see why we are justified in taking \\(x_0\\) as the initial position.)2\nStill one more way to write the dynamics of falling under the influence of gravity …. Recognizing that \\(v(t) = \\partial_t x(t)\\), we can see that \\(\\partial_t v(t) = \\partial_{tt} x(t)\\). So the original differential equation could be written:\n\\[\\partial_{tt} x = -g\\] This is an example of a second-order differential equation, so called because of the appearance of a second derivative, \\(\\partial_{tt}x\\).\nIn this chapter, we will study second-order differential equations in a variety of contexts. But, as for Newton, movement under the influence of gravity will be a focus. Since the second-order differentiation can be interpreted as representing the balance between force and acceleration, we will call these force-balance equations.\nIn general, a force-balance equation has the form \\[\\partial_{tt} x = f(\\partial_t x, x)\\], the acceleration is a function both of position and velocity. In the above example, the dynamical function has a particularly simple form: \\(f(\\partial_t x, x) \\equiv -g\\).\nSecond-order differential equations can always be written as a pair of first-order differential equations. To see this, let one of the first-order equations be \\[\\partial_t x = v\\ .\\] The other equation, \\(\\partial_{tt} x = f(\\partial_t x, x)\\) can be re-written using \\(v\\): \\[\\partial_t v = f(v, x)\\ .\\]\nSince we know how to solve sets of first-order differential equations by Euler’s method, we can always find the solution \\(x(t)\\) to any second-order differential equation.\nA lot of the theory of second-order differential equations was developed in the setting of a ball being set off with an initial velocity from an initial position. Such a focus on the flight of balls might seem trivial. Fortunately, language allows us to construct a scientific-sounding word by adding the suffix “istic” to the root “ball.” This suffixing produces the word ballistics.\nThe importance of ballistics to Newton can be seen by a famous diagram he drew, shown in Figure 47.1. In the diagram, Newton traces the path of a ball shot horizontally from a cannon placed at the top of a mountain.\nSince the motion in Newton’s diagram has both vertical and horizontal components, we will need two second-order differential equations:\n\\[\\text{Horizontal}: \\ \\ \\partial_{tt} x = 0\\\\\n\\ \\ \\ \\text{Vertical}: \\ \\ \\ \\ \\ \\ \\partial_{tt} y = -g\\] The zero on the right-hand side of the equation of horizontal movement reflects that gravity does not act horizontally.\nWe found a solution for the vertical equation in the previous section, \\[y(t) = -\\frac{1}{2} g\\,t^2 + 0\\,t + y_0\\ .\\] The \\(0\\, t\\) component to the solution reflects that the vertical component of the ball, coming out of the cannon, is zero.\nThe solution for the horizontal component of motion can be found by anti-differentiating both sides of the equation of hortizontal motion: \\[\\int \\partial_{tt} x(t)\\, dt = \\partial_t x(t) = \\int 0\\, dt = v_0\\] where \\(v_0\\) is the initial horizontal velocity. A second stage of anti-differentiation gives \\(x(t)\\) itself: \\[\\int \\partial_t x(t) = \\int v_0 dt = v_0\\, t + x_0\\]\nRegrettably, symbolic anti-differentiation works only in simple cases. To support more realistic models of ballistics, let’s see how to translate the two second-order differential equations into sets of first-order equations. The state variables will be \\(x(t)\\) and \\(y(t)\\), but we also have to add another pair, \\(u(t)\\) and \\(v(t)\\) standing for the horizontal and vertical velocities respectively. The first-order equations will be: \\[\\partial_t x = u\\\\\n\\partial_t y = v\\\\\n\\partial_t u = 0\\\\\n\\partial_t v = -g\n\\] To illustrate, we will solve this set of four first-order equations numerically. We need to specify the initial values for \\(x_0\\), \\(y_0\\), \\(u_0\\) and \\(v_0\\). We will let the cannon be located at horizontal position \\(x_0 = 0\\) and vertical position \\(y_0 = 100\\) meters. The vertical velocity is, initially, zero, so \\(v_0 = 0\\). And suppose the cannon produces an initial horizontal velocity of \\(u_0 = 250\\) meters/sec. The constant \\(g\\) is known to be 9.8 meters/sec2.\nFigure 47.2 shows the trajectory as calculated by integrateODE().\ntraj &lt;- integrateODE(\n  dx ~ u, dy ~ v, \n  du ~ 0, dv ~ -9.8, #dynamics\n  x=0, y=100, u = 250, v=0, # initial conditions\n  bounds(t=0:5)\n)\ntraj_plot(y(t) ~ x(t), traj)\ntraj_plot(v(t) ~ u(t), traj)\nThe left panel in Figure 47.2 shows that the trajectory is a parabola. At about \\(t=4.4\\) secs the \\(y\\) position is zero. Since zero is the location of the ground, the part of the trajectory for \\(4.4 &lt; t\\) is invalid, since the ball has already hit the ground. The ball travels a little more than 1100 meters horizontally before hitting the ground.\nThe right panel might seem somewhat strange. You can see that the vertical component of velocity, \\(v(t)\\) starts out at zero and increases linearly with time, becoming more and more negative as gravity continuous to accelerate the ball downward. The vertical velocity, \\(u(t)\\), stays constant at \\(u(t) = 250\\) meters per second. This is because there is no horizontal force on the ball.\nThe ballistics of real world artillery shells is more complex than the simple model we constructed earlier. What’s missing from that model is air resistance, which is a function of the shell’s velocity and altitude. To illustrate, let’s add in a simple model of air resistance to the earlier ballistic model. In this model, the force of air resistance is a vector pointing in the opposite direction to overall velocity and proportional to velocity squared.\nThe velocity vector is simply \\(\\left[\\begin{array}{c}u\\\\v\\end{array}\\right]\\). The air resistance force will be \\[-\\alpha\\sqrt{\\strut u^2 + v^2} \\left[\\begin{array}{c}u\\\\v\\end{array}\\right]\\ .\\] Consequently, the horizontal component of the air-resistance vector is \\(-\\alpha\\, u \\sqrt{\\strut u^2 + v^2}\\) and the vertical component is \\(-\\alpha\\, v \\sqrt{\\strut u^2 + v^2}\\).\nRepresenting air resistance by the function \\(r(u, v)  \\equiv \\alpha \\sqrt{\\strut u^2 + v^2}\\), the dynamics are \\[\\begin{eqnarray}\n\\partial_t x & = u\\\\\n\\partial_t y & = v\\\\\n\\partial_t u & = -u\\ r(u,v)\\\\\n\\partial_t v &= -g - v\\ r(u,v)\\\\\n\\end{eqnarray}\\]\nintegrateODE() carries out the calculation in R/mosaic.\nr &lt;- makeFun(alpha*sqrt(u^2 + v^2) ~ u & v, alpha = 0.003)\ntraj2 &lt;- integrateODE(\n  dx ~ u, dy ~ v,           # dynamics\n  du ~ -u*r(u,v),           #   \"\n  dv ~ -9.8 - v*r(u,v),     #   \"\n  x=0, y=100, u = 250, v=0, # initial conditions\n  domain(t=0:6)\n)",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Force-balance equations</span>"
    ]
  },
  {
    "objectID": "Dynamics/46-second-order.html#sec-ballistics",
    "href": "Dynamics/46-second-order.html#sec-ballistics",
    "title": "47  Force-balance equations",
    "section": "",
    "text": "Figure 47.1: Newton’s diagram showing ballistic motion under the force of gravity.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 47.2: Trajectory of the cannon ball shot with an initial horizontal velocity and no initial vertical velocity. The trajectory is plotted in slices of state space: position \\((x, y)\\) and velocity \\((u, v)\\). The time at which the ball reaches the points marked in the boxes.\n\n\n\n\n\n\n\n\n\n\n\nCalculus history—Computing trajectories\n\n\n\nThe world’s first programmable, electronic, general-purpose digital computer was started up in 1945 at the University of Pennsylvania, where it is still on display. The date and location have something to say about why the computer was built. 1945 is, of course, at the end of World War II. The computer was built to carry out some important war-time calculations. The place, Philadelpha, Pennsylvania, has to do with the location of the US Army’s center for developing and testing ordnance: the Aberdeen Proving Ground which is only 75 miles from the University of Pennsylvania.\nThe name given to the computer, ENIAC, has a science-fiction flavor but is in fact rooted in its purpose: the Electronic Numerical Integrator and Computer. ENIAC was constructed to calculate the trajectories of artillery shells. Knowing the trajectory is essential to being able to fire artillery accurately.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 47.3: Adding air resistance to the model changes the trajectory. For reference, the trajectory without air resistance is plotted in \\(\\color{orange}{\\text{orange}}\\). Air resistance causes the cannon ball to travel a shorter horizontal distance before hitting the ground and to arrive with a much reduced velocity.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Force-balance equations</span>"
    ]
  },
  {
    "objectID": "Dynamics/46-second-order.html#the-harmonic-oscillator",
    "href": "Dynamics/46-second-order.html#the-harmonic-oscillator",
    "title": "47  Force-balance equations",
    "section": "47.2 The harmonic oscillator",
    "text": "47.2 The harmonic oscillator\nConsider the motion of a weight attached to a spring, as in Figure 50.2. We will denote the vertical position of the mass by \\(y(t)\\). Such a spring-mass system has a fixed point where the spring is stretched just enough to cancel out gravity and the velocity is zero. We will measure \\(y\\) relative to this fixed point.\n\n\n\n\n\n\nFigure 47.4: A spring-mass system in motion. Source: Svjo CC BY-SA via Wikimedia Commons\n\n\n\nAccording to Hooke’s Law, a stretched or compressed spring exerts a force that is proportional to the amount of extension or compression. With our measuring \\(y\\) relative to the fixed point, the Hooke’s Law force will be \\[m\\, \\partial_{tt} y = - s\\, y\\ ,\\] where \\(m\\) is the amount of mass and \\(s\\) is the stiffness of the spring. This force-balance equation corresponds to the second-order differential equation \\[\\partial_{tt} y = - \\frac{s}{m} y\\ .\\]\nYou can see that the motion is oscillatory, which suggests that the solution to the differential equation will be of the form \\(y(t) = A \\sin(\\omega t)\\). Taking this as an ansatz leads to finding a value of \\(\\omega\\), which is called the angular frequency of the oscillation. (The period of oscillation \\(P\\) corresponds to the angular frequency is \\(\\omega = 2 \\pi/P\\).)\nTo find \\(\\omega\\), plug in the ansatz to the differential equation:\n\\[\\partial_{tt} A \\sin(\\omega t) = - \\frac{s}{m}\\, A \\sin(\\omega t)\\] Differentiating \\(\\sin(\\omega t)\\) once let’s us re-write the left-hand side of the equation as a first derivative\n\\[\\partial_{t} A \\omega\\, \\cos(\\omega t) = - \\frac{s}{m}\\, A \\sin(\\omega t)\\] Differentiating again gives \\[- \\omega^2 A\\sin(\\omega\\, t) = - \\frac{s}{m}\\, A\\sin(\\omega t)\\ .\\] Simplifying this by cancelling out the \\(A \\sin(\\omega t)\\) term gives \\(\\omega^2 = \\frac{s}{m}\\), where \\(\\omega\\) is the angular frequency of the oscillation.\n\n\n\n\n\n\nTip\n\n\n\nInstead of using \\(A \\sin(\\omega t)\\) as the ansatz we could have used \\(A \\sin(\\omega t) + B \\cos(\\omega t)\\). Working through this ansatz would produce the same result, that \\(\\omega^2 = \\frac{s}{m}\\). So the solution to the spring-mass system will be, in general, a linear combination of the sine and the cosine functions with angular frequency \\(\\omega\\).",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Force-balance equations</span>"
    ]
  },
  {
    "objectID": "Dynamics/46-second-order.html#exponential-or-sinusoid",
    "href": "Dynamics/46-second-order.html#exponential-or-sinusoid",
    "title": "47  Force-balance equations",
    "section": "47.3 Exponential or sinusoid?",
    "text": "47.3 Exponential or sinusoid?\nChapter 46 established that solutions to second-order linear differential equations have the form \\(m_1 e^{\\lambda_1 t} + m_2 e^{\\lambda_2 t}\\). Yet in the previous section, we saw one linear second-order differential equation, \\(\\partial_{tt} y = - \\omega^2 y\\) where the solution is a linear combination of a sine and a cosine function: \\(y(t) = A \\sin(\\omega t) + B \\cos(\\omega t)\\) with \\(\\omega = \\sqrt{\\frac{s}{m}}\\).\nHow is it possible for the solution to be both in the form of a linear combination of exponentials and a linear combination of sine and cosine? Sinusoids oscillate up and down and up and down, whereas exponentials are monotonic.\nTo find out what might be the relationship between an exponential and a sinusoid, let’s plug an exponential ansatz \\(y(t) = A e^{\\lambda t}\\) into the spring-mass system \\(\\partial_{tt} y = -\\omega^2 y\\).\n\\[\\partial_{tt} A e^{\\lambda t} = \\lambda^2 A e^{\\lambda t} = -\\omega^2 A e^{\\lambda t}\\ .\\] As before, we will cancel out the common term \\(A e^{\\lambda t}\\) to get a simple relationship: \\[\\lambda^2 = -\\omega^2\\ \\ \\ \\implies\\ \\ \\ \\lambda = \\pm \\sqrt{\\strut-1}\\  \\omega \\ .\\] Generally, the symbol \\(i\\) is used to stand for \\(\\sqrt{\\strut -1}\\), so our eigenvalues can be written \\(\\lambda = \\pm i \\omega\\). The solution to the spring-mass system, according to this analysis, is: \\[y(t) = m_1 e^{i\\omega t} + m_2 e^{-i \\omega t}\\]\nIn other words, \\(e^{i \\omega t}\\)—notice the \\(i\\) in the argument to the exponential—is a sinusoid with angular frequency \\(\\omega\\).",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Force-balance equations</span>"
    ]
  },
  {
    "objectID": "Dynamics/46-second-order.html#exponentials-with-imaginary-inputs",
    "href": "Dynamics/46-second-order.html#exponentials-with-imaginary-inputs",
    "title": "47  Force-balance equations",
    "section": "47.4 Exponentials with “imaginary” inputs",
    "text": "47.4 Exponentials with “imaginary” inputs\nThe “imaginary” in the section title is used in its mathematical sense. In interpreting the word “imaginary,” you should keep in mind a long history in mathematics of assigning insulting names to mathematical objects that, at the time they were first introduced. That is why some numbers are vilified as “negative,” and some as “irrational.” The insult is even more dire for numbers like \\(i\\), which are called the “imaginary” numbers. Regrettably, the word “imaginary” leads many people to shy away from them, just as many people avoid genres such as fantasy fiction. That imaginary numbers are introduced as kind of freakish—is there a numerical value for \\(\\sqrt{\\strut -1}\\)?—and rarely touched until advanced calculus, means that students are unused to them.\nYou will only get comfortable with “imaginary” numbers when you start to work with them extensively, as happens in physics and engineering courses. Our goal here is merely to increase your awareness of imaginary numbers and some of the ways they are used in the sciences. To that end, we offer three different approaches to understanding the function \\(e^{i\\omega t}\\).\n\nBasic, pragmatic understanding. This is the level of understanding that you must have to make sense of the rest of this chapter. Here it is: \\[e^{i\\omega t}\\ \\text{is simply a shorthand for}\\ \\cos(\\omega t).\\] So whenever you see \\(e^{i \\omega t}\\), think of \\(\\cos(\\omega t)\\).\nAlgebraic understanding via Taylor Polynomials. (optional) This level of understanding can give you confidence that the basic, pragmatic understanding in (1) has honest roots. It also shows the way that (1) is not 100% on target (although good enough for a large fraction of mathematical work). But for many people, algebra is a rocky road to understanding.\n\nThe starting point for the algebraic understanding is the Taylor polynomial approximation for \\(e^{\\omega t}\\). Recall from Chapter 27 that \\[e^{\\omega t} = 1 + \\omega t + \\frac{1}{2!}\\omega^2 t^2 + \\frac{1}{3!}\\omega^3 t^3 + \\frac{1}{4!} \\omega^4 t^4 + \\frac{1}{5!} \\omega^5 t^5 + \\frac{1}{6!} \\omega^6 t^6 + \\cdots\\] You may also recall the Taylor polynomial expansion of sine and cosine: \\[ \\cos(\\omega t) = 1 - \\frac{1}{2!} \\omega^2 t^2 + \\frac{1}{4!}\\omega^4 t^4 - \\frac{1}{6!} \\omega^6 t^6 + \\cdots\\]\n\\[\\color{magenta}{\\sin(\\omega t) = \\omega t - \\frac{1}{3!}\\omega^3 t^3 + \\frac{1}{5!} \\omega^5 t^5 +  \\cdots}\\] You can see some association between \\(e^{wt}\\), \\(\\cos(\\omega t)\\), and \\(\\sin{\\omega t}\\) by looking at \\[\\cos(\\omega t) + \\color{magenta}{i \\sin(\\omega t)} = 1 + \\color{magenta}{i \\omega t} -\\frac{1}{2!} \\omega^2 t^2 - \\color{magenta}{i \\frac{1}{3!} \\omega^3 t^3} + \\frac{1}{4!}\\omega^4 t^4 + \\color{magenta}{i \\frac{1}{5!} \\omega^5 t^5} - \\frac{1}{6!}\\omega^6 t^6 + \\cdots\\] Now consider the Taylor polynomial for \\(e^{i\\omega t}\\). This will be the same as the Taylor polynomial for \\(e^{\\omega t}\\) but everywhere substituting \\(i \\omega\\) in place of the plain \\(\\omega\\). That is:\n\\[e^{i \\omega t} = 1 + \\color{magenta}{i\\omega t} + \\frac{1}{2!}i^2\\omega^2 t^2 + \\color{magenta}{\\frac{1}{3!}i^3\\omega^3 t^3} + \\frac{1}{4!} i^4\\omega^4 t^4 + \\color{magenta}{\\frac{1}{5!} i^5\\omega^5 t^5} + \\frac{1}{6!} i^6\\omega^6 t^6 + \\cdots\\] Since \\(i\\equiv \\sqrt{\\strut -1}\\), we have the following facts for the powers \\(i^n\\):\n\\[i^2 = -1\\ \\ \\ \\ \\ \\color{magenta}{i^3 = -i}\\ \\ \\ \\ \\ i^4 = 1\\ \\ \\ \\ \\ \\color{magenta}{i^5 = i}\\ \\ \\ \\ \\ i^6 = -1\\ \\ \\text{and so on}.\\] Substitute these facts about \\(i^n\\) into the Taylor polynomial for \\(e^{i\\omega t}\\):\n\\[e^{i \\omega t} = 1 + \\color{magenta}{i\\omega t} - \\frac{1}{2!}\\omega^2 t^2 - \\color{magenta}{i \\frac{1}{3!}\\omega^3 t^3} + \\frac{1}{4!} \\omega^4 t^4 + \\color{magenta}{i \\frac{1}{5!} \\omega^5 t^5} - \\frac{1}{6!} \\omega^6 t^6 + \\cdots\\] which exactly matches the Taylor polynomial for \\(\\cos{\\omega t} + \\color{magenta}{i \\sin(\\omega t)}\\).\n\nThe arithmetic of complex numbers. (optional) A complex number is a number like \\(2 - 3i\\) which consists of two parts: the real-part \\(2\\) and the imaginary part \\(-3\\). When you multiply one complex number by another you get a complex number (although either the real or imaginary parts might happen to be zero.) For example: \\[(2 + 3i)^2 = (2+3i)(2+3i) = \\underbrace{4}_{2\\times 2} + \\underbrace{ \\ 6 i\\ }_{2 (3i)} +   \\underbrace{\\ 6 i\\ }_{(3i)2}\\ \\  \\underbrace{- 9}_{(3i)(3i)}\\  = -5 +12 i.\\] R knows the rules for arithmetic on complex numbers. To demonstrate, consider the oscillations that result from raising a complex number to successive powers.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNotice that the real part of the result oscillates between negative and positive. The imaginary part also oscillates, but delayed a bit from the real part. Just like sine and cosine.\nWe can get a clearer picture by plotting \\(e^{i\\omega t}\\) over the domain \\(0 &lt; t &lt; 10\\). As an example, in Figure 47.5 we will set \\(\\omega = 2\\). We need to be a little careful, since our plotting functions are not arranged to display complex numbers. But there is an easy workaround: plot the “real” and “imaginary” parts separately. The R operators Re() and Im() do this work.\n\n\n\n\nf &lt;- makeFun(exp(1i * omega * t) ~ t, omega = 2)\nslice_plot(Re(f(t)) ~ t, \n           domain(t=0:10), color = \"magenta\") |&gt;\n  slice_plot(Im(f(t)) ~ t, color=\"brown\")\n\n\n\n\n\n\n\n\n\n\nFigure 47.5: The real and imaginary parts of \\(e^{i \\omega t}\\) plotted as a function of \\(t\\).",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Force-balance equations</span>"
    ]
  },
  {
    "objectID": "Dynamics/46-second-order.html#damping",
    "href": "Dynamics/46-second-order.html#damping",
    "title": "47  Force-balance equations",
    "section": "47.5 Damping",
    "text": "47.5 Damping\nIt is common for there to be friction, called damping, in a spring mass system. To keep things very simple, we will consider that the friction is proportional to the velocity and, as in the cannonball example, in the direction opposite to velocity. That is: \\[\\partial_{tt} y = -r\\, \\partial_t y -b y\\ ,\\] where \\(b\\) would be the positive number \\(\\frac{s}{m}\\) and \\(r\\) is another positive number reflecting the magnitude of friction. (Think of \\(r\\) as standing for “resistance.”)\nAs always, this second-order differential equation can be written as a pair of first-order differential equations. One of the first-order differential equations will be \\[\\partial_t y = v\\ ,\\], which is just the definition of velocity \\(v\\). The other first-order equation will be \\[\\partial_t v = -r v  - b y\\ .\\] Both equations are linear.\nIn the previous chapter, we wrote such a pair of linear first-order differential equations as a vector \\[\\vec{w(t)} = \\left[\\begin{array}{c}v(t)\\\\y(t)\\end{array}\\right]\\ .\\] Using the vector \\(\\vec{w(t)}\\), the dynamics can be written in vector/matrix form: \\[\\partial_t \\vec{w} = \\left[\\begin{array}{c}-r \\ \\ \\  -b\\ \\ \\\\1 \\ \\ \\ \\ \\ \\ \\ 0\\end{array}\\right]\\, \\vec{w}\\ .\\] This form suggests, at least to the avid reader of the previous chapter, that we look for a solution \\(y(t) = m_1\\, e^{\\lambda_1\\, t} + m_2\\, e^{\\lambda_2\\, t}\\) written using the eigenvectors and eigenvalues of the matrix \\(\\left[\\begin{array}{cc}r & b\\\\1 & 0\\end{array}\\right]\\).\nWe used the R function eigen() to compute the eigenvalues and eigenvectors of the matrix, given numerical values for \\(r\\) and \\(b\\). Let’s now try to find an algebraic formula for the eigenvalues. After all, it is the eigenvalues that determine the stability of the fixed point.\nAs an ansatz for the for the original second-order differential equation \\[\\partial_{tt} y = r\\, \\partial_t y + b y\\ ,\\] let’s use \\(y(t) = A e^{\\lambda t}\\), a simple exponential function. Plugging in the ansatz to the differential equation gives: \\[A \\lambda^2 e^{\\lambda t} = - r A \\lambda e^{\\lambda t} - b A e^{\\lambda t}\\ .\\] We can cancel out the common term \\(A e^{\\lambda t}\\) from all the terms in the equation, and bring all the terms to the left-hand side of the equation, leaving us with \\[\\lambda^2 + r \\lambda + b = 0\\ .\\] This is a quadratic polynomial in \\(\\lambda\\), so we can use the “quadratic formula” to find values for \\(\\lambda\\) that are consistent with the parameters \\(a\\) and \\(b\\). In applying the quadratic formula you have to remember that the standard statement is for the roots of \\(a x^2 + b x + c = 0\\) and make adjustment for the fact that our polynomial uses the parameter names differently: \\(\\lambda^2 + r \\lambda + b = 0\\).\n\\[\\lambda = \\frac{- r \\pm \\sqrt{\\strut r^2 - 4 b}}{2}\\ .\\] Recall that the parameter \\(r\\) describes the amount of friction or resistance in the system; it is a positive number. Similarly, the nature of springs is that \\(b\\) is a positive number. The relative values of \\(r\\) and \\(b\\) determine the motion of the system.\nSuppose the stiffness of the spring is much larger than the friction. Then \\(r^2 &lt; 4b\\). This being the case, the \\(\\sqrt{\\strut r^2 - 4 b}/2\\) is an imaginary number. Altogether, the eigenvalues is \\(\\lambda = -\\frac{r}{2} \\pm {i \\omega}\\). The solution is \\[y = m_1 e^{\\lambda_1 t} + m_2 e^{\\lambda_2 t}\\] \\[= m_1 e^{-\\frac{r}{2}t + i \\omega t} + m_2 e^{\\frac{r}{2} - i\\omega t}\\] \\[= m_1 e^{-r t/2} e^{i\\omega t} + m_2 e^{-r t/2} e^{-i \\omega t}\\] \\[= e^{-r t/2}\\underbrace{\\left[m_1 e^{i \\omega t} + m2 e^{i\\omega t}\\right]}_{\\text{sinusoid}(\\omega t)}\\] Result: an exponentially decaying sinusoid.\nTo graph this function, we need to choose appropriate numerical values for \\(r\\) and \\(b\\). Let’s set \\(r=1\\). Since \\(r^2 &lt; 4b\\), we must have \\(\\frac{1}{4} &lt; b\\): we will choose \\(b = 6\\) which meets this criterion. Figure 47.6 shows the solution to the differential equation:\n\n\n\n\ntraj &lt;- integrateODE(dv~ -r*v - b*y, dy ~ v, \n                     v=10, y=0, r=1, b=6, \n                     bounds(t=0:20))\n## Solution containing functions v(t), y(t).\ntraj_plot(y(t) ~ t, traj)\n\n\n\n\n\n\n\n\n\n\nFigure 47.6: An exponentially decaying sinusoid arising from \\(r = 1\\) and \\(b = 6\\).\n\n\n\nThis is the situation with a swinging door. You shove it to swing open, after which it oscillates with a decreasing amplitude.\nIn contrast, suppose the spring is weak compared to the damping such that \\(4b &lt; r^2\\). Now \\(\\sqrt{\\strut r^2 - 4b}\\) is a positive number, not imaginary. What’s more, since \\(b\\) is positive, \\(\\sqrt{\\strut r^2 - 4 b} &lt; r\\). This means that both eigenvalues are negative. We will illustrate the situation with \\(r=1, b=0.2\\):\n\n\n\n\ntraj2 &lt;- integrateODE(dv~ -r*v - b*y, dy ~ v, \n                      v=10, y=0, r=1, b=0.1, \n                      bounds(t=0:20))\n## Solution containing functions v(t), y(t).\ntraj_plot(y(t) ~ t, traj2) %&gt;%\n  gf_lims(y = c(0, NA))\n\n\n\n\n\n\n\n\n\n\nFigure 47.7: A heavily damped spring-mass system with \\(r = 1\\) and \\(b = 0.1\\).\n\n\n\nThe situation in Figure 47.7 is the sort of behavior one expects when giving a shove to an exit door in theater or stadium. The shove causes the door to swing open, after which it slowly returns to the closed position. That gives plenty of time for the people following you to get to the door before it closes.\nFinally, consider the case where \\(r^2 - 4 b = 0\\), a balance between resistance and springiness. In this case, both eigenvalues are \\(\\lambda = -r/2\\).\n\n\n\n\ntraj3 &lt;- integrateODE(dv~ -r*v - b*y, dy ~ v, v=10, y=0, r=1, b=0.25, bounds(t=0:20))\n## Solution containing functions v(t), y(t).\ntraj_plot(y(t) ~ t, traj3) %&gt;%\n  gf_lims(y = c(0, NA))\n\n\n\n\n\n\n\n\n\n\nFigure 47.8: A critically damped oscillation with \\(r=1\\), \\(b=0.25\\).\n\n\n\nThis is a situation called critically damped. The door swings open, then closes as fast as it can without any oscillation.\n\nApplication area 47.1  \n\n\n\n\n\n\n\nApplication area 47.1 Stability and eigenvalues\n\n\n\nConsider the second-order linear differential equation \\[\\partial_{tt}\\ y + 2\\, \\partial_t\\, y - 3\\, y = 0\\ .\\] Is the equilibrium point for this system stable?\nFor this system, \\(a=2\\) and \\(b = - 3\\), so the eigenvalues are \\[\\lambda = \\left(-2 \\pm \\sqrt{\\strut 4 + 12}\\right)/2 = 1 \\pm \\sqrt{\\strut 16}/2 = -1 \\pm 2\\] In other words, \\(\\lambda_1 = -3\\) and \\(\\lambda_2 = +1\\). This indicates that the system is a saddle: unstable in one direction and stable in the other.\nTo confirm our work, let’s use eigen() to find the eigenvalues of the matrix \\(\\left[\\begin{array}{cc}2 & 3\\\\1 & 0\\end{array}\\right]\\):\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nAlthough R is doing all the calculations for us, it is possible to write the direction corresponding to an eigenvalue using the the corresponding eigenvector: \\[\\vec{\\Lambda_1} = \\left[\\begin{array}{c}\\lambda_1\\\\1\\end{array}\\right]\\ \\ \\text{and}\\ \\ \\vec{\\Lambda_2} = \\left[\\begin{array}{c}\\lambda_2\\\\1\\end{array}\\right]\\]\nFor the system with \\(\\lambda_1 = 3\\) and \\(\\lambda_2 = -1\\), you can confirm that the eigenvectors calculated with this formula point in the same directions as the eigenvectors reported by eigen().\n\n\nLet’s return to the car-following control system introduced in Chapter 46. Recall that \\(x\\) was defined to be the distance between two cars and \\(x_0\\) the distance to be maintained. Translated to a new variable \\(y = x - x_0\\) the system was \\[\\partial_{tt} y = - b y\\ .\\] You can see that this system has no damping; \\(y(t)\\) will be a sinusoidal oscillation. The ride will be more pleasant, however, if the oscillations can be damped out. To accomplish this, we should add a new term to the second-order differential equation, a damping term to give \\[\\partial_{tt} y = -a\\, \\partial_t y- b\\, y\\ .\\] We should set the parameters \\(a\\) and \\(b\\) to make the real part of the eigenvalues negative. Only then will we have designed a workable control system.\n\nApplication area 47.2 —Following the car in front using derivatives.\n\n\n\n\n\n\n\nApplication area 47.2 Driving a car\n\n\n\nFor a human driver, following a car at a steady distance requires careful attention but in practice is not too difficult a task. Could it be the case that drivers have an intuitive understanding of the need for damping? Perhaps complex eigenvalues ought to be a standard topic in driving schools? That might be, but there is a more down-to-earth explanation of how humans handle the car-following task.\nThe quantity \\(\\partial_{tt} y\\) is the acceleration, and the control pedal that leads to positive acceleration is called the “accelerator.” But the pedal does not set acceleration. In reality, the pedal sets velocity as well as acceleration. A simple model is \\(\\text{pedal} = r \\partial_t y + s \\partial_{tt} y\\), where \\(y\\) is the velocity of the car and \\(r\\) and \\(s\\) are positive parameters.\nTo understand this model of the pedal, think what happens when you press the accelerator and hold it. The car accelerates, but only up to the point where a steady state velocity is reached. Or, consider what happens if you partially release the pedal. The car slows down until it reaches a new, slower, steady-state velocity.\nWith a human driver, the control system is not \\(\\partial_{tt} y = - b y\\). Instead, the control system is \\[\\text{pedal} - p_0 =  - b y\\ .\\] For steady-state driving at the desired velocity \\(\\partial_t y\\) we press the pedal by an amount \\(p_0\\). To perform the car-following task, we push down or lighten up on the pedal, depending on whether we are farther or closer to the car ahead than our desired distance.\nCombining the models for how \\(\\text{pedal}\\) is controlled and how \\(\\text{pedal}\\) relates to velocity and acceleration, we have \\[r \\partial_t y + s \\partial_{tt} y  - p_0 = -b y\\] or, re-arranging terms \\[ \\partial_{tt} y = \\underbrace{- \\frac{r}{s} \\partial_t y}_{\\text{damping}} - \\frac{b}{s} y + p_0\\ .\\] The nature of the gas pedal itself leads to a damping term in the dynamics, without our having to think about it consciously.",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Force-balance equations</span>"
    ]
  },
  {
    "objectID": "Dynamics/46-second-order.html#footnotes",
    "href": "Dynamics/46-second-order.html#footnotes",
    "title": "47  Force-balance equations",
    "section": "",
    "text": "it is remarkable that the same \\(m\\) appears both in Newton’s Second Law and in the description of the force of gravity. There was no mathematical theory for this until Albert Einstein (1879-1955)↩︎\nAnother bit of physics which is still not included in the differential equation is that the dynamics hold only until the object hits the ground, at which point the force of gravity will be counter-acted by the force of the ground on the object.↩︎",
    "crumbs": [
      "BLOCK V. Dynamics",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Force-balance equations</span>"
    ]
  },
  {
    "objectID": "manifestations-part.html",
    "href": "manifestations-part.html",
    "title": "BLOCK VI. Manifestations",
    "section": "",
    "text": "The ideas of calculus are used throughout science and technology. Previous blocks have introduced the mathematical ideas themselves, often illustrated with examples of real-world systems. In this Block, we will explore some of the ways calculus concepts are manifested in different fields and for different uses. Of course, each reader may find some fields more interesting than others. Think of this Block as a sampler containing a handful of the myriad ways that calculus concepts are used.",
    "crumbs": [
      "BLOCK VI. Manifestations"
    ]
  },
  {
    "objectID": "Manifestations/47-operations.html",
    "href": "Manifestations/47-operations.html",
    "title": "48  Operations on functions",
    "section": "",
    "text": "48.1 Task: Solve\nBlock 1 introduced the idea of mathematical modeling: creating a representation of some aspect of the world out of mathematical “stuff.” The relevant “stuff” includes the concept of a function with its inputs and output, units and dimensions of quantities, frameworks such as the basic modeling functions and ways of combining functions via linear combination, composition, and multiplication.\nOur emphasis in calculus has been and will continue to be functions. This contrasts with high-school algebra where the emphasis was on equations and manipulations such as the movement of quantities from one side of the equal sign to another.\nIt pays to think a little about what equations mean and what information they are intended to convey. Consider an equation like \\[{\\mathbf{\\text{equation:}}}\\ \\ \\ x^2 - 4 x + 3 = 0\\] which you might see in a beginning algebra text. What does this equation mean?\nA simple equation like \\(3 + 4 = 7\\) is a statement of fact: three plus four is indeed the same as seven. But \\(x^2 - 4 x + 3 = 0\\) is not a fact. The equality might be true or false, depending on what \\(x\\) happens to be. In an algebra course, the equation is intended to be an instruction to a person: \\[\\text{Given}\\ x^2 - 4 x + 3  = 0, \\ \\ \\text{find x.}\\] or, equivalently, \\[\\text{Solve}\\ x^2 - 4 x + 3  = 0\\ \\ \\text{for}\\ x.\\] “Find \\(x\\)” or “Solve for \\(x\\)” direct you to determine which numerical values (if any) when substituted for \\(x\\) in the equation will produce a true statement.\n“Solve for \\(x\\)” is an example of a mathematical task. We undertake such tasks to extract useful information from a mathematical object. For instance, textbook “word problems” involve two phases: i) a modeling phase where you translate a verbal description of a situation—often involving paddling canoes across a flowing river—into a matching mathematical form and ii) having constructed a suitable mathematical form, you apply some mathematical task to the form to reveal the answer you seek.\nThis chapter looks at a small list of mathematical tasks, calling them operations on functions. These operations, combined in various ways, enable you to extract relevant information from the functions you build in your models. A simple important part of this introduction is to give a name to each task. That way, confronted with a mathematical problem, you will be able to look down the short mental menu of opertions to decide which ones are applicable to your circumstance. Even better, once each operation has a name, you can tell a computer to do it for you.\nHere are four common mathematical tasks that you’ve already encountered in Blocks 1 through 3 of this book:\nIn this chapter, we focus on the following operations on functions that you may not yet have mastered.\nThese seven tasks allow you to perform the mathematical work of extracting useful information from a model. Human judgement and creativity is needed to construct the model. And judgement and experience is needed to figure out which tasks to perform and in what order. But carrying out the tasks does not require judgement, experience, or creativity. Performing the tasks requires only an algorithm and the tools to step through the algorithm. Computers are excellent for this; you just have to give them the function and whatever additional input is required (e.g. the name of a with-respect-to input), and then tell the computer which task it is to perform.\nStarting materials:\nIdeal result from the algorithm: A new candidate \\(\\color{magenta}{x^\\star}\\) such that \\(f(\\color{magenta}{x^\\star}) = v\\) or, equivalently, that \\[\\left\\|\\strut f(\\color{magenta}{x^\\star}) - v \\right\\| = 0\\ .\\]\nRealistic result from the algorithm: The new candidate \\(\\color{magenta}{x^\\star}\\) will be better than \\(x_0\\), that is, \\[ \\left\\|\\strut f(\\color{magenta}{x^\\star}) - v\\right\\|\\ \\  {\\mathbf &lt;}\\ \\  \\left\\|\\strut f(\\color{brown}{x_0}) - v\\right\\|\\] One algorithm for the operation involves approximating the function \\(f(x)\\) with a straight-line function \\(\\widehat{f}(x) = a x + b\\). For straight-line functions, the solution \\(x^\\star\\) can be found by simple arithmetic:\n\\[a x^\\star + b - v = 0 \\ \\ \\implies \\ \\ \\ x^\\star = \\frac{b-v}{a}\\] You saw in Block 2 how to construct the straight-line approximation to a function \\(f()\\) in a region of interest near \\(x_0\\) by evaluating the function and its derivative at \\(x_0\\). In other words, \\[\\widehat{f}(x) \\equiv f(x_0) + \\partial_x f(x_0) \\left[\\strut x - x_0 \\right]\\ .\\]\nBecause \\(\\widehat{f}(x)\\) is a straight-line function, it is easy to find an input \\(x_1\\) that will generate exactly the desired output value \\(v\\). In other words, to solve \\(\\widehat{f}(x_1) = v\\) for \\(x_1\\).\n\\[\\begin{equation}\nx_1 = x_0 + \\frac{v-f(x_0)}{\\partial_x f(x_0)}\n\\end{equation}\\]\nAlthough \\(x_1\\) is an exact solution to the approximate problem, all we can hope is that for nonlinear \\(f(x)\\), \\(x_1\\) will be an approximate solution to the actual problem. In particular, we want \\(x_1\\) to be a better guess than \\(x_0\\): \\[\\|f(x_1) - v\\| \\underbrace{\\ \\ &lt;\\ \\ }_\\text{We hope!} \\|f(x_0) - v\\|\\]\nThis (hopefully) improved solution \\(x_1\\) can become the starting guess for a new round of improvement based on the straight-line approximation to \\(f(x)\\) around \\(x_1\\). The refinement of \\(x_1\\) will be calculated as \\[\\begin{equation}\nx_2 = x_1 + \\frac{v-f(x_1)}{\\partial_x f(x_1)}\n\\end{equation}\\]\nEach round of improvement—that is, “iteration”—calculates a new value \\(x_{i+1}\\) from the previous \\(x_i\\). The improvement can be encapsulated as a function, which we will call solve_step(): \\[\\text{solve-step}(z) \\equiv z + \\frac{v-f(z)}{\\partial_x f(z)}\\ .\\]\nThis particular form of solve_step() is called a Newton step. The idea is to take successive steps, each refining the previous approximation, to get closer and closer (hopefully!) to the actual answer \\(x^\\star\\):\n\\[x_1 = \\text{solve-step}(x_0)\\] \\[x_2 = \\text{solve-step}(x_1)\\] \\[x_3 = \\text{solve-step}(x_2)\\] \\[\\vdots\\] \\[x_{i+1} = \\text{solve-step}(x_{i})\\] \\[\\text{until eventually}\\ \\|f(x_{i+1}) - v\\|\\ \\text{is practically zero.}\\]\nNewton’s method involves creating a custom solve_step() function for each new problem. The process is simple enough that we can create such functions automatically:\nLet’s test it out with this function:\nConstruct the take-a-step function:\nTake three steps starting at \\(x_0 = 3\\):\nThe Newton-step process is not guaranteed to work. By exploring cases where it fails, computational mathematicians1 have developed strategies for increasing the range of situations for which it works. Some of these strategies are incorporated in the R/mosaic function Zeros().",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Operations on functions</span>"
    ]
  },
  {
    "objectID": "Manifestations/47-operations.html#task-solve",
    "href": "Manifestations/47-operations.html#task-solve",
    "title": "48  Operations on functions",
    "section": "",
    "text": "a function \\(f(x)\\),\n\na known output value \\(v\\), and\n\na candidate for a suitable input value \\(\\color{brown}{x_0}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 48.1: A Newton-step calculation seen graphically. The brown function is approximated as a straight-line function at the initial point \\(x_0\\). The resulting \\(x_1\\) is where that straight line crosses the value \\(v\\) on the output scale. Here, \\(x_1\\) is a little to the left of the actual place where \\(f()\\) crosses \\(v\\). The Newton step produced an improved guess, since \\(\\|x_1 - x^\\star\\|\\) is smaller than \\(\\| x_0 - x^\\star\\|\\).\n\n\n\n\n\n\n\n\n\n\n\nTry it! 48.1\n\n\n\n\n\n\n\n\n\nTry it! 48.1 Taking a Newton step\n\n\n\nConstruct the Newton-step function for finding zeros of the function \\[f(x) \\equiv x^2 - x\\ \\]\nSince \\(\\partial_x f(x) = 2 x - 1\\), the custom-built Newton-step function will be: \\[\\text{solve-step}(z) = z - \\frac{z^2 - z - 4}{2 z - 1}\\]\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe algorithm requires a starting guess. We will use \\(x_0 = 2\\). After each application of solve_step(), we will print out the refined value as well as the function output at that refined value.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe output \\(f(x_3)\\) is practically the desired \\(v=4\\) so we have our result: \\(x^\\star = 2.56155\\)!\nAfter the first Newton step, producing \\(x_1 = 2.666666\\), the function output and \\(f(x_1) = 4.44444\\) was not sufficiently close to the desired output for us to take \\(x_1\\) as the solution. You can think of the problem like the task of digging a well. You need to start with the first shovelful. Then take another and another and … until you have your well.\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\nZeros()\n\n\n\nZeros() takes two arguments: a function and a domain. The function is specified, as with other R/mosaic operators such as D(), slice_plot(), etc., as a tilde expression. Zeros() searches the domain for an input which makes the value of the function zero. If, instead, you want to find an input that makes the function value some other value, say \\(f(x^\\star) = v\\), you construct an intermediate expression f(x) - v ~ x. Finding the zero of the intermediate function corresponds to finding \\(f(x^star) = v\\).\nSometimes there will be multiple zeros on the specified domain. To handle such situations, Zeros() returns a data frame with two columns. The first gives input values that correspond to an output near zero. The second column, named .output. calculates the output (and will be near zero). We will illustrate by solving \\(x^3 = 6\\) for \\(x\\).\n\n\n\n\n\n\n\n\n\n\nTry it! 48.2\n\n\n\n\n\n\n\n\n\nTry it! 48.2 “Solving” by finding zeros.\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Operations on functions</span>"
    ]
  },
  {
    "objectID": "Manifestations/47-operations.html#task-argmax",
    "href": "Manifestations/47-operations.html#task-argmax",
    "title": "48  Operations on functions",
    "section": "48.2 Task: Argmax",
    "text": "48.2 Task: Argmax\nThe task of finding the input value that corresponds to a local maximum is called argmax finding. We don’t need to know the value of the local maximum to solve this problem. Instead, we designate a locale by specifying an initial guess \\(x_0\\) for the argmax. For argmax finding of an objective function \\(f(x)\\), we seek a \\(x^\\star\\) such that \\(f(x^\\star) &gt; f(x_0)\\).\nTo accomplish this, we will approximate \\(f(x)\\) with a low-order polynomial, as we so often do. We will call the approximation \\(\\widehat{f(x)}\\). In the solving task, the approximation was with a first-order polynomial. But first-order polynomials—that is, straight-line functions—don’t have a local argmax. We need to use a second-order polynomial. Easy enough: construct the second-order Taylor polynomial around \\(x_0\\):\n\\[\\widehat{f}(x) \\equiv f(x_0) + f'(x_0) \\left[x - x_0\\right] + \\frac{1}{2} f''(x_0) \\left[x-x_0\\right]^2\\] Remember that \\(f(x_0)\\), \\(f'(x_0)\\) and \\(f''(x_0)\\) are all fixed quantities; the output of the functions for the specific input \\(x_0\\).\nTo find the argmax of \\(\\widehat{f}(x)\\), differentiate it with respect to \\(x\\) and find the zero of the derivative: \\[\\partial_x \\widehat{f(x)} = f'(x_0) \\underbrace{\\partial_x\\left[x - x_0\\right]}_{{\\large\\strut}1} +\n\\frac{1}{2} f''(x_0) \\underbrace{\\partial_x\\left[x-x_0\\right]^2}_{2 \\left[x - x_0\\right]} = 0\n\\]\nThis gives \\[f'(x_0) + f''(x_0) \\left[x - x_0\\right] = 0\\ .\\] We will solve this equation for \\(x\\) and, having in mind the iterative process of the previous section, call the result \\(x_1\\) \\[x_1 = x_0 - \\frac{f'(x_0)}{f''(x_0)}\\ .\\] In other words, our new guess \\(x_1\\) will be a step away from the old guess \\(x_0\\), with the step being \\(-f'(x_0) / f''(x_0)\\). This also is called a Newton step. What’s different from the Newton step of the previous section is that the function whose zeros are being sought is not \\(f(x)\\) but \\(f'(x)\\).\n\n\n\n\n\n\n\n\nTry it! 48.3\n\n\n\n\n\n\n\n\n\nTry it! 48.3 Searching for an argmax\n\n\n\nUse the R/mosaic argM() function to find argmaxes and argmins. Like other R/mosaic calculus functions, the first argument is a tilde expression defining the objective function. The second argument is the domain to search.\nTo illustrate, the following code creates a randomly shaped function (displayed in Figure 48.2) and calls argM() to generate the argmaxes and argmins.\n\nf &lt;- doodle_fun(~ x, 3215)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 48.2: Dots mark the \\(x\\)-coordinates of the argmax and argmin. The maximum and minimum are the function values at those points.\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNotice that argM() identified both a local maximum and a local minimum, that is, one argmax and one argmin. Visually, it is easy to tell which one is which. In terms of the data frame returned by argM(), the sign of the concavity does the identification for you: positive concavity points to an argmin, negative concavity to an argmax. The name argM() refers to this versatility of finding both argmins and argmaxes.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Operations on functions</span>"
    ]
  },
  {
    "objectID": "Manifestations/47-operations.html#task-iterate",
    "href": "Manifestations/47-operations.html#task-iterate",
    "title": "48  Operations on functions",
    "section": "48.3 Task: Iterate",
    "text": "48.3 Task: Iterate\nIn everyday language, to iterate means simply to repeat: to do something over and over again. In mathematics and in computing, “iterate” has a more specific meaning: to repeatedly perform an operation, each time taking the output from the previous round as the input to the current round.\nFor our purposes, it suffices to define iteration as the use of a function \\(g(x)\\). The function \\(g(x)\\) must be such that the output of the function can be used as an input to the function; the output must be the same kind of thing as the input. The iteration starts with a specific value for the input. We will call this value \\(x_0\\). Iteration then means simply to compose the function with itself starting with \\(x_0\\) as the initial input. Here, for instance, is a four-step iteration: \\[g(g(g(g(x_0))))\\] Or, you might choose to iterate for ten steps: \\[g(g(g(g(g(g(g(g(g(g(x_0))))))))))\\] However many iteration steps you take, the output from the final step is what you work with.\nIteration is the mathematical engine behind many function operations. You’ve already seen it at work for the “solve” task and the “argmax” task.\n\n\n\n\n\n\n\n\nTry it! 48.4\n\n\n\n\n\n\n\n\n\nTry it! 48.4 Iterating automatically\n\n\n\nThe R/mosaic function Iterate() provides a very simple way to see the results of iteration. Typically when iteration is used as part of a function operation, the software has been written specifically for that task and includes logic about when to stop or start over or handle a variety of troublesome cases. The function Iterate() is provided in R/mosaic just for demonstration purposes.\nIterate() takes arguments specifying the function to be iterated (as a tilde expression), the starting \\(x_0\\), and the number \\(n\\) of steps to take. To illustrate, we will iterate a famous function called the logistic map: \\(f(x) \\equiv \\mu x (1-x)\\). Depending on the value of the parameter \\(\\mu\\), the iterates can show different patterns.\nEventually reaching a fixed point, as in Active R chunk 48.1\n\n\n\nActive R chunk 48.1\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nEventually reaching a periodic oscillation, as in Active R chunk 48.2:\n\n\n\nActive R chunk 48.2\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nOr even a never-ending, random-seeming fluctuation, called mathematical chaos, as in Active R chunk 48.3\n\n\n\nActive R chunk 48.3\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Operations on functions</span>"
    ]
  },
  {
    "objectID": "Manifestations/47-operations.html#software-for-the-tasks",
    "href": "Manifestations/47-operations.html#software-for-the-tasks",
    "title": "48  Operations on functions",
    "section": "48.4 Software for the tasks",
    "text": "48.4 Software for the tasks\nEvaluation of a function—number one in the list at the head of this chapter—is so central to the use of computer languages generally that every language provides a direct means for doing so. In R, as you know, the evaluation syntax involves following the name of the functions by a pair of parentheses, placing in those parenthesis the values for the various arguments to the function. Example: log(5)\nThe other six operations on functions listed above, there is one (or sometimes more) specific R/mosaic functions. Every one of them takes, as a first argument, a tilde expression describing the function on which the operation is to be formed; on the left side is a formula for the function (which can be in terms of other, previously defined functions), on the right side is the with-respect-to input.\n\nDifferentiate: D(). Returns a function.\nAnti-differentiate: antiD(). Returns a function.\nIntegrate: Integrate(). Returns a number.\nSolve: Zeros(). Returns a data frame with one row for each solution found.\nArgmax: argM() Finds one argmax and one argmin in the domain. local_argM() looks for all the local argmaxes and argmins. Returns a data frame with one row for each argmax or argmin found.\nIterate: Iterate() Returns a data frame with the value of the initial input and the output after each iteration.\n\nEach of operations 4-6 involves the specification of a domain. For Integrate(), this is, naturally, the domain of integration: the upper and lower bounds of the integral\nFor Zeros() and argM() the domain specifies where to search for the answer. Iterate() is slightly different. After the tilde expression comes an initial value \\(x_0\\) and then n= which you use to set the number of times to iterate.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Operations on functions</span>"
    ]
  },
  {
    "objectID": "Manifestations/47-operations.html#footnotes",
    "href": "Manifestations/47-operations.html#footnotes",
    "title": "48  Operations on functions",
    "section": "",
    "text": "A traditional name for such a person is “numerical analyst.”↩︎",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Operations on functions</span>"
    ]
  },
  {
    "objectID": "Manifestations/48-splines.html",
    "href": "Manifestations/48-splines.html",
    "title": "49  Data-driven functions",
    "section": "",
    "text": "49.1 Generating smooth motion\nAs early as Chapter 3 of this book, we noted that a function can be described as a table where each row stands for one set of input values together with a corresponding output value. We did not, however, make much use of the table-as-function concept. Instead, we used data tables to motivate the choice of parameters, as in linear combinations of the basic modeling functions. We called this function fitting: constructing a function that stays near the data values. (We will say more about function fitting in Block 5 where we introduce new tools for treating functions as geometrical objects.)\nThis chapter introduces yet another important method for constructing functions that match with data. What’s different here is that each data point will be a mandate; the function is required to go through each and every data point exactly.\nAs a motivating example, consider the programming of robotic arms of the sort displayed in Figure 49.1:\nSince this isn’t a robots course, we will simplify. The arm has a resting position. When a car frame comes into place, the arm moves so that its welding electrodes are at a specific, known place in space near the car body. Then it moves in sequence to other places where a weld is required, perhaps passing through waypoints to avoid obstacles.\nThe problem of converting the discrete list of weld and waypoints into a continuous signal for the actuator is an instance of a mathematical process called interpolation. In real robot arms, there are multiple joints that need to be controlled simultaneously. For our illustration, we will use a simple setup (see Figure 49.2) where the robot hand rolls along a set of rails in the y-direction and another x-rail running crosswise to the y direction.\nThe task for our example robot will be to visit the points shown in Figure 49.3 in order, taking 15 seconds to traverse the whole path.\nFigure 49.3 shows a continuous path in \\((x,y)\\) coordinates together with discrete labels indicating when each waypoint is to be reached. Note that the path is not a function \\(y(x)\\). Mathematical functions are required to be single valued, meaning that for each value of the input (in the function domain) there can be only one, unique output value. The path in Figure 49.3 often involves two or more different \\(y\\) values for a single \\(x\\) value. There is even a small domain of \\(x\\) near \\(x=900\\) where the path at any given \\(x\\) crosses six different \\(y\\)-values.\nEven so, functions can be a useful way of describing the \\((x,y)\\) path. The key is the plural: functions. For the path in Figure 49.3 we need two quantities varying with time in a coordinated way. One approach, familiar to navigators, is to specify direction of movement and velocity at each instant of time. Perhaps not as familiar, but more fundamental mathematically, is to specify \\(x\\) as a function of time and, separately, \\(y\\) as a function of time. Using this formalism, the trajectory of the robot arm will consist of two functions, \\(x(t)\\) and \\(y(t)\\). To build those functions, we start with the waypoints stored in the data frame Robot_stations.\nThe \\(x(t)\\) and \\(y(t)\\) functions in this table aren’t complete enough to operate the robot. We need to provide the \\(x,y\\)-location data in the form of two continuous functions of \\(t\\) so that the robot, at any time \\(t\\), can look up where it is supposed to be, what its velocity should be, and how that velocity should be changing in time (acceleration).\nOne strategy is to construct the functions as piecewise linear functions of \\(t\\), like this:\nIt can be difficult at first glance to see the relationship between the \\(x(t)\\) and \\(y(t)\\) functions and the path shown in Figure 49.3. As an exercise, look specifically at the segment \\(9 \\leq t \\leq 10\\). In Figure 49.4, this is the segment connecting points 9 and 10. In the path view, you can see that on this segment \\(x\\) changes a lot while \\(y\\) changes only a little. Correspondingly, in the function view, \\(\\partial_t x(t)\\) is large in magnitude compared to \\(\\partial_t y(t)\\).\nEach functions in Figure 49.4 is an interpolating function. You’re entitled to think of the \\(x(t)\\) function as connecting with lines the sequence of \\(x\\) versus \\(t\\) coordinates from the table and similarly for \\(y(t)\\). Each of the two functions is continuous. But, based on your work in Blocks 1 through 3, you have a richer set of concepts for interpreting those two functions.\nFor instance, let’s look at \\(\\partial_t y(t)\\). Since \\(y\\) is a position along the cross rail, \\(\\partial_t y(t)\\) is the velocity in that direction. Figure 49.5 shows the velocity versus time for both the \\(x\\) and \\(y\\) components of the movement.\nThe speed of the robot arm maxes out at about 600 mm-per-second. You can get a sense for this by moving your finger two feet in 1 second: a normal human speed of movement.\nSince the original \\(x(t)\\) and \\(y(t)\\) functions are piecewise linear, it makes sense that the derivatives with respect to time are piecewise constant. But the robot hand is a physical thing; it has to have a velocity at every instant in time. It cannot instantaneously have an undefined velocity.\nThink about what it is that causes the change from one velocity step to another. There is a motor that is spinning and changing its rate of spin, perhaps using a pulley and a belt to move the robot hand to the right position at any instant of time. Changing the velocity requires a force to create an acceleration. We can differentiate the velocity to see what the acceleration must be to create the simple piecewise linear function shown in Figure 49.4.\nMathematically, the second derivatives \\(\\partial_{tt} x(t)\\) and \\(\\partial_{tt} y(t)\\) do not exist, because \\(\\partial_{t} x(t)\\) and \\(\\partial_{t} y(t)\\) are discontinuous. There is no physical amount of force that will change the velocity in an instant.\nAs an accommodation to the physical existence of the robot hand, we’ve softened the transition between consecutive velocity segments to allow it to take 0.2 seconds, ramping up from zero force 0.1 second before the hand reaches the station, to maximum force at the station, then back down to zero 0.1 second after the hand reaches the station. Consequently the actual motion is smoother and the maximum acceleration is about half that of gravity. Figure 49.7 shows the resulting trajectory which can be likened to that of a baseball player rounding a base.\nA consequence of smoothing the trajectory is that the robot hand comes near, but does not touch the station. It misses by about 2 mm. For many human tasks that might be good enough, but for precision manufacturing a miss by 2 mm is about 1000 times more than allowed.\nIf you like working with practical problems, you might find a simple solution to the problem. For instance, we could have aimed the robot hand 2 mm further to the right than the actual station. In falling short by 2mm, the hand would miss the new target but cross right over the originally intended station.\nSolutions like this are sometimes called ad hoc, meaning that they are so specifically tailored to one situation that they do not generalize well to slightly different problems. The next section introduces a superior approach that is much more general.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Data-driven functions</span>"
    ]
  },
  {
    "objectID": "Manifestations/48-splines.html#generating-smooth-motion",
    "href": "Manifestations/48-splines.html#generating-smooth-motion",
    "title": "49  Data-driven functions",
    "section": "",
    "text": "Figure 49.1: Robotic arms on the factory floor.\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.2: A CNC (Computer Numerical Control) carving machine. The cutting head (which plays the role of our “hand” in the narrative) is at the bottom of the rig mounted vertically. It can ove left-to-right along the bar labeled “XCARVE”. That bar, in turn, can move to-and-fro along the two parallel panels.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.3: The waypoints on a path the robot hand is supposed to follow. All the action is taking place in roughly 1x1 meter area.\n\n\n\n\n\n\n\n\n\n\n\nt\nx\ny\n\n\n\n\n1\n496\n191\n\n\n2\n1037\n138\n\n\n3\n1251\n191\n\n\n... 16 rows in total ...\n\n\n\n\n\n\n\n15\n928\n432\n\n\n16\n737\n240\n\n\n\n\n\nThe data frame `Robot_stations”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.4: Two functions \\(x(t)\\) and \\(y(t)\\) which describe the path shown in Figure 49.3.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.5: Velocity versus time time along the path defined by \\(x(t)\\) and \\(y(t)\\) as shown in Figure Figure 49.4.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.6: Acceleration in the x and the y direction for the piecewise linear path. There is a huge change in velocity at the juncture between linear segments.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.7: A smoothed x-trajectory near station 2. The position of the station is marked with a dot.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Data-driven functions</span>"
    ]
  },
  {
    "objectID": "Manifestations/48-splines.html#piecewise-but-smooth",
    "href": "Manifestations/48-splines.html#piecewise-but-smooth",
    "title": "49  Data-driven functions",
    "section": "49.2 Piecewise but smooth",
    "text": "49.2 Piecewise but smooth\nThe approach we will take to smoothly connect the points on the path is based on ideas of derivatives and on the construction of low-order polynomials. In Block 2, we emphasized low-order polynomials up to the square term, and we will pick that up again here for demonstration purposes. For this example, we will construct only the \\(y(t)\\) function. Constructing \\(x(t)\\) would be done using the same procedure.\nOur task is to find a function \\(y(t)\\) to interpolate discrete points such as those shown in Figure 49.8. The discrete points are called knots1 in the language of interpolating functions. Each knot is a coordinate pair \\((t_i, y(t_i))\\) shown as an orange dot in Figure 49.8.\nThe piecewise linear interpolating function is easily constructed and is shown as a dotted curve. As we saw in the previous section, such a function has a discontinuous first derivative. We would like something smoother, with a continuous first derivative. A curve such as the one we seek is shown as the multi-colored function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.8: Two interpolating functions of the four discrete points (orange). One is piecewise linear (dotted curve), the other is piecewise quadratic (multi-color curve).\n\n\n\nThe framework we will adopt for the smooth interpolating function is piecewise quadratic segments between adjacent knots. There are four knots, requiring three segments. We will call the segment \\(p_1(y)\\) connecting the first knot to the second, with \\(p_2(y)\\) connecting the second to the third knot and \\(p_3(y)\\) connecting the third to the fourth knot. Each of those segments will be a second-order polynomial. To keep things organized, we will use coefficient names systematically:\n\\[p_1(t) \\equiv a_1 + b_1 \\left[t - t_1\\right] + c_1 \\left[t - t_1\\right]^2\\] \\[p_2(t) \\equiv a_2 + b_2 \\left[t - t_2\\right] + c_2 \\left[t - t_2\\right]^2\\] \\[p_3(t) \\equiv a_3 + b_3 \\left[t - t_3\\right] + c_3 \\left[t - t_3\\right]^2\\] The four knots are \\[\\left[\\begin{array}{c}\\left(t_1, x_1\\right)\\\\\n\\left(t_2, y_2\\right)\\\\\n\\left(t_3, y_3\\right)\\\\\n\\left(t_4, y_4\\right)\\\\\n\\end{array}\\right]\\] which you can think of as two columns of a data frame, one with the \\(t\\)-coordinates of the knots and the other with the \\(y\\)-coordinates. For the knots in Figure 49.8 the data table is\n\n\n\n\n\nt\ny\n\n\n\n\n1\n0.0\n\n\n2\n2.0\n\n\n3\n0.5\n\n\n4\n1.7\n\n\n\n\n\n\n\nConstructing the interpolating function is a matter of making good choices for \\(a_1,\\) \\(a_2,\\) \\(a_3,\\) \\(b_1,\\) \\(b_2,\\) \\(b_3,\\) \\(c_1,\\) \\(c_2,\\) and \\(c_3\\).\nWe require these things of each of the interpolating polynomials:\n\nIt passes exactly through the two knots marking the segment’s endpoints. That is \\(p_1(t_1) = y_1\\) and \\(p_1(t_2) = y_2 = p_2(t_2)\\) and \\(p_2(t_3) = y_3 = p_3(t_3)\\) and, finally, \\(p_3(t_4) = y_4\\). Note that at the interior knots where two polynomials join, the left-hand polynomial and the right-hand polynomial should exactly match the function value and each other.\nThe derivative (with respect to \\(t\\)) should match where the segments join. That is, \\(\\partial_t p_1(t_1) = \\partial_t p_2(t_2)\\) and \\(\\partial_t p_2(t_3) = \\partial_t p_3(t_3)\\). Thus, the function we want to build will be \\(C^1\\), that is, have a continuous first derivative.\n\nHow to accomplish (1) and (2)?\nNotice first that because we wrote each of the polynomials in the style of Taylor polynomials, we can read the values of \\(a_1\\), \\(a_2\\), and \\(a_3\\) directly from the data table:\n\\[p_1(t_1) = a_1 = y_1\\] \\[p_2(t_2) = a_2 = y_2\\] \\[p_3(t_3) = a_3 = y_3\\]\nWe can find other coefficients from the requirement that the right side of each segment pass through the knot on that side. This gives:\n\\[p_1(t_2) = y_2 = a_1 + b_1 \\left[t_2-t_1\\right] + c_1\\left[t_2-t_1\\right]^2\\] \\[p_2(t_c) = y_3 = a_2 + b_2 \\left[t_3-t_2\\right] + c_2\\left[t_3-t_2\\right]^2\\] \\[p_3(t_c) = y_4 = a_3 + b_3 \\left[t_4-t_3\\right] + c_3\\left[t_4-t_3\\right]^2\\] (Notice that \\(t_2 - t_1\\) and the like are simply numbers that can be computed from the known knot points.)\nAnother two conditions are that the derivatives of the polynomials from either side of each interior knot point must match at the knot point. Finding the derivatives of the segments is a simple exercise:\n\\[\\partial_t p_1(t) = b_1 + 2 c_1 \\left[t - t_1\\right]\\] \\[\\partial_t p_1(t) = b_2 + 2 c_2 \\left[t - t_2\\right]\\] \\[\\partial_t p_1(t) = b_3 + 2 c_3 \\left[t - t_3\\right]\\]\nMatching these derivatives at the \\(t_2\\) and \\(t_3\\) knot points—the interior knots where two segments come together—gives two more equations: \\[\\partial_t p_1(t_2) = b_1 + 2 c_1 \\left[t_2 - t_1\\right] = b_2 = \\partial_t p_2(t_2)\\] \\[\\partial_t p_2(t_3) = b_2 + 2 c_2 \\left[t_3 - t_2\\right] = b_3 = \\partial_t p_3(t_3) \\] All together, we have five equations in six unknowns: \\(b_1, b_2, b_3\\) and \\(c_1, c_2, c_3\\).\nPlugging in the specific values \\(t_1\\) through \\(t_4\\), and \\(x_1\\) through \\(x_4\\) from the data table translates the equations for the polynomial values and derivatives gives this system of equations:\n\\[b_1 + c_1 = x_2 - x_1 = \\ \\ \\ \\ \\ \\ 2\\] \\[b_2 + c_2 = x_3 - x_2 = -1.5\\] \\[b_3 + c_3 = x_4 - x_3= \\ \\ 1.2\\] \\[b_1 + 2 c_1 - b_2 = 0\\] \\[b_2 + 2 c_2 - b_3 = 0\\]\nThis is not the place to go into the details of solving the five equations to find the six unknowns. (Block 5 introduces the mathematics of such things, which turns out to the same math used to find model parameters to “fit” data.) But there are some simple things to say about the task.\nFirst, you may recall being told in high-school mathematics that to find six unknowns you need six equations. We have only five equations to work with. But it is far from true that there is no solution for six unknowns with five equations. There are in fact an infinite number of solutions. (Again, Block 5 will show the mathematics behind this statement.) Essentially, all we need to do is make up a sixth equation to identify a particular one of the infinite number of solutions. It is nice if this made-up equation reflects something interpretable about the curve.\nWe will choose to have the sixth equation specify what the derivative of the interpolating function should be at the far right end of the graph. That right-most derivative value will be \\[\\partial_t p_3(t_4) = b_3 + 2 c_3 \\left[t_4 - t_3\\right]\\ .\\] We can set this value to anything we like. For instance, in Figure 49.8 the right-most derivative is set to zero; you can see this from the curve being flat at the right-most knot point.\n\n\n\n\n\n\n\n\n\n## Warning: All aesthetics have length 1, but the data has 101 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n\n\n\n\n\n\n\n\n\n\n## Warning: All aesthetics have length 1, but the data has 101 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n\n\n\n\n\n\n\n\n\n\n## Warning: All aesthetics have length 1, but the data has 101 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n\n\n\n\n\n\n\n\n\n\n## Warning: All aesthetics have length 1, but the data has 101 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.9: Four different \\(C^1\\) piecewise quadratic functions that interpolate the knot points. The functions have different values of the derivative at the right end of the domain.\n\n\n\nKeeping in mind the piecewise nature of the interpolating polynomial, it may seem surprising that changing the slope at \\(t_4\\) leads to a change in value of the function almost everywhere. Yet the stiffness of the parabolic segments means that conditions in one segment have an impact on adjacent segments. In turn, the segments adjacent to these also change, a change that percolates down to every segment in turn.\n\n\n\n\n\n\n\n\nTry it! 49.1\n\n\n\n\n\n\n\n\n\nTry it! 49.1 Splining through the robot stations\n\n\n\nQuadratic spline functions can be created with the R/mosaic qspliner() function. The second argument is a data frame giving the knot locations. The first argument is a tilde expression specifying the variables to use from the data frame.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nHaving constructed the functions, we can plot them in the ordinary way, here adding orange dots at the \\(x\\)-values of the knots.\n\n\n\nActive R chunk 49.1\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nThe graph produced by Active R chunk 49.1 quadratic spline interpolation of the \\(x\\)-coordinates of the robot-path knots. The data hardly speak for themselves, since the interpolationg function tends to alternate between concave up and concave down in adjacent segments between the knots.\n\n\nPutting together the \\(x(t)\\) and \\(y(t)\\) interpolating functions, each of which has that extremum between knot points, leads to an absurdly complicated path, as seen in Figure 49.10.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.10: Connecting the robot-path knots with a piecewise quadratic polynomial, constructed to be \\(C^1\\). The path is pretty perhaps, but absurd.\n\n\n\nQuadratic splines are rarely used in practice. (A cubic spline provides helpful flexibility. See Section 49.3.) In Figure 49.9 you can see one of the reasons: the quadratic form is so stiff that the interpolating function tends to shift from concave up to concave down (or vice versa) at each knot point. This results in the interpolating function tending to have a local minimum or maximum between adjacent knots, even if the data themselves to not indicate such a structure.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Data-driven functions</span>"
    ]
  },
  {
    "objectID": "Manifestations/48-splines.html#sec-cubic-splines",
    "href": "Manifestations/48-splines.html#sec-cubic-splines",
    "title": "49  Data-driven functions",
    "section": "49.3 C2 smooth functions",
    "text": "49.3 C2 smooth functions\nIn the previous section, we arranged the functions \\(x(t)\\) and \\(y(t)\\) composed from the piecewise quadratic segments to be \\(C^1\\) smooth. (Recall that \\(C^1\\) smooth means that the derivatives \\(\\partial_t x(t)\\) and \\(\\partial_t y(t)\\) are continuous.) We established this continuity by make sure that each segment has a value of the derivative at its end-point know that matches the derivative of the adjacent segment.\nTo arrange \\(C^2\\) continuity requires that the segment include a new parameter. Most commonly, this is done by moving from quadratic segments to cubic segments. This can be done by an approach similar to that of the previous section but somewhat more elaborate. Such a \\(C^2\\) interpolating function is called a cubic spline. Cubic splines are very commonly encountered in applications requiring interpolation.\nWith the ability to match piecewise cubic polynomials to a set of knots, we can easily construct the smooth path to connect the knots in Figure 49.3. Figure 49.11 shows a \\(C^2\\) path connecting the knots. The path is constructed by plotting simultaneously the output of two functions, \\(x(t)\\) and \\(y(t)\\), with the input \\(t\\) on the domain \\(1 \\leq t \\leq 16\\).\n\n\n\n\n\n\nCubic splines have smoother joints\n\n\n\nCubic spline functions can be created with the R/mosaic spliner() function. The second argument is a data frame giving the knot locations. The first argument is a tilde expression specifying the variables to use from the data frame.\n\nxfun &lt;- spliner(x ~ t, data = Robot_stations)\nyfun &lt;- spliner(y ~ t, data = Robot_stations)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.11: Connecting the robot-path knots with a piecewise cubic polynomial, constructed to be \\(C^2\\). This is a much smoother path than produced by interpolation with quadratic polynomials.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.12: A cubic spline interpolation of the \\(x\\)-coordinates of the robot-path knots. The cubic spline respects the monotonicity of consecutive knot points.\n\n\n\n\n\n\nApplication area 49.1 —Continuous second derivatives keep motion smooth.\n\n\n\n\n\n\n\nApplication area 49.1 Avoiding infinite force\n\n\n\nWe saw that using a line-segment interpolation produces discontinuity in the derivative of the function. Mathematically, discontinuity in the velocity can be thought of as an infinite acceleration, requiring an infinite force. In the physical world, accelerations must be finite. Even if a force is large, there is often slack in connections between components and the components are not perfectly rigid.\nFigure 49.13 shows motion of a robotic dog. At the start, the motors in the robot are being asked to make a straight-line transition between waypoints. The result is vibration and a tremor-like movement.\nThen the motors are given a smoothly interpolated signal. (Much of the video shows the programming involved. You can skip directly to time 13:02 to see the motion with smoothly interpolated waypoints.) The smooth interpolation produces a gentle and vibration-free movement.\n\n\n\n\n\n\nFigure 49.13: Link to entire video by James Bruton.\n\n\n\n\n49.4 Bézier splines\nThe sort of interpolating functions described in the previous two sections were designed to be smooth at the \\(C^1\\) level (quadratic spline) or the \\(C^2\\) level (cubic spline). Such smoothness makes sense for, say, robotic motion where we want at all times to keep the force on each robot joint small.\nNot all path-related design problems require such smoothness. Indeed, in some settings, non-smoothness is called for. For instance, Figure 49.14 shows the outline of a familiar shape\n\n\n\n\n\n\nFigure 49.14: The outline of a letter in a computer font is often specified by a series of knot points (red dots). The path passes smoothly through some of the knot points, but has a discontinuous derivative at others.\n\n\n\nFor both quadratic and the more commonly used cubic splines, matching the derivatives on either side of a knot is essential to constructing the function. For Bézier splines, each segment is mathematically independent from every other segment. It is up to the human designer of the curve to determine whether the curves derivative should be continuous or discontinuous at the knot point between two segments.\nThe shape of a Bézier spline segment is established by four independent points. The first and last points determine the endpoints of the segment. Each endpoint is associated with a control point that sets the angle and “speed” with which the path leaves or enters the endpoint. You can interact with the graph in Figure 49.15 to develop an intuition.\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.15: A single Bézier segment is defined by two endpoints and two control points. Drag the control points to see how the shape of the curve is defined by them.\n\n\n\nThe curve for a given segment is gratifyingly smooth. The real power of Bézier splines stems from how segments can be connected in various ways. Figure 49.16 shows two Bézier segments that have been initialized to have a smooth junction at endpoints 4 and 5. The smoothness is set by the corresponding control points (marked 3 and 6). So long as those four points (3, 4, 5, 6) are colinear and in order, the junction will be smooth. You can alter control points 2 and 7 in any way you like; the junction will remain smooth.\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.16: Two Bézier segments can be arranged in to create a smooth or non-smooth junction between them.\n\n\n\nConsider the path followed by a Bézier curve as it leaves one of the endpoints. Figure 49.16 has been initialized so that the tangent to the curve is horizontal at the right endpoint and almost vertical at the left endpoint. The further the control point is from the endpoint, the longer the Bézier curve will stay close to the tangent line. Another way to think of this is that the position of a control point has little impact on the shape of the curve near the opposite endpoint. You can observe this on the canvas by, say, moving control point 2 and observing the relatively little change near endpoint 4.\n\n\n\n\n\n\n\n```\n\n\nFigure 49.17: A Bézier curve leaves each endpoint in a direction that is tangent to the line drawn between the endpoint and its control point.\n\n\n\n\n\n\nAlgebraically, each Bézier segement is a pair of cubic functions, \\(x(t)\\) for the x-coordinate and \\(y(t)\\) for the y-coordinate. The input \\(t\\) varies between 0 and 1 for each segment. The coordinate pair \\(\\left({\\large\\strut} x(0), y(0)\\right)\\) is one endpoint of the curve, while \\(\\left({\\large\\strut} x(1), y(1)\\right)\\). Each intermediate value of \\(t\\) corresponds to a point on the interior of the curve.\nThe \\(x(t)\\) and \\(y(t)\\) functions have the same form, the difference between the functions being only the values of the end values (\\(x_1\\) and \\(x_4\\) for the \\(x(t)\\) function, and similarly \\(y_1\\) and \\(y_4\\) for the \\(y(t)\\) function), as well as the control-point values (\\(x_2\\) and \\(x_3\\) for one function, \\(y_2\\) and \\(y_3\\) for the other.)\n\\[x(t) = (1-t)^3\\, x_1 + 3(1-t)^2 t\\, x_2 + 3(1-t) t^2\\, x_3 + t^3\\, x_4\\] and \\[y(t) = (1-t)^3\\, y_1 + 3(1-t)^2 t\\, y_2 + 3(1-t) t^2\\, y_3 + t^3\\, y_4\\]\n\n\n\n\n\n\nCalculus history—Splines in wood and metal\n\n\n\nBefore the advent of digital design and manufacturing, smooth curves were described by clay or wooden models hand-crafted by skilled workers. Material was removed to conform to the models by machine tools directed by cams running over the models, by hand sanding and polishing, as shown in this video of propeller manufacture during World War II.\n\n\n\n\n\n\nVideo 49.1: Machining in the pre-digital world: airplane propellers\n\n\n\nSpline functions and digital actuators have largely replaced such analog models.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Data-driven functions</span>"
    ]
  },
  {
    "objectID": "Manifestations/48-splines.html#bézier-splines",
    "href": "Manifestations/48-splines.html#bézier-splines",
    "title": "49  Data-driven functions",
    "section": "49.4 Bézier splines",
    "text": "49.4 Bézier splines\nThe sort of interpolating functions described in the previous two sections were designed to be smooth at the \\(C^1\\) level (quadratic spline) or the \\(C^2\\) level (cubic spline). Such smoothness makes sense for, say, robotic motion where we want at all times to keep the force on each robot joint small.\nNot all path-related design problems require such smoothness. Indeed, in some settings, non-smoothness is called for. For instance, Figure 49.14 shows the outline of a familiar shape\n\n\n\n\n\n\nFigure 49.14: The outline of a letter in a computer font is often specified by a series of knot points (red dots). The path passes smoothly through some of the knot points, but has a discontinuous derivative at others.\n\n\n\nFor both quadratic and the more commonly used cubic splines, matching the derivatives on either side of a knot is essential to constructing the function. For Bézier splines, each segment is mathematically independent from every other segment. It is up to the human designer of the curve to determine whether the curves derivative should be continuous or discontinuous at the knot point between two segments.\nThe shape of a Bézier spline segment is established by four independent points. The first and last points determine the endpoints of the segment. Each endpoint is associated with a control point that sets the angle and “speed” with which the path leaves or enters the endpoint. You can interact with the graph in Figure 49.15 to develop an intuition.\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.15: A single Bézier segment is defined by two endpoints and two control points. Drag the control points to see how the shape of the curve is defined by them.\n\n\n\nThe curve for a given segment is gratifyingly smooth. The real power of Bézier splines stems from how segments can be connected in various ways. Figure 49.16 shows two Bézier segments that have been initialized to have a smooth junction at endpoints 4 and 5. The smoothness is set by the corresponding control points (marked 3 and 6). So long as those four points (3, 4, 5, 6) are colinear and in order, the junction will be smooth. You can alter control points 2 and 7 in any way you like; the junction will remain smooth.\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.16: Two Bézier segments can be arranged in to create a smooth or non-smooth junction between them.\n\n\n\nConsider the path followed by a Bézier curve as it leaves one of the endpoints. Figure 49.16 has been initialized so that the tangent to the curve is horizontal at the right endpoint and almost vertical at the left endpoint. The further the control point is from the endpoint, the longer the Bézier curve will stay close to the tangent line. Another way to think of this is that the position of a control point has little impact on the shape of the curve near the opposite endpoint. You can observe this on the canvas by, say, moving control point 2 and observing the relatively little change near endpoint 4.\n\n\n\n\n\n\n\n```\n\n\nFigure 49.17: A Bézier curve leaves each endpoint in a direction that is tangent to the line drawn between the endpoint and its control point.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Data-driven functions</span>"
    ]
  },
  {
    "objectID": "Manifestations/48-splines.html#footnotes",
    "href": "Manifestations/48-splines.html#footnotes",
    "title": "49  Data-driven functions",
    "section": "",
    "text": "Called such possibly because the curves are tied together at each of the knots.↩︎",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Data-driven functions</span>"
    ]
  },
  {
    "objectID": "Manifestations/49-optimization.html",
    "href": "Manifestations/49-optimization.html",
    "title": "50  Optimization and constraint",
    "section": "",
    "text": "50.1 Gradient descent\nChapter 24 introduced optimization and some of the terms used to describe optimization problems:\nA simple optimization problem has three main phases:\nTo illustrate, consider this simple but unrealistic problems found in hundreds of calculus texts: Finding the configuration to construct the rectangular box with the largest possible volume out of a piece of cardboard. The modeling phase starts with a representation of the box-construction and volume-finding process. Suppose, for the sake of simplicity, that we are working with a piece of cardboard fixed at 20 inches by 30 inches. For box construction, we will propose cutting out squares from each corner of the box of some side length \\(x\\). Those squares will be discarded and the box formed by folding up the flaps generated by the squares’ removal, as in Figure 50.1.\nFor the volume of the box, we will multiply the area of the bottom of the box by the height \\(x\\). Examination of Figure 50.1 should be enough to convince you that the volume \\(V\\) is a function of \\(x\\):\n\\[V(x) = \\underbrace{\\strut x}_\\text{height} \\cdot \\underbrace{(20-2x)}_\\text{width}\\cdot \\underbrace{(30-2x)}_\\text{length} =  x(600 - 100 x + 4 x^2)\\ .\\] Since the goal is to find the maximum possible volume, \\(V(x)\\) is our objective function.\nThe solution phase can be completed by drawing a graph of \\(V(x)\\) and finding the \\(x\\) corresponding to the peak value of \\(V(x)\\). We will leave this for you to do in a sandbox; you can figure out the relevant domain by noting that the corner squares cannot overlap. Calculus texts typically emphasize another approach, using symbolic differentiation to examine \\(\\partial_x V(x)\\) and solve for \\(x^\\star\\) such that \\(\\partial_x V(x^\\star) = 0\\). The derivative is \\[\\partial_x V(x) = 600 - 200 x + 12 x^2\\ .\\] The symbolic solution task is to find the zeros of \\(\\partial_x V(x)\\). They work out to be \\(x_1^\\star = 3.92\\) or \\(x_2^\\star = 12.74\\).\nThe third phase of an optimization problem, evaluation phase, can start with plugging in the objective function the values of \\(x^\\star\\).\n\\[V(x_1^\\star) = 1056.3\\ \\text{in}^3 \\ \\ \\ \\ \\text{and}\\ \\ \\ \\ V(x_2^\\star) = -315.6\\ \\text{in}^3\\] It is common sense that \\(x_2^\\star\\) is not a viable solution. The negative volume at \\(x_2^\\star\\) is a consequence of looking at \\(V(x)\\) beyond the sensible domain for cardboard boxes. More generally, as part of the evaluation phase we can look at the value of the convexity \\(\\partial_{xx} V(x^\\star)\\) to find out whether an \\(x^\\star\\) value is an argmax or an argmin. Since \\(\\partial_{xx} V(x) = 24 x - 200\\) we see that \\(\\partial_{xx} V(x_1^\\star) &lt; 0\\), corresponding to an argmax. Alternatively, instead of computing the convexity, we could check whether we have an argmin or an argmax by evaluating the objective function at a nearby input.\nAdditional examination of the phase-two solution can give useful information, such as an indication of how sensitive the output is to small changes of the input near the argmax (or argmin). For example, setting \\(x=4\\) in will produce a volume output \\(V(4) = 1056\\) in2, hardly different than the “exact” maximum of 1056.3 in3 and perhaps preferred for the person who wants to make standard-size boxes.\nThe evaluation phase in a genuine application (as opposed to a textbook toy example) should also include a reflection on how well the model reflects the real-world situation. For example we’ve neglected the creases that arise from folding cardboard, so a more complete examination would estimate this effect. And the person skeptical about calculus-book chestnuts might wonder whether the object is really to create a box without a top!\nCommonly, optimization problems involve much more complicated objective functions with many inputs. The next section considers the basis for a more general and practical approach to the solving phase of optimization. Later sections examine how this more general approach leads to methods for approaching the sort of real-world optimization problem where there are multiple objectives.\nThe general approach we will take to the solving phase of optimization problems will be iterative as in Chapter 48. Start with an initial guess for an argmin and then construct a new function that can improve the guess. Applying this improvement function iteratively leads to better and better estimates of the true argmin.\nFor illustration purposes, we will use optimization problems where the objective function has two inputs. Such objective functions can be graphed on paper or a display screen and it is possible to see the action of the iterative improvement process directly. For optimization in problem with many inputs, the improvement can be monitored from the objective function output at each step.\nFigure 50.2 shows a mechanical system consisting of a mass suspended from a fixed mounting by three nonlinear springs.\nThe mass is shown by a black circles. Springs are the zig-zag shapes. The bold bar is the fixed mounting, as if from a beam on the ceiling of a room. The system has an equilibrium configuration where the springs are stressed sufficiently to balance each other left to right and to balance the gravitational force downward on the mass.\nWe want to calculate the equilibrium position. The basic strategy is to model the potential energy of the system, which consists of:\nSince the configuration of the system is set by the coordinate \\((x_1, y_1)\\), the potential energy is a function \\(E(x_1, y_1)\\). For brevity, we will leave out the physics of the formulation of the potential-energy function; shown in Figure 50.3.\nThe potential energy function \\(E(x,y)\\) has a bowl-like shape. The bottom of the bowl—the argmin—is near \\((x=1.7, y=-1.3)\\). Referring to Figure 50.2, the equilibrium position is a bit upward and to the right of the position shown in the figure.\nWith a graph of the objective function like Figure 50.3, the solution phase is simple; a graph will do the job. But for more complicated objective functions, with more than 2 inputs, drawing a complete graph is not feasible. For example, in the spring-mass system shown in Figure 50.4, the potential energy function has six inputs: \\(x_1, y_1, x_2, y_2, x_3, y_3\\). In genuine applications of optimization, there are often many more inputs.\nIn a multi-input optimization problem, we don’t have a picture of the whole objective function. Instead, we are able merely to evaluate the objective function for a single given input at a time. Typically, we have a computer function that implements the objective function and we are free to evaluate it at whatever inputs we care to choose. It is as if, instead of having the whole graph available, the graph is covered with an opaque sheet with a loophole, as in Figure 50.5.\nWe can see the function only in a small region of the domain and need to use the information provided there to determine which way to move to find the argmin.\nThe situation is analogous to standing on the side of a smooth hill in a dense fog and finding your way to the bottom. The way forward is to figure out which direction is uphill, which you can do directly from your sense of balance by orienting your stance in different ways. Then, if your goal is the top of the hill (argmax) start walking uphill. If you seek a low point (argmin), walk downhill.\nThe mathematical equivalent to sensing which direction is uphill is to calculate the gradient of the objective function. Chapter 25 uses partial differentiation with respect to each of the input quantities to assemble the gradient vector, denoted \\(\\nabla f() = \\left({\\large \\strut} \\partial_x f(), \\ \\partial_y f()\\right)\\). In Figure 50.5, where we are standing at about \\((x_i=0.8, y_i=-2.3)\\), we would evaluate the each of the partial derivatives in the gradient vector at \\((0.8, -2.3)\\).\nThe gradient points in the steepest direction uphill so, once you know the direction, take a step in that direction to head toward the argmax, or a step in the opposite direction if you seek the argmin. The process of following the gradient toward the top of the hill is called gradient ascent. Correspondingly, following the gradient downhill is gradient descent.\nFor humans, the length of a step is fixed by the length of our legs and the size of our feet. The mathematical step has no fixed size. Often, the modeler gains some appreciation for what constitutes a small step from the modeling process. Referring to Figure 50.4 for example you can see that a small increment in \\(x\\) is, say, \\(0.1\\), and similarly for \\(y\\). There is little point in taking an infinitesimal step—that gets you almost nowhere! Instead, be bold and take a finite step. Then, at your new location, calculate the gradient vector again. If it is practically the same as at your earlier position, you can wager on taking a larger step next time. If the new gradient direction is substantially different, you would be well advised to take smaller steps.\nFortunately, a variety of effective ideas for determining step size have been implemented in software and packaged up as algorithms. The modeler need only provide the objective function in a suitable forma starting guess for the inputs.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Optimization and constraint</span>"
    ]
  },
  {
    "objectID": "Manifestations/49-optimization.html#gradient-descent",
    "href": "Manifestations/49-optimization.html#gradient-descent",
    "title": "50  Optimization and constraint",
    "section": "",
    "text": "Tip\n\n\n\nSpring-mass systems: an example context\nAs our example context for discussing the optimization process, we will consider how to use optimization to calculate the configuration of simple mechanical systems consisting of interconnected springs and masses. Such configuration problems are especially important today in understanding the structure and function of proteins, but we will stick to the simpler context of springs and masses.\n\n\n\n\n\n\n\n\n\nFigure 50.2: A mass suspended from three springs.\n\n\n\n\n\n\nthe gravitational potential energy of the mass.\nthe energy stored in stretched or compressed springs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 50.3: The potential energy of the spring-mass system in Figure 50.2.\n\n\n\n\n\n\n\n\n\n\n\nFigure 50.4: A more complicated spring-mass system.\n\n\n\n\n\n\n\n\n\n\nFigure 50.5: A more realistic view of what we can know about a function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 50.6: The gradient provides information about the shape of the local function in a convenient form to guide the step to the next locale in your journey toward the argmin or argmax.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry it! 50.2\n\n\n\n\n\n\n\n\n\nTry it! 50.2 Using argM()\n\n\n\nThe R/mosaic function argM() is set up to find argmins and argmaxes using the familiar tilde-formula/domain style of arguments used throughout this book. For instance, the potential energy of the spring-mass system shown in Figure 50.2 is available as mosaicCalc::PE_fun1()\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nApplication area 50.1 —Algorithms can implement formulas, but the reverse is not necessarily true.\n\n\n\n\n\n\n\nApplication area 50.1 Formulas and algorithms\n\n\n\nTextbook formulas in physics, chemistry, engineering, and economics often have a root in an optimization problem. Since a formula is the desired result, symbolic differentiation is used in the solution phase. This allows parameters to be represented with symbols rather than as specific numbers. Usually the objective functions involved are simple. And to make the objective functions simple enough for symbolic work, it is common to make approximations, for example by replacing functions like \\(\\sin(x)\\) with \\(x\\) and \\((1+p)^n\\) with \\(1+np\\). But simplifying the objective function should really be considered part of the solution phase rather than the modeling phase.\nNumerical techniques are the most widely used in practice. Optimization is an important operation in both science and management and much human ingenuity has gone into the development of effective algorithms. The modeler rarely if ever needs to reach beyond the software provided in technical computing environments such as R, MATLAB, Mathematica, or the many packages available for Python.\nIn data science and machine learning, often advanced solution-phase software is provided as web services and APIs (application programming interfaces). An example is the Google technology product TensorFlow used to find optimal parameters for functions in the machine technique called “deep learning.”\n\n\n\n\nApplication area 50.2  \n\n\n\n\n\n\n\nApplication area 50.2 Minimum potential energy\n\n\n\nThe potential energy function of the spring-mass system in Figure 50.4 is available as the R/mosaic function PE_fun2(). This potential energy function has six inputs: the \\(x\\) and \\(y\\) coordinates of each of the three masses. The code below shows how to use the R/mosaic argM() function to locate an argmin of the potential energy.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe argM() function reports the final result, the end of the path followed in descending the gradient field. Figure 50.7 gives a movie of the path as it is being followed.\n\n\n\n\n\n\nFigure 50.7: The path to equilibrium for the 3-body spring-mass system shown in Figure 50.4. The top two frames show a 2-dimensional slice through the 6-dimensional gradient field. The bottom frame translates the current point on the path into a picture of the spring-mass locations.\n\n\n\nAt the start of the movie, the masses are (absurdly) misplaced and far from their equilibrium position. As system configuration moves downhill toward the argmin of the potential energy function, the masses sort themselves out.\nThe two gradient-field frames show a different two-dimensional slices of the potential energy function which has six inputs. Watch the gradient-fields carefully to see that the field itself is changing as time goes by. All six inputs are changing. At each point in time, we are plotting the gradient field as a function of the two inputs shown on the axes. These stay the same through the whole movie, but the other four inputs are changing as the system moves along the gradient descent path. The last frame shows the gradient field at the final position in six-dimensional space. You can see that the early parts of the path are not aligned with the end-of-path gradient fields, but they were aligned at the earlier time when each point in the path was passed.\nThe familiar tilde-expression format used by argM() and the other R/mosaic functions is suitably compact for function of one or two arguments, but for functions with many inputs it becomes ungainly. For objective functions with many inputs, a different programming style is more appropriate that packages up the multiple inputs into a single, vector input. Since this is not a programming book, we won’t go into the vector-input programming style, but be aware that in professional-level work, learning new tools for programming becomes essential.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Optimization and constraint</span>"
    ]
  },
  {
    "objectID": "Manifestations/49-optimization.html#objectives-and-constraints",
    "href": "Manifestations/49-optimization.html#objectives-and-constraints",
    "title": "50  Optimization and constraint",
    "section": "50.2 Objectives and Constraints",
    "text": "50.2 Objectives and Constraints\nMany real-world decision-making settings do not fit neatly into the framework of constructing an objective function and then finding the argmin (or argmax). A common situation is having multiple objectives. These objectives often compete and the output of the respective objective functions may not necessarily be directly comparable. For instance, in health care one objective is to save lives, while another is to minimize costs. But lives and money are not directly comparable.\nOften, the original problem statement does not include all of the objectives and the modeler needs to be perceptive to discern important objectives left out of the initial description of the problem. When such missing objectives become apparent, it is necessary to visit the modeling phase of the problem to insert the new objectives. By adopting the right approach to modeling, such situations can be readily handled and, even better, the modeling phase can bring new insight into the real-world problem.\nTo illustrate, let’s returning to the mathematically simplified problem of constructing an optimal cardboard box. Before, we stipulated that the raw cardboard stock has dimension 20 inches by 30 inches. Now we will generalize and work with a piece of cardboard that has edges of length \\(y\\) from which, as before, we will cut out square corners of length \\(x\\) on a side. (See Figure 50.8). Our objective is to make a box with the largest possible volume. (This will be an argmax problem.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 50.8: The cardboard cut lines and the eventual shape of the folded box.\n\n\n\nThe area of the bottom of the box is \\((y - 2x)^2\\) and the box height is \\(x\\). The objective function is the volume of the box, area times height: \\[V(x, y) \\equiv x (y - 2x)^2\\ .\\] There are two inputs, \\(x\\) and \\(y\\), so a simple plot should suffice to find the argmax.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 50.9: The volume of the box (in cubic inches) constructed by cutting corners of size \\(x\\)-by\\(x\\) out of a \\(y\\)-by-\\(y\\) piece of cardboard.\n\n\n\nScanning Figure 50.9 reveals a couple of things that you might not have anticipated. First, the argmax is in the extreme lower-right corner of the graphics frame, not in the center as in previous examples. Second, the argmax in this corner, \\((y=0, x=10)\\) is logically inconsistent with the idea of a cardboard box.\nThe inconsistency stems from an inadmissible value for \\(x\\). For \\(2x &gt; y\\), the bottom of the box would have negative edge length. But because the objective function \\(V(x,y)\\) squares this negative quantity—in the \\((y - 2x)^2\\) term—the output of the objective function does not signal that anything is wrong. The source of the problem is not the objective function formula itself, but neglecting to consider carefully what is the proper practical domain for the function.\nTo make the calculation realistic, we should search for the argmax only in that region of the graphics frame where \\(y &gt; 2x\\). That restriction on the search domain is called a constraint. In this case, the constraint takes the form of an inequality \\(y &gt; 2x\\) so we call it an inequality constraint. (Later, we will work with equality constraints.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 50.10: The inequality constraint that \\(y &gt; 2x\\) renders much of the graphics frame inadmissible as a possible solution. The inadmissible region is shaded in blue. The argmax must be sought in the unshaded region of the frame.\n\n\n\nWith the \\((x,y)\\)-domain restricted to the values that are physically realistic, we can see that the argmax is still on the edge of the frame, at \\(y=30\\) and \\(x\\approx 5\\), where the volume of the box will be about 1800 in3. This result should cause you pause, since there was nothing in the problem statement that limited \\(y\\) to be 30” or less. If we replotted with a larger domain for \\(y\\), we should see still larger boxes, without any limit.\nThe interpretation of the problem as originally posed is: With enough cardboard we can make a box of any size! Since the goal was to recommend the “best” size, this conclusion is not so useful. The weak conclusion stems from a fault in the problem statement. The statement omitted an important second objective function: use as little cardboard as possible.\nIf using as little cardboard as possible were our sole objective, the optimization problem has an easy-to-find solution: we would make a zero-volume box out of zero-area of cardboard. What we want, somehow, is to make as big a box as possible out of as little cardboard as possible: we have two objectives! In this case, the objectives are in conflict: making a bigger box (good) uses more cardboard (bad).\nCommon sense tells us to balance the two objectives, but how to represent this mathematically? Ideally—note that “ideally” is sometimes far from “realistically” or “productively”—we would know how much box-volume is worth to us and how much cardboard costs, and we could construct an objective function that incorporates both value and cost. For instance, if each cubic inch of volume is worth 1 cent, and each square inch of cardboard costs 3 cents, then the objective function will be the following (with output in cents):\n\\[\\text{Profit}(x,y) \\equiv 1\\, x (y-2x)^2 - 3 y^2\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 50.11: The “profit” (value minus cost) of the cardboad box (cents).\n\n\n\nEven with including the cardboard cost in the objective function, we will still want to make \\(y\\) as large as possible. Not much guidance there!\nBut let’s imagine a new factor coming into play. At the meeting where the box-design decisions are being made and where you are presenting your analysis in Figure 50.11, the graphic designer speaks up. “The trending shape for this year is cubic. We want the box, whatever it is size, to be a cube.”\nLuckily, you the modeler can quickly incorporate this into your analysis. To be a cube, the height \\(x\\) of the box has to be the same as the width and depth \\(y - 2x\\). So you can incorporate the designer’s wish into the model of the decision factors by adding a new constraint:\n\\[x = y - 2x \\ \\ \\ \\implies y-3x=0\\ \\ \\ \\ \\text{constraint: box must be cubic}\\] This is called an equality constraint. Figure 50.12 shows the equality constraint in green: to be a cube, \\(x\\) and \\(y\\) must be somewhere along the green line.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 50.12: The profit function shown in more detail along with the equality constraint (green) for the box to be cube-shaped.\n\n\n\nFollow the green path uphill. As \\(x\\) gets larger along the constraint, the output of the objective function grows, reaching a level of 1350 cents when \\((x=15, y=45)\\) at the right border of the graphics frame.\nIt is worth pointing out, for later use, that the be-a-cube constraint is almost parallel to the objective function contours.\n\nApplication area 50.3 —Using a budget optimally.\n\n\n\n\n\n\n\nApplication area 50.3 Budgets\n\n\n\nMany organizations use a budget mechanism to manage their affairs. The organization defines divisions or projects, and each of these is given a dollar budget to stay within. The individual division or project manager can arrange things more or less as she thinks best, so long as she stays within the budget. This is a kind of constraint: a budget constraint.\nSuppose you have been tasked to set up a new factory and given a budget of $5,500,000 to do so. You were given this task because you have a particular expertise in how best to set up the factory, but your design will of course depend on the relative prices of the different inputs to the production process.\nFor simplicity, let’s imagine that there are two main inputs: labor \\(L\\) and capital/equipment \\(K\\). It would be silly to spend all the budget on labor and none on capital; the workers would have no tools to work with. Similarly, capital without labor has no productive value. The best design for the factory will be a mix of labor and capital.\nSince the purpose of the factory is to make things for sale, a good objective function will be the sales value of the output produced by the factory. Economists have a favored form for production functions of this sort, a power-law called the Cobb-Douglas function. The essential insight behind the Cobb-Douglas function is that doubling both capital and labor (as if you built a second factory alongside the first) should double production. The Cobb-Douglas form for production as a function of capital and labor is \\[Q(L, K) = p b L^a K^{1-a}\\ .\\] You will use your expertise to set the values of the \\(a\\) and \\(b\\) parameters. The price \\(p\\) of each unit of output will be set by the market: Let’s assume for planning purposes that it is \\(p - \\$450\\) per unit. Suppose you have determined that \\(a=0.3\\) and \\(b=40\\) are appropriate. This production function is shown in Figure 50.13.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 50.13: A Cobb-Douglas production function for factory output with \\(p=100\\), \\(b=40\\) and \\(a=0.3\\). (Output units in millions of dollars).\n\n\n\nAs you can see from Figure 50.13, the more labor and the more capital you use, the higher the production. Notice that the production function itself does not have an argmax interior to the domain being plotted. It is one of those “more is better” situations.\nSuppose that labor costs $6000 per person-month. Capital, in units of production stations, costs $13,000 per unit. Your budget constraint reflects the total cost of capital and labor: \\(6000 \\cdot L + 15000 \\cdot K \\leq 5500000\\). This constraint is graphed in Figure 50.14.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 50.14: The production function for factory output with the budget constraint shown in green.\n\n\n\nAny mixture of labor and capital that falls outside the green zone stays within your budget. What’s the best mixture? The one that gives the largest production. You can read this off the graph, \\(L\\approx 650\\) person-months and \\(K\\approx 125\\) workstations which gives slightly more than $7 million dollars in production.\nThe argmax is right on the frontier of the constraint region. Put into more operational terms: You will want to spend your entire budget to maximize production. This is hardly a surprise to anyone who has to work within a budget. Knowing that you’re going to use the whole budget, you might as well have found the argmax by walking along the constraint frontier from left to right. As you start near \\(K=250\\) and \\(L=380\\), the path you walk goes uphill on the graph of the production function. The path continues uphill until you reach the argmax. Near the argmax, the path is level. After the path crosses the argmax, it is leading downhill. At the argmax, the production function contours are parallel to the constraint boundary.\nYou might like to think of this as bicycling along a path in hilly terrain. (The hill is shown in Figure 50.14 as the contours, the path is the boundary of the green area, running diagonally from top left to bottom right in the graph.) When you reach the local high point on the path, you may not be at the top of the hill. But you will be on a flat spot on the path, meaning that the path is parallel to the contour of the hill at that point.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Optimization and constraint</span>"
    ]
  },
  {
    "objectID": "Manifestations/49-optimization.html#constraint-cost",
    "href": "Manifestations/49-optimization.html#constraint-cost",
    "title": "50  Optimization and constraint",
    "section": "50.3 Constraint cost",
    "text": "50.3 Constraint cost\nOptimization techniques have an important role to play as aids to human decision making. Let’s see how the mathematical representation of a constraint as a function can facilitate the human decision-making process.\nIn the previous section, the box designer’s request that the box be cubic was translated into an equality constraint, \\(y-3x=0\\), shown as the green line in Figure 50.12. The skilled modeler can bring additional power to the analysis by translating that constraint, \\(y-3x=0\\) into a function, for example \\[\\text{Equation:}\\ \\  \\ y - 3x = 0\\ \\ \\longrightarrow\\ \\ \\ \\text{Function:}\\ \\ \\text{cube-box}(x, y) = y / 3x\\ .\\] Any \\((x^+, y^+)\\) that produces \\(\\text{cube-box}(x^+, y^+) = 1\\) is a pair that satisfies the constraint. In other words, the equality constraint amounts the 1-contour of the cube_box() function.\nTranslating the constraint into a function provides the opportunity to reframe the situation from the mandate, “the box must be a cube,” into a question, “How cubic-like is the box?” If the value of \\(\\text{cube-box}(x,y) &gt; 1\\), the box is flatter than a cube; something in the direction of a pizza box. If \\(\\text{cube-box}(x,y) &lt; 1\\) the box is taller than a cube, as if flowers were being shipped in it.}\nThe constraint-to-function translated situation is shown in Figure 50.15:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 50.15: Zooming in on the objective function Profit() and showing the function version of the constraint, cube_box() using magenta contours, with the heavy green line being the contour at cube_box(x,y)=1.\n\n\n\nEarlier, we saw that if restricted to inputs on the contour \\(\\text{cube-box}(x,y) = 1\\), the optimal output value of Profit() is about $13.50. Now we have a broader picture. For instance, suppose we allow a “little” deviation in box shape from a cube, say, cube_box(x,y) = 1.05. If we allowed this, the value of the Profit() function could be increased from $13.50 to about $22.50 .\nWhether the $9 increase in value justifies the deviation from a cube by 5% is a matter of judgement. We don’t have an immediate way to translate the output of cube_box() into the same units as the output of profit(). The two different units are said to be incommensurate, meaning that they cannot be directly compared. Nonetheless, we now have a basis for a conversation. It might go like this:\nModeler to Designer: I realize that from your perspective, a cube is the optimal shape for the box.\nDesigner: Right. Cubes are in fashion this year. Last year it was the Golden Ratio.\nModeler: It might sound surprising, but we find that so long as you are close to the optimal, it does not much matter if you are exactly on it. How close to a perfect cube would be good enough?\nDesigner: What’s really important is that the box be perceived as a cube in our sales material. I think that most customers would think “cube” so long as the edge lengths are within about 15% of one another.\nModeler: That’s very helpful. Let’s see if I can translate that into the cube_box() function.\n[Modeler does some scribbling while mumbling to himself. “\\(y-2x\\) is the base width and depth of the box, and \\(x\\) is the height of the box. So if \\(y-2x = 1.15 x\\) then \\(y = 3.15 x\\). \\(\\text{cube-box}(x, 3.15 x) = 1.05\\).]\nModeler: [to Designer] The 15% deviation corresponds to an output of 1.05 from \\(\\text{cube-box}()\\).\nModeler: [To product manager] Making that change in shape increases profit per box from $13.50 to $22.50.\nProduct manager: Good job! How about a 30% deviation? That let’s us get up to about $33 in profit.\nDesigner: But it would make the box shaped like a brick! Bricks are so 1990s!\nModeler: It sounds like a 15% deviation would be about right.\nMaking the constraint negotiable by representing it with a function, broadens the scope of the discussion and points to new ways of improving the result.\n\nApplication area 50.4 — Finding an optimal mixture of capital and labor.\n\n\n\n\n\n\n\nApplication area 50.4 Factory production\n\n\n\nLet’s return to a previous example about determining optimal levels of labor and capital in a factory. In that example, the objective function was the money value of the product produced. There was also a budget constraint. Translating the budget constraint into a function, which we will call expenditure(K, L), we have \\(\\text{expenditure}(K, L) = 6000 L + 13000 K\\). Our budget amounts to enforcing \\(\\text{expenditure}(K, L) = \\$5,500,000\\).\nA manager presented with a budget knows that she should work within the constraints of that budget. Mathematically, however, it is easy to imagine the budget being changed, either relaxed or tightened. This mathematical possibility provides the means to extract new information that can be helpful in making decisions at a higher level, that is, above the rank of the manager. This can be helpful to higher management—the people who are responsible for seeing the bigger picture.\nFigure 50.16 show the production function plotted along with the expenditure function. The budget constraint corresponds to a single output level of the expenditure function, that is, a single contour of the expenditure function. Other contours correspond to different values for the budget constraints. Such a graph makes it easy to calculate the consequences for relaxing (or tightening) the constraint.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 50.16: The production function for factory output (blue, curved lines) and the labor/capital expenditure function (magenta, straight lines). Contour labels are in millions of dollars. Contour levels for expenditure (magenta: 3.1, 3.9, 4.6, etc.) were selected so that the magenta contours are nearly tangent to the blue factory-output contours. This makes it easy to see the K/L position for optimal factory output for each of the indicated expenditure levels.\n\n\n\nWith the budget fixed at $5.5 million—that is, on the $5.5-million contour of the expenditure function—the maximum production was $7.1 million.\nWhat happens if we pretend that the budget level was different? Doing so is a matter of looking at a different contour of the expenditure function. For example, if the budget had been smaller, say only $5 million, then production also goes down, to $6.5 million. On the other hand, if we had the means to increase the budget to $6 million, production would go up to $7.7 million.\nIn this example we see that an increase in budget of $500K produces an increase in production worth $600K. It might seem logical that it is worth raising the budget to harvest the extra production, but that is not necessarily the case. To see why, recall that we use constraints such as the budget constraint in this problem to represent a real-world situation where there are multiple objectives, not just the particular objective represented by the objective function. There is a budget in the first place because there are other, competing uses for the money. For instance, the money might be better spent in some other product line that is even more profitable. Or perhaps higher management, taking a long-term strategic view, would prefer to spend the funds on research and development.\nThe point of exploring theoretical changes in the budget is to provide information to the higher-level decision makers, the people who set the budget as opposed to the managers who have to work within the budget. The format that most non-technical people would find accessible is simple: a $500K increase in the budget will result in $600K greater production.\nIn mathematical presentations, this same information is often formatted differently, as a ratio called the Lagrange multiplier. The Lagrange multiplier in this example would be 600/500, that is, 1.2. There is no intrinsic advantage of the Lagrange multiplier format over the common sense format, but the Lagrange multiplier is the format used in many textbooks, so it is worthwhile to know the nomenclature. Some economists have a more evocative name for the ratio: the shadow price of the constraint. Thus, the theoretical exploration of relaxing the constraint provides a straightforward way to put a cost on the constraint. This gives a reasonable way to determine the value of something when there is no direct market for it.\nAn important example of a shadow price comes in the setting of life-saving interventions. For example, increasing spending on highway safety can save lives. If $7.5 billion in increased expenditures saves 1000 lives, the shadow price is $7.5 M per life. People who misinterpret the constraint-to-function methodology often think that it is about cravenly putting a money value on life. In reality, the method merely reveals the money value on life implicit in decisions such as budget allocations. Knowing that the shadow price is $7.5 M does not say what the value of life should be. But it provides a mechanism for comparing different uses for the money. For instance, if the shadow price for increased regulation of toxic industrial chemicals is $11.3 M per life, the relative shadow prices provide an indication that budget money might reasonably be shifted from chemical regulation to highway safety. Economists and epidemiologists who undertake such calculations reveal that the mixture of spending on different life-preserving interventions is far from optimal.\n\n\n\n\n\n\n\n\nTip\n\n\n\nGenerations of calculus students have been taught a method of mathematical optimization in the presence of constraints that involves positing a Lagrange multiplier, typically written as \\(\\lambda\\), and carrying out a series of differentiations followed by equation solving to find an argmax, which simultaneously provides a numerical value for \\(\\lambda\\). It is easier to understand the motivation behind this by considering the gradient of the objective function and the gradient of the constraint function. If the goal is, say, to maximize production and simultaneously minimize expenditures, we would want to walk up the production gradient and down the expenditure gradient.\nFigure 50.17 shows two gradient fields, one for the production function in the factory-design example and one for expenditure. (The negative of the expenditure gradient is shown, since the goal is to keep expenditures small.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 50.17: The production and expenditure functions displayed as gradient fields. Expenditure is brown, production is magenta.\n\n\n\nAt each point in the graphics frame, the two gradient vectors form an angle. For example, near the point labeled (a) the angle is roughly 140 degrees, while near (b) the angle is 180 degrees.\nAny value of \\(K\\) and \\(L\\) where the angle is less than 180 degrees is sub-optimal or dominated by some other choice of \\(K\\) and \\(L\\). For instance, near label (a), you could improve both production and expenditures by moving to the southeast. When the angle is 180 degrees, the objective and constraint functions are in complete opposition to one another; any movement in favor of one comes at the cost in the other.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Optimization and constraint</span>"
    ]
  },
  {
    "objectID": "Manifestations/49-optimization.html#note-other-optimization-algorithms",
    "href": "Manifestations/49-optimization.html#note-other-optimization-algorithms",
    "title": "50  Optimization and constraint",
    "section": "50.4 Note: Other optimization algorithms",
    "text": "50.4 Note: Other optimization algorithms\nContemporary work often involves problems with tens, hundreds, thousands, or even millions of inputs. Even in such large problems, the mechanics of finding the corresponding gradient vector are straightforward. Searching through a high-dimensional space, however, is not generally a task that can be accomplished using calculus tools. Instead, starting in the 1940s, great creativity has been applied to develop algorithms with names like linear programming, quadratic programming, dynamic programming, etc. many of which are based on ideas from linear algebra such as the qr.solve() algorithm that you will meet in Block 5, or ideas from statistics and statistical physics that incorporate randomness as an essential component. An entire field, operations research, focuses on setting up and solving such problems. Building appropriate algorithms requires deep understanding of several areas of mathematics. But using the methods is mainly a matter of knowing how to set up the problem and communicate the objective function, constraints, etc. to a computer.\nPurely as an example, let’s examine the operation of an early algorithmic optimization method: Nelder-Mead, dating from the mid-1960s. (There are better, faster methods now, but they are harder to understand.)\nNelder-Mead is designed to search for maxima of objective functions with \\(n\\) inputs. The video shows an example with \\(n=2\\) in the domain of a contour plot of the objective function. Of course, you can simply scan the contour plot by eye to find the maxima and minima. The point here is to demonstrate the Nelder-Mead algorithm.\nStart by selecting \\(n+1\\) points on the domain that are not colinear. When \\(n=2\\), the \\(2+1\\) points are the vertices of a triangle. The set of points defines a simplex, which you can think of as a region of the domain that can be fenced off by connecting the vertices.\nEvaluate the objective function at the vertices of the simplex. One of the vertices will have the lowest score for the output of the objective. From that vertex, project a line through the midpoint of the fence segment defined by the other \\(n\\) vertices. In the video, this is drawn using dashes. Then try a handful of points along that line, indicated by the colored dots in the video. One of these will have a higher score for the objective function than the vertex used to define the line. Replace that vertex with the new, higher-scoring point. Now you have another simplex and can repeat the process. The actual algorithm has additional rules to handle special cases, but the gist of the algorithm is simple.\n\n\n\n\n\n\nVideo 50.1: A demonstration of successive steps in the Nelder-Mead optimization algorithm. Based on the objective function’s values at the three vertices in the previous triangle, a new triangle is formed by discarding the worst of the previous vertices and adding a new vertex in the direction of improvement. Source: Miles Chen, Department of Statistics, UCLA",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Optimization and constraint</span>"
    ]
  },
  {
    "objectID": "Manifestations/50-probability.html",
    "href": "Manifestations/50-probability.html",
    "title": "51  Probability and evidence",
    "section": "",
    "text": "51.1 Probability density\nWe often deal with situations of uncertainty, situations where only partial predictions are possible. For instance, we can say whether a person may be at high risk for a disease, say, diabetes or lung cancer. But this does not let us predict with certainty whether the person will get the disease. Instead, the term “high risk” indicates that we know something but not everything about the situation: not whether or not the person will get the disease but whether they are “likely” to have or to get it. Another example: a car might be said to be “unreliable.” We do not mean by this that the car cannot be used. Rather we are thinking that from time to time the car might fail to start or run. A car where this happens once over a few year span is reliable, a car where this happens on a month-to-month basis is not reliable.\nYou may well have had some textbook exposure to probability as an intellectual field. Typical examples used to illustrate concepts and methods are coins being flipped, dice being tossed, and spinners spun. Colored balls are drawn from urns, slips of paper from hats, and so on. Each of these is a physical representation of an idealized mechanism where we feel sure we understand how likely each possible outcome is to happen.\nIn this chapter, we will use two basic imagined settings where uncertainty comes into play: the risk of disease before the disease is diagnosed and the safety of a self-driving car as it comes out of the factory. The word “imagined” signals that you should not draw conclusions about the facts of any particular disease or any particular self-driving car; we are merely using the imagined settings to lay out concepts and methods for the mathematical presentation and analysis of uncertainty and risk. Of particular importance will be the mathematical means by which we represent our knowledge or belief in these settings and the way we can properly update our knowledge/belief as new information becomes available.\nA probability, as you may know, is a dimensionless number between zero and one (inclusive). In this chapter, you will be dealing with functions relating to probabilities. The input to these functions will usually be a quantity that can have dimension, for instance, miles driven by a car. For some of the functions we will see in this chapter, the output will be a probability. For other functions in this chapter, the output will be a probability density.\nProbability relates to the abstract notion of an event. An event is a process that produces an outcome. For instance:\nAn event with a discrete outcome—coin flip, medical screening test—can be modeled by assigning a probability number to each of the possible outcomes. To be a valid probability model, each of those assigned numbers should be greater than or equal to zero. In addition, the sum of the assigned numbers across all the possible outcomes should be 1.\nFor events with a continuous outcome, such as the dart toss where the outcome is distance from the center, the probability model takes the form of a function whose domain is the possible outcomes. For the model to be a valid probability model, we require that the function output should never be less than zero. There is another requirement as well: the integral of the function over the entire domain should be 1. For the dart-toss event, if we denote the distance from the bullseye as \\(r\\) and the assigned number for the probability model as \\(g(r)\\), the integral requirement amounts to \\[\\int_0^\\infty g(r) dr = 1\\ .\\]\nNote that the output \\(g(r)\\) is not a probability, it is a probability density. To see why, let’s use the fundamental theorem of calculus to break up the integral into three segments:\nThe total integral is \\[\\int_0^\\infty g(r) dr = 1\\ = \\int_0^a g(r) dr + \\int_a^b g(r) dr + \\int_b^\\infty g(r) dr.\\] The probability that the dart lands at a distance somewhere between \\(a\\) and \\(b\\) is \\[\\int_a^b g(r) dr\\ .\\] Since \\(r\\) is a distance, the dimension \\([r] =\\ \\)L. Suppose the units of \\(r\\) are centimeters. We need \\(\\int g(r) dr\\) to be a dimensionless number. Since the dimension of the integral is \\([r] \\cdot [g(r)] = [1]\\), it must be that \\([g(r)] = [1/r] = \\text{L}^{-1}\\). Thus, \\(g(r)\\) is not a probability simply because it is not dimensionless. Instead, in the dart example, it is a “probability-per-centimeter.” This kind of quantity—probability per something—is called a probability density and \\(g(r)\\) itself is a probability density function.\nTo show the aptness of the word “density,” let’s switch to a graphic of a function that uses literal density of ink as the indicator of the function value. Figure 51.1) shows what the dart toss’s \\(g(r)\\) probability density function might look like:\nFigure 51.3 shows the contest entries from three competitors.\nThe functions called for by the contest instructions are relative density functions. The “relative” means that the function indicates where the probability is more or less dense, but the function has not yet been scaled to be a probability density function. Suppose \\(h(x)\\) is a relative density function such that \\[\\int_{-\\infty}^\\infty h(x)\\, dx = A \\neq 1\\ .\\] Although \\(h(x)\\) is not a probability density function, the very closely related function \\(\\frac{1}{A} h(x)\\) will be a probability density function. We will use the term normalizing to refer to the simple process of turning a relative density function into a probability density function.\nA relative density function is entirely adequate for describing the distribution of probability. However, when comparing two or more probability distributions, it is important that they all be on the same scale. Normalizing the relative density functions to probability density functions accomplishes this. Figure 51.4 compares the three relative probability functions in Figure 51.3. Johnny makes the density large over a narrow domain and zero elsewhere, while Louisa specifies a small density over a large domain. All three competitors’ functions have an area-under-the-curve of dimensionless 1.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Probability and evidence</span>"
    ]
  },
  {
    "objectID": "Manifestations/50-probability.html#probability-density",
    "href": "Manifestations/50-probability.html#probability-density",
    "title": "51  Probability and evidence",
    "section": "",
    "text": "Flipping a coin is an event where the possible outcomes of H and T.\nTaking a medical screening test is an event where the outcomes are “positive” or “negative.”\nThrowing a dart at a bullseye is an event where the outcome is the distance of the impact point from the center of the bullseye.\n\n\n\n\n\nclose to the bullseye: \\(0 \\leq r \\leq a\\)\nfar from the bullseye: \\(b &lt; r\\)\nnot close but not far: \\(a &lt; r \\leq b\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 51.1: Showing a probability density function for the dart distance in two modes: 1) an ordinary function graph and 2) the density of ink.\n\n\n\n\nApplication area 51.1  \n\n\n\n\n\n\n\nApplication area 51.1 How much money is in the jar?\n\n\n\nConsider a simple competition of the sort you might encounter at a fund-raising fair. There is a jar on display, filled with coins that have been donated by one of the fair’s sponsors. You pay $1 (which goes to a good cause) to enter the contest. Your play is to describe how much money is in the jar, writing your description down along with your name on an entry form. At the end of the day, an official will open the jar, count the money, and announce who made the best estimate. The winner gets the money in the jar.\n\nIn the usual way these contests are run, the contestants each write down a guess for the amount they think is in the jar, say $18.63. The winner is determined by seeing whose guess was closest to the actual value of the coins in the jar.\nIn reality, hardly anyone believes they can estimate the amount in the jar to the nearest penny. The person guessing $18.63 might prefer to be able to say, “between 18 and 19 dollars.” Or, maybe “$18 \\(\\pm\\) 3.” To communicate what you know about the situation, it is best to express a range of possibilities that you think likely.\nIn our more mathematical contest, we ask the participants to specify a function that describes their beliefs about the money in the jar. The instructions state, “On the graph-paper axes below, sketch a continuous function expressing your best belief about how much money is in the jar. The only requirement is that the function value must be zero or greater for all inputs.”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 51.2: The entry form for the money-in-the-jar contest.\n\n\n\nTake a minute to look at the picture of the jar and draw your function on the axes shown above. Think about why the contest form appropriately does not ask you to scale the vertical axis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 51.3: Three contestants’ contest entries.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 51.4: Comparing the contest entries by normalizing each of them to a probability density function.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Probability and evidence</span>"
    ]
  },
  {
    "objectID": "Manifestations/50-probability.html#three-density-functions",
    "href": "Manifestations/50-probability.html#three-density-functions",
    "title": "51  Probability and evidence",
    "section": "51.2 Three density functions",
    "text": "51.2 Three density functions\nThree commonly used families of probability density functions are:\n\nthe gaussian density function\nthe exponential density function\nthe uniform density function.\n\nFigure 51.5 shows their shapes.\n\n\n\n\n\n\n\n\n\n## Warning: All aesthetics have length 1, but the data has 301 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n\n\n\n\nGaussian density\n\n\n\n\n\n\n\nExponential density\n\n\n\n\n\n\n\n\n\nUniform density\n\n\n\n\n\n\n\nFigure 51.5: Three probability density functions that are often used in applied work.\n\n\n\nThe uniform density function, \\(u(x, a, b)\\) is more or less the equivalent of the constant function. The family has two parameters \\(a\\) and \\(b\\) with the function defined as: \\[\\text{unif}(x, a, b) \\equiv \\left\\{\\begin{array}{cl}\\frac{1}{b-a} & \\text{for}\\ a \\leq x \\leq b\\\\0& \\text{otherwise} \\end{array}\\right.\\] This function is used to express the idea of “equally likely to be any value in the range \\([a, b]\\).” For instance, to describe a probability that a January event is equally likely to occur at any point in the month, you can use \\(u(x, 0, 31)\\) where \\(x\\) and the parameters \\(a\\) and \\(b\\) have dimension T and are in units of days. Notice that the density itself has dimension T-1 and units “per day.”\nThe gaussian density function, \\(\\dnorm(x, \\text{mean}, \\text{sd})\\) is familiar to you from previous blocks in this book: the bell-shaped function. It is known also as the normal distribution because it is so frequently encountered in practice. It is a way of expressing, “The outcome of the event will likely be close to this particular value.” The parameter named mean specifies “this particular value.” The parameter sd specifies what’s mean by “close.” The gaussian density function is smooth. It is never zero, but \\(\\lim_{x \\rightarrow \\pm \\infty} \\dnorm(x, \\text{mean}, \\text{sd}) = 0\\).\nTo use an analogy between physical density (e.g., kg per cubic-meter), where density times size gives mass, we can say that the total mass of a probability density function is always 1. For the gaussian density, 68% of of the total mass is within \\(\\pm 1\\)sd of the mean, 95% is within \\(\\pm 2\\)sd of the mean, 99.7% within \\(\\pm 3\\)sd, and 99.99% within \\(\\pm 4\\)sd.\nThe exponential probability density is shaped just like an exponential function \\(e^{-kx}\\). It is used to describe events that are equally likely to happen in any interval of the input quantity, and describes the relative probability that the first event to occur will be at \\(x\\).",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Probability and evidence</span>"
    ]
  },
  {
    "objectID": "Manifestations/50-probability.html#sec-expected_value",
    "href": "Manifestations/50-probability.html#sec-expected_value",
    "title": "51  Probability and evidence",
    "section": "51.3 Expectation value, mean and variance",
    "text": "51.3 Expectation value, mean and variance\nProbability theory was originally motivated by problems in gambling, specifically, figuring out what casino games are worth betting on. A feature of casino games—roulette, slot machines, blackjack, Texas hold’em, etc.—is that they are played over and over again. In any one round of play, you might win or you might lose, that is, your “earnings” might be positive or they might be negative. Over many plays, however, the wins and loses tend to cancel out. One way to summarize the game itself, as opposed to the outcome of any single play, is by the average earnings per play. This is called the expected value of the game.\nThis logic is often applied to summarizing a probability density function. If \\(x\\) is the outcome of the random event described by a probability density \\(f(x)\\), the expected value of the probability density is defined as \\[\\mathbb{E}\\!\\left[{\\strut} x\\right] \\equiv \\int_{-\\infty}^\\infty x\\, f(x) \\, dx\\ .\\] Section 53.4 uses this same form of integral for computing the center of mass of an object.\n\n\n\n\n\n\nWhy square braces for expected value?\n\n\n\nWhy are you using square braces \\(\\left[\\strut\\ \\ \\right]\\) rather than parentheses \\(\\left(\\strut \\ \\  \\right)\\).\nWe always used parentheses to indicate that the enclosed quantity is the input to a function. But \\(\\mathbb{E}\\!\\left[{\\strut} x\\right]\\) is not a function, let alone a function of \\(x\\). Instead, \\(\\mathbb{E}\\!\\left[{\\strut} x\\right]\\) is a numerical summary of a probability density function \\(f(x)\\).\n\n\n\n\n\n\n\n\n\n\nTry it! 51.1\n\n\n\n\n\n\n\n\n\nTry it! 51.1 Computing expected value\n\n\n\nFind the expected value of the gaussian probability density \\(\\dnorm(x, \\text{mean}=6.3, \\text{sd}= 17.5)\\). Using the R/mosaic Integrate() function, we have\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nThe expected value of a gaussian is the same as the parameter called mean which describes the argmax of the gaussian.\n\n\nAnother important quantity to describe data or probability distributions is the variance, which is the average of the square distance from the mean. In math notation, this looks like \\[\\mathbb{E}\\!\\left[{\\large\\strut} (x - \\mathbb{E}[x])^2\\right] = \\int_{-\\infty}^{\\infty} \\left(\\strut x - \\text{mean}\\right)^2\\, \\dnorm(x, \\text{mean}, \\text{sd})\\, dx\\ .\\]\n\n\n\n\n\n\n\n\nTry it! 51.2\n\n\n\n\n\n\n\n\n\nTry it! 51.2 Computing variance\n\n\n\nCompute the variance of a gaussian probability density \\(\\dnorm(x, \\text{mean}=6.3, \\text{sd}= 17.5)\\).\nTo do this, we must first know the mean, then we can carry out the integration.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nAgain, you might have anticipated this result, since the variance is the square of the standard deviation (sd) and we were using a particular gaussian distribution with sd equaling 17.5. Of course, \\(17.5^2 = 306.25\\).\n\n\nTo illustrate the calculations in another setting, we will use an exponential probability function. Just as the R function dnorm() gives the density of the “normal”/gaussian distribution, the R function dexp() outputs the density of the exponential distribution. We used \\(k\\) as the parameter in the exponential distribution. In R, the parameter is stated using the rate at which events happen, that is, the expected number of events per unit time. For instance, the following integrals compute the mean and standard deviation of an exponential process where events happen on average twice per time unit.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nThe result shouldn’t surprise you. If events are occurring on average twice per unit time, the average time between events should be 0.5 time units.\nHere’s the variance of the same distribution\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nIt works out that for an exponential distribution with parameter \\(k\\), the mean is \\(1/k\\) and the standard deviation (square root of the variance) is also \\(1/k\\).\nFinally, let’s look at the mean and variance of a uniform distribution with, say, \\(a=0\\) and \\(b=10\\). We can do this symbolically or numerically. For the mean: \\[\\int_{-\\infty}^\\infty x\\  \\text{unif}(x, 0, 10)\\, dx = \\int_0^{10} \\frac{x}{10-0}\\, dx = \\left.{\\Large\\strut} \\frac{x^2}{20}\\right|_{x=0}^{10} \\\\= \\frac{100}{20} - \\frac{0}{20} = 5\\] For the variance, \\[\\begin{eqnarray}\n\\ \\ \\ & \\!\\!\\!\\!\\!\\!\\int_{-\\infty}^\\infty (x-5)^2\\  \\text{unif}(x, 0, 10)\\, dx\\\\\n& = \\int_0^{10} \\frac{(x-5)^2}{10-0}\\, dx\\\\\n& = \\left.{\\Large\\strut}\\frac{(x-5)^3}{30}\\right|_{x=0}^{10}\\\\\n& =\\frac{5^3}{30} - \\frac{(-5)^3}{30} = \\frac{125}{30} - \\frac{-125}{30} = 8 \\tiny{\\frac{1}{3}}\n\\end{eqnarray}\\]\nOr, numerically\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\nWhere to look first on an infinite domain?\n\n\n\nNumerical integrals from \\(-\\infty\\) to \\(\\infty\\) of functions that are zero almost everywhere are challenging. The computer has to figure out where, out of the whole number line, the function has non-zero output. We’ve given the computer a head start by using 0 in the limits of integration. This would not be a problem for the exponential or gaussian distribution, which are non-zero everywhere (for the gaussian) or for half the number line (for the exponential).",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Probability and evidence</span>"
    ]
  },
  {
    "objectID": "Manifestations/50-probability.html#likelihood-and-data",
    "href": "Manifestations/50-probability.html#likelihood-and-data",
    "title": "51  Probability and evidence",
    "section": "51.4 Likelihood and data",
    "text": "51.4 Likelihood and data\nIn this section, we will examine the accepted technique for combining data with probability density functions to combine previous knowledge with new observations. The technique, called Bayesian inference, is used throughout science and engineering.\nRecall that a relative density function is a format to describe the relatively likeliness of possible outcomes from a random event. The domain for a relative density function is the complete set of possible outcomes from the event. An example: The distance of a dart’s impact from the bullseye.\nThe output of a relative density function is a non-negative number. For an expert dart thrower, the relative density will be high for small distances and low for large distances. This is just a way of quantifying that the expert’s is likely to hit close to the bullseye.\nIn comparing two relative density functions, for instance the function for an expert dart thrower versus that for an amateur, it is helpful to normalize them so that the integral of the relative density over the entire domain is dimensionless 1. The normalized version of a relative density function is called a probability density functions. Note that the probability density function contains the same information as the relative density function.\nIn this section, we introduce a new type of function that is important in probability calculations involving data. This new type of function is, perhaps confusingly, called a likelihood function.\nLikelihood functions always involve hypothetical reasoning. The idea is to construct a model world whose characteristics are exactly known. In that world, we can imagine constructing a function that gives the probability or probability density of any possible value of a measurement.\nFor instance, Johnny, Louisa, and Geoff each created hypothetical worlds that describe the amount of money in the jar. For each contestant, their personal hypothesis states a probability density over all the theoretically possible amounts of money in the jar.\nThe domain of a likelihood function is all the competing hypotheses. Take a moment to digest that. The domain of money-in-jar likelihood function is not the amount of money in the jar, it is instead the three hypotheses: Johnny’s, Louisa’s, and Geoff’s.\nIt is conventional to denote name a likelihood function \\({\\cal L}()\\). For the competition, a likelihood function will be \\({\\cal L}(\\text{contestant})\\), where \\(\\text{contestant}\\) will be one of “Johnny” or “Louisa” or “Geoff” in our example.\nThere are many likelihood functions that might be relevant to the money-in-jar situation. There is one likelihood function for each possible amount of money in the jar. For instance, \\({\\cal L}_{\\$10}(\\text{contestant})\\) is relevant if there were ten dollars in the jar. Another likelihood function \\({\\cal L}_{\\$11.50}(\\text{contestant})\\) would be relevant if there were eleven dollars and fifty cents in the jar.\nThis notation of naming functions using a subscript can get awkward when there are a huge number of functions. For instance, for the money-in-jar contest there will be a likelihood function for $0.01, $0.02, $0.03, and all other possibilities such as $21.83 or \\(47.06\\). If we want to be able to refer to the whole set of likelihood functions, better to replace the dollar amount in the subscript with a symbol, say \\(m\\) for money. Then the whole set of likelihood functions potentially relevant to the contest would be written \\({\\cal L}_m(\\text{contestant})\\).\n\n\n\n\n\n\nNotation styles\n\n\n\nThere is another style for notation that you may encounter in your future work. In the alternative style, for example, instead of \\({\\cal L}_m(\\text{contestant})\\) the likelihood function would be written \\({\\cal L}(\\text{contestant}\\, {\\mathbf |} m )\\). The vertical bar is pronounced “given” and is part of a notational system often used in probability calculations.\n\n\nSince the output of any likelihood function is a probability or a probability density depending on context, we know that the output will be a non-negative quantity.\nLikelihood functions provide the link between data and hypotheses. The idea is that when data become available, it is possible to choose the relevant likelihood function.\nTo illustrate, let’s return to the jar-of-money contest and the three competitors’ entries as shown in Figure 51.4. For convenience, that Figure is reproduced here:\n\n\n\n\n## Warning: No shared levels found between `names(values)` of the manual scale and the\n## data's colour values.\n\n\n\n\n\n\n\n\n\n\nFigure 51.6: The contest entries shown in Figure 51.4.\n\n\n\nThe functions shown in the Figure are not likelihood functions. But we can use them to construct whatever likelihood function turns out to be relevant in the money-in-jar contest.\n\nApplication area 51.2  \n\n\n\n\n\n\n\nApplication area 51.2 Who won? (Bayesian style)\n\n\n\nIt is time to calculate who won the jar-of-coins contest! That is, we will calculate whose entry is best. The word “best” should remind you of optimization and indeed the winner of the contest will be the argmax of the relevant likelihood function. At this point, remember that the likelihood functions are \\({\\cal L}_m(\\text{contestant})\\), so the argmax will be one of the contestants!\nFirst, we need to pick the relevant likelihood function. Common sense tells us that you can only pick a winner when the jar has been opened and the money counted. That is, we need some data.\nHere’s the data: The officials have opened the jar and carefully counted the money. There was $32.14 in the jar. This tells us that the relevant likelihood function is \\({\\cal L}_{\\$32.14}(\\text{contestant})\\).\nThe output of \\({\\cal L}_{\\$32.14}(\\text{contestant})\\) is the probability density assigned by the contestant to the observed value 32.14. You can read this from Figure 51.6. For your convenience, the observation \\(32.14\\) has been annotated with a faint brown vertical line.\nHere’s a tabular version of \\({\\cal L}_{\\$32.14}(\\text{contestant})\\).\n\n\n\n\\(\\text{contestant}\\)\n\\({\\cal L}_{\\$32.14}(\\text{contestant})\\)\n\n\n\n\nJohnny\n0.000 per dollar\n\n\nLouisa\n0.010 per dollar\n\n\nGeoff\n0.066 per dollar\n\n\n\nIn statistics, likelihood functions are used to describe how to estimate a quantity given some data about the quantity. The techique is called maximum likelihood estimation: the estimate is the argmax of the likelihood function. For the coins-in-jar contest, the argmax is Geoff. Therefore, Geoff wins!\nIn the spirit of “Monday morning quarterbacking,” let’s look carefully at Johnny’s entry. If his bar-shaped probability density function were shifted just a little to the right, he would have won. This illustrates a weakness in Johnny’s logic in constructing his probability density function. The function indicates that he thought the probability of the amount being $23 was the same as being 30 dollars. In other words, he was uncertain to a considerable extent. But given this uncertainty, why would he insist that $30.01 is impossible (that is, has probability density 0 per dollar). Wouldn’t it make more sense to admit nonzero density for $30.01, and similarly for $30.02 and upward, with the density gradually decreasing with the amount of money. This is why, absent very specific knowledge about the circumstances, probability densities are so often framed as Gaussian distributions, as in Geoff’s entry.\n\n\nThe previous example is intended to give you an idea about what a likelihood function is. In that example, we use the calculus operator argmax to find the contest winner.\nLet’s turn now to another important use of likelihood functions: their role in the Bayesian inference process. The example concerns figuring out the risk of disease transmission.\n\nApplication area 51.3 —Functions suited to modeling risk.\n\n\n\n\n\n\n\nApplication area 51.3 Beliefs about risk\n\n\n\nConsider the situation in November 2019 at the start of the COVID-19 pandemic. At that time, there was almost no information about the illness or how it spreads. In the US and many other countries, most people assumed that the spread of illness outside its origin in Wuhan, China, would be prevented by standard public health measures such as testing, contact tracing, quarantine, and restrictions on international travel. Intuitively, most people translated this assumption into a sense that the personal risk of illness was small.\nIn communicating with the public about risk, it is common to present risk as a number: a probability. This is an adequate presentation only when we have a solid idea of the risk. To form a solid idea, we need evidence.\nBefore there is enough evidence responsibly to form a solid idea, it is best to present risk not as a probability but as a probability density function. To illustrate, Figure 51.7) shows three different examples of what such probability density functions might look like for vague, preliminary ideas of risk.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 51.7: Three different opinions about the risk of a disease.\n\n\n\nPanel (A) in Figure 51.7 is a strong statement that the risk is believed to be small. Even so, the density function is non-zero even for values of the risk near 100%. This is an honest admission that, as with COVID-19, something that we don’t know might be going on. In the case of COVID-19, what most people didn’t realize is 1) that the reported numbers were completely unrepresentative of the extent of spread, since most cases are asymptomatic and 2) that the illness can spread even by those who are asymptomatic. Epidemiologists and other public health workers knew enough from previous experience to be aware of their lack of knowledge about (1) and (2), but the rest of us, including many policy makers, didn’t even know what they didn’t know. The word “unk-unk” is sometimes used by engineers to refer to such an “unknown unknown”.\nPanel (B) says, “I have no idea!” This can often be an honest, useful appraisal of the situation. But experts who are honest in this way are often regarded by the public and policy makers as lacking credibility.\nPanel (C) expresses the belief that the risk might well be small but also might be large.\nAny of the three probability density functions would be reasonable statements about what we knew and didn’t know about COVID-19 at the very beginning of the pandemic, before there was much data. Such statements are called priors; summaries of what we know up to the present.\nIn Bayesian inference, as data become available we can revise or update the priors, giving a better informed description of the risk.\nFor COVID-19, data eventually came in many different forms: estimates of incubation periods, testing to determine what fraction of cases are asymptomatic, and so on.\nFor our presentation of Bayesian reasoning, we will consider a simplified situation where data come in only one form: screening tests for the illness. Imagine that you are conducting a contact-tracing study. Whenever a patient presents with COVID-19 symptoms and has a positive PCR test, that patient’s close contacts are given a screening test for COVID. The objective is to estimate how transmissible the virus is by figuring out what proportion of close contacts become infected.\nWe cannot know which of the three priors in Figure 51.7 is most appropriate. After all, until rich enough data become available, each prior is just an opinion. So we will repeat the update-with-data analysis for each of the three priors. If, in the end, the results from the three priors substantially agree, then we can conclude that the data is shaping the results, rather than the prior.\nThe unknown here is the risk \\(R\\) of transmission. We will denote the three priors as \\(\\text{prior}_A (R)\\), \\(\\text{prior}_B (R)\\), and \\(\\text{prior}_C (R)\\). But, in general, we will write \\(\\text{prior}(R)\\) to stand for any of those three specific priors.\n\n\nIn Bayesian inference, the prior represents the starting point for what we know (or, more precisely, “believe”) about the risk of transmission. It has the form of a relative density function. As data come in, we update our prior beliefs on the basis of the data.\nAfter we have updated our prior, our state of knowledge is called a posterior belief. Think of the prior as “pre-data” belief and the posterior as “post-data” belief. The posterior also has the form of a relative density function.\nThe formula for updating is called Bayes’ Rule: posterior is likelihood times prior. \\[\\text{posterior}(R) = {\\cal L}_\\text{data}(R) \\times \\text{prior}(R)\\ .\\] Recall that the output of a likelihood function is a non-negative quantity. Since the prior is a relative density function, it too is non-negative for all \\(R\\). Therefore the posterior will have a non-negative output and be a valid relative density function.\n\n\n\n\n\n\nPrior as relative density functions\n\n\n\nMost texts prefer to define priors and posteriors as probability density functions rather than relative density functions. The only difference, of course, is the normalization. But that can be performed at any time, so to streamline the updating process, we will let posteriors and priors be relative density functions.\n\n\nNotice that the posterior has just one input, the parameter \\(R\\). That is because the \\(\\text{data}\\) is fixed by our observations: the posterior only makes sense once we have the data available to choose the relevant likelihood function.\nOur task now is to construct the appropriate likelihood function that reflects how the screening test works. To outline the process, let’s consider a group of 1000 people who are taking the screening test. If we knew the parameter \\(R\\), we could split those 1000 people into two groups: one group with the illness and one group without.\n\nWhole group of 1000, made up of\n\n1000 \\(R\\) with the illness\n1000 \\((1-R)\\) without the illness\n\n\nFor instance, if \\(R=0.2\\), then out of the whole group of 1000 people, 200 would have the illness and 800 would not.\nAfter taking the screening test, each person will have either a positive test result (we will write this “+”) or a negative test result (we will write “-”).\nto make sense of a screening test, you need to know two probabilities. These are:\n\nThe probability of a + test in a group of people with the disease. We will call this \\(p_d(+)\\).\nThe probability of a - test in a group of people without the disease. we will call this \\(p_h(-)\\).\n\nNote that the subscript indicates whether we are referring to the probability in the has-the-illness group (\\(p_d\\)) or in the no-illness (“healthy”) group (\\(p_h\\)).\nYou may know that the result of a screening test is not definitive. That is, a person with a \\(+\\) result may not have the illness. Likewise, a \\(-\\) result is no guarantee that the person does not have the illness. The word “screening” is meant to emphasize the imperfections of such tests. But often the imperfect test is the best we have available.\nAfter the screening test has been taken by the 1000 people in our example group, we can divide them further\n\nWhole group of 1000, made up of\n\n1000 \\(R\\) with the illness, made up of\n\n1000 \\(R\\ p_d(+)\\) who had a correct positive test result\n1000 \\(R\\ (1-p_d(+))\\) who had a negative result despite having the illness\n\n1000 \\((1-R)\\) without the illness, made up of\n\n1000 \\((1-R)\\ (1-p_h(-))\\) who had a positive test result despite being healthy\n1000 \\((1-R)\\ p_h(-)\\) who had a correct negative result.\n\n\n\n\nApplication area 51.4  \n\n\n\n\n\n\n\nApplication area 51.4 Likelihood of illness\n\n\n\nSuppose that \\(R=0.2\\), \\(p_d(+) = 80\\%\\), and \\(p_h(-) = 70\\%\\). Then the division would be:\n\nWhole group of 1000, made up of\n\n200 with the illness, of whom\n\n160 who got a correct \\(+\\) result\n40 who got a \\(-\\) result, despite having the illness\n\n800 without the illness, of whom\n\n240 who got a \\(+\\) result, despite not having the illness\n560 who got a correct \\(-\\) result.\n\n\n\nThere are two likelihood functions reflecting the two different possible test results: \\({\\cal L}_+ (R)\\) and \\({\\cal L}_- (R)\\). We will need to construct both of these functions since the test result is going to be \\(+\\) for some people and \\(-\\) for others.\nRecall now that each likelihood function is based on a hypothesis about the world. In this case, the hypothesis is a particular value for \\(R\\). Let’s look at the situation for the hypothesis that \\(R=0.2\\). We can figure out the values of both \\({\\cal L}_+ (R=0.2)\\) and \\({\\cal L}_- (R=0.2)\\) from the breakdown given in the above example.\n\\({\\cal L}_+ (R=0.2)\\) is the probability of observing the given data (\\(+\\)) in the hypothetical world where \\(R=0.2\\). Out of the 1000 people, 160 will get a correct test result \\(+\\) and 240 people will get a \\(+\\) despite not having the illness. Therefore, \\[{\\cal L}_+ (R=0.2) = \\frac{160+240}{1000} = 40\\%\\ .\\]\nSimilarly, \\({\\cal L}_- (R=0.2)\\) is the probability of observing the given data (\\(-\\)) in the hypothetical world where \\(R=0.2\\). In the above breakdown, altogether 600 people received a \\(-\\) result: 560 of these were indeed healthy and 40 had the illness but nonetheless got a \\(-\\) result. So, \\[{\\cal L}_- (R=0.2) = \\frac{40+560}{1000} = 60\\%\\ .\\]\n\n\nThe above example calculated the output of the likelihood function for both \\(+\\) and \\(-\\) results when \\(R=0.2\\). We can repeat the calculation for any other value of \\(R\\). The results, as you can confirm yourself, are\n\\[{\\cal L}_+(R) = p_d(+)\\ R + (1-p_h(-))\\ (1-R) \\] and\n\\[{\\cal L}_-(R) = (1-p_d(+))\\ R + p_h(-)\\, (1-R)\\ .\\] In a real-world situation, we would have to do some experiments to measure \\(p_h(-)\\) and \\(p_d(+)\\). For our example we will set \\(p_d(+) = 0.8\\) and \\(p_h(-) = 0.7\\).\nNow that we have constructed the likelihood functions for the two possible observations \\(+\\) and \\(-\\), we can use them to update the priors.\nSuppose our first observations are the results of screening tests on ten randomly selected individuals, as in Table 51.1.\n\n\n\nTable 51.1: Test outcomes on ten patients. There were three \\(+\\) tests.\n\n\n\n\n\nSubject ID\nTest outcome\n\n\n\n\n4349A\n\\(+\\)\n\n\n7386A\n\\(-\\)\n\n\n6263E\n\\(+\\)\n\n\n5912C\n\\(-\\)\n\n\n7361C\n\\(-\\)\n\n\n9384C\n\\(-\\)\n\n\n6312A\n\\(-\\)\n\n\n3017C\n\\(+\\)\n\n\n1347B\n\\(-\\)\n\n\n9611D\n\\(-\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing prior (A)\n\n\n\n\n\n\n\nUsing prior (B)\n\n\n\n\n\n\n\nUsing prior (C)\n\n\n\n\n\n\n\nFigure 51.8: Posteriors (blue) consider only the first two screening tests, one \\(+\\) and one \\(-\\), for each of the priors in Figure 51.7. The prior itself is drawn in gray.\n\n\n\nAfter the first test outcome is available we can calculate the posterior: \\[\\text{posterior}_1 (R) = {\\cal L}_+(R) \\times \\text{prior(R)}\\ .\\] After the second test outcome, the new posterior is \\[\\text{posterior}_2 (R) = {\\cal L}_-(R) \\times {\\cal L}_+(R) \\times \\text{prior(R)}\\ .\\] And after the third (a \\(+\\) result!) it will be \\[\\text{posterior}_3 (R) = {\\cal L}_+(R) \\times {\\cal L}_-(R) \\times {\\cal L}_+(R) \\times \\text{prior(R)}\\ .\\]\nWith just two rows of data considered, the posteriors depend very much on the particular prior selected. This shouldn’t be a surprise; one test result from an imperfect screening test is not going to tell us much.\nMore data might help things! We can continue on in this way through all ten rows of the data to get the posterior distribution after all 10 test results have been incorporated, as in Figure 51.9.\n\n\n\n\n\n\n\n\n\n\n\n\nUsing prior (A)\n\n\n\n\n\n\n\nUsing prior (B)\n\n\n\n\n\n\n\nUsing prior (C)\n\n\n\n\n\n\n\nFigure 51.9: Posteriors (blue) after the first ten rows of data, for each of the priors in Figure 51.7. The prior itself is drawn in gray.\n\n\n\nAfter the first 10 rows of data have been considered, the posteriors are similar despite the different priors.\nLet’s go on to collect more data, on 100 patients.\n\n\n\n\n\n\n\n\n\n\n\n\nUsing prior (A)\n\n\n\n\n\n\n\nUsing prior (B)\n\n\n\n\n\n\n\nUsing prior (C)\n\n\n\n\n\n\n\nFigure 51.10: Posteriors (blue) after 100 subjects have been screen, with 30 \\(+\\) results.\n\n\n\nAs data accumulates, the priors become irrelevant; the knowledge about the risk of disease is being driven almost entirely by the data.\nRemarkably, even though 30% of the 100 tests were positive, all the posteriors place almost all the probability density on transmission risks less than 20%. This is because the likelihood functions correctly take into account the imperfections of the screening test.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Probability and evidence</span>"
    ]
  },
  {
    "objectID": "Manifestations/51-future-value.html",
    "href": "Manifestations/51-future-value.html",
    "title": "52  Present and future value",
    "section": "",
    "text": "52.1 Present value\nComparison is an important element of human decision making. Often, comparison amounts to translating each of the options at hand into a scalar score. (Recall that a scalar is just an ordinary quantity, e.g. 73 inches. As we get more deeply involved with vectors it will become more important to be perfectly clear when we are talking about a vector and when about a scalar. So we will be using “scalar” a lot.) Scalars are easy to compare; the skill is taught in elementary school. Scalar comparison is completely objective. Putting aside the possibility of error, everyone will agree on which of two numbers is bigger. Importantly, comparison of scalars is transitive, that is if \\(a &gt; b\\) and \\(b &gt; c\\) then it is impossible that \\(a \\leq c\\).\nThe comparison of scalars can be extended to situations where the options available form a continuum and the score for each option \\(x\\) is represented by a function, \\(f(x)\\). Chapter 24 and Chapter 50 showed that selecting the highest scoring option is a matter of finding the argmax.\nMany decisions involve options that have two or more attributes that are not directly comparable. Expenditure decisions have this flavor: Is it worth the money to buy a more reliable or prettier or more capable version of a product? The techniques of constrained optimization (Chapter 50) provide one useful approach for informing decisions when there are multiple objectives.\nElections are a form of collective decision making. The options—called, of course, “candidates”—have many attributes. Three such attitudes are attitudes toward social policy, toward fiscal policy, and toward foreign policy. Perceived honesty and trustworthiness as well as the ability to influence other decision makers and perceived ability to win the election are other attributes that are considered and balanced against one another. Ultimately the decision is made by condensing the diverse attributes into a single choice for each voter. The election is decided by how many votes the candidate garners. With this number attached to each of the candidates, it is easy to make the decision, \\[\\text{winner} \\equiv\\mathop{\\text{argmax}}_{i}\\left({\\large\\strut} \\text{votes}(\\text{candidate}_i)\\right)\\ .\\] There are many voting systems. For instance, sometimes the winner is required to have a majority of votes and, if this is not accomplished, the two highest vote-getting candidates have a run-off. Some jurisdictions have introduced “rank-choice” voting, where each voter can specify a first choice, a second choice, and so on. US federal elections involve a primary system where candidates compete in sub-groups before the groups complete against one another. It is tempting to think that there is a best voting system if only we were clever enough to find one and convince authorities to adopt it. But two mathematical theorems, Arrow’s impossibility theorem and the Gibbard-Satterthwaite theorem demonstrate that this is not the case for any system that involves three or more candidates. All such voting systems potentially create situations where a voter’s sincere interests are best served by an insincere, tactical vote choice. A corollary is that voters can be tricked into making a sincere choice that violates their interests.\nThis chapter is about a very common situation where there are multiple attributes that need to be condensed into a single score. The setting is simple: money. The question is how to condense a future income/expense stream—a function of time—into an equivalent value of money in hand at the present. This is called the present value problem. There is a solution to the problem that is widely accepted as valid, just as voting is a hallowed process of social optimization. But like voting, with its multiplicity of possible forms, there is a subtlety that renders the result somewhat arbitrary. This is not a situation where there is a single, mathematically correct answer but rather a mathematical framework for coming to sensible conclusions.\nPeople and institutions often have to make decisions about undertakings where the costs and benefits are spread out over time. For instance, a person acquiring an automobile or home is confronted with a large initial outlay. The outlay can be financed by agreeing to pay amounts in the future, often over a span of many years.\nAnother example: Students today are acutely aware of climate change and the importance of taking preventive or mitigating actions such as discouraging fossil fuel production, investing in renewable sources of energy and the infrastructure for using them effectively, and exploring active measures such as carbon sequestration. The costs and benefits of such actions are spread out over decades, with the costs coming sooner than the benefits. Policy makers are often and perhaps correctly criticized for overvaluing present-day costs and undervaluing benefits that accrue mainly to successive generations. There are many analogous situations on a smaller scale, such as setting Social Security taxes and benefits or the problem of underfunded pension systems and the liability for pension payments deferred to future taxpayers.\nThe conventional mechanism for condensing an extended time stream of benefits and costs is called discounting. Discounting is based on the logic of financing expenditures via borrowing at interest. For example, credit cards are a familiar mechanism for financing purchases by delaying the payment of money until the future. An expense that is too large to bear is “carried” on a credit card so that it can be paid off as funds become available in the future. This incurs costs due to interest on the credit-card balance. Typical credit-card interest rates are 18-30% per year.\nAs notation, consider a time stream of income \\({\\cal M}(t)\\), as with the apartment building example. We will call \\({\\cal M}(t)\\) the nominal income stream with the idea that the money in \\({\\cal M}(t)\\) is to be counted at face value. “Face value” is the literal amount of the money. In the case of cash, a $20 bill has a face value of $20 regardless of whether it becomes available today or in 50 years. The word “nominal” refers to the “name” on the bill, for instance $20.\nThe big conceptual leap is to understand that the present value of income in a future time is less than the same amount of income at the present time. In other words, we discount future money compared to present money. For example, if we decide that an amount of money that becomes available 10 years into the future is worth only half as much as that same amount of money if it were available today, we would be implying a discounting to 50%. In comparison, if that money were available 20 years in the future, it would make sense to discount it by more strongly to, say, 25%.\nTo represent the discounting to present value as it might vary with the future time horizon, we multiply the nominal income stream \\({\\cal M}(t)\\) by a discounting function \\({\\cal D} (t)\\). The function \\({\\cal D}(t)\\, {\\cal M}(t)\\) gives the income stream as a present value rather than a nominal value. It is sensible to insist that \\({\\cal D} (t=0) \\equiv 1\\) which is merely to say that the present value of money available today is the same as the nominal value.\nThe net present value (NPV) of a nominal income stream \\({\\cal M}(t)\\) is simply the sum of the stream discounted to the present value. Since we are imagining that the income stream is a function of continuous time, the sum amounts to an integral: \\[\\text{NPV} \\int_\\text{now}^\\text{forever} {\\cal M}(t)\\ {\\cal D}(t)\\, dt\\ .\\]",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Present and future value</span>"
    ]
  },
  {
    "objectID": "Manifestations/51-future-value.html#sec-discount-functions",
    "href": "Manifestations/51-future-value.html#sec-discount-functions",
    "title": "52  Present and future value",
    "section": "52.2 Discounting functions",
    "text": "52.2 Discounting functions\nWhat should be the shape of the discounting function?\nRecall that the purpose of the discounting function is to help us make comparisons between different income streams, that is, between the various options available to an entrepreneur. Each individual can in principle have his or her own, personal discounting function, much as each voter is entirely free to weight the different attributes of the candidates when deciding whom to vote for. As a silly example, a person might decide that money that comes in on a Tuesday is lucky and therefore worth more than Thursday money. We won’t consider such personalized forms further and instead emphasize discounting functions that reflect more standard principles of finance and economics.\nAs a thought experiment, consider the net present value of an income stream, that is \\[\\text{NPV}_\\text{original} = \\int_0^\\infty {\\cal M}(t)\\ {\\cal D}(t)\\, dt\\ .\\] Imagine now that it has been proposed to delay the income stream by \\(T=10\\) years. This new, delayed income stream is \\({\\cal M}(t-T)\\) and also has a net present value: \\[\\text{NPV}_\\text{delayed} = \\int_0^\\infty {\\cal M}(t - T)\\ {\\cal D}(t)\\, dt\\ .\\] There are at least two other ways to compute \\(\\text{NPV}_\\text{delayed}\\) that many people would find intuitively reasonable:\n\nSimply discount the original NPV to account for it becoming available \\(T\\) years in the future, that is, \\[\\text{NPV}_\\text{delayed} = {\\cal D}(T)\\ \\text{NPV}_\\text{original} = \\int_0^\\infty {\\cal M}(t)\\ {\\cal D}(T)\\ {\\cal D}(t)\\, dt\\ \\]\nApply to \\({\\cal M}(t)\\) a discount that takes into account the \\(T\\)-year delay. That is:\n\\[\\text{NPV}_\\text{delayed} = \\int_0^\\infty {\\cal M}(t)\\ {\\cal D}(t + T)\\, dt\\ .\\] For (1) and (2) to be the same, we need to restrict the form of \\({\\cal D}_r(t)\\) so that \\[{\\cal D}(T)\\ {\\cal D}(t) = {\\cal D} (t+T)\\ .\\] The form of function that satisfies this restriction is the exponential, that is \\({\\cal D}(t) \\equiv e^{kt}\\).",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Present and future value</span>"
    ]
  },
  {
    "objectID": "Manifestations/51-future-value.html#compound-interest",
    "href": "Manifestations/51-future-value.html#compound-interest",
    "title": "52  Present and future value",
    "section": "52.3 Compound interest",
    "text": "52.3 Compound interest\nA more down-to-earth derivation of the form of \\({\\cal D}(t)\\) is to look at how financial transactions take place in the everyday world: borrowing at interest. Suppose that a bank proposes to lend you money at an interest rate of \\(r\\) per year. To receive from the bank one dollar now entails that you pay the bank \\(1+r\\) dollars at the end of the year.\nFor this proposition to be attractive to you, the present value of \\(1+r\\) dollars to be paid in one year must be less than or equal to the present value of one dollar today. In other words, \\[{\\cal D}_\\text{you}(1)\\ (1+r) \\leq 1\\ .\\] From the bank’s perspective, the present value of your payment of \\((1+r)\\) dollars in a year’s time must be greater than one dollar today. That is\n\\[1 \\leq {\\cal D}_\\text{bank}(1)\\ (1+r) \\ .\\] It is perfectly reasonable for you and the bank to have different discounting functions, just as it is perfectly legitimate for you and another voter to have different opinions about the candidates. It is convenient, though to imagine that the two discounting functions are the same, which will be the case if \\[{\\cal D}(1) = \\frac{1}{1+r}\\ .\\]\nIf you were to borrow money for two years, the bank would presumably want to charge more for the loan. A typical practice is to charge compound interest. Compound interest corresponds to treating the loan as having two phases: first, borrow one dollar for a year and owe \\(1+r\\) dollars at the end of that year. At that point, you will borrow \\((1+r)\\) dollars at the interest rate \\(r\\). At the end of year two you will owe \\((1+r) (1+r) = (1+r)^2\\) dollars. In general, if you were to borrow one dollar for \\(t\\) years you would owe \\((1+r)^t\\) dollars. In order for the loan to be attractive to both you and the bank, the discounted value of \\((1+r)^t\\) should be one dollar: \\[{\\cal D}(t) = \\frac{1}{(1+r)^t} = (1 + r)^{-t}\\ .\\]\n\nApplication area 52.2 —Is the interest rate too high?\n\n\n\n\n\n\n\nApplication area 52.2 Building apartments (2)\n\n\n\nLet’s return to the entrepreneur considering the apartment-building project in Application area 52.1.\nThe entrepreneur goes to the bank with her business plan and financial forecasts. The bank proposes to lend money for the project at an interest rate of 7% per year.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 52.2: Blue curve: The apartment project income stream discounted at 7% per year. Black curve: The undiscounted income stream.\n\n\n\nFigure 52.2 shows the income stream discounted at 7% per year. The net present value of the income stream for the apartment project is the accumulation of the discounted income stream: \\[\\text{NPV}(7\\%) = \\int_0^{30} (1 + 0.07)^{-t} {\\cal M}(t) \\ dt \\ .\\] This works out to be negative $1,095,000. As things stand, the apartment building will be a money pit.\nThe entrepreneur responds to the bank’s offer with some business jargon. “I’ll have to go back to the drawing board, put the team’s heads together, and see how to get the numbers to work. I’ll circle back with you tomorrow.” This might involve reconsidering her model of the income stream. Perhaps she should have taken into account yearly rent increases. Maybe she can re-negotiate with the city about zoning height restrictions and build a 12-unit building, which will cost $4 million, lowering the cost per apartment?",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Present and future value</span>"
    ]
  },
  {
    "objectID": "Manifestations/51-future-value.html#mortgages",
    "href": "Manifestations/51-future-value.html#mortgages",
    "title": "52  Present and future value",
    "section": "52.4 Mortgages",
    "text": "52.4 Mortgages\nA mortgage is a form of loan where you pay back the borrowed amount at a steady rate, month by month. At the end of a specified period, called the term of the mortgage, your have completely discharged the debt.\nSuppose you decide to buy a product that costs \\(\\$\\cal P\\), say \\(\\cal P=\\) $10,000 for a used car. Let’s say you need the car for your new job, but you don’t have the money. So you borrow it.\nYour plan is to pay an amount \\(A\\) each month for 30 months—the next \\(2\\,\\small\\frac{1}{2}\\) years. What should that rate be?\nYou can put the purchase on your credit card at an interest rate of \\(r=2\\%\\) per month. This corresponds to \\(k = \\ln(1+r) = 0.0198\\) per month. The net present value of your payments over 30 months at a rate of \\(A\\) per month will be\n\\[\\int_0^{30} A {\\cal D}_r(t)\\, dt = \\int_0^{30} e^{-kt} A = -\\frac{A}{k} e^{-kt}\\left.{\\Large\\strut}\\right|_{t=0}^{30}\\] \\[=- A \\left(\\strut \\frac{e^{-30k}}{k} - \\frac{e^{-0k}}{k} \\right)\\\\ = - A\\left(\\strut27.88 - 50.50\\right) = 22.62 A\\]\nThe loan will be in balance if the net present value of the payments is the same as the amount \\({\\cal P}\\) you borrowed. For this 30-month loan, this amounts to \\[22.62 A = {\\cal P}\\ ,\\] from which you can calculate your monthly payment \\({\\cal A}\\). For the $10,000 car loan, your monthly payment will be $10,000/22.62 or $442.09.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Present and future value</span>"
    ]
  },
  {
    "objectID": "Manifestations/52-mechanics.html",
    "href": "Manifestations/52-mechanics.html",
    "title": "53  Mechanics",
    "section": "",
    "text": "53.1 Work\nIn Block 2, we introduced the ideas of instantaneous rate of change and infinitesimal intervals of time. These are mathematical concepts introduced in the 17th century for describing motion. (In the 16th century, Galileo’s measurements of motion involved averages over finite time intervals.) With this new mathematical tool, Newton understood that the velocity of an object is its instantaneous rate of change of position with respect to time, and to define acceleration as the instantaneous rate of change of velocity with respect to time. With these definition, Newton was able to connect motion to the palpable forces acting on objects. Thus was born the field of dynamics, the study of forces and their effects on motion.\nIn contrast, statics is about physical systems that do not change. A non-changing system is said to be in equilibrium or balance. Static systems are incredibly important in everyday life; a bridge that is not static is one that you do not want to cross! The equilibrium in a bridge is the balance between the downward force of gravity and the compressive and tensile forces in the materials that make up the bridge.\n“Mechanics” is a catch-all term for the combination of statics and dynamics studied in physics and used in engineering and design. The sense of the word is the study of machines, with the “-ic” signifying “practice of” in the sense of scientific, physics, mathematics, optics, chiropractic, and such. This starts with simple machines—simple devices that change the direction or strength of a force— such as the lever, wheel and axle, pulley, inclined plane, wedge, and screw. Mechanics goes on to deal with more complicated machine components such as a gas-filled cylinder and piston, flywheel, valve, turbine, etc. Many concepts originally developed for the theory of machines are familiar, and intuitive to the modern mind: force, pressure, momentum.\nThis chapter illustrates the central role of calculus concepts and methods in mechanics.\n“Work” is a familiar, everyday concept, but a nuanced one; one person’s work can be another person’s play. In mechanics, work has a much more specific meaning stemming from the study of simple machines. A lever, for instance, can be used to move an object that is otherwise too heavy to handle. It still takes toil and effort to move the object, but the effort is eased by the mechanics of the lever.\nOur intuitive sense of work is perhaps rooted in physiology: effort, fatigue, muscle pain. For instance, it takes work to pick up a heavy object, but it is also work to hold the object steady even without moving it. Generations of thinking about machines has brought us to a different notion of work that does not involve human subjectivity. In mechanics, holding an object steady, no matter how heavy, does not involve work. Although a human tasked to hold a heavy load will become exhausted, the same duty can be accomplished by placing the load on a table, completely eliminating the effort. In mechanics, work and motion go hand in hand; without motion there is no mechanical work.\nThe table holding the heavy load does no work. Work is done only when the load is moved, and the amount of work depends on how the load is moved. For instance, moving a block along level ground involves a lot of work, but pulling a cart filled with blocks can be almost effortless. In mechanics, work combines both the amount of motion and the force needed to accomplish the motion.\nConsider, for instance, the work involved in lifting a mass \\(m\\) to table height \\(h\\).\nThe lifting is accomplished by applying an upward force to counter the force of gravity. The gravitational force on the mass is \\(m g\\), where \\(g\\) is the instantaneous acceleration of an object released to fall freely (about 9.8 m/s2 near the Earth’s surface). The distance traveled is \\(h\\). So the work performed on the mass is \\(m g h\\).\nNotice that the mechanical work has nothing to do with the speed with which the mass is moved up to the table. Lift it fast or lift it slow, it amounts to the same mechanical work. (Of course, to human perception, lifting an object very slowly up to table height involves more effort than snapping it up quickly. But human effort is only peripherally related to mechanical work.)\nLet’s introduce a machine to the situation in the form of a ramp or a pulley. The purpose of the machine is to ease human labor by changing the strength or direction of forces. You can perhaps intuit that rolling the mass up the ramp will be an easier task than lifting it. How so?\nThe ramp can be seen as a sort of partial table. The ramp does most of what’s needed to hold the mass up. To keep the mass in place on the ramp the human worker need only supply a modest additional force parallel to the ramp surface. Calculating that modest additional force can be accomplished by a basic mathematical technique in mechanics: decomposing a vector.\nYou encountered vectors (in Section 24.003) in the context of the gradient vector of a function, say, \\(f(x,y)\\). At any given input \\((x,y)\\) the gradient vector, written \\(\\nabla f(x,y)\\), points in the steepest uphill direction of the function \\(f(x,y)\\). Recall that the gradient vector was written as a set of values; the partial derivative of \\(f()\\) with respect to each of its inputs in turn. That is, \\[\\nabla f(x,y) = \\left({\\large\\strut} \\partial_x f(x,y),\\ \\  \\partial_y f(x,y)\\right)\\ .\\] In this representation, the vector \\(\\nabla f(x, y)\\) is decomposed into two components: \\(\\partial_x f(x,y)\\) and \\(\\partial_y f(x,y)\\).\nTo decompose the vector of gravitational forces, we can place a coordinate grid over the gravity vector. In Figure 53.1 this grid has been arranged so that one cardinal direction is aligned with the ramp itself and the other is perpendicular—that is, “normal”—to the ramp. Merely by noting the coordinates of the gravitational vector in the coordinate grid, we decompose that vector into two components, one along the surface of the ramp and the other perpendicular to the ramp.\nWe return to the idea of vector decomposition in much more detail in Block 5 of this course; it has a major (though perhaps unexpected) role to play in fitting models to data. But for now, we will simply examine the right triangle in Figure 53.1. In that right triangle, the gravitational force vector \\(F_{gravity} = m g\\) is the hypotenuse. The component tangential to the ramp is \\(m \\sin(\\theta) g\\). The worker pushing the mass up the ramp need provide only tangential component of force which is smaller than the force imposed on the worker picking up the mass without a ramp. Thus human effort is reduced by the machine.\nWhat about the mechanical work? Is that also reduced? Remember that mechanical work is the product of force times distance. The force has been reduced to \\(m \\sin(\\theta) g\\), but the distance \\(D_{ramp}\\) along the ramp is much longer than the distance \\(h\\) from floor to table top.\nAgain, referring to the ramp itself as a right triangle, you can see that \\(D_{ramp}\\sin(\\theta) = h\\) or, \\(D_{ramp} = h / \\sin(\\theta)\\). The total mechanical work, the product of applied force times distance moved is \\[m \\sin(\\theta) g \\times D_{ramp} = m \\sin(\\theta) g \\times \\frac{h}{\\sin(\\theta)} = m g h\\ .\\] The ramp does nothing to reduce the mechanical work needed to lift the mass!\nWe usually think of ramps as an inclined plane. But, from Blocks 1 to 3 we have the tools to figure out the work for a (smooth) ramp with any shape at all. We will do this not because odd-shaped ramps are encountered frequently, but to provide an example in a relatively familiar setting of some techniques we will use elsewhere in this chapter.\nThe ramp we have in mind has a surface whose height \\(f(x)\\) is zero at the foot (\\(x=a\\)) and reaches \\(f(x=b) = h\\) where it joins the table.\nThe slope of the ramp at any location \\(x\\) is, as you know, \\(\\partial_x f(x)\\). It is helpful to convert this rise/run formulation of slope into the slope-angle form we used to study the simple ramp. As you can see from the diagram, which zooms in on one place on the ramp, rise over run amounts to \\(L\\sin(\\theta) / L\\cos(\\theta) = \\partial_x f(x) = \\tan(\\theta)\\), with the result: \\[\\theta = \\arctan({\\large\\strut}\\partial_x f(x))\\ .\\] Consequently, the force that needs to be applied parallel to the ramp’s surface is \\(m \\sin(\\arctan(\\partial_x f(x))) g = m \\sin(\\theta) g\\). To find the work done in pushing the mass an infinitesimal distance along the ramp we need to know the instantaneous length of the ramp. This is potentially confusing to the reader since we’ve already said that the distance is infinitesimal. As you know, infinitesimal is different from zero. We will write \\(dx\\) as an infinitesimal increment along the floor, but the zoomed-in length \\(dL\\) of the corresponding part of the ramp is the hypotenuse of a right triangle where one leg has length \\(dx\\) and the other leg has length \\(\\partial_x f(x) dx\\): slope times distance.\nThe hypotenuse of the infinitesimal segment of the ramp has length \\(dL = \\sqrt{\\strut dx + \\partial_x f(x) dx}\\), or \\(dL = \\sqrt{\\strut 1 + \\partial_x f(x)}\\ dx\\). Things are a bit simpler if we write \\(dL\\) using the slope angle \\(\\theta(x)\\). Since \\(dx = \\cos(\\theta(x)) dL\\), we know \\(dL = dx/\\cos(\\theta(x))\\). Consequently the infinitesimal of work is \\[dW \\ = \\ m g \\frac{\\sin(\\theta(x))}{\\cos(\\theta(x))}\\ dx\\  = \\ m g \\tan(\\theta(x)) dx \\ .\\]\nThe total work is the accumulation of \\(dW\\) over the extent of the ramp. In other words, \\[\\int_a^b m g \\tan(\\theta(x))\\ dx\\ = \\ \\int_a^b m g \\tan(\\arctan(\\partial_x f(x)))\\ dx = \\int_a^b m g \\partial_x f(x) dx\\ ,\\] where we’ve used the formula \\(\\theta(x) = \\arctan(\\partial_x f(x))\\). From the “fundamental theorem of calculus” we know that\n\\[\\int_a^b m g\\ \\partial_x f(x)\\ dx \\ = \\ \\left.m g \\ f(x){\\Large\\strut}\\right|_a^b = mg \\left[\\strut f(b) - f(a)\\right] = mg h\\ .\\]\nWhat’s remarkable is that pushing the mass up the \\(f(x)\\)-shaped ramp involves an amount of work, \\(m g h\\), that does not depend on \\(f(x)\\), only on \\(f(b) - f(a)\\), the net height comprised by the ramp.\nWe haven’t yet said what this notion of work is good for and we’ve given no detailed justification for the definition of mechanical work as force times distance. You could imagine a dictatorial authority deciding to measure work as the square-root of force times distance squared. But … that particular measure is not going to make sense if we think about the dimension of the quantity. Force has dimension [force] = M L T-2. Square root of force times length squared would have dimension [sqrt(force) \\(\\times\\) length-squared] = M1/2 L5/2 T-2. The non-integer exponents mean that this is not a legitimate physical quantity.\nThe dimension of force-times-length are straightforward: [force \\(\\times\\) length] = M L2 T-2, that is, energy. The particular definition of work as force times length will make sense in the context of a more comprehensive mechanical theory of energy. The significance of energy itself is that, as a fundamental proposition of physics, the various forms of energy are interchangeable but conserved; energy is neither created nor destroyed, just moved around from one form to another and one place to another.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Mechanics</span>"
    ]
  },
  {
    "objectID": "Manifestations/52-mechanics.html#work",
    "href": "Manifestations/52-mechanics.html#work",
    "title": "53  Mechanics",
    "section": "",
    "text": "Work is force times displacement.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 53.1: Decomposing the vector of gravitational force into two perpendicular components, one tangent to the ramp and the other perpendicular to it.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApplication area 53.1  \n\n\n\n\n\n\n\nApplication area 53.1 Lifting very high\n\n\n\nNear the surface of the Earth, gravitational acceleration is approximately constant regardless of latitude or longitude. But gravity varies with distance \\(r\\) from the Earth’s center. Newton’s law of universal gravitation gives the force on an object of mass \\(m\\) due to the Earth’s gravity as \\[F = \\frac{m M_e G}{r^2}\\] where \\(M_e = 5.972 \\times 10^{24}\\) kg is the mass of the Earth and \\(G = 6.674 \\times 10^{-11}\\) N m2 / kg2 is the universal gravitational constant. The Earth’s radius is roughly \\(6,370,000\\) m, so the force on a 1 kg object near the surface of the Earth is \\(F = 1 \\text{kg} (5.972 \\times 10^{24} \\text{kg}) (6.674 \\times 10^{-11})/ (6.37 \\times 10^6 \\text{m})^2\\) N m2 kg-2. Carrying out the arithmetic and consolidating the units gives \\[F = 9.823 N\\] for the 1 kg object.\nSuppose we want to lift the 1 kg object from the Earth’s surface to 10000 km away, that is, to a distance of 1,6370,000 m from the center of the Earth. For the purpose of the example, we will ignore the gravitational force exerted by the Sun, Moon, planets, and other galaxies, etc. The work performed in the lifting is\n\\[\\int_{6.47\\times 10^6}^{16.47\\times 10^6} \\frac{1 \\text{kg}\\ M_e\\ G}{r^2}\\ dr = -\\left.  {\\Large\\strut}\\frac{1 \\text{kg}\\ M_e\\ G}{r}\\right|_{6.47\\times 10^6}^{16.47\\times 10^6}\\] \\ \\ \\ \\[= -\\ 3.986 \\times 10^{14}\\left[\\strut \\frac{1}{16.47 \\times 10^6} - \\frac{1}{6.47 \\times 10^6}\\right] \\text{N m} \\\\\\ \\\\\\ \\\\= 37,405,840\\ \\text{J}.\\]\nA Newton-meter (N m) is also known as a Joule (J), a unit of energy. With 37,000,000 J, you could toast about 2000 pieces of bread. (A toaster uses about 300 W of power and takes about 60 seconds to process a slice of bread. \\(\\text{300 W} \\times \\text{60 s} = 18,000 \\text{J}\\).",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Mechanics</span>"
    ]
  },
  {
    "objectID": "Manifestations/52-mechanics.html#energy",
    "href": "Manifestations/52-mechanics.html#energy",
    "title": "53  Mechanics",
    "section": "53.2 Energy",
    "text": "53.2 Energy\nMechanical work, as discussed in the previous section, is a form of energy. When we lift a object, we put energy into the object. But we cannot say from examining the object how much work was done to place it on the table. The amount of work depends on how the object came to be on the table: lifted from the floor (positive work; force is positive upward, displacement is also positive upward) or perhaps lowered from a helicopter (negative work: force exerted by the cable is positive upward but the displacement is downward, therefore negative). We might call the work energy latent, the word meaning “unobservable,” “hidden,” “concealed,” “dormant.” To have an operational meaning, the work-energy that we assign to an object at rest must be with respect to some “ground state.” A convenient ground state here is to imagine the object resting on the ground. The assigned energy will then be the work that would have to be performed to raise the object to table height. Once at table height, the energy is again latent.\nHow then to measure the work energy that is latent in the object resting on the table? The idea is to return the object to its ground state, which we could do by lowering it—a negative displacement—to the ground, measuring the force needed to support the object (upward, so positive) and multiplying this by the displacement.\nAnother idea for measuring the latent energy is to let the object fall freely back toward its ground state and see what changes about the object. Perhaps you have already caught on to what will happen: the object’s speed increases steadily until the instant before it hits the ground.\n“Latent” is an apt but unusual word to express the energy imbued in the object resting on the table. We might equally say that the energy is “associated with position (at the height the table),” or we could call it “gravitational energy.” The term that is generally used is a near synonym of “latent.” We call the energy of the stationary object on the table potential energy. More precisely it can be called gravitational potential energy to distinguish it from the potential energy created by other forms of work, for instance pulling apart magnets or electric charges or compressing a gas into a cylinder.\nThere is also a form of energy associated with motion. We could call this “energy of motion,” but the conventional term is kinetic energy. (A dictionary definition of “kinetic” is “relating to or resulting from motion.” so we might as well say simply that kinetic energy is “energy relating to motion.)\nVelocity is a good way to observe motion. We can use dimensional analysis to anticipate how velocity and kinetic energy are related. Recall that energy has dimension M L2 T-2 and velocity has dimension L/T. Consequently, if an object’s kinetic energy at any instant stems from its mass and its velocity, then the energy must be mass times velocity squared, perhaps multiplied by a scalar, that is: \\[E_{kinetic} = \\alpha\\, m v^2\\ .\\]\nTo find the scalar \\(\\alpha\\), we can use calculus and accumulation. We know that the acceleration of a free-falling object due to gravity is \\(-g\\) (where the negative sign reflects the downward direction). Starting from rest (that is zero velocity so zero kinetic energy) the newly released mass will have a velocity that is the accumulated acceleration over time. In other words: \\[v(t) = \\int_0^t - g\\ dt = -\\left.g \\ t{\\large\\strut}\\right|_0^t = -g\\ t\\ .\\] Correspondingly, the position at time \\(t\\) will be the accumulated velocity: \\[x(t) = x(t=0) + \\int_0^t v(t) dt \\\\ =\nh  + \\int_0^t -g\\ t\\ dt\\] \\[= h - \\frac{1}{2} \\left.g\\ t^2{\\Large\\strut}\\right|_0^t \\ \\ =\\ \\  h - \\frac{1}{2} g\\ t^2 \\ .\\] The mass reaches the ground at time \\(t_g\\) such that \\(h - \\frac{1}{2} g\\ t_g^2 = 0\\). Solving this for \\(t_g\\) gives \\(t_g = \\sqrt{\\strut 2 h/g}\\).\nNow that we know the time when the object reaches its ground state, we can calculate the velocity at that instant: \\[v(t_g) = -g\\ t_g = - g\\ \\sqrt{\\strut 2 h / g} = - \\sqrt{\\strut 2 g h}\\] As the object reaches its ground state, its gravitation potential energy is zero (because it is at the ground state) and, since total energy is conserved, the kinetic energy will be the same size as the potential energy at \\(t=0\\) when the object was released from the table, that is\n\\[E_{kinetic}(t_g) = \\alpha\\ m\\ v(t_g)^2 =\n= \\alpha\\ m \\left(\\sqrt{\\strut 2 g h\\ }\\ \\right)^2 = \\]\n\\[2 \\alpha\\, m g\\,h\\  = m\\, g\\, h = E_{potential}(t=0)\\]\nSolving \\(2 \\alpha\\ m\\,g\\,h = m\\,g\\,h\\) gives \\(\\alpha = \\frac{1}{2}\\). Thus, the kinetic energy as a function of mass \\(m\\) and velocity \\(v\\) is \\(\\frac{1}{2} m\\, v^2\\).\n\n\n\n\n\n\n\n\nTry it! 53.1\n\n\n\n\n\n\n\n\n\nTry it! 53.1 … getting there\n\n\n\nIn the previous section, we calculated the potential energy of a 1 kg object at an altitude of 10,000 km above the Earth’s surface: 37,405,840 J. How fast would the 1 kg object need to be moving to have this much kinetic energy?\n\\[\\frac{1}{2} (1 \\text{kg}) v^2 = 37,\\!405,\\!840 \\text{J} = 37,\\!405,\\!840 \\ \\text{kg}\\ \\text{m}^2\\ \\text{t}^{-2}\\]\nSolving for \\(v\\) we get \\(v^2 = 2 \\times 37,\\!405,\\!840 \\text{kg}\\ \\text{m}^2\\ \\text{t}^{-2}\\ \\text{kg}^{-1}\\) or \\[v = 8649.4\\ \\text{m}/\\text{s}\\ ,\\] about eight-and-a-half kilometers per second.\n\n\n\nApplication area 53.2 —Mechanical work always involves movement, even when it doesn’t seem like it.\n\n\n\n\n\n\n\nApplication area 53.2 Work without movement?\n\n\n\nFigure 53.2 shows a simple exercise: holding a dumbbell out horizontally.\n\n\n\n\n\n\nFigure 53.2: Is he working even when the barbells are held still? Of course! Photo source\n\n\n\nAs anyone who does this exercise can tell you, even when there is no movement of the dumbbell, there is a strong sense of work being done. Your muscle fatigues and, for most people, the dumbbells can be held in place for only a short time.\nWe’ve said that mechanical work always involves motion; no motion, no work. So how come the exercise feels like work even though the hands do not move?\nTo perform the exercise, you contract the muscles of the shoulder and upper arm. There is no skeletal joint that can be locked in place (unlike, say, the knee). It is only the muscle force that holds the arms in place.\nOn the size scale that we normally perceive, it can appear that nothing is moving during the exercise. But zoom in to the molecular scale to see the action by which force is generated by muscle. The functional unit of muscle force involves two proteins, actin and myosin, that interact in a complicated way. The animation (from the online textbook by Michael D. Mann, The Nervous System in Action, chapter 14) shows the situation. The “head” of a myosin unit (red) acts like an oar. It attaches to a site on the actin molecule (orange) causing the head to contract and pull on the actin. Once contracted, a molecule of ATP (green sphere) binds to the myosin, releasing the head and preparing it for another stroke. ATP is an organic molecule that serves as a primary energy carrier and is found in all known forms of life. Transformation of ATP to ADP releases the energy. The ADP is then cycled, though other metabolic processes, back into ATP. This happens rapidly. Humans recycle approximately their own body weight in ATP each day.\n\n\n\n\n\n\nFigure 53.3: Animation of the generation of force by the interaction of actin and myosin, from The Nervous System in Action.\n\n\n\nWhen muscle is under tension, the actin can slip back in between strokes of the myosin head. Thus, a constant-length muscle in tension on a macroscopic scale is steadily consuming energy, in much the same way as an oarsman on an anchored boat can do work via the movement of oars against the water even when the boat itself is not moving.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Mechanics</span>"
    ]
  },
  {
    "objectID": "Manifestations/52-mechanics.html#momentum",
    "href": "Manifestations/52-mechanics.html#momentum",
    "title": "53  Mechanics",
    "section": "53.3 Momentum",
    "text": "53.3 Momentum\nIn the previous sections we looked at force \\(\\times\\) distance. Dimensional analysis showed that [force \\(\\times\\) distance] = energy and, in the setting of lifting an object and letting it fall back toward its ground state, we traced out the conversion of the energy of position (“potential energy”) into the energy of velocity (“kinetic energy”).\nNow consider a somewhat different quantity: force \\(\\times\\) time. Dimensional analysis gives \\[\\underbrace{M^1 L^1 \\ T^{-2}}_\\text{[force]}\\  \\times\\ \\underbrace{T}_\\text{[time]} = \\underbrace{M^1}_\\text{[mass]} \\underbrace{L^1 T^{-1}}_\\text{[velocity]} \\] The product of force times time is dimensionally equivalent to the product of mass times velocity. The quantity is called momentum. Newton’s second law of motion, often written using acceleration, \\(F = m a\\), is more fundamentally written in terms of momentum: \\(F = \\partial_t\\, m\\, v\\). The conservation of momentum refers to the situation when outside forces on a system are nil. In such case, momentum of the system does not change with time; momentum is constant or “conserved.”\nAn example of such a system is a deep-space probe, sufficiently far from other matter that gravitational force is negligible. to speed up or slow down (or turn), the probe is made to throw out fast moving molecules of burnt fuel. These particles have “new” momentum, but since momentum of the whole system is conserved, the body of the probe gains “new” momentum in the opposite direction. This is the operating principle of the rocket engine.\n\n\n\n\n\n\n\n\nTurbojet\n\n\n\n\n\n\n\n\n\nTurbofan\n\n\n\n\n\n\nFigure 53.4: A turbojet engine uses air for combustion and emits a relatively low amount of mass at high velocity. A turbofan engine uses a fan blade (1) to convert some of the combustion energy into a large mass of relatively slow velocity, unburnt air.\n\n\n\nAircraft jet engines work in a similar matter, burning fuel to create energy. Whereas the force generated by a rocket engine is entirely produced by the newly created momentum of the burnt fuel, aircraft engines have an additional material to work with: air. The earliest jet engines, turbojet engines, were small in diameter, bringing in air mainly as a fuel for combustion. (Figure 53.4 (left)1 Today’s more efficient engines are large diameter: turbofan engines. (Figure 53.4 (right)2) In addition to using air for combustion, they use large fan blades to convert the energy of combustion into a large mass of relatively slowly moving, uncombusted air. This moving air carries momentum; more than that contained in the fast moving particles generated directly through combustion.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Mechanics</span>"
    ]
  },
  {
    "objectID": "Manifestations/52-mechanics.html#sec-center-of-mass",
    "href": "Manifestations/52-mechanics.html#sec-center-of-mass",
    "title": "53  Mechanics",
    "section": "53.4 Center of mass",
    "text": "53.4 Center of mass\nIn considering a physical object of extended shape, it can be a great simplification to be able to treat the whole extended object as if it were a simple point object at a single location. For instance, Figure 53.5 imagines a space probe (orange dot) coasting through the edge of a galaxy.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 53.5: An imagined space probe (orange dot) on the outer edges of a galaxy.\n\n\n\nWhat is the gravitational attraction of the galaxy on the probe? One way to find this is by adding up the individual gravitational attractions of the individual stars. Another is to find the center of mass of the galaxy and calculate the force as if all the mass were at that point. The two calculations give the same answer.\nFor the galaxy, the center of mass is located at a point \\((\\bar{x},\\bar{y})\\) where \\[\\bar{x} \\equiv \\sum_\\text{galaxy} m_i x_i\\ \\ \\ \\text{and}\\ \\ \\ \\ \\bar{y} \\equiv \\sum_\\text{galaxy} m_i y_i\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 53.6: An irregular shape used in the example. The \\((x,y)\\) coordinates of closely spaced points on the boundary are available as Blob1 in the sandbox software.\n\n\n\nFor a continuous shape, such as in Figure 53.6 (left) we can describe the center-of-mass calculation as an accumulation of the mass-density function \\(\\rho(x, y)\\) over the entire shape \\(S\\). The mass of the object is the accumulation of mass-density itself \\[M = \\int_\\text{S} \\rho(x,y)\\ d\\text{S}\\] while the components of the center of mass are the accumulation of \\(x\\ \\rho(x,y)\\) and \\(y\\ \\rho(x,y)\\), that is: \\[\n\\bar{x} = \\int_\\text{S} x\\ \\rho(x,y)\\ d\\text{S} / M\\\\\n\\bar{y} = \\int_\\text{S} y\\ \\rho(x,y)\\ d\\text{S} / M\n\\] where \\(S\\) refers to the whole object and \\(d\\)S is a differential of the object, that is, a tiny piece of the object.\nThere are many ways to split an object up into differentials so that they can be accumulated to give the whole integral. One simple way, shown in Figure 53.6 (right), is to divide the object into a set of discrete, non-overlapping, adjacent rectangles (or cubes for a three-dimensional object). Then, as with adding up the stars, just add up \\(x \\rho(x, y) d\\)S or \\(y \\rho(x,y) d\\)S contained in each of the rectangular \\(d\\)A regions. For the rectangle located at $(x_i, y_i), the mass \\(m_i\\) will be \\(m_i = \\rho(x_i, y_i) d\\)S: density times area of each rectangle. This turns the integrals in Eq. @ref(eq:cm-integral) into a sum:\n\\[\\bar{x} \\approx \\sum_\\text{rectangles} m_i x_i/ M\\ \\ \\ \\text{and}\\ \\ \\ \\ \\bar{y} \\approx \\sum_\\text{rectangles} m_i y_i / M\\] where \\[M = \\sum_\\text{rectangles} m_i\\ .\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 53.7: A continuous shape can be approximated by a set of rectangles within the borders of the shape. Integrating over the shape is a matter of adding up across all of the rectangles the relevant quantity for each rectangle.\n\n\n\nFor the center of mass calculation, the relevant quantity for \\(\\bar{x}\\) for each rectangle is the mass times the \\(x\\)-position. Similarly, for \\(\\bar{y}\\) the relevant quanty is the mass times the \\(y\\)-position.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(y\\) component\n\n\n\n\n\n\n\n\\(x\\) component\n\n\n\n\n\n\n\nFigure 53.8: For the \\(y\\)-component of the center of mass (left panel), the \\(x\\)-coordinate of each rectangle is irrelevant. It is as if all the rectangles were moved to \\(x=0\\). Similarly for the \\(x\\)-component of the center of mass (right panel).\n\n\n\n\n\n\n\n\n\n\n\n\nTry it! 53.2\n\n\n\n\n\n\n\n\n\nTry it! 53.2 Center of mass\n\n\n\nCompute the center of mass of the object Blob1 shown in Figure 53.6, assuming the mass-density \\(\\rho(x,y) = 10\\).\nThe mass of the object is \\[M = \\int_\\text{Blob1} \\rho(x, y)\\, dA\\] The \\(x\\)-component of the center of mass is\n\\[\\bar{x} = \\int_\\text{Blob1} x \\rho(x, y)\\, dA / M\\] and similarly for \\(\\bar{y}\\).\nTo find the center of mass, we first need to know the total mass of the object. We will carry out the calculation by dividing the object into a series of rectangles, computing the mass of each rectangle, then adding together the masses. The R/mosaic function box_set() takes as input the density function, a data frame with points on the boundary of the object, and a size for the boxes, which we will set to \\(dx=0.1\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nEach row of Boxes is one box. The x and y columns give the location of the center of that box, dx and dy are the lengths of the box sides in the \\(x\\) and \\(y\\) directions. The value ofthe function being accumulated is in the column labelled .output. column dA gives the area of each box (which is simply \\(dA = dx\\, dy\\)).\nAs the notation \\[\\int_\\text{Blob1} \\rho(x, y) dx dy\\] suggests, to accumulate the results for the individual boxes we just multiply the .output. by dA and sum.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nComputing the \\(x\\)-component of the center of mass, \\(\\bar{x}\\), is much the same but now the function being integrated is \\(x \\rho(x,y)\\) instead of just \\(\\rho(x,y)\\):\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe \\(y\\) component of the center of mass, \\(\\bar{y}\\) is computed almost identically, but substituting 10*y ~ x & y as the function to be integrated. In the next line, we will tell box_set() to do the summation over all the boxes directly, instead of our having to do it with the with(..., sum(.output. * dA)) command.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nTip\n\n\n\nRecall that the summation over the boxes provides an approximation to the integral. The quality of the approximation depends on the boxes being small enough. It is responsible to check the result by using smaller box size. (This involves more calculation, so be patient.)\n\n\n\nActive R chunk 53.1\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nFrom this, we conclude that a box size dx = 0.01 gives 4 digits precision, but dx = 0.1 was not small enough.\nRepeat the calculation for \\(\\bar{x}\\) to get the same precision:\n\n\n\nActive R chunk 53.2\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nThere is more than one way to describe the perimeter of an object, and the manner of integration has to be selected to match the description.\n\n\nConsider this shape that might be the design of a panel in a large sculpture. The panel will be cut out of 3mm thick sheet aluminum \\(x\\) and \\(y\\) are given in meters. In order for the panel to be balanced, it will be mounted at its center of mass\n\n\n\n\n## Warning: All aesthetics have length 1, but the data has 101 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n\n\n\n\n\n\n\n\n\nFigure 53.9: A panel to locate center of mass\n\n\n\nThe shape is defined by two functions, \\(f(x)\\) and \\(g(x)\\), one of which sets the top edge and the other the bottom edge. The side edges are defined by the leftmost and rightmost values of \\(x\\). Here, that is \\(x_\\text{left} = - 1.5\\) and \\(x_\\text{right} = 4.0\\).\nThe area of the object can be found using the integration techniques from Block 3. For the purpose of finding the center of mass, we need to calculate the mass of the object. For 3mm sheet aluminum the density is about 8.1 kg/m2. Here, that is simply \\[\\text{mass} = \\int_{-1.5}^{4.0} 8.1 \\left[\\strut f(x) - g(x)\\right] dx \\approx 880 \\text{kg}\\ .\\]\nWhat about the center of mass? Because the shape is not described as a set of boxes, as we did earlier in this section, we need a way to perform the accumulation that uses only the information in the functions.\nThe differential \\(8.1 \\left[\\strut f(x) - g(x)\\right] dx\\) gives the mass of each vertical slice of the object, several of which are shown in magenta in Figure 53.10. The tops and bottoms of those slices don’t align exactly with the boundaries of the object. That is because we’ve drawn them at a finite width so that you can see them. But the actual differentials being accumulated will have negligible width, and so will fit exactly.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 53.10: The panel can be divided into vertical slices, a few of which are shown here. The vertical mid-point of each slice is marked with a blue dash. Accumulating the slices’ mid-point coordinates times the slices’ mass, and dividing by the panel’s mass, gives the \\(y\\)-component of the center of mass.\n\n\n\nThe horizontal positions of the vertical slices are given by \\(x\\). The \\(x\\)-component of the center of mass of each individual vertical slice will be \\[x \\left[\\strut \\text{top}(x) - \\text{bottom}(x)\\right] dx\\ .\\] The center of mass of the entire object will be the accumulation \\[\\bar{x}= \\frac{1}{\\text{mass}}\\int_{-1.5}^{4.0} x\\ \\text{density} \\left[\\strut \\text{top}(x) - \\text{bottom}(x)\\right] dx \\approx 1.72 \\text{m}\\ .\\]\nFinding the \\(y\\)-component of the center of mass could be done in a similar way, by constructing horizontal slices. However, we would have to calculate the functions \\(\\text{left}(y)\\) and \\(\\text{right}(y)\\) that bound each slice.\nAn easier way is to find the vertical center of each slice. For a slice at position \\(x\\), the vertical center is \\[y_{\\text{mid}}(x) \\equiv\\left[\\strut \\text{top}(x) + \\text{bottom}(x)\\right]/ 2\\ ,\\] the average of the top and bottom positions. (These are marked in blue in Figure 53.10.) The \\(y\\)-component of the center of mass is \\[\\begin{eqnarray}\n\\bar{y} &=& \\frac{1}{\\text{mass}} \\int_{-1.5}^{4.0} \\text{density}\\ y_\\text{mid}(x)\\ \\left[\\strut \\text{top}(x) - \\text{bottom}(x)\\right] dx = \\\\\n&=&\\frac{1}{\\text{mass}} \\int_{-1.5}^{4.0} \\text{density}\\ \\frac{\\left[\\strut \\text{top}(x) + \\text{bottom}(x) \\right]}{2}\\ \\left[\\strut \\text{top}(x) - \\text{bottom}(x)\\right] dx =\\\\\n&=&\\frac{1}{\\text{mass}} \\int_{-1.5}^{4.0} \\text{density}\\ \\frac{\\left[\\strut \\text{top}(x)^2 - \\text{bottom}(x)^2 \\right]}{2}\\ \\approx -5.43 \\text{m}\\ .\n\\end{eqnarray}\\]\nThe center of mass, \\((x=1.72, y=-5.43)\\), is plotted as \\(\\color{blue}{\\Large\\mathbf{\\text{+}}}\\) on the object.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Mechanics</span>"
    ]
  },
  {
    "objectID": "Manifestations/52-mechanics.html#angular-momentum-and-torque",
    "href": "Manifestations/52-mechanics.html#angular-momentum-and-torque",
    "title": "53  Mechanics",
    "section": "53.5 Angular momentum and torque",
    "text": "53.5 Angular momentum and torque\nThe relationship between force and momentum is familiar: \\[F = \\partial_t\\, m\\, v =\\ \\underbrace{m \\ \\partial_t\\  v}_\\text{if mass is constant}\\ .\\] Of course, the derivative of velocity with respect to time is also called “acceleration.”\nConsider the following situation. A space probe is being acted on by a constant force, as in Figure 53.11. The mass of the probe is \\(m\\), the thrust from the rocket engine provides the force \\(F\\). Starting from velocity \\(\\partial_t y(t=0)\\) and position \\(y(t=0) = 0\\), the thrust produces an acceleration \\(\\partial_{tt} y(t) = F/m\\). Integrating the acceleration gives the velocity as a function of time \\[\\partial_t y(t) = \\frac{F}{m} t + C\\ .\\]\n\n\n\n\n\n\nFigure 53.11: A space probe accelerating along a linear course. The position at time \\(t\\) can be written as \\(y(t)\\) or as \\(\\theta(t)\\).\n\n\n\nThe function \\(y(t)\\) is not the only way to represent where the probe is as a function of \\(t\\). Suppose that the probe is being observed by a telescope which measures the angle \\(\\theta(t)\\) with respect to the equatorial plane. If the distance to the probe is \\(D(t)\\), then the pair \\(\\left(\\strut\\theta(t), D(t)\\right)\\) gives the position of the probe. As \\(y(t)\\) increases, so do \\(\\theta(t)\\) and \\(D(t)\\).\nWe know the laws of motion in terms of \\(y(t)\\). Can we translate these laws to an expression in terms of \\(\\theta(t)\\) and \\(D(t)\\)? That is, can we find the function \\[\\partial_{t} \\theta(t) = {\\Large ?}\\] If we can find the laws of motion in the language of \\(\\theta(t)\\) and \\(D(t)\\), we will have a way to describe the motion of spinning bodies.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 53.12: A space probe (orange) accelerating in a straight line, but observed as a change in angle \\(\\theta\\) seen from a planet (green).\n\n\n\nThe derivation of \\(\\partial_{t} \\theta(t)\\) will not be obvious, but you will be able to see how calculus operations come into play.\nThree points in the diagram describe a right triangle: the probe’s position at \\(t=0\\), the center of the planet, and the probe’s position at time \\(t\\). The length of the horizontal leg of the triangle is \\(D(t=0)\\) which not a function of time, so we will drop the unnecessary parentheses and write it as \\(D_0\\). The vertical leg has length \\(y(t)\\), and the hypotenuse has length \\(D(t)\\). The Pythagorean theorem tells us that \\[D(t)^2 = y(t)^2 + D_0^2\\ .\\]\nStep 1: Differentiate both sides with respect to \\(t\\): \\[\\partial_t \\left[\\strut D(t)^2\\right] = \\partial_t \\left[\\strut y(t)^2\\right] \\ . \\] Using the chain rule and the fact that \\(D_0\\) does not depend on \\(t\\), gives \\[2\\, D(t)\\, \\partial_t D(t) = 2\\, y(t)\\ \\partial_t y(t)\\ \\ \\implies\\ \\ \\partial_t y(t) = \\frac{D(t)}{y(t)}\\,\\partial_t D(t)\\ .\\]\nStep 2: Trigonometry allows us to see a relationship among the functions \\(y(t)\\), \\(\\theta(t)\\), and \\(D(t)\\):\n\\[y(t) = D(t) \\sin\\left(\\strut\\theta(t)\\right) \\ .\\] Plugging this form of \\(y(t)\\) into the equation for \\(\\partial_t y(t)\\) in Step 1 produces \\[\\partial_t y(t) = \\frac{1}{\\sin(\\theta(t))} \\partial_t D(t)\\] and \\[D_0 = D(t) \\cos\\left(\\strut\\theta(t)\\right)\\ \\ \\implies\\ \\ D(t) = \\frac{D_0}{\\cos\\left(\\strut\\theta(t)\\right)}\\]\nStep 3: Another fact from trigonometry is that \\[D(t) = D_0/\\cos(\\theta(t))\\ .\\] Differentiating both sides (chain rule again!) with respect to \\(t\\) gives another form for \\(\\partial_t D(t)\\): \\[\\partial_t D(t) = D_0\\, \\partial_t \\left(\\frac{1}{\\cos\\left(\\strut\\theta(t)\\right)}\\right) = D_0 \\frac{\\sin\\left(\\strut\\theta(t)\\right)}{\\cos\\left(\\strut\\theta(t)\\right)^2}\\, \\partial_t \\theta(t)\\]\nStep 4: Combining the results from Steps 1 and 3 gives \\[\\partial_t y(t) =  \\frac{1}{\\sin\\left(\\strut\\theta(t)\\right)}D_0 \\frac{\\sin\\left(\\strut\\theta(t)\\right)}{\\cos\\left(\\strut\\theta(t)\\right)^2}\\, \\partial_t \\theta(t)\\ .\\] Cancelling out the \\(\\sin(\\theta(t))\\) terms and remembering that \\(\\partial_t y(t) = \\frac{F}{m} t\\) gives\n\\[\\frac{F}{m} t = \\frac{D_0}{\\cos\\left(\\strut \\theta(t)\\right)^2}\\, \\partial_t\\theta(t) \\] Multiplying both sides by \\(D_0\\) and recalling that \\(D(t) = D_0/\\cos(\\theta(t))\\) we arrive at \\[\\frac{F D_0}{m} t = \\frac{D_0^2}{\\cos\\left(\\strut \\theta(t)\\right)^2}\\, \\partial_t\\theta(t) = D(t)^2 \\partial_t \\theta(t)\\ . \\]\nAgain re-arranging to find \\(\\partial_t\\, \\theta(t)\\): \\[\\partial_t\\,\\theta(t) = \\frac{F\\, D_0}{m D(t)^2}\\ t\\ .\\]\nTo summarize, we have two equivalent expressions for the dynamics of the space probe:\n\\[\\partial_t y(t) = \\frac{\\overbrace{\\ F}^\\text{force}}{\\underbrace{M}_\\text{mass}} t \\ \\ \\text{and}\\ \\ \\partial_t \\theta(t) = \\frac{\\overbrace{F\\, D_0}^\\text{torque}}{\\underbrace{m D(t)^2}_\\text{moment of inertia}}\\ t\\ .\\] In rectangular \\((x,y)\\) coordinates, the velocity is the accumulation of force divided by mass: the usual statement of Newton’s second law of motion. In the angular coordinates \\((\\theta, D)\\) the angular velocity is the accumulation of torque divided by ***moment of inertia.\nThe angular coordinate representation is helpful when studying the rotation of objects. To illustrate, imagine a different configuration for the system than that in in Figure 53.11 where the space probe was free to accelerate in a straight line. Instead, suppose the rocket is mounted on a carousel, that is a wheel whose axle goes through the center as in Figure 53.13.\n\n\n\n\n\n\nFigure 53.13: The space probe mounted on a rigid wheel that can spin around its center.\n\n\n\nIn the rocket-on-wheel configuration, \\(D(t)\\) is a constant; the rocket stays the same distance from the center. Therefore, the moment of inertia is \\(m D_0^2\\). Following the previous formula, \\[\\partial_t \\theta(t) = \\frac{F D_0}{m D_0^2} t\\] which is easily differentiated to give the angular acceleration \\[\\partial_{tt} \\theta(t) = \\frac{F D_0}{m D_0^2}\\ .\\]\nIn a typical wheel, there is mass density throughout the wheel, not just at distance \\(D_0\\). To find the moment of inertia of such a distributed mass system, break the wheel down into small pieces and find the moment of inertia due to each bit. Then accumulate the pieces’ moments of inertia to find the total moment of inertia:\n\\[\\text{moment of inertia} = \\int_\\text{wheel} \\rho(x,y) \\left(\\strut x^2 + y^2\\right) d \\text{wheel}\\ .\\]\n\n\n\n\n\n\n\n\nTry it! 53.3\n\n\n\n\n\n\n\n\n\nTry it! 53.3 Calculating the moment of inertia\n\n\n\nCompute the moment of inertia of Blob1.\nIt is not enough to say, “compute the moment of inertia.” We also have to specify what is the reference location—the wheel axle in the configuration. We will first do the calculation around the center of mass \\((\\bar{x}, \\bar{y})\\) which we computed earlier as xbar and ybar. (Make sure you have run the code in Interactive R chunks 53.2 and 53.1 before running this code.)\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Mechanics</span>"
    ]
  },
  {
    "objectID": "Manifestations/52-mechanics.html#footnotes",
    "href": "Manifestations/52-mechanics.html#footnotes",
    "title": "53  Mechanics",
    "section": "",
    "text": "Source: Jeff Dahl, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=3235265↩︎\nSource: https://commons.wikimedia.org/wiki/File:Geared_Turbofan_NT.PNG↩︎",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Mechanics</span>"
    ]
  },
  {
    "objectID": "Manifestations/53-diffusion.html",
    "href": "Manifestations/53-diffusion.html",
    "title": "54  Diffusion",
    "section": "",
    "text": "54.1 Origins of the gaussian function\nFrom the start of MOSAIC Calculus we have used a small set of basic modeling functions which will by now be familiar to you:\nThis chapter gives a more detailed introduction to the gaussian function and provides a specific algebraic formula that composes an exponential with a low-order polynomial.\nStart with the low-order polynomial: \\[h(x) = - \\frac{(x-\\mu)^2}{2 \\sigma^2}\\] This is, of course a parabola with an argmax \\(x^\\star = \\mu\\) and a maximum value 0. It is written this way by convention, the point of which is to give names to two features of the function:\n(Recall that the Greek letters \\(\\mu\\) and \\(\\sigma\\) are pronounced “mu” and “sigma” respectively.)\nA reasonable person can point out that the domain of the low-order polynomial is \\(-\\infty &lt; x &lt; \\infty\\). It is sensible to define “width” to refer to that part of the domain where the function’s value is non-zero and, better, where “most” of the function is. One way to define “how much” of the function there is uses the area under the curve. The convention that you are about to see defines the width as the domain that contains the central 2/3 of the overall area.\nThe sandbox defines the function \\(h(x)\\) and graphs it, with particular emphasis on the range (vertical scale!) from -1 to 0. The graph is annotated with blue and red horizontal lines with y-intercept of 0 and \\(-\\frac{1}{2}\\) respectively.\nsigma &lt;- 1\nmu &lt;- 0\nh &lt;- makeFun(  -(x-mu)^2/(2*sigma^2) ~ x, mu=mu, sigma=sigma)\nslice_plot(h(x) ~ x, bounds(x=sigma*c(-1.5, 1.5))) %&gt;%\n  gf_hline(yintercept = c(0,-0.5), color=c(\"dodgerblue\", \"orange3\"))\nThe width of the parabola is based on the length of the horizontal line segment between the two branches of the parabola. Specifically, the width is defined as half the length of this line segment. to avoid confusion, we will use “width” in the usual English sense and a special term, standard deviation, to refer to half the length of the line segment. (“Standard deviation” is a good candidate for the worst name ever for a simple concept: a length. Another, equivalent term you will hear for this length is “root mean square,” which is almost as bad. Still, those are the standard terms and you should be careful to use instead of non-standard alternatives.)\n**Part A**  In the above sandbox, set `sigma &lt;- 2`. What *standard deviation* corresponds to $\\sigma = 2$? \n1/4 1 \\(\\sqrt{2}\\) 2 4\n**Part B**  Still holding $\\sigma = 2$, what is the *variance* of the function? \n1/4 1 \\(\\sqrt{2}\\) 2 4\n**Part C**  Set $\\sigma = 3$, and read off the graph what is the *standard deviation* of the function? \n1/3 1 \\(\\sqrt{3}\\) 3 9\n**Part D**  When $\\sigma = 3$, what is the *variance* parameter? \n1/3 1 \\(\\sqrt{3}\\) 3 9\n**Part E**  Pick a $\\sigma$ of your choice, try a few non-zero values for $\\mu$. Read off from the graph the standard deviation. How does the standard deviation depend on $\\mu$? \n\nThe standard deviation is \\(\\mu + \\sigma\\).\nThe standard deviation is \\(\\sqrt{\\strut\\mu + \\sigma^2}\\).\nThe standard deviation is \\(\\ln(\\mu + e^\\sigma)\\).\nThe standard deviation is not affected by \\(\\mu\\).\n**Part F**  What is the relationship between the *variance* $\\sigma^2$ and the *standard deviation* $\\sigma$?  \n\nThe standard deviation is the square root of the variance.\nThe standard deviation is 1 over the variance.\nThey are completely unconnected concepts.\nThe variance is the square of the mean, \\(\\mu\\).\nOne of the features that make bump functions useful is that they are local, the function is practically zero except on a domain of limited width. The parabola \\(h(x) \\equiv - \\frac{(x - \\mu)^2}{2 \\sigma^2}\\) is non-zero everywhere except at \\(x = \\mu\\), so not at all local.\nTo produce our pattern-book bump function, we compose an exponential function \\(e^x\\) with the polynomial \\(h(x)\\) to get \\[f(x) \\equiv e^{h(x)} = \\exp\\left(-\\frac{(x - \\mu)^2}{2 \\sigma^2}\\right)\\] This is not yet the “official” gaussian function, but are getting close!\nThe sandbox defines \\(f(x)\\) and graphs it. As with the previous sandbox, the graph is annotated with a blue horizontal line that touches the curve at the argmax and a red horizontal line with a y-intercept at \\(e^{-1/2}\\).\nsigma &lt;- 1\nmu &lt;- 0\nf &lt;- makeFun(  exp( -(x-mu)^2/(2*sigma^2) ) ~ x, mu=mu, sigma=sigma)\nslice_plot(f(x) ~ x, bounds(x=sigma*c(-3.5, 3.5))) %&gt;%\n  gf_hline(yintercept = c(0,exp(-0.5)), color=c(\"dodgerblue\", \"orange3\"))\nsigmoid &lt;- antiD(g(x) ~ x, lower.bound = -Inf)\n# Graph the sigmoid on the domain -10 &lt; x &lt; 10\nNotice that the vertical range of the function is \\(0 &lt;  f(x) \\leq 1\\). The argmax is \\(\\mu\\), so \\(f(\\mu) = 1\\). This seems easy and convenient, but one of the purposes of the standard gaussian function is to define a standard sigmoid function. It is the sigmoid that we want to have a range from 0 to 1.\nHow to scale the gaussian \\(f(x)\\) to produce a sigmoid with the range 0 to 1? Recall that a sigmoid is the anti-derivative of the corresponding gaussian. In the sandbox, we use antiD() to compute \\[\\int_{-\\infty}^x f(u) du\\], which we called sigmoid(). From the graph of sigmoid() you can read off the scaling factor that will make the vertical range of the resulting sigmoid zero to one.\n**Part G**  Where did the variable $u$ come from in $$\\int_{-\\infty}^x f(u) du ?$$ \n\nThe instructors made a mistake. They will try to be more careful in the future!\nYou can use any name for the variable of integration. The function \\(\\int_{-\\infty}^x f(u) du\\) will be a function of \\(u\\), not \\(x\\).\nYou can use any name for the variable of integration. The function \\(\\int_{-\\infty}^x f(u) du\\) will be a function of \\(x\\), not \\(u\\).\n\\(u\\) is the Latin equivalent of \\(\\mu\\).\n**Part H**  This question will take a bit of detective work in the sandbox. Make a table of a few combinations of different values for $\\mu$ and $\\sigma$. For each combination, find the maximum value of the corresponding `sigmoid()` function. Using this data, choose the correct formula for the maximum value of the sigmoid as a function of $\\mu$ and $\\sigma$. \n\n\\(\\sqrt{\\strut 2 \\pi \\sigma^2}\\)\n\\(\\pi \\sigma\\)\n\\(\\sqrt(\\strut \\mu \\pi \\sigma)\\)\n\\(\\sqrt(\\strut \\pi \\sigma^2 / \\mu)\\)\nPutting all this together, we arrive at our “official” standard gaussian function, called the Gaussian function:\n\\[g(x) \\equiv \\frac{1}{\\sqrt{\\strut2\\pi\\sigma^2}} e^{-(x-\\mu)^2 / 2 \\sigma^2}\\] The Gaussian function is important to many quantitative disciplines and has a central role in statistics. The function is named after physicist and mathematician Carl Friedrick Gauss (1777-1855). But in the social sciences, it is usually called the “normal” function; that is how common it is.\nIn R, the Gaussian function is provided as dnorm(x, mu, sigma). (The corresponding sigmoid, that is, the anti-derivative of dnorm() with respect to x is available as pnorm(x, mu, sigma).\nThe Gaussian function is so important, that it is worth pointing out some recognizable landmarks in the algebraic expression. Knowing to look for such things is one trait that defines an expert.\n**Part I**  Go back to the sandbox where we defined the function `f(x)` and graphed it. The function has a distinctive shape which is almost always described in terms of a familiar word. Which one is it? \nbreadloaf-shaped ghost-shaped bell-shaped sombrero-shaped\nOptional background. Now for a bit of irony. We’ve taken a lot of care to define a specific form of gaussian function with a formula that strikes many people as complicated. It must be that all these specifics are important, right? In reality, any function with a roughly similar shape would work in pretty much the same way. We could have defined our “official” gaussian function in any of a number of ways, some of which would be algebraically simpler. But the specific gaussian shape is a kind of fixed point in the differential equation that we will study next.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Diffusion</span>"
    ]
  },
  {
    "objectID": "Manifestations/53-diffusion.html#origins-of-the-gaussian-function",
    "href": "Manifestations/53-diffusion.html#origins-of-the-gaussian-function",
    "title": "54  Diffusion",
    "section": "",
    "text": "exponential\nlogarithm: inverse to exponentials\npower-law\nsinusoid\ngaussian and sigmoid\n\n\n\n\nThe mean, \\(\\mu\\), is the argmax of the function.\nThe variance, \\(\\sigma^2\\), how “fat” the parabola is.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe term \\(1/\\sqrt{2 \\pi \\sigma^2}\\) is not a function of \\(x\\), it is a constant related to the variance \\(\\sigma^2\\). It is just a number arranged to make \\[\\int_{-\\infty}^\\infty g(x) dx = 1 .\\]\nThe exponential means that, whatever the values of \\(\\mu\\) and \\(\\sigma\\), the function value \\(0 &lt; g(x)\\).\nThe argmax is at \\(x^\\star = \\mu\\). This is also the “center” of the function, which is symmetrical around the argmax.\nThe variance \\(\\sigma^2\\) appears directly in the formula. In no place does \\(\\sigma\\), the standard deviation, appear without the exponent 2. This is a hint that the variance is more fundamental than the standard deviation.",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Diffusion</span>"
    ]
  },
  {
    "objectID": "Manifestations/53-diffusion.html#net-flux",
    "href": "Manifestations/53-diffusion.html#net-flux",
    "title": "54  Diffusion",
    "section": "54.2 Net flux",
    "text": "54.2 Net flux\nRecall Newton’s Law of Cooling, \\(\\dot{T} = - k (T - T_a)\\) Although temperature was hardly understood as a physical phenomenon in Newton’s era, with today’s sensibility you can understand that energy is flowing into or out of the object from the ambient environment. The word flux is often used in mathematics to refer to such flows.\nWe’ve studied cooling in a spatially discrete context, the cooling of a single point (e.g. “a cup of coffee”) in an environment that has only one property, the “ambient” temperature.\nLet’s switch to a spatially continuous context, a bar of iron with one end lying in a bed of hot coals and the other end in the open air, as in the picture:\n\n\n\n\n\n\nFigure 54.1: Source\n\n\n\nThe iron rod is incandescent at the right end and cooler toward the left. If the picture were a movie, you likely would be able to predict what the action will be: Heat will flow down the rod from right to left. The free end of the rod will eventually get burning hot.\nThe temperature \\(T\\) at each point \\(x\\) in the rod is a function of position. But at any given position, the temperature is a function of time \\(t\\). That is, the temperature of the rod is a function of two variables: \\(T(x,t)\\).\nIf we were thinking about the movie frame-by-frame, we might prefer to treat \\(t\\) as a discrete variable and could write \\(T(x,t)\\) as \\(T_t(x)\\). It does not really matter which, but it helps to think about \\(T()\\) as a function of \\(x\\) whose shape evolving in time.\nNow back to Newton’s Law of Cooling. The flux of heat is the difference between the object’s temperature and the ambient temperature. But in the continuous spatial system, the difference in temperature between two infinitely close neighboring points is zero. That suggests no flux. Of course, a major theme in Calculus is to provide means to discuss the rate of difference of a value at two infinitely close points: the derivative \\(\\partial_x T(x, t)\\). This derivative gives the flux of heat from right to left.\n\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\nIt might be tempting to translate this directly into the terms of Newton’s Law of Cooling and claim, wrongly that \\(\\partial_t T(x, t) = -k \\partial_x T(x, t)\\). Why is this wrong? In the spatially discrete setting—cup of coffee, ambient environment—there are only two spatial points of interest. But along the continuous iron rod, there are at least three points to be considered. Let’s call them \\(x_A\\), \\(x_B\\), and \\(x_C\\) as in the diagram. We are interested in \\(\\partial_t T(x_B, t)\\). This change in temperature at point B is driven by the flux from point C to point B and is proportional to \\(\\partial_x T(x_B, t)\\). But the change in temperature at point B is equally influenced by the flux from B to A. That is, the change in temperature at point B is set by the difference in flux, the flux coming from A to B and the flux going from B to C.\nIn the spatially continuous context, the net flux or difference in differences (A to B, B to C) is represented by the second derivative with respect to \\(x\\). That is, along the rod, Newton’s Law of Cooling amounts to \\[\\partial_t T(x, t) = k\\, \\partial_{xx} T(x, t)\\] This is called the heat equation and was introduced in 1807 by Jean-Baptiste Joseph Fourier (1786-1830). The same equation is now also called the diffusion equation.\nSome people might be more comfortable thinking about the discrete-time dynamics of the movie, which could be written \\[T_{t+h}(x) \\approx T_t(x) + h\\,k\\, \\partial_{xx} T_t(x)\\]\nExercise: Turn away from the iron rod of the picture and imagine being presented with four new rods each of which has been heated in some way to produce a temperature profile at time 0, that is \\(T_0(x)\\) as shown in the four graphs below.\n\n\n\n\n\n\n\n\n\n## Warning: All aesthetics have length 1, but the data has 101 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n\n\n\n\n\n\n\n\n\n\n## Warning: All aesthetics have length 1, but the data has 101 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n\n\n\n\n\n\n\n\n\n\n## Warning: All aesthetics have length 1, but the data has 101 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n\n\n\n\n\n\n\n\n\n\n## Warning: All aesthetics have length 1, but the data has 101 rows.\n## ℹ Please consider using `annotate()` or provide this layer with data containing\n##   a single row.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 54.2\n\n\n\n\nFor the function \\(T(x, t=0)\\) shown in Graph (A), how will the temperature at \\(x=0\\) change instantaneously? (That is, what is \\(\\\\partial_t T(x, t=0)\\)?)\n\n“+Temperature decreases+”, “No change in temperature”, “Temperature increases”, “Temperature oscillates” = “The temperature cannot oscillate instantaneously. The value of a derivative function at a point is a real number, not an oscillation.” , random_answer_order = FALSE )\n\nFor the function \\(T(x, t=0)\\) shown in Graph (B), how will the temperature at \\(x=0\\) change instantaneously? (That is, what is \\(\\\\partial_t T(x, t=0)\\)?) “Temperature decreases”, “No change in temperature”, “+Temperature increases+”, “Temperature oscillates” = “The temperature cannot oscillate instantaneously. The value of a derivative function at a point is a real number, not an oscillation.” , random_answer_order = FALSE )\nFor the function \\(T(x, t=0)\\) shown in Graph (C), how will the temperature at \\(x=0\\) change instantaneously? (That is, what is \\(\\\\partial_t T(x, t=0)\\)?) “Temperature decreases”, “+No change in temperature+”, “Temperature increases”, “Temperature oscillates” = “The temperature cannot oscillate instantaneously. The value of a derivative function at a point is a real number, not an oscillation.” , random_answer_order = FALSE )\n\naskMC( 4. For the function \\(T(x, t=0)\\) shown in Graph (D), how will the temperature at \\(x=0\\) change instantaneously? (That is, what is \\(\\\\partial_t T(x, t=0)\\)?) “Temperature decreases”, “+No change in temperature+”, “Temperature increases”, “Temperature oscillates” = “The temperature cannot oscillate instantaneously. The value of a derivative function at a point is a real number, not an oscillation.” , random_answer_order = FALSE )",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Diffusion</span>"
    ]
  },
  {
    "objectID": "Manifestations/53-diffusion.html#diffusion",
    "href": "Manifestations/53-diffusion.html#diffusion",
    "title": "54  Diffusion",
    "section": "54.3 Diffusion",
    "text": "54.3 Diffusion\nRecall that the heat equation describes how the temperature along a approximately 1-dimensional object—an iron bar, for example—changes with time due to spatial differences in temperature from point to point. The heat equation is\n\\[\\underbrace{\\partial_t T(x, t)}_\\text{change in time} = \\ \\ \\ k\\, \\underbrace{\\partial_{xx} T(x, t)}_\\text{pattern in space}\\]\n\n\n**Part J**  Suppose the inputs to the temperature function have units centimeters (for $x$) and seconds (for $t$) and that the output is in degrees Kelvin (which we will write \"K\"). What are the units of the coefficient $k$? \n\\(K cm^{1}\\) \\(K cm^{2} s^{-1}\\) \\(cm^{2} s^{-1}\\) \\(s cm^{2}\\)\n\n\nAt the time Fourier was working, there was no molecular theory of matter and very little understanding of what the “heat substance” might consist of. Now we know that heat is the energy of molecular vibrations. This energy diffuses through the material.\nSimilarly, “diffusion” is one mode of physical motion of material, for example movement of sugar molecules within a cell. Other things can diffuse as well, for example the action of viscosity in fluids can be seen as the diffusion of momentum.\nStarting in the 20th century and in support of the developing molecular theory of gasses, mathematicians and physicists undertook to follow the trajectories of individual diffusing particles and to develop a means to describe them mathematically. This included the concept of a “random walk,” movement of a particle shifting direction and speed randomly as it collides with other moving molecules and particles in its environment.\nThe movie shows a simulation of a few particle (in yellow) undergoing a random walk. The path followed by each diffusing particle is shown in blue; the velocity of one particle is indicated with a red vector.\n\nCC BY-SA 3.0, Link\nThe idea of random walks has become especially important in economics and finance. The walking “particle” might be the price of a stock or other derivative asset. The “collisions” happen as individual trades are made in the market, each trade being influenced by some news or situation or the passing whims, fancies, or fears of investors. The work on this point of view started about 1900 by a mathematics graduate student, Louis Bachelier, who undertook to study the movements of the Bourse, the Parisian stock exchange. The 1997 Nobel Prize in economics was awarded for a “new method to determine the value of [market] derivatives.”\nFor these reasons, we will focus and the mathematics of diffusion instead of the equivalent but historically prior mathematics of heat. We will work with a function \\(C(x, t)\\) which stands for the concentration of particles in some medium such as a gas as a function of space and time. In economics, \\(x\\) will stand for the value of some asset such as an investment, and \\(C()\\) gives a probability density for each possible value of \\(x\\).\nFor the sake of visualization, suppose some odor molecules are released in at the midpoint of a pipe with absolutely still air. Over time, the molecules will diffuse throughout the along the extent of the pipe.\nIf \\(C(x, t)\\) is the concentration of odor molecules at each point \\(x\\) and time \\(t\\), then the change in concentration with time is:\n\\[\\partial_t C(x, t) = D\\, \\partial_{xx} C(x, t)\\] This is called the “Diffusion equation.” \\(D\\) is called the “diffusion coefficient” and depends on the size of the molecule and the temperature in the pipe. The “diffusion equation” is the same as the “heat equation,” with different names used for the quantities involved.\n\n\n**Part K**  Suppose the inputs to the concentration function have units centimeters (for $x$) and seconds (for $t$) and that the output is in nanograms per cubic centimeter ($ng\\ cm^{-3}$) . What are the units of the coefficient $k$? \n\\(ng cm^{-1}\\) \\(ng cm^{1} s^{-1}\\) \\(cm^{2} s^{-1}\\) \\(s cm^{1}\\)\n\n\nMany people have difficulty imagining the sorts of frequent collisions that are behind diffusion. They think, for instance, that in still air the molecules are pretty much still. This is wrong. A typical velocity of a water molecule in air at room temperature and pressure is 650 m/sec. (The speed of sound is about 350 m/sec.) But the time between molecular collisions is on the order of \\(10^{-10}s\\), so the typical distance travelled between collisions is about \\(10^{-7}m\\). For a root mean square distance of 1m, we need roughly \\(10^{14}\\) collisions, which would occur in \\(10^{4}\\) seconds (a couple of hours).",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Diffusion</span>"
    ]
  },
  {
    "objectID": "Manifestations/53-diffusion.html#dynamics-of-variance",
    "href": "Manifestations/53-diffusion.html#dynamics-of-variance",
    "title": "54  Diffusion",
    "section": "54.4 Dynamics of variance",
    "text": "54.4 Dynamics of variance\nIn this section, we will explore the connection between diffusion and the gaussian function. Recall that we modeled the temperature along a one-dimensional spatial domain (a “bar of iron”) as it evolves in time as a function of both position and time: \\(C(x, t)\\). The same sort of function—of position and time—can be used to describe the concentation of particles freely diffusing along a medium such as air in a pipe.\nWe constructed a differential equation to describe the dynamics of \\(C(x, t)\\) called the “heat” equation or the “diffusion” equation, depending on context. Before using that differential equation, let’s explore a little bit what we might mean about the “dynamics of a function.”\nIn studing dynamics we worked first with time taken discretely, e.g. a sequence of states \\({\\mathbf x}_0\\), \\({\\mathbf x}_1\\), \\({\\mathbf x}_2\\), \\(\\ldots\\), \\({\\mathbf x}_n\\), \\(\\ldots\\). The vector \\({\\mathbf x}_n\\) is the “state” of the system at time step \\(n\\). In our work, we looked at 1-dimensional and 2-dimensional states, tracing out a trajectory from one time step to the next and the next and so on. Exactly the same ideas would apply to 3- and higher-dimensional states, say an ecosystem involving growing grass, and populations of mice, rabbits, foxes, birds, and so on.\nIn our present contexts, heat or diffusion, we are working with functions. Let’s return to the earlier metaphor of a movie of \\(C(x, t)\\) with the frames taken every \\(h\\) seconds. The movie is the sequence of frames \\(C_0(x)\\), \\(C_h(x)\\), \\(C_{2h}(x)\\), \\(\\ldots\\), \\(C_t(x)\\), \\(\\ldots\\).\nTo describe the dynamics—that is, the change from frame to frame in the movie—we write a finite-difference equation, generically: \\[C_{n+1}(x) = f(C_n(x))\\] In the movie of diffusion, that equation will be this:\n\\[C_{t+h}(x) = C_n(x) + h\\, \\alpha\\,  \\partial_{xx} C_n(x)\\] The term \\(\\partial_{xx} C(x)\\) tells us the net flux of heat/particles/probability into the point \\(x\\).\nIn English, this says, “The concentration at \\(x\\) in one frame of the movie is the amount that was there in the previous frame plus the net flux of heat/particles from the neighboring points.”\nNow imagine making the movie using an ultra-high-speed camera that takes a new frame every \\(h\\) microseconds. We will label the time of one frame as \\(t\\) and the time of the next frame as \\(t+h\\). The frame-to-frame change is therefore \\[C_{t+h}(x) = C_t(x) + h\\, \\alpha \\partial_{xx} C_t(x)\\] We can equally well write \\(C_t(x)\\) as \\(C(x, t)\\), our notation for functions on a continuous domain. Doing this, and re-arranging the formula, gives: \\[\\frac{C(x,t+h) - C(x,t)}{h} = \\alpha \\partial_{xx} C(x, t)\\] The left side of this equation is the differencing operator applied to \\(C(x, t)\\) (with respect to \\(t\\)). In the limit as \\(h\\rightarrow 0\\) (that is, as you turn the video frame rate faster and faster) we can replace the left side of the equation with the partial derivative \\(\\partial_t C(x,t)\\). That is the heat/diffusion equation.\nWe will find the solution \\(C(x,t)\\) to the differential equation using Euler’s method. In other words, we will make movies of the functions \\(C_t(x)\\) one frame at a time. We know the dynamics; to start we need is an initial condition, the function \\(C_0(x)\\).\n\nC_funs &lt;- list()\nflux_funs &lt;- list()\ntmp &lt;- list()\nxpts &lt;- seq(-50, 50, length=1000)\nC_funs[[1]] &lt;- tibble::tibble(\n  x = xpts,\n  y = dnorm(x, 0, 1)\n)\n\nfor (k in 1:500) {\n  tmp &lt;- spliner(y ~ x, data = C_funs[[k]])\n  foo &lt;- flux_funs[[k]] &lt;- D(tmp(x) ~ x + x, .hstep=1)\n  C_funs[[k+1]] &lt;- tibble::tibble(\n    x = xpts,\n    y = tmp(x) + 0.1*foo(x)\n  )\n}\n\nImagine that we have a long, thin pipe filled with still air and we inject at position \\(x=0\\) a concentration of particles, say 1600 per cm^3. Equivalent, you could picture a freezing-cold iron bar and, at time \\(t=0\\), we place a white-hot coal on the center point, heating it to 1600 (deg C) and then removing the coal. The initial condition looks like this:\n\ntop &lt;- max(C_funs[[1]]$y)\ngf_line(y ~ x, data = C_funs[[1]] |&gt; filter(abs(x) &lt; 25)) %&gt;%\n  gf_labs(y=\"Temperature (deg C)\", x=\"Position along bar\", title=\"Initial condition C(x,t=0)\") %&gt;%\n  gf_hline(yintercept = exp(-.5)*top, color=\"orange3\")\n\nThe red horizontal line is positioned to enable you to read off the standard deviation of the bell-shaped function.\nThe next frame of the movie will show \\(C_h(x)\\). To construct that, we will compute the net flux into each point on the bar.\n\ntmp &lt;- spliner(y ~ x, data = C_funs[[1]])\nfoo &lt;- D(tmp(x) ~ x & x, .hstep=1)\nslice_plot(foo(x) ~ x, bounds(x=c(-25, 25)), npts=500) %&gt;%\n  gf_labs(y=\"Net flux (deg C/cm^2)\", x=\"Position along bar\", title=\"Net flux at time t=0\")\n\nYou can see that there is a strong net flux out of the center point and a net flux in to neighboring regions: the heat will be spreading out. Far from the center point, the net flux is zero. In the next graphs, we will zoom in on the center of the domain, \\(-2.5 \\leq x \\leq 2.5\\).\nTo find the next Euler step, that is, the function \\(C_h(x)\\), we add the net flux (scaled by \\(h \\alpha\\)) to \\(C_0(x)\\). As usual, we take one Euler step after the other to reach whatever time \\(t\\) we want.\nHere is the solution \\(C(x, t=0.5)\\) shown with \\(C(x, t=0)\\) superimposed in blue. (We set \\(\\alpha=2\\) and used 5 Euler steps with \\(h=0.1\\).)\n\ndraw_C &lt;- function(n, compare_n=1, show_sd=TRUE, domain=5) {\n  Dat &lt;- C_funs[[n]] |&gt; filter(abs(x) &lt; domain)\n  ylab &lt;- glue::glue(\"C(x, t={(n-1)*0.1}) in deg C.\")\n  title &lt;- glue::glue(\"C(x, t={(n-1)*0.1})\")\n  redline &lt;- exp(-0.5) * max(Dat$y)\n  P &lt;- gf_line(y ~ x, data = C_funs[[n]] |&gt; filter(abs(x) &lt; domain)) %&gt;%\n    gf_labs(y=ylab, \n                  x=\"Position along bar\", title=title) \n  \n  if (compare_n &gt; 0) {\n    P &lt;- P |&gt; \n      gf_line(y ~ x, alpha=0.5, color=\"dodgerblue\", \n            data = C_funs[[compare_n]] |&gt; filter(abs(x) &lt; domain)) \n  }\n  \n  if (show_sd) {\n    P |&gt; gf_hline(yintercept=redline, color=\"orange3\")\n  } else {\n    P\n  }\n}\ndraw_C(6, domain=25, show_sd=FALSE)\n\nAt time \\(t=0.5\\) , the temperature at the center has gone down. Less obviously, \\(C(x, h)\\) is a tiny bit wider than \\(C(x,t=0)\\). That is, heat has spread out a bit from the center.\nHere is the function \\(C(x,t)\\) at \\(t=1, 2, 3, 4\\). You can see the function spreading out as \\(t\\) increases. We’ve zoomed in on the x-axis to where the action is.\naxis5 &lt;- function(P, xlim=5, dxlim=xlim/50) {\n  P |&gt; gf_refine(scale_x_continuous(breaks=(-xlim):xlim, \n                               minor_breaks = seq(-xlim,xlim,by=dxlim)))\n}\ndraw_C(11, domain=5, show_sd=TRUE) |&gt; axis5()\ndraw_C(21, domain=5, show_sd=TRUE) |&gt; axis5()\ndraw_C(31, domain=5, show_sd=TRUE) |&gt; axis5()\ndraw_C(41, domain=5, show_sd=TRUE) |&gt; axis5()\n\n\n\nUse the intersection between the red horizontal line and the black curve to find the width of the black curve, that is, the standard deviation. Which of these sequences correspond to the standard deviation at times 1, 2, 3, and 4?\n“\\(\\\\sigma_1 = 1.7, \\\\sigma_2 = 2.1, \\\\sigma_3 = 2.5, \\\\sigma_4 = 2.9\\)”, “+\\(\\\\sigma_1 = 1.7, \\\\sigma_2 = 2.2, \\\\sigma_3 = 2.6, \\\\sigma_4 = 2.9\\)+”, “\\(\\\\sigma_1 = 1.7, \\\\sigma_2 = 2.2, \\\\sigma_3 = 2.7, \\\\sigma_4 = 3.2\\)”, “\\(\\\\sigma_1 = 1.7, \\\\sigma_2 = 2.2, \\\\sigma_3 = 2.8, \\\\sigma_4 = 3.5\\)”,\nHere is a similar set of graphs for \\(t=10, 20, 30, 40\\):\n\ndraw_C(101,0, domain=15, show_sd=TRUE) |&gt; axis5(xlim=15, dxlim=0.5)\ndraw_C(201,0, domain=15, show_sd=TRUE) |&gt; axis5(xlim=15, dxlim=0.5)\ndraw_C(301,0, domain=15, show_sd=TRUE) |&gt; axis5(xlim=15, dxlim=0.5)\ndraw_C(401,0, domain=15, show_sd=TRUE) |&gt; axis5(xlim=15, dxlim=0.5)\n\nWhich of these sequences correspond to the standard deviation at times 10, 20, 30, and 40?\n“+\\(\\\\sigma_{10} = 4.5, \\\\sigma_{20} = 6.4, \\\\sigma_{30} = 7.7, \\\\sigma_{40} = 9.0\\)+”, “\\(\\\\sigma_{10}=  4.5, \\\\sigma_{20} = 6.0, \\\\sigma_{30} = 7.5, \\\\sigma_{40} = 9.0\\)”, “\\(\\\\sigma_{10} = 4.5, \\\\sigma_{20} = 5.8, \\\\sigma_{30} = 7.1, \\\\sigma_{40} = 8.4\\)”, “\\(\\\\sigma_{10} = 4.9, \\\\sigma_{20} = 6.0, \\\\sigma_{30} = 7.9, \\\\sigma_{40} = 9.3\\)”,\nThe standard deviation increases with time. Which one of these power-law relationships best corresponds to the standard deviations you measured off the graphs?\n“\\(\\\\sigma_t = k t\\)”, “\\(\\\\sigma_t = k t^2\\)”, “+\\(\\\\sigma_t = k \\\\sqrt{t}\\)+”, “\\(\\\\sigma_t = k t^{1.5}\\)”\nEssay: A more fundamental way to measure the increase in width of \\(C(x, t)\\) with time is to use the variance rather than the standard deviation. Describe briefly the very simple pattern that the variance of \\(C(x, t)\\) follows over time. (You don’t have to give a formula, just say what function it is.)",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Diffusion</span>"
    ]
  },
  {
    "objectID": "Manifestations/53-diffusion.html#random-walk",
    "href": "Manifestations/53-diffusion.html#random-walk",
    "title": "54  Diffusion",
    "section": "54.5 Random walk",
    "text": "54.5 Random walk\nThe solution to diffusion differential equation gives the concentration of the diffusing particles \\(C(x, t)\\) at any \\(x\\) and \\(t\\). At any given value of \\(t= t_1\\), the shape of \\(C(x,t=t_1)\\) tends to a smooth bell-shaped curve: the gaussian function. The width of the gaussian is a function of \\(t\\). Conventionally, the width corresponds to the variance parameter \\(\\sigma^2\\). This changes with \\(t\\) in a very simple way: \\[\\sigma(t)^2 = D t\\]\nEverything is smooth and nicely behaved. But this is an abstraction which summarizes the concentation of a theoretically infinite population of particles. Looking at the position of a single particle as a function of time gives a very different impression. The figure shows individually the trajectories of 50 (randomly selected) particles.\n\n\n\n\n\n\n\n\n\nDo take note that the position of each particle in the pipe at time \\(t\\) is shown on the vertical axis.\nEach of the trajectories is called a random walk, as if a walker were randomly taking steps forward or backward.\nr insert_calcZ_exercise(“XX.XX”, “KktWcD”, “Exercises/Dynamics/fox-write-oven.Rmd”)",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Diffusion</span>"
    ]
  },
  {
    "objectID": "Manifestations/53-diffusion.html#investment-volatility",
    "href": "Manifestations/53-diffusion.html#investment-volatility",
    "title": "54  Diffusion",
    "section": "54.6 Investment volatility",
    "text": "54.6 Investment volatility\nWhen people invest money, they expect a return. Generally, the return is measured as a percentage per year. An \\(r=10\\%\\) annual return—that is pretty high these days—means that at the end of the year your investment of, say, $500 will be worth $550. And remember, saying \\(r=10\\%\\) is the same thing as saying \\(r=0.10\\).\nBanks and such do things in discrete time, e.g. crediting your savings account with interest once a month. But this is calculus, so we focus on continuous time. (And, of course, Nature does things in continuous time!)\nIf \\(S\\) is the amount of money you have invested at a return of \\(r\\), the evolution of \\(S\\) over time is given by a familiar, linear differential equation: \\[\\dot{S} =  r S\\ \\ \\ \\implies \\ \\ \\  S(t) = S_0 e^{r t}\\]\nQuick review questions:\n\nWhich of those two equations is the differential equation and which is the solution?\nWhat symbol is being used to stand for the “state” of the system?\nWhat is the form of the dynamical function of state?\nIs there a fixed point? If so, is it stable?\n\nInvestments in the stock market provide two types of return. We will focus on the return that comes from the changing price of the stock, which can go up or down from day to day. The other kind of return is dividends, the typically quarterly payment made by the company to stock holders. In investments, dividends should not be ignored, but we aren’t interested in them here.\nNow imagine that you expect, for whatever reason, the stock price to go up by 2% per year (\\(r=0.02\\)) on average. Of course, the price is volatile so the 2% is by no means guaranteed. We will imagine the volatility is 25% per year.\nThis situation, which includes volatility, is modeled by a modification of differential equations called “stochastic differential equations.” (“Stochastic” comes from the Greek word for “aiming”, as in aiming an arrow at a target. You won’t necessarily hit exactly.) The math is more advanced and we will not go into details. Our point here is to warn you: Now that you are expert about (ordinary) differential equations, you need to be aware that things are somewhat different in a stochastic situation.\nTo that end, we will show you trajectories that follow the mathematics of stochastic exponential growth (with \\(r=0.2\\) per year and volatility \\(\\sigma = 0.25\\) per year). On top of that, we will show in red the trajectory from (ordinary, non-stochastic) exponential dynamics \\(\\dot{S} = r S\\). In blue, we will show the theoretical average stochastic dynamics. In all cases, we will set the initial condition to be \\(S_0 = 100\\). We will follow the trajectories for three years.\n\n\n\n\n\n\n\n\n\nThe eye is drawn to the trajectories leading to large returns. That is misleading. Although there are a few trajectories that show a 3-year return above 50% (that is, to $150 or higher) in fact the majority of trajectories fall below that of a purely deterministic \\(r=2\\%\\) annual return exponential process. The volatility causes a decrease in the median return.\nIt is easy to understand why the stochastic shocks cause a loss in return. Consider a stock with an initial price of $100 that in two successive days goes up by 50% and down by 50%. These ups and downs should cancel out, right? that is not what happens:\n\\(\\$100 \\times 1.5 \\times 0.5 = \\$75\\)\n\nSome class notes in www/2021-03-31-classnotes.Rmd",
    "crumbs": [
      "BLOCK VI. Manifestations",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Diffusion</span>"
    ]
  },
  {
    "objectID": "Glossary.html",
    "href": "Glossary.html",
    "title": "55  Glossary",
    "section": "",
    "text": "apply the function,1,01 applying,1,01 axis,1,01 convert,1,01 coordinate plane,1,01 degree of freedom,1,01 dimension,2,01 domain,4,01 functions,3,01 inputs,1,01 maps,1,01 mathematical models,1,01 model,1,01 name of an input,1,01 not a symbolic name,1,01 number line,2,01 output,4,01 quantity,4,01 range,1,01 scale,1,01 trajectory,3,01 application of a function to an input,1,02 apply a function,1,02 argument,1,02 constants,1,02 defining,1,02 evaluate a function on,1,02 formula,3,02 notation,1,02 parameters,4,02 period,4,02 special inputs,2,02 declarative,1,03 do some action,1,03 evaluating,1,03 format,1,03 imperative,1,03 only,1,03 storage arrow,1,03 storage,1,03 tilde expression,2,03 constant value,1,04 contour plot,2,04 function,12,04 graph,3,04 graphical domain,1,04 graphs,1,04 one,1,04 output space,1,04 slice,3,04 space,1,04 surface,2,04 base,1,05 bit,1,05 decade,1,05 doubling time,1,05 gaussian,1,05 inversely proportional,1,05 monomials,2,05 natural logarithm,2,05 negative powers,1,05 non-integer powers,1,05 normal function,1,05 not,8,05 pattern-book functions,2,05 power-law functions,1,05 sigmoidal,1,05 argmax,8,06 argmin,5,06 asymptotic,1,06 at,1,06 change in slope,1,06 change,1,06 concave down,3,06 concave up,4,06 concavity,5,06 “continuous on an interval (a,b)”,1,06 continuous,4,06 cycle,1,06 decrease,1,06 divided by,1,06 extrema,1,06 horizontal asymptote,1,06 increase,1,06 inflection points,1,06 input,5,06 local maximum,3,06 local minimum,3,06 monotonic,2,06 negative concavity,1,06 not include 0,1,06 of,1,06 optimization,2,06 periodic,1,06 positive concavity,1,06 rate of change,3,06 vertical asymptote,2,06 continuous domain,1,07 data frame,3,07 discrete,1,07 evaluate,2,07 explanatory variables,2,07 interpolation,4,07 machine learning,1,07 point plot,1,07 response,1,07 statistics,1,07 variable,1,07 black,4,08 input scaling,2,08 output scaling,2,08 parameter,2,08 pure number,1,08 pure numbers,1,08 quantities,2,08 coefficients,2,09 constant of proportionality,1,09 discontinuity,2,09 exterior function,2,09 fitting the function,1,09 function composition,1,09 inner,1,09 interior function,2,09 linear combination,4,09 local behavior,1,09 outer,1,09 piecewise function,1,09 wave packet,1,09 without lifting the pencil from the paper,1,09 displacement,3,10 just the identity function,1,10 multiplication,1,10 10,1,11 amplitude,1,11 baseline,1,11 fitting,1,11 half,1,11 layers,1,11 modeling cycle,4,11 newton’s law of cooling,2,11 polishing,1,11 regression,1,11 constant term,2,12 demand functions,2,12 eight simple shapes,6,12 interaction term,3,12 linear terms,1,12 order,1,12 polynomial,1,12 production functions,2,12 quadratic terms,2,12 information,1,13 inverting,1,13 iterate,4,13 iteration,1,13 maximization,1,13 minimization,1,13 zero finding,2,13 decades,1,14 difference,1,14 log-log,1,14 logarithm,1,14 lux,1,14 magnitude,2,14 negative values,1,14 order of magnitude,1,14 orders of magnitude,1,14 ratio,1,14 scientific notation,1,14 semi-log,1,14 units,1,14 [acceleration],1,15 [area],1,15 [energy],1,15 [force],1,15 [momentum],1,15 [power],1,15 [velocity],1,15 [volume],1,15 coefficients must themselves have dimension,1,15 compound dimensions,1,15 degrees,1,15 density,2,15 dimensionless,1,15 divide,3,15 flavors of one,1,15 fundamental dimensions,1,15 gravity,2,15 length,4,15 mass,3,15 multiply,2,15 never,1,15 radians,2,15 time,3,15 unit,2,15 work,2,15 data,1,16 empirical models,1,16 extrapolation,1,16 hypothetico-deductive model,1,16 mechanistic models,1,16 observation,1,16 residuals,1,16 scientific method,1,16 systematic,1,16 54,1,17 differentiation,4,17 elliptical,1,17 flow,2,17 water volume,1,17 with-respect-to,1,17 average rate of change,1,18 instantaneous slope function,1,18 instantaneous slope,1,18 model the model,1,18 objective function,4,18 rate,1,18 slope function,1,18 velocity,2,18 non-standard analysis,1,19 derivative,2,19 theory of limits,1,19 finite-difference,1,20 second derivative,4,20 static,1,20 symbolic differentiation,1,20 change in the slope,1,21 constant,1,21 curvature,3,21 first derivative,1,21 radius of curvature,1,21 smooth,1,21 smoothness,3,21 third derivative,2,21 transition,2,21 twice,1,21 you are not expected to memorize it,1,21 c-infinity,1,22 continuity,1,22 continuous sets,1,22 cubic splines,1,22 differencing operator,1,22 discrete sets,1,22 example of non-\\(c^2\\) functions:,1,22 examples of \\(c^\\infty\\) functions,1,22 heaviside function,1,22 ramp function,1,22 real numbers,2,22 small change in input,1,22 small change in output,1,22 adding,1,23 chain rule,1,23 composition of two functions,1,23 composition,1,23 derive,1,23 h-theory,1,23 left graph,1,23 linear operator,1,23 per capita,1,23 per,8,23 percent,1,23 power-law rule,1,23 product of two functions,1,23 product rule,2,23 proportional increase,1,23 right graph,1,23 rules,1,23 scaling,2,23 step 1,1,23 step 2,1,23 step 3,1,23 arg,1,24 critical points,1,24 decision quantity,1,24 differential equations,3,24 max,2,24 maximize,1,24 maximum,2,24 micron,1,24 minimize,1,24 minimum,1,24 multiple objectives,1,24 negative,2,24 objectives,1,24 only part,1,24 “out of 55,000”,1,24 parabola,1,24 positive,1,24 precision,1,24 revenue,1,24 slope,4,24 value of the second derivative,1,24 value,1,24 all other things being equal,1,25 all other things held constant,1,25 blue,1,25 facet,1,25 gradient field,1,25 gradient vector,2,25 gradient,2,25 green,1,25 held constant,1,25 orientation,1,25 partial derivative,1,25 partial derivatives,1,25 sum,1,25 tangent plane,1,25 linear regression,1,26 linear,1,26 local,2,26 mixed partial derivatives,1,26 quadratic,1,26 second-order polynomial,1,26 error,2,27 exact,1,27 far,1,27 indeterminant form,1,27 l’hopital’s rule,1,27 least squares,1,27 n-th order,1,27 near,3,27 tail domains,1,27 taylor polynomial,2,27 well behaved,1,27 wriggly domain,1,27 ** componentwise ,1,28 alignment,1,28 angle,2,28 column vector,1,28 column,1,28 correlation coefficient,1,28 cosine of the angle,2,28 degrees of freedom,1,28 direction,2,28 dot product,3,28 embedded,1,28 functional notation,1,28 functional,1,28 infix notation,1,28 linear algebra,1,28 matrix mechanics,1,28 mean,3,28 negative correlation,1,28 orthogonal,2,28 positively correlated,1,28 protractor,1,28 row vector,1,28 ruler,1,28 scalar,2,28 scale a vector,1,28 square length,1,28 step,2,28 subspace,4,28 tail,1,28 tip,1,28 variance,6,28 vector space,1,28 vectors,3,28 zero correlation,1,28 bold-faced,1,29 componentwise multiplication,1,29 matrix multiplication,1,29 matrix,2,29 projected onto,1,29 scaled,1,29 subspace spanned,1,29 subspaces,1,29 subtraction,1,29 table,1,29 tail to tail,1,29 vector,1,29 at the same time,1,30 decomposing,1,30 decomposition,1,30 model space,1,30 model vector,1,30 mutually orthogonal,2,30 not mutually orthogonal,1,30 projecting,1,30 projection,1,30 residual vector,1,30 target vector,1,30 unit length vector,1,30 both,4,31 columns of data,1,31 projection problem,2,31 target problem,4,31 accounted for,1,32 analysis of variance,1,32 artificial intelligence,1,32 body mass index,1,32 genetics,1,32 intercept,1,32 maximum likelihood estimation,2,32 r-squared,1,32 response variable,1,32 unaccounted for,1,32 unexplained,1,32 variation,1,32 anti-differentiation,2,33 brackets,1,33 with-respect-to input,1,33 age of the universe,1,35 anti-derivative,1,35 apply,2,35 bold-face,3,35 bounds of integration,1,35 constant of integration,3,35 definite integral,3,35 dummy variable,1,35 first fundamental theorem of calculus,1,35 flavor of one,1,35 force accumulated over time,1,35 force,2,35 indefinite integral,1,35 lower bound of integration,1,35 momentum,2,35 net change,2,35 second fundamental theorem of calculus,1,35 upper bound of integration,1,35 analyze,1,36 decompose,1,36 fourier transform,2,36 frequency domain,1,36 harmonics,1,36 knots,2,36 molecular vibrational spectroscopy,1,36 periodicity,1,36 time domain,1,36 waveforms,1,36 cumulative sum,1,37 cumulative,1,37 dynamics,3,37 euler steps,1,37 gauss-legendre quadrature,1,37 integrand,1,37 series of quantities,1,37 cataloged functions,2,38 elementary functions,1,38 helper function,1,38 no such rules,1,38 normal pdf,1,38 parts,2,38 slope function visualization,1,38 symbolic anti-derivatives,1,38 symbolic derivatives,1,38 techniques of integration,1,38 u-substitution,1,38 a trajectory can never cross itself,1,39 change in state,1,39 chaos,2,39 “continuous-time, continuous-space system”,1,39 deterministic,2,39 differential equation,1,39 dimension of the state space,1,39 “discrete-time, discrete-state dynamical system”,1,39 dynamical functions,1,39 dynamical state,1,39 dynamical system,1,39 dynamical systems,1,39 essential,1,39 first-order differential equations,2,39 fixed point,3,39 function of time,1,39 gradient ascent,2,39 initial condition,3,39 instantaneous state,2,39 oscillates,1,39 path,1,39 position,2,39 second-order differential equation,3,39 solution,1,39 speed,2,39 state space,4,39 state velocity,3,39 state,1,39 stochastic,2,39 system of differential equations,1,39 system,1,39 time series,1,39 accumulating,1,40 ansatz,2,40 continuous-time function,1,40 euler formula,1,40 euler method,1,40 graphical method,1,40 integrating,1,40 limit definition of the derivative,1,40 logistic equation,1,40 method of ansätze,1,40 solutions,1,40 substitution methods,1,40 catastrophe theory,1,41 tipping point,1,41 evaluated,1,41 fixed points,2,41 flow field,1,41 linear approximation,1,41 linearization,1,41 net,1,41 stability,1,41 stable,2,41 sustainability,1,41 unstable,1,41 neutral stability,1,42 nullclines,1,42 saddle,1,42 saddles,1,42 sink,1,42 source,1,42 vector field,1,42 bounded stable model,1,43 bounded unstable model,1,43 carrying capacity,1,43 chain reaction,1,43 conserved,2,43 coupled,1,43 critical mass,1,43 explosion,1,43 fissile,1,43 framework,1,43 function shape,1,43 interaction,1,43 linear stable model,1,43 linear unstable model,1,43 love,1,43 near zero,1,43 phase plane,1,43 predator-prey model,1,43 radioactive decay,1,43 reactor,1,43 sir model,1,43 square of the army size,1,43 the same problem,1,43 twice as capable,1,43 two,1,43 warfare,1,43 “ in the language of dynamical systems, the equilibrium state of a system is called a “,1,44 equilibrium,2,44 steady state,1,44 transient,2,44 cruise control,1,45 eigenvalues,2,45 eigenvectors,1,45 angular frequency,1,46 ballistics,1,46 complex number,1,46 critically damped,1,46 damping,1,46 force-balance equations,1,46 fundamental theorem of calculus,1,46 law ii,1,46 must have,1,46 theory of general relativity,1,46 accumulate,1,47 anti-differentiate the function,1,47 argmax finding,1,47 computational mathematicians,1,47 differentiate the function,1,47 equations,1,47 evaluating a function,1,47 ideal result,1,47 integrate the function,1,47 logistic map,1,47 mathematical modeling,1,47 mathematical task,1,47 name,1,47 newton step,1,47 number,1,47 operations on functions,1,47 realistic result,1,47 solving,1,47 zero-finding,1,47 ad hoc,1,48 control point,1,48 cubic segments,1,48 cubic spline,2,48 five,1,48 function fitting,1,48 interpolating function,1,48 mandate,1,48 s,2,48 single valued,1,48 six,1,48 stiff,1,48 stiffness,1,48 aids to human decision making,1,49 budget constraint,1,49 cobb-douglas,1,49 constraint,1,49 dominated,1,49 dynamic programming,1,49 equality constraint,1,49 equality constraints,1,49 evaluation phase,1,49 gradient descent,1,49 incommensurate,1,49 inequality constraint,1,49 iterative,1,49 lagrange multiplier,2,49 linear programming,1,49 min,1,49 modeling phase,1,49 operations research,1,49 parallel,1,49 potential energy,1,49 quadratic programming,1,49 relaxed,1,49 shadow price,1,49 simplex,1,49 solution phase,1,49 spring-mass systems: an example context,1,49 sub-optimal,1,49 vector input,1,49 arithmetic mean,1,50 bayes’ rule,1,50 bayesian inference,1,50 center of mass,1,50 event,1,50 expected value,2,50 exponential probability density,1,50 gaussian density function,1,50 hypothesis,1,50 hypothetical reasoning,1,50 likelihood function,1,50 likelihood functions,1,50 normal distribution,1,50 normalize,1,50 normalizing,1,50 not a probability,1,50 not definitive,1,50 outcome,1,50 posterior functions,1,50 posterior,1,50 prior functions,1,50 prior,1,50 priors,1,50 probability density function,1,50 probability density functions,2,50 probability density,3,50 probability,1,50 relative density,1,50 relative density function,1,50 relative density functions,1,50 risk of disease transmission,1,50 sd,1,50 standard deviation,2,50 uncertainty,1,50 under construction,1,50 uniform density function,1,50 update,1,50 compound interest,1,51 constrained optimization,1,51 discount,1,51 discounting function,1,51 discounting,1,51 interest rate,1,51 interest rates,1,51 interest,1,51 mortgage,1,51 net present value,1,51 nominal income stream,1,51 present value,3,51 stream,1,51 term,1,51 transitive,1,51 votes,1,51  more precisely it can be called ,1,52 angular acceleration,1,52 area,1,52 conservation of momentum,1,52 decomposed,1,52 decomposing a vector,1,52 energy,1,52 finite,1,52 forces,1,52 infinitesimal,1,52 instantaneous length,1,52 instantaneous rate of change,1,52 kinetic energy,1,52 latent,1,52 mechanics,1,52 rotation,1,52 statics,1,52 study of machines,1,52 torque,1,52 turbofan engines,1,52 turbojet engines,1,52 diffusion equation,1,53 essay,1,53 heat equation,1,53 optional background,1,53 quick review questions,1,53 random walk,1,53 three,1,53 wrongly**,1,53",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Glossary</span>"
    ]
  }
]