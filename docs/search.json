[
  {
    "objectID": "Preliminaries/01-quant-fun-space.html",
    "href": "Preliminaries/01-quant-fun-space.html",
    "title": "1  Quantity, function, space",
    "section": "",
    "text": "1.1 Quantity vs number\nA mathematical quantity is an amount. How we measure amounts depends on the kind of stuff we are measuring. The real-world stuff might be time, speed, force, price inflation, physiological and ecological systems … anything to which arithmetic can be applied!\nEveryone learns that arithmetic provides a set of patterns and rules for manipulating numbers. A number is a special, abstract kind of quantity. Among people who work with applied mathematics, experience has shown that numbers lack something that is important in many modeling situations: units. A quantity consists of two parts: (1) a number and (2) the units of measurement. From this perspective, a number is merely a quantity that has no units.\nWe learn about units in elementary school, where we study the kinds of units appropriate for measuring tangible stuff. For example, the capacity of a water bottle might be presented in liters, cups, fluid ounces, gallons, bushels, and so on. The area of a field or apartment can be measured in square-meters, hectares, square-feet, square yards, acres, and so on. One of the things school-children learn about units is to identify what kind of stuff is measured by any given unit. For example, a gallon and a cup measure the same kind of stuff: volume. A meter and an inch measure the same kind of stuff: length. Some students also learn how to convert between units. For example, a meter is about 40 inches, a gallon is exactly 16 cups and a cup is exactly 48 teaspoons. But there is no way to convert, say, an inch into a cup. These lessons on units are useful in everyday life, even if most adults have only a limited recollection of them.\nMoving beyond stuff like length, area, volume, and time can be difficult. You probably recognize kilometers-per-hour (or miles-per-hour) as a unit of velocity, but many student struggle when they first encounter something like meters-per-second-per-second, which is one way to measure acceleration.\nOne of the great uses of Calculus is to convert between different kinds of stuff. A simple, everyday example is relevant to transportation. You can convert from speed (say, miles-per-hour) to distance travelled by multiplying by the time needed to cover that distance. Or, for some purposes, you might need to convert into speed two different measurements: distance travelled and time taken. In later chapters, we will encounter many situations where such conversions between types of stuff is important.\nKeeping track of the type of stuff is an important habit when using Calculus for such conversions. In other words, you need to keep track of the units of each kind of stuff.\nWhen talking about the methods of Calculus, however, there is a shorthand that enables you to see what kind of conversion is being done. This shorthand can help you select the appropriate arithmetical or Calculus method for whatever model manipulation you need to do, and it is especially helpful for spotting and correcting errors. The shorthand involves explicitly noting the “dimension” of the quantity.\nUnits and dimensions are closely related. Knowing the units, you can readily figure out the dimension. But knowing the dimension makes it easier to design and check your calculations. Chapter 15 will describe units and dimension more thoroughly. For now, however, we will give just a hint, enough to understand why knowing and understanding dimensions will help tremendously in your calculation.\nIn particular, we want to emphasize the reason to think about using quantities rather than mere numbers. Whenever you see a quantity, you should expect to see units or their shorthand, dimension.\nAlthough there are hundreds of different kinds of units, we will need only four basic dimensions for most of the applications presented in this book. These are:\nConsider the familiar rules of arithmetic. The basic actions—addition, subtraction, multiplication, division—apply to any two numbers (although division by zero is not allowed). So 17 + 1.3 is 18.3, whatever those numbers are meant to represent. But the two quantities 17 inches and 1.3 seconds (L and T, respectively) cannot be meaningfully added. To apply addition and subtraction, the two quantities must be in the same unit, hence the same dimension. If you encounter someone adding quantities with different dimensions, you know you have spotted an error.\nOn the other hand, division and multiplication can work with any two quantities, regardless of their respective dimensions. In fact, division and multiplication are the basic arithmetic that enables us to construct new kinds of stuff from old kinds of stuff. For insight into the use of the word “dimension,” consider that a length times a length (L \\(\\times\\) L) gives an area (L2) and an area times a length (L2 \\(\\times\\) L) gives a volume (L^3). These correspond to one-, two- and three-dimensional objects respectively.\nThe mathematics of units and dimension are to the technical world what common sense is in our everyday world. For instance (and this may not make sense at this point), if people tell me they are taking the square root of 10 liters, I know immediately that either they are just mistaken or that they haven’t told me essential elements of the situation. It is just as if someone said, “I swam across the tennis court.” You know that person either used the wrong verb—walk or run would work—or that it wasn’t a tennis court, or that something important was unstated, perhaps, “During the flood, I swam across the tennis court.”",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quantity, function, space</span>"
    ]
  },
  {
    "objectID": "Preliminaries/01-quant-fun-space.html#sec-quant-vs-number",
    "href": "Preliminaries/01-quant-fun-space.html#sec-quant-vs-number",
    "title": "1  Quantity, function, space",
    "section": "",
    "text": "time, denoted T\nlength, denoted L\nmass, denoted M\nmoney, denoted V (for “value”)",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quantity, function, space</span>"
    ]
  },
  {
    "objectID": "Preliminaries/01-quant-fun-space.html#sec-functions",
    "href": "Preliminaries/01-quant-fun-space.html#sec-functions",
    "title": "1  Quantity, function, space",
    "section": "1.2 Functions",
    "text": "1.2 Functions\n\n\nOther examples of relationships:\n\nthe input is the altitude on your hike up Pikes Peak; the output is the air temperature. Typically, as you gain altitude the temperature goes down.\nthe input is the number of hours past noon; the output is the brightness of sunlight. As the afternoon progresses, the light grows dimmer, but only to a point.\n\nFunctions, in their mathematical and computing sense, are central to calculus. The introduction to this Preliminaries Block starts, “Calculus is about change, and change is about relationships.” The idea of a mathematical function gives a definite perspective on this. The relationship represented by a function is between the function’s input and the function’s output. The input might be day-of-year2 and the output cumulative rainfall up to that day. Every day it rains, the cumulative rainfall increases.\nA function is a mathematical concept for taking one or more inputs and returning an output. In calculus, we will deal mainly with functions that take one or more quantities as inputs and return another quantity as output.\n\n\nPay careful attention to our use of “input” and “output.” We avoid using the word “variable” because it is too vague. (For instance, it does not distinguish between what goes in and what comes out of a function.) There are two contexts in which we will use “variable,” neither of which has to do with inputs to functions. In talking about data, we will use “variable” in the statistical sense, meaning “a type of quantity” like height or pH. And in the final part of the text, involving systems whose configuration changes in time, we will use “variable” in the sense of “a quantity that varies over time.” Try to put the word “variable” out of mind for the present, until we get to discussing the nature of data.\nBut sometimes we will work with functions that take functions as input and return a quantity as output. And there will even be functions that take a function as an input and return a function as output.\nIn a definition like \\(f(x) \\equiv \\sqrt{\\strut x}\\), think of \\(x\\) as the name of an input. So far as the definition is concerned, \\(x\\) is just a name. We could have used any other name; it is only convention that leads us to choose \\(x\\). The definition could equally well have been \\(f(y) \\equiv \\sqrt{\\strut y}\\) or \\(f(\\text{zebra}) \\equiv \\sqrt{\\strut\\text{zebra}}\\).\nNotation like \\(f(x)\\) is also used for something completely different from a definition. In particular, \\(f(x)\\) can mean apply the function \\(f()\\) to a quantity named \\(x\\). You can always tell which is intended—function definition or applying a function—by whether the \\(\\equiv\\) sign is involved in the expression.\n\n\nLater in this Preliminaries Block, we will introduce the “pattern-book functions.” These always take a pure number as input and return a pure number as output. In the Modeling Block, we will turn to functions that take quantities—which generally have units—as input and return another quantity as output. The output quantity also generally has units.\nOne familiar sign of applying a function is when the contents of the parentheses are not a symbolic name but a numeral. For example, when we write \\(\\sin(7.3)\\) we give the numerical value \\(7.3\\) to the sine function. The sine function then does its calculation and returns the value 0.8504366. In other words, \\(\\sin(7.3)\\) is utterly equivalent to 0.8504366.\nIn contrast, using a name on it is own inside the parentheses indicates that the specific value for the input is being determined elsewhere. For example, when defining a function we often will be combining two or more functions, like this: \\[g(x) \\equiv \\exp(x) \\sin(x)\\] or \\[h(y,z) \\equiv \\ln(z) \\left(\\strut\\sin(z) - \\cos(y)\\right)\\ .\\] The \\(y\\) and \\(z\\) on the left side of the definition are the names of the inputs to \\(h()\\).3 The right side describes how to construct the output, which is being done by applying \\(\\ln()\\), \\(\\sin()\\) and \\(\\cos()\\) to the inputs. Using the names on the right side tells us which function is being applied to which input. We won’t know what the specific values those inputs will have until the function \\(h()\\) is being applied to inputs, as with \\[h(y=1.7, z=3.2)\\ .\\]\nOnce we have specific inputs, we (or the computer) can plug them into the right side of the definitionto determine the function output: \\[\\ln(3.2)\\left(\\sin(3.2) - \\strut \\cos(1.7)\\right) = 1.163(-0.0584 + 0.1288) =-0.2178\\ .\\]\n\n\nWe will introduce the idea of “spaces” in ?sec-spaces-intro. A function maps each point in the function’s input space into a single point in the function’s output space. The input and output spaces are also known respectively as the “domain” and “range” of the function.\n\n\n\n\n\n\nMath in the World: Functional gunnery\n\n\n\nThe various mathematical functions that we will be studying in this book are in the service of practical problems. But there are so many such problems, often involving specialized knowledge of a domain or science, engineering, economics, and so on, that an abstract mathematical presentation can seem detached from reality.\nVideo 1.1 is a training movie from 1945 for gunners in B-29 bombers. The gunner tries to position the gun (the function output) so that a shell and the plane will intersect. There are many inputs to the function, which has been implemented by electronics. The function itself is literally a black box. The inputs are provided by a human gunner training a telescope on a target and setting control dials. The ultimate output is the deflection of the guns in a remote turret. The main function is composed of several others, such as a function that outputs target range given the target size based on knowledge of the size of the target and how large it appears in the telescopic sight.\n\n\n\n\n\n\n\nVideo 1.1: A training video from World War II: Gunnery in the B-29: How to Shoot.\n\n\n\nDividing the gunnery task into a set of inputs and a computed output allows for a division of labor. The gunner can provide the skills properly trained humans are good at, such as tracking a target visually. The computer provides the capabilities—mathematical calculation—to which electronics are well suited. Combining the inputs with the calculation provides an effective solution to a practical problem.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quantity, function, space</span>"
    ]
  },
  {
    "objectID": "Preliminaries/01-quant-fun-space.html#sec-space-intro",
    "href": "Preliminaries/01-quant-fun-space.html#sec-space-intro",
    "title": "1  Quantity, function, space",
    "section": "1.3 Spaces",
    "text": "1.3 Spaces\nCalculus is largely about change, and change involves movement. To move, as you know, means to change location. But we often use location as a metaphor for other things. Consider, for example, the everyday expression, “The temperature is getting higher.” This is not pointing to a thermometer rising up in the air, but to a quantity as it changes.\nTo represent a quantity as it changes we need to provide scope for movement. One way to do this is familiar from schooling: the number line. Each location on the number line corresponds to a possible value for a quantity. The particular value of that quantity at a specific moment in time is represented as a tiny bead on that line. The line as a whole encompasses many other locations. The changing quantity is the movement of the bead.\nFigure 1.1 shows a number line as conventionally drawn. I have placed two different colored dots on the line to correspond to two different quantities: \\(\\color{blue}{6.3^\\circ\\ \\text{C}}\\) and \\(\\color{magenta}{-4.5^\\circ \\text{C}}\\). (You only know that the line is about temperature because I told you.)\n\n\n\n\n\n\n\n\nFigure 1.1: A number line drawn in the conventional style suitable for representing temperatures near freezing.\n\n\n\n\n\nThe graphical element representing the set of possibilities is the horizontal line segment. The tick marks and labels are added to enable you to translate any given location into the corresponding quantity. In mathematics texts, it’s common to put arrowheads at the ends of the line segment, perhaps to remind you that a line is infinite in length. But in other disciplines, no infinity or arrowheads are needed: the picture is just a scale to help people translate location into quantity.\nI could have placed many dots on the line to represent many different particular quantitities. Of course, all the dots would be representing temperatures in \\(^\\circ\\text{C}\\) since that is the sort of quantity that the number line in Figure 1.1 represents.\nA more general way for representing pairs of quantities is the coordinate plane, which Figure 1.2 shows in the mathematics-text style. (In Chapter 4, we will switch to another style more commonly used across disciplines.)\n\n\n\n\n\n\nFigure 1.2: The coordinate plane drawn in the style common to mathematics texts. Source: Wikipedia\n\n\n\nEvery location in the coordinate plane is a possibility. A specific pair of quantities is displayed by placing a dot. Figure 1.2 has four such dots, corresponding to four distinct pairs of quantities.\nThe space annoted by the coordinate axes and grid is two-dimensional, analogous to a table-top or a piece of paper or a computer display’s surface. Two-dimensional space accommodates change in each of the two quantities.\nEveryday life acquaints us well with three-dimensional spaces, where each location corresponds to a possible value for each of three quantities. Displaying a three-dimensional space is difficult to do well because conventional displays show only two dimensions. Figure 1.3 shows one style that uses perspective and shading to create the impression of a 3-D scene.\n\n\n\n\n\n\nFigure 1.3: One of many styles for displaying a three-dimensional space. Source\n\n\n\nDrawings of one-dimensional space (Figure 1.1) or two-dimensional space (Figure 1.2) make it straightforward to read off the quantitative value corresponding to any location. Already in 3-dimensional space, reading quantitative coordinates is difficult and requires conscious mental effort. We will make only limited use of 3-D displays.\nConsider now what the different dimensional spaces permit in terms of movement, that is, how quantities can change. The number line permits one kind of movement, left-right in Figure 1.1. Even though we use two words to name the kind of movement—“left” and “right”—we still consider movement in either opposing direction as one kind of movement. Left is the opposite of right. The coordinate plane permits two kinds of movement: left-right and up-down. A three-dimensional space permits three kinds of movement: left-right, up-down, nearer-farther. We often say that each type of movement is along an axis. as is conventional for one- and two-dimensional spaces. The number line has one axis, the coordinate plane has two axes, and three-dimensional space has three.\nFigures ?fig-number-line, 1.2, and 1.3 are conventional drawings of one-, two-, and three-dimensional spaces respectively. What about four- or higher-dimensional spaces? Many people put their foot down here and refuse to accept such a thing. Some others will point to the Theory of Relativity where an essential concept is “space-time,” a four-dimensional space sometimes denoted as \\((x, y, z, t)\\). True though this be, it does not much appeal to intuition and for a good reason. We are free to move objects in their x-, y-, and z-coordinates, but we have no control over time.\nEngineers, statisticians, physicists, and others often use the phrase “degree of freedom” to refer to a type of movement. In English, we have many phrases for different types of movement: in-and-out, back-and-forth, clockwise-and-counter-clockwise, nearer-and-farther, up-and-down, left-and-right, north-and-south, east-and-west, and so on. When imagining time machines, or showing photos from our recent trip, we speak of going forward in time or going back in time: forward-and-back. (Note the word “going,” which emphasizes the idea of movement rather than position.)\nHealth professions learn additional names for kinds of movement: adduction-abduction, flexion-extension, internal-vs-external rotation.\nThinking in terms of movement, it’s easier to construct a depiction of even four-, five-, and higher-dimensional space. Figure 1.4 shows the movement of a robotic hand with six degrees of freedom:\n\n\n\nActive R chunk 1.1: Six degrees of freedom for the robot hand.\n\n\n\n\nswiveling of the base\n\n2-4. rotation around each of the three knuckles\n\nswiveling at the wrist\n\n\nfingers moving closer together or farther apart\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.4: A robot hand with six degrees of freedom. Source\n\n\n\nThere can be multiple ways of forming coordinates for the same space. In terms of the mechanism for the robotic hand, it’s configuration can be specified by the six degrees of freedom enumerated in Active R chunk 1.1. But from the point of view of a person training the robot, it might be more convenient to think about the configuration in terms of the six quantities given in Active R chunk 1.2.\n\n\n\nActive R chunk 1.2: A different way of describing the configuration of the robot hand that is better suited for a user training the robot in some action.\n\n\n\n1-3. The location of the wrist in everyday three-dimensional space. (x,y,z)\n4-5. The direction that the fingers point in. (Two angles often called “azimuth” and “elevation.”)\n\nThe distance between the fingers (d).\n\n\n\n\n\nTo train the robot to perform a specific movement, the engineer specifies a sequence of configurations. Each of the configurations corresponds to a dot in the six-dimensional space. An important Calculus method, introduced in ?sec-splines, is to create a smooth, continuous path connecting the dots called a trajectory which, in this case, depicts movement through six-dimensional space.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quantity, function, space</span>"
    ]
  },
  {
    "objectID": "Preliminaries/01-quant-fun-space.html#all-together-now",
    "href": "Preliminaries/01-quant-fun-space.html#all-together-now",
    "title": "1  Quantity, function, space",
    "section": "1.4 All together now",
    "text": "1.4 All together now\nThe three mathematical concepts we’ve been discussing—quantities, functions, spaces—are used together.\nA quantity can be a specific value, like 42.681\\(^\\circ\\)F. But you can also think of a quantity more broadly, for instance, “temperature.” Naturally, there are many possible values for temperature. The set of all possible values is a space. And, using the metaphor of space, the specific value 42.681\\(^\\circ\\)F is a single point in that space.\nFunctions relate input quantities to a corresponding output quantity. A way to think of this—which will be important in Chapter 4 —is that a function is a correspondence between each point in the input space and a corresponding point in the output space. By mathematical convention, the output space in Calculus is always one-dimensional.\nEvery function has a set of legitimate potential inputs, a region in the input space. This input-space region is called the domain of the function. For some functions, the possible output values occupy the whole of the one-dimensional output space. Other functions use only regions of the one-dimensional output space. The set of possible outputs is called the range of the function.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quantity, function, space</span>"
    ]
  },
  {
    "objectID": "Preliminaries/01-quant-fun-space.html#exercises",
    "href": "Preliminaries/01-quant-fun-space.html#exercises",
    "title": "1  Quantity, function, space",
    "section": "1.5 Exercises",
    "text": "1.5 Exercises",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quantity, function, space</span>"
    ]
  },
  {
    "objectID": "Preliminaries/01-quant-fun-space.html#footnotes",
    "href": "Preliminaries/01-quant-fun-space.html#footnotes",
    "title": "1  Quantity, function, space",
    "section": "",
    "text": "Source: Oxford Languages↩︎\n“Day-of-year” is a quantity with units “days.” It starts at 0 on midnight of New Year’s Eve and ends at 365 at the end of day on Dec. 31.↩︎\nSometimes, we will use both a name and a specific value, for instance \\(\\sin(x=7.3)\\) or \\(\\left.\\sin(x)\\Large\\strut\\right|_{x=7.3}\\)↩︎",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quantity, function, space</span>"
    ]
  },
  {
    "objectID": "Preliminaries/04-graphs-and-graphics.html",
    "href": "Preliminaries/04-graphs-and-graphics.html",
    "title": "4  Visualizing functions",
    "section": "",
    "text": "4.1 Drawing function graphs with R/mosaic\nIn technical vocabulary, the word “graph” refers to the kind of display in ?fig-function-graph-1. The word “graphic” is not technical and is more general than graph. A “graphic” is any kind of visual presentation of art or information. This is important to note, since the majority of graphics in this Calculus book will not be “graphs.”\nA “graph” is a suitable format for displaying a function with one input. (Remember, all functions in this book will have one output.) In modeling, it’s typical to use functions of multiple inputs. Consequently, graphics involving functions will typically not be “graphs.” But, the reader is undoubted familiar with graphs from high school, so we start there so that we can focus on the computer commands before we introduce other modes.\nActive R chunk 4.1 shows an R/mosaic graphics command that produces a graphic showing the graph of a function. (Make sure to press “Run code” to see the graphic.)\nYou see the typical components of an R/mosaic command: a function name (slice_plot()) followed by a pair of parentheses. Inside the parentheses are two arguments, both of which are required.\nA tilde expression suitable for slice_plot() must have a single input name on the right-hand side (“RHS” for short). The left-hand side (“LHS”) is an expression of the sort you have already used in makeFun().\nThe second argument to slice_plot() will always be domain() with a named argument matching the name from the tilde expression RHS. The value given for the named argument contains the left- and right-bounds for the horizontal axis.\nRemember that a function’s domain is the space of all possible valid inputs to the function. The domain for most of the functions you studied in high school is infinite, for instance extending from \\(-\\infty\\) to \\(\\infty\\). A graphical domain, in contrast, is usually a finite part of the function domain.\nThe R/mosaic command you will use to graph a mathematical function is slice_plot(). (We will get into the reasons for this name later.) slice_plot() takes two arguments which are, as always, separated by a comma. Here are two examples producing graphs of different functions:\nThe first argument to slice_plot() tells what function is to be plotted. The argument is a tilde expression of the same sort you saw in Chapter 3.\nThe second element establishes the graphics domain. It will always be the name domain() with a named argument that matches the name for the input on the RHS of the tilde expression. In (a) the argument name is x while in (b) the argument name is y.\nNamed arguments always consist of a name followed by an equal sign. After the equal sign comes the value of the argument. In (a), the value is 0 : 5 which means “zero to five.” In (b), the value is -3.5 : 3 standing for “minus three-point-five to three.” Each of these values is written with a colon (:). This is a bit of punctuation meaning “to,” as in “zero to five.”1",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/04-graphs-and-graphics.html#drawing-function-graphs-with-rmosaic",
    "href": "Preliminaries/04-graphs-and-graphics.html#drawing-function-graphs-with-rmosaic",
    "title": "4  Visualizing functions",
    "section": "",
    "text": "Active R chunk 4.1\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\na tilde expression for the function you wish to graph\nthe graphical domain to be covered by the horizontal axis\n\n\n\n\n\n\nslice_plot(sin(x) ~ x, domain(x = 0 : 5)\nslice_plot(3 * y + b  ~ y, domain(y = -3.5 : 3)",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/04-graphs-and-graphics.html#sec-drawing-contour-plots",
    "href": "Preliminaries/04-graphs-and-graphics.html#sec-drawing-contour-plots",
    "title": "4  Visualizing functions",
    "section": "4.2 Drawing contour plots of functions",
    "text": "4.2 Drawing contour plots of functions\nIn general, the functions we use in modeling can have multiple inputs. So best not to get to fixated on the format in Active R chunk 4.1. That format is appropriate only for functions that have one input.\nOur preferred visual format for a function of two inputs is the “contour plot.” You might be familiar with the idea if you have ever had to use a topographic map for hiking. Here’s an example:\n\n\n\n\nh &lt;- makeFun(3*x - 2*x*y - y^2 ~ x & y)\ncontour_plot(h(x, y) ~ x & y, domain(x = -1:1, y = 0:2))\n\n\n\n\n\n\n\n\n\n\nFigure 4.2\n\n\n\nThe function being plotted is named h(). Since h() takes two inputs, the graphical domain needs to be a space with two dimensions. We signal this by giving two arguments to domain(), one for each dimension of the input space. Of course, the names used within domain() have to match the names used in the tilde expression. Note that we use only one domain() call. It’s the two arguments that establish that the domain will have two dimensions.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/04-graphs-and-graphics.html#spaces-and-graphs-of-functions",
    "href": "Preliminaries/04-graphs-and-graphics.html#spaces-and-graphs-of-functions",
    "title": "4  Visualizing functions",
    "section": "4.3 Spaces and graphs of functions",
    "text": "4.3 Spaces and graphs of functions\nWe are going to step back from the instructions for drawing graphics like ?fig-function-graph-1 in order to give you a better perspective on the essentials of making graphical displays of functions. This will help in reading and interpreting graphics, especially graphics for functions of multiple inputs such as in Figure 4.2.\nAs you know, the domain of a function is the set of all possible valid inputs to that function. In Chapter 1 we defined a space to be a set of possibilities. Thus, a function domain can be seen as a space, the “input space” for the function.\nIn ?fig-function-graph-1, the input space is a number line. Each point in the space is a possible input value to the function.\n\n\n\n\n\n\nFigure 4.3: The input space for the function graphed in ?fig-function-graph-1.\n\n\n\nThere is also an output space for a function, the set of all possible outputs. Figure 4.4 shows a number line that is the output space for the function graphed in ?fig-function-graph-1.\n\n\n\n\n\n\nFigure 4.4\n\n\n\nThe function itself tells us, for every point in the input space, what is the corresponding point in the output space. Figure 4.5 gives an example. The input and output spaces are shown as number lines, while the function is indicated by the thin colored lines.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.5: A function relates points from the input space to their corresponding location in output space.\n\n\n\nFor example, the function displayed in Figure 4.5 translates an input value of -4 (see the green line) into an output value of 3.7.\nThe display in Figure 4.5 is very hard to interpret. Also, to avoid the display being filled up with colored ink, we can show only a few of the input/output pairs.\nA function graph (like ?fig-function-graph-1) is much easier to read. But let’s be clear about where the spaces are shown. In a function graph, the input space is shown horizontally and the output space is shown vertically as in Figure 4.6.\n\n\n\n\n\n\nFigure 4.6: A function graph shows the input space horizontally and the output space vertically.\n\n\n\nNow for every point in the input space the function specifies a corresponding point in the output space. We mark the correspondence with a dot. The horizontal coordinate tells us what is the input value. The vertical coordinate tells us what is the corresponding output value. To show the function as a whole—the output corresponding to every input—we would need a lot of dots! So many dots that they collectively give an appearance of a thin curve, the curve seen in ?fig-function-graph-1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.7: The mapping from Figure 4.5 translated into the form of a graph. The input space is marked by the horizontal axis and the output space by the vertical axis. Each of the arrows in Figure 4.5 is represented by a point, whose x-coordinate is the position of the tail of the arrow in the input space and whose y-coordinate is the position of the head of the arrow in the output space. This is the graph of the function.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/04-graphs-and-graphics.html#input-and-output-spaces",
    "href": "Preliminaries/04-graphs-and-graphics.html#input-and-output-spaces",
    "title": "4  Visualizing functions",
    "section": "4.4 Input and output spaces",
    "text": "4.4 Input and output spaces\nThis background about spaces is important for understanding functions because, for functions with two or more inputs, the input space and the output space can be depicted in different ways. For example, in the contour plot of Figure 4.2, the input space is the two-dimensional space in the plane of the display (that is, the paper or screen depending on how you are reading this). The output space for the contour plot is depicted using curves, colors, and labels. There is one curve for each output value. Only a handful of output values are shown, but you can get pretty close to estimating the output value even for inputs not on the curve.\nThe space of all possibilities (y, z, output) is three-dimensional, but very few of those possibilities are consistent with the function to be graphed. You can imagine our putting dots at all of those consistent-with-the-function points, or our drawing lots and lots of continuous curves through those dots, but the cloud of dots forms a surface; a continuous cloud of points floating over the (y, z) input space.\nFigure 4.8 displays this surface. Since the image is drawn on a two-dimensional screen, we have to use painters’ techniques of perspective and shading. In the interactive version of the plot, you can move the viewpoint for the image which gives many people a more solid understanding of the surface.\nMath textbooks—but not so much this one!—often display functions with two inputs using a three-dimensional space. This space is made by laying the two-dimensional input space on a table, and sticking the one-dimensional output space perpendicular to the table. Each point in the input space has a corresponding value in the output space which could, in principle, be marked with a dot. The whole set of dots, one for each value in the input space, appears as a surface floating over the table.\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.8: A function g(x, y) ~ x & y drawn as a surface. The function output value, for any given \\((x, y)\\) pair, is the height of the surface above the \\((x, y)\\)-plane.\n\n\n\nA variety of drawing techniques such as transparency, color, and interactive annotation are used to help us perceive a two-dimensional surface embedded in a three-dimensional space. (Place your cursor within the space delimited by the x, y, z axes to see the annotations.)\nPretty as such surfaces are, a contour plot (Figure 4.9) provides a good view of the same function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.9: The function from Figure 4.8 shown in contour-plot format.",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/04-graphs-and-graphics.html#why-slice_plot",
    "href": "Preliminaries/04-graphs-and-graphics.html#why-slice_plot",
    "title": "4  Visualizing functions",
    "section": "4.5 Why “slice_plot()”?",
    "text": "4.5 Why “slice_plot()”?\nSaying “graph” for a display of \\(f(x)\\) versus \\(x\\) is correct and reasonable. But in MOSAIC Calculus we have another point to make.\nAlmost always, when mathematically modeling a real-world situation or phenomenon, we do not try to capture every nuance of every relationship that might exist in the real world. We leave some things out. Such simplifications make modeling problems easier to deal with and encourage us to identify the most important features of the most important relationships.\n\n\n\n\n\n\n\n\nFigure 4.10: A hypothetical relationship among three quantities.\n\n\n\n\n\nIn this spirit, it is useful always to assume that our models are leaving something out and that a more complete model involves a function with more inputs than the present candidate. The present candidate model should be considered as a slice of a more complete model. Our slice leaves out one or more of the input quantities in a more complete model.\nAs you become practiced reading contour plots, you might prefer to read this one as a hilltop (shaded yellow) side-by-side with a hollow or bowl (shaded purple), with green, almost level flanks at the left and right edges of the frame.\nTo illustrate this, suppose that the actual system involves relationships among three quantities, which we represent in the form of a function of two inputs, as shown in Figure 4.10. (The third quantity in the relationship is the output of the function.)\nThe most common forms of slice involve constructing a simpler function that has one input but not the other. For example, our simpler function might ignore input 22. There are different ways of collapsing the function of two inputs into a function of one input. An especially useful way in calculus is to take the two-input function and set one of the inputs to a constant value.\nFor instance, suppose we set input 22 to the constant value 1.5. This means that we can consider any value of input 1, but input 2 has been replaced by 1.5. In Figure 4.11, we’ve marked in red the points in the contour plot that give the output of the simplified function.\n\n\n\n\n\n\n\n\n(a) Contour plot of a function with two inputs. The red path shows points in the input space where input_2 is held constant at 1.5.\n\n\n\n\n\n\n\n\n\n(b) Values of the function at the points along the red path in (a). Since there is effectively one input, the function can be presented as a graph.\n\n\n\n\n\n\nFigure 4.11\n\n\n\nEach point along the red line corresponds to a specific value of input #1. From the contours, we can read the output corresponding to each of those values of input #1. This relationship, output versus input #1 can be drawn as a mathematical graph (to the right of the contour plot). Study that graph until you can see how the rising and falling parts of the graph correspond to the contours being crossed by the red line.\nSlices can be taken in any direction or even along a curved path! The blue line in Figure 4.12 shows the slice constructed by letting input 2 vary and holding input 1 at the constant value 0.\n\n\n\n\n\n\n\n\n\n(a) The same function as shown in Figure 4.11 but with a slicing path drawn by holding input_1 at zero.\n\n\n\n\n\n\n\n\n\n(b) A graph of the value of the function along the blue slice. Note the violation of convention: The graph has been flipped on its side so that the input axis aligns with the direction of the slice.\n\n\n\n\n\n\n\nFigure 4.12",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/04-graphs-and-graphics.html#exercises",
    "href": "Preliminaries/04-graphs-and-graphics.html#exercises",
    "title": "4  Visualizing functions",
    "section": "4.6 Exercises",
    "text": "4.6 Exercises",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizing functions</span>"
    ]
  },
  {
    "objectID": "Preliminaries/04-graphs-and-graphics.html#footnotes",
    "href": "Preliminaries/04-graphs-and-graphics.html#footnotes",
    "title": "4  Visualizing functions",
    "section": "",
    "text": "R experts should note that the colon has a different meaning within domain() than it does generally in R.↩︎",
    "crumbs": [
      "PRELIMINARIES",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizing functions</span>"
    ]
  },
  {
    "objectID": "preliminaries-part.html",
    "href": "preliminaries-part.html",
    "title": "PRELIMINARIES",
    "section": "",
    "text": "Calculus is about change, and change is about relationships. Consider the complex and intricate network of relationships that determine climate: a changing climate implies that there is a relationship between, say, global average temperature and time. Scientists know temperature changes with levels of CO2 and methane which themselves change due to their production or elimination by atmospheric and geological processes. A change in one component of climate (e.g., ocean acidification or pH level) provokes change in others.\nTo describe and use the relationships we find in the natural or designed world, we build mathematical representations of them. We call these mathematical models. On its own, the word “model” signifies a representation of something in a format which serves a specific purpose. A blueprint describing the design of a building is an everyday example of a model. The blueprint represents the building but in a way that is utterly different from the building itself. Blueprints are much easier to construct or modify than buildings, they can be carried and shared easily. Two of the purposes of a blueprint is to aid in the design of buildings and to communicate that design to the people securing the necessary materials and putting them together into the building itself.\n\n\nModels provide the link between the real world and the abstractions of mathematics.\nAtmospheric scientists build models of climate whose purpose is to explore scenarios for the future emission of greenhouse gasses. The model serves as a stand-in for the Earth, enabling predictions in a few hours of decades of future change in the climate. This is essential for the development of policies to stabilize the climate.\n\n\nA concise definition of a “model” is a representation for a purpose. Defining the purpose for your model is a crucial first step in building a mathematical representation that will serve that purpose. Useful models of the same real-world setting can be very different, depending on the purpose. For instance, one routine use for a model is to make a prediction. But other models are intended for exploring the connections among the components of the system being modeled.\nDesigning a building or modeling the climate requires expertise and skill in a number of areas. Nonetheless, constructing a model is relatively easy compared to the alternative. Models make it relatively easy to extract the information that is needed for the purpose at hand. For instance, a blueprint gives a comprehensive overview of a building in a way that is hard to duplicate just by walking around an actual building.\nModels are easy to manipulate compared to reality, easy to implement (think “draw a blueprint” versus “construct a building”), and easy to extract information from. We can build multiple models and compare and contrast them to gain insight into the real-world situation behind the models.\nA mathematical model is a model made out of mathematical and computational stuff. Example: a bank’s account books are a model made mostly out of numbers. But in technical areas—science and engineering are obvious examples, but there are many other fields, too—numbers don’t get you very far. By learning calculus, you gain access to important mathematical and computational concepts and tools for building models and extracting information from them.\nA major use of mathematics is building models constructed out of mathematical concepts and objects. The chapters in this Preliminaries section of this book introduce some of the fundamental mathematical entities that are the heart of modeling.",
    "crumbs": [
      "PRELIMINARIES"
    ]
  },
  {
    "objectID": "Modeling/16-modeling-scientific-method.html",
    "href": "Modeling/16-modeling-scientific-method.html",
    "title": "16  Modeling and the scientific method",
    "section": "",
    "text": "16.1 Example: Cooling water\nChapter 11 presented data on water cooling from near boiling to room temperature. (See Figure 11.2.) Prof. Stan Wagon of Macalester College collected the data with a purpose in mind: to see whether Newton’s Law of Cooling provides a good description of the actual physical process. In particular, Wagon knew that a simple application of Newton’s Law implies that the water temperature will decay exponentially.\nFigure 16.1 shows an exponential function fitted to the water-cooling data. The fitted function is stored as Mod1_fun() with the 1 identifying it as the first of possibly several models.\nAt first glance, the exponential function seems a good match to the data. But what do we mean by “good?” Is “good” good enough? And “good enough” for what? The experienced modeler should always have in mind the criterion by which to evaluate “good enough.” These criteria, in turn, are shaped by the purpose of building the model.\nBeware of general-purpose criteria. For instance, a general-purpose criterion is to compare the shape of the function with the data. The exponential model slopes and curves in a similar manner to the data. The deviation of the model output from the data is, at worst, just a few degrees. This is small compared to the 50\\(^\\circ\\)C change in temperature throughout the experiment.\nAnother widely used general-purpose statistical measure of the quality of the match is the R2 (R-squared) value. Mathematically, R2 is always between 0 and 1. (See ?sec-stat-modeling, where the mathematics of R2 is introduced.) When R2 = 1, the match is perfect. For the Mod1_fun() and the cooling water data, R2 = 0.991. Many scientists would interpret this as “almost perfect,” although things are sometimes more complicated … as here.\nWagon’s purpose was not simply to see if an exponential curve resembles the data. Instead, he wanted to know if Newton’s Law of Cooling was consistent with the observed cooling over time. The exponential shape of temperature versus time is just one consequence of Newton’s Law. Another is that the water temperature should eventually reach equilibrium at room temperature.\nThe fitted exponential model fails here, as you can see by zooming in on the right tail of Figure 16.1. In Figure 16.2, we zoom in on the data’s left and right tails.\nIn the scientific method, one takes a theory (Newton’s Law of Cooling), makes predictions from that theory, and compares the predictions to observed data. If the predictions do not match the theory—here, that the water should cool to room temperature—then a creative process is called for, replacing or modifying the theory. The same applies when a mathematical model fails to suit its original purpose.\nThe creative process of constructing or modifying a theory is not primarily a matter of mathematics, it usually involves expert knowledge about the system being modeled. In the case of a cooling mug of water, the “expertise” can be drawn from everyday experience. For instance, everybody knows that it is not entirely a matter of the water cooling off; the mug gets hotter and then cools in its own fashion to room temperature.\nWagon’s initial modification of the theory went like this: Newton’s Law still applies, but with the water in contact with both the room and, more strongly, the mug. To build this model, he needed to draw on the mathematics of dynamical systems (to be introduced in Block 5), producing a new parameterized formula of water temperature versus time.\nWagon went on to check the water-mug-room theory against the data. He found that the improved model did not completely capture the temperature-versus-time data. He applied more expertise, that is, he considered something we all observe: water vapor (“steam”) rises from the water in the mug. This prompted some experimental work where a drop of oil was placed on the water surface to block the vapor. This experiment convinced him that a new revision to the model was called for that would include the cooling due to evaporation.\nWithout being able to anticipate the settings and purposes for your model building, we can’t pretend to teach you the real-world expertise you will need. But we can do something to help you move forward in your work. The remaining sections of this chapter introduce some general-purpose mathematical approaches to refining models. For instance, rather than using a single exponential function, Wagon used a linear combination of exponentials for his modeling. We will also try to warn you of potential pitfalls and ways you can mislead yourself.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling and the scientific method</span>"
    ]
  },
  {
    "objectID": "Modeling/16-modeling-scientific-method.html#example-cooling-water",
    "href": "Modeling/16-modeling-scientific-method.html#example-cooling-water",
    "title": "16  Modeling and the scientific method",
    "section": "",
    "text": "Mod1_fun &lt;- mosaic::fitModel(temp ~ A*exp(-k*time) + C, \n                             data = CoolingWater,\n                             start = list(C = 30, k = 1/20, A = 70))\ngf_point(temp ~ time, data = CoolingWater, alpha = .15 ) |&gt;\n  slice_plot(Mod1_fun(time) ~ time, color = \"blue\") |&gt;\n  gf_labs(x = \"Time (minutes)\", y=\"Temperature (deg. C)\")\n\n\n\n\n\n\n\n\n\n\nFigure 16.1: An exponential model (blue) fitted to the CoolingWater data. \\[T(t) \\equiv 27.0 + 62.1\\ e^{-0.021 t}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.2: The exponential model does not capture the water’s initial almost-boiling temperature. To judge from the \\(A\\) coefficient presented in the caption of Figure 16.1, the room temperature is 27\\(^\\circ\\)C, while the data themselves indicate a room temperature a little less than 25\\(^\\circ\\)C.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling and the scientific method</span>"
    ]
  },
  {
    "objectID": "Modeling/16-modeling-scientific-method.html#identifying-systematic-discrepancies",
    "href": "Modeling/16-modeling-scientific-method.html#identifying-systematic-discrepancies",
    "title": "16  Modeling and the scientific method",
    "section": "16.2 Identifying systematic discrepancies",
    "text": "16.2 Identifying systematic discrepancies\nRefer to Figure 16.1, which shows the measured temperature data along with the initial fitted exponential model. The data systematically differ from the model in four places: the model underestimates the temperature early in the process, then overestimates for an interval, returns later to an underestimation, and finishes up with an overestimation.\nWhen assessing a model, it is a good practice to calculate the difference between the observed data and the model value. Figure 16.3 shows this difference, calculated simply by subtracting from each data point the model value at the corresponding time. In statistical nomenclature, such differences are called the “residuals” from the model.\n\n\n\n\ngf_point(temp - Mod1_fun(time) ~ time, data = CoolingWater) |&gt;\n  gf_labs(x = \"Time (minutes)\", \"Residual (deg C)\")\n\n\n\n\n\n\n\n\n\n\nFigure 16.3: Residuals versus time. The comparatively smooth structure indicates systematic deviations between the model and the data.\n\n\n\nThe presence of noise in any measurement is to be expected. You can see such noise in Figure 16.3 in the small, irregular, trembling fluctuations. Conversely, any smooth, slowly changing pattern in the residuals is considered systematic variation. Here, those smooth variations are much larger in amplitude than the irregular noise, indicating that we should investigate the systematic variation more closely.\nThis is a chance for the modeler to speculate on what might be causing the systematic deviations. A good place to start is with the largest residuals. In this case, that’s at the beginning of the temperature recording. Note that early in the recording, the recorded temperature falls much faster than in the model. This is perhaps clearer with reference to Figure 16.1.\nWhat physical process might lead to the initial fast cooling of the water? Answering this question requires both detailed knowledge of how the system works and some creativity. As mentioned above, Wagon speculated that the water might be cooling in two ways: i) heat moving from the water to the air in the room and ii) heat moving from the water into the adjacent mug. This second process can be fast, making it a candidate for explaining the residuals in the early times of the experiment.\nA quick test of the speculation is to construct a linear combination of two exponential processes, one fast and one slower. This would be difficult to do by eye using the techniques of Chapter 8, but fitModel() can accomplish the task so long as we can give a rough estimate of suitable numerical values for the parameters. Figure 16.4 shows the resulting function and the residuals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.4: The water-cooling model with two exponential processes, one fast and one slow, and the residuals from the observed data.\n\n\n\nFigure 16.4 shows a much-improved match between the model and the data. The residuals are about one-tenth as large as those from the original model.\nDepending on the purpose for which the modeling is being done, this might be the end of the story. The model is within a few tenths of a degree C from the data, good enough for purposes such as prediction. For Prof. Wagon, however, the purpose was to investigate how complete an explanation Newton’s Law of Cooling is for the physical process. He concluded that the residuals in Figure 16.4 still show systematic patterns.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling and the scientific method</span>"
    ]
  },
  {
    "objectID": "Modeling/16-modeling-scientific-method.html#refine-the-data-refine-the-experimental-technique",
    "href": "Modeling/16-modeling-scientific-method.html#refine-the-data-refine-the-experimental-technique",
    "title": "16  Modeling and the scientific method",
    "section": "16.3 Refine the data, refine the experimental technique",
    "text": "16.3 Refine the data, refine the experimental technique\nThe initial model building often suggests how new, informative data might be collected. It’s impossible to generalize this to all situations, but in Prof. Wagon’s work, two possibilities arise:\n\nMeasure the temperature of the mug directly, as well as the temperature of the water.\nStifle other possible physical processes, such as evaporation of water from the top surface of the liquid, and collect new data\n\nProf. Wagon managed (2) by putting a small drop of oil on the water right after it was poured into the mug. This created a thin layer that hindered evaporation. He expected that canceling this non-Newtonian process would make the Newton model a better fit to the new data, providing an estimate of the magnitude of the evaporation effect.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling and the scientific method</span>"
    ]
  },
  {
    "objectID": "Modeling/16-modeling-scientific-method.html#including-new-influences",
    "href": "Modeling/16-modeling-scientific-method.html#including-new-influences",
    "title": "16  Modeling and the scientific method",
    "section": "16.4 Including new influences",
    "text": "16.4 Including new influences\nIn many situations, there is data on more than one factor that might come into play in determining the response variable. The mathematical methods relating to linear combinations covered in Block 3 provide the basic toolkit for considering such new influences. In Block 5, you will see many examples of situations where the system’s behavior depends critically on the interaction between two competing factors.\nThe closely related topic of statistical modeling provides essential concepts and tools for deciding whether to incorporate an additional input to a model function. Part of the importance of a statistical approach comes from a mathematical paradox that will be discussed in Block 4. Whenever you have a model of a response variable in terms of explanatory variables there will be residuals. Adding in a new response variable, even if it is utterly unrelated to the system, will make the residuals smaller. Statistical method provides the means to avoid mistakenly including the new response variable by comparing the actual reduction in residuals to what would be expected from a generic random variable.\nAnother important statistical topic is causality: reasoning about what causes what. Often, the modeler’s understanding of how the system under study works can guide the choice of additional variables to include in a model.\nSpace precludes detailed consideration of the statistical phenomena in this book, although the techniques used are well within the capabilities of anyone who has completed this book through Block 3.\nIt’s worth point out a regretable fact of life in the academic world covering the quantitative science. At almost all institutions (as of 2024), calculus is taught in a manner that is totally disconnected from statistics and data science. In part, this is because the professional training of mathematics instructors does not include an emphasis on statistics. This is an artifact of history and is slowly being addressed. MOSAIC Calculus is the result of a successful program to integrate modeling, statistics, calculus, and computing—the M S C of MoSaiC. To fit in with the academic structure at most universities, it was necessarily to deliver the integrated material in a form that could be slotted into existing Calculus curricula and, separately, into existing statistics curricula. The statistics part of the integrated curriculum is packaged into another free, online textbook: Lessons in Statistical Thinking.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling and the scientific method</span>"
    ]
  },
  {
    "objectID": "Modeling/16-modeling-scientific-method.html#interpolation-and-extrapolation",
    "href": "Modeling/16-modeling-scientific-method.html#interpolation-and-extrapolation",
    "title": "16  Modeling and the scientific method",
    "section": "16.5 Interpolation and extrapolation",
    "text": "16.5 Interpolation and extrapolation\nModels are most reliable when the domain of the functions is considered to be the span of the data or observations underlying them. Evaluating a function inside the span of the data is called “interpolation.” Conversely, sometimes functions are evaluated at inputs far outside the span of the data used to construct the function. This is “extrapolation.”\nExtrapolation is unreliable. However, sometimes, the purpose of a model is to help us consider how a system might behave when inputs are outside of the span of already-observed data. To satisfy such a purpose, extrapolation is unavoidable.\nStill, it is possible to adopt mathematical methods that mitigate the impact of extrapolation. It’s key, for instance, to avoid high-order polynomials, as these produce some of the most extreme and misleading behavior. (See ?sec-polynomials.) More reliable alternatives include using only low-order polynomials (as already described in Chapter 12), localized functions such as the sigmoid or gaussian, or splines (?sec-splines).\n\n\n\n\n\n\nExample: Extrapolating gravitation\n\n\n\nIsaac Newton famously developed his universal theory of gravitation by examining the orbits of planets. Planets close to the sun feel a strong tug of the Sun’s gravity, and farther-out planets have receive a weaker tug. Similarly, the motion of satellites around the Earth is determined mainly by the Earth’s gravitational pull. Per Newton’s theory, the acceleration due to gravity goes as the inverse-square of distance from the center of the Earth. Far-away satellites have long orbits, close-in satellites orbit with far lower orbital duration.\nPlanets orbit outside the Sun. Satellites orbit outside the Earth. Application of the inverse-square law to forces outside the Sun or outside the Earth are interpolation. But there are no observations of orbits or forces inside the Sun or Earth. So using the inverse-square law for inside forces is an extrapolation. This extrapolation produces catastrophically misleading models. In reality, the gravitational force on the inside—as you might imagine in a tunnel going through the center of the Earth—increases linearly with distance from the center. (Newton’s theory explains the mechanics of this.)",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling and the scientific method</span>"
    ]
  },
  {
    "objectID": "Modeling/16-modeling-scientific-method.html#sec-mechanism-vs-curve",
    "href": "Modeling/16-modeling-scientific-method.html#sec-mechanism-vs-curve",
    "title": "16  Modeling and the scientific method",
    "section": "16.6 Mechanism versus curve",
    "text": "16.6 Mechanism versus curve\nIt is important to distinguish between two basic types of model:\n\nEmpirical models which are rooted in observation and data.\nMechanistic models such as those created by applying fundamental laws of physics, chemistry, etc.\n\nWe will put off mechanistic models for a while, for two reasons. First, the “fundamental laws of physics, chemistry, and such” are often expressed with the concepts and methods of calculus. We are heading there, but at this point you don’t yet know the core concepts and methods of calculus. Second, most students don’t make a careful study of the “fundamental laws of physics, chemistry, and such” until after they have studied calculus. So examples of mechanistic models will be a bit hollow at this point.\nEmpirical models (sometimes deprecatingly called “curve fitting”) are common in many vital areas such as the social sciences and economics, psychology, clinical medicine, etc. A solid understanding of statistics greatly enhances the reliability of conclusions drawn from empirical models.\nIn addition to physical law, geometry provides many examples of mechanistic connections between elements. An example is the geometric calculation of the forward force on an arrow being pulled back by a bow’s draw.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling and the scientific method</span>"
    ]
  },
  {
    "objectID": "Modeling/16-modeling-scientific-method.html#sec-polishing-parameters",
    "href": "Modeling/16-modeling-scientific-method.html#sec-polishing-parameters",
    "title": "16  Modeling and the scientific method",
    "section": "16.7 Polishing parameters",
    "text": "16.7 Polishing parameters\nIn Section 11.6, we referred to a way to improve parameters in order to better match patterns in data: polishing. Polishing can be carried out by software such as fitModel(). It is usually worthwhile because it is easy and mostly automatic.\nPolishing becomes harmful when the modeler misinterprets it as increasing a model’s reliability. Polishing can add a digit or two to a model parameter. Still, those digits may be irrelevant from a statistical perspective and do not provide any warranty that the fitted function is an appropriate approximation to the real-world situation.\nAn analogy might help. Polishing your teeth might make you marginally more attractive, and can be part of good hygiene, but it will not make you a better person.\n\nApplication area 16.1 We look at attemps in the 1970s to predict when oil production and consumption will reach its peak.\n\n\n\n\n\n\n\nApplication area 16.1 Peak Oil?\n\n\n\nNowadays, everybody knows about the problems with oil consumption and climate change; bringing world oil consumption down to zero by 2050 is an often-stated goal of countries throughout the world. The basic theory of climate change is that increased CO\\(_2\\)_ from the consumption of fossil fuels causes warming via the greenhouse effect. (There are other greenhouse gasses as well such as methane.) The CO\\(_2\\)_ greenhouse phenomenon was first modeled by Svante Arrhenius in 1896. His model indicated that if atmospheric CO\\(_2\\)_ doubled, global temperature would rise by 5-6\\(^\\circ\\)C. As of today, atmospheric CO\\(_2\\)_ has increased by 50% from historic levels at the start of the industrial revolution. A simple linear interpolation of Arrhenius’ prediction would put global temperature increase at about 2.5\\(^\\circ\\)C. This is eerily close to the present-day target of an increase by 2 degrees.\nDespite Arrhenius’s prescience, widespread understanding of greenhouse gas effects emerged only around 1990. Up until then, the possibility of concern was that the world might run out of oil and freeze the economy. Policy makers attempted to use past data to predict future oil. These were entirely empirical models: curve fitting. The curve selected was sensible: a sigmoidal function. The exhaustion of oil corresponds to reaching the sigmoid’s upper horizontal asymptote is reach.\nFigure 16.5 shows cumulative oil consumption globally. The value for each year corresponds to all consumption from 1900 up to the given year. It’s revealing to look at a fitted curve using data up to 1973. (In 1973, the world’s oil economy started to change rapidly due to embargoes and oligopolistic behavior by leading oil producers.) The blue curve in Figure 16.5 shows a sigmoidal function as might have been fitted in 1973.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.5: The total cumulative amount of oil produced (units: Terra-watt-hours) globally for each year since 1900. Blue: a sigmoidal model fitted to the data up to 1973. Green: sigmoidal model fitted with data through 2023.\n\n\n\nSuch curve fitting is a fool’s errand for several reasons. Although it might be sensible to imagine cumulative oil production over time as a sigmoid, there is no indication in the data that the curve is leveling off. Nor is there good reason to assume that the mechanisms that shaped the oil economy of 1920 were still in action in 1973, let alone in 2023. Not only is the model an extrapolation, but it depends on the details of polishing of sigmoidal parameters. To illustrate this sensitivity, consider the green curve in Figure 16.5 which is fitted to the data through 2023. The observed data in 2023 is only about one-third the level of the 1973 prediction for 2023.\nNow consider a different purpose for such modeling. Suppose we take at face value international claims of a “net-zero” CO\\(_2\\) economy by 2050 and we want to anticipate the total cumulative effect as of that time. Draw a smooth curve continuing the oil consumption data up through 2100, with a leveling off around 2050. There is no precise way to do this, since there is no reason to think that the growth part of the sigmoidal curve will be the same as the leveling-off part. There is a range of plausible scenarios, but since we are already close or even at the target of a 1.5 degree increase in global temperature, it seems entirely possible that the horizontal asymptote will be above 2 or even 2.5 degrees.\nFortunately, unlike in 1973, there are now detailed mechanistic climate models relating CO\\(_2\\) to global temperatures. We will have to pay careful attention to their predictions as we head toward the goal of net-zero emissions.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling and the scientific method</span>"
    ]
  },
  {
    "objectID": "Modeling/08-parameters.html",
    "href": "Modeling/08-parameters.html",
    "title": "8  Parameters",
    "section": "",
    "text": "8.1 Matching numbers to quantities\nThe coordinate axes in Figure 8.1 represent quantities. On the horizontal axis is time, measured in days. The vertical axis is denominated in “10000 cases,” meaning that the numbers on the vertical scale should be multiplied by 10000 to get the number of cases.\nThe exponential function takes as input a pure number and produces an output that is also a pure number. This is true for all the pattern-book functions. Since the graph axes don’t show pure numbers, it is no surprise then that the pattern-book exponential function doesn’t align with the COVID case data.\nIf we want the input to the model function \\(\\text{cases}(t)\\) to be denominated in days, we will have to convert \\(t\\) to a pure pure number (e.g. 10, not “10 days”) before the quantity is handed off as the argument to \\(\\exp()\\). We do this by introducing a parameter.\nThe standard parameterization for the exponential function is \\(e^{kt}\\). The parameter \\(k\\) will be a quantity with units of “per-day.” Suppose we set \\(k=0.2\\) per day. Then \\(k\\, t{\\LARGE\\left.\\right|}_{t=10 days} = 2\\). This “2” is a pure number because the units on the 0.2 (“per day”) and on the 10 (days) cancel out: \\[0.2\\, \\text{day}^{-1} \\cdot 10\\, \\text{days} = 2\\ .\\] The use of a parameter like \\(k\\) does more than handle the formality of converting input quantities into pure numbers. Having a choice for \\(k\\) allows us to stretch or compress the function to align with the data. Figure 8.2 plots the modeling version of the exponential function to the COVID-case data:\nFigure 8.2: Using the function form \\(A e^{kt}\\) with parameters \\(k=0.19\\) per day and \\(A = 0.0573\\) cases (in 10000s) matches the COVID-case data well.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parameters</span>"
    ]
  },
  {
    "objectID": "Modeling/08-parameters.html#matching-numbers-to-quantities",
    "href": "Modeling/08-parameters.html#matching-numbers-to-quantities",
    "title": "8  Parameters",
    "section": "",
    "text": "Recall that pure numbers, like 17.32, do not have units. Quantities, on the other hand, usually do have units, as in 17.3 days or 34 meters.\n\nIn every case, these parameters are arranged to translate a with-units quantity into a pure number suitable as an input to the pattern-book function. Similarly, parameters will translate the pure-number output from the pattern-book function into a quantity with units.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parameters</span>"
    ]
  },
  {
    "objectID": "Modeling/08-parameters.html#parallel-scales",
    "href": "Modeling/08-parameters.html#parallel-scales",
    "title": "8  Parameters",
    "section": "8.2 Parallel scales",
    "text": "8.2 Parallel scales\n\n\n\n\n\n\n\n\n\nAt the heart of how we use the pattern-book functions to model the relationship between quantities is the idea of conversion between one scale and another. Consider these everyday objects: a thermometer and a ruler.\n\n\n\n\n\n\n\n\n\nEach object presents a read-out of what’s being measured—temperature or length—on two different scales. At the same time, the objects provide a way to convert one scale to another.\nA function gives the output for any given input. We represent the input value as a position on a number line—which we call an “axis”—and the output as a position on another output line, almost always drawn perpendicular to one another. But the two number lines can just as well be parallel to one another. To evaluate the function, find the input value on the input scale and read off the corresponding output.\nWe can translate the correspondance between one scale and the other into the form of a straight-line function. For instance, if we know the temperature in Fahrenheit (\\(^\\circ\\)F) and want to convert it to Celsius (\\(^\\circ C\\)) we have the following function: \\[C(F) \\equiv {\\small\\frac{5}{9}}(F-32)\\ .\\] Similarly, converting inches to centimeters can be accomplished with \\[\\text{cm(inches)} \\equiv 2.54 \\, (\\text{inches}-0)\\ .\\] Both of these scale conversion functions have the form of the straight-line function, which can be written as \\[f(x) \\equiv a x + b\\ \\ \\ \\text{or, equivalently as}\\ \\ \\ \\ f(x) \\equiv a(x-x_0)\\ ,\\] where \\(a\\), \\(b\\), and \\(x_0\\) are parameters.\nIn Section 8.3, we will use the \\(ax + b\\) form of scale conversion, to scale the input to pattern-book functions, but we could equally well have used \\(a(x-x_0)\\).\nIn Section 8.4 we will introduce a second scale conversion function, for the output from pattern-book functions. That scaling will also be in the form of a straight-line function: \\(A x + B\\). The use of the lower-case parameter names (\\(a\\), \\(b\\)) versus the upper-case parameter names (\\(A\\), \\(B\\)) will help us distinguish the two different uses for scale conversion, namely input scaling versus output scaling.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parameters</span>"
    ]
  },
  {
    "objectID": "Modeling/08-parameters.html#sec-input-scaling",
    "href": "Modeling/08-parameters.html#sec-input-scaling",
    "title": "8  Parameters",
    "section": "8.3 Input scaling",
    "text": "8.3 Input scaling\nFigure 8.3 is based on the data frame RI-tide which is a minute-by-minute record of the tide level in Providence, Rhode Island (USA) for the period April 1 to 5, 2010. The level variable is measured in meters; the hour variable gives the time of the measurement in hours after midnight at the start of April 1.\n\n\n\n\n\n\n\n\nFigure 8.3: Tide levels oscillate up and down over time. This is analogous to the \\(\\sin(t)\\) pattern-book function.\n\n\n\n\n\nThe pattern-book \\(\\sin()\\) and the function \\(\\color{magenta}{\\text{level}}\\color{blue}{(hour)}\\) have similar shapes, so it seems reasonable to model the tide data as a sinusoid. However, the scale of the axes is different on the two graphs.\nTo model the tide with a sinusoid, we need to modify the sinusoid to change the scale of the input and output. First, let’s look at how to accomplish the input scaling. Specifically, we want the pure-number input \\(t\\) to the sinusoid be a function of the quantity \\(hour\\). Our framework for this re-scaling is the straight-line function. We will replace the pattern-book input \\(t\\) with a function \\[t(\\color{blue}{hour}) \\equiv a\\, \\color{blue}{hour} + b\\ .\\]\nThe challenge is to find values for the parameters \\(a\\) and \\(b\\) that will transform the \\(\\color{blue}{\\mathbf{\\text{blue}}}\\) horizontal axis into the black horizontal axis, like this:\n\n\n\n\n\n\n\n\n\nBy comparing the two axes, we can estimate that \\(\\color{blue}{10} \\rightarrow 4\\) and \\(\\color{blue}{100} \\rightarrow 49\\). With these two coordinate points, we can find the straight-line function that turns \\(\\color{blue}{\\mathbf{\\text{blue}}}\\) into black by plotting the coordinate pairs \\((\\color{blue}{0},1)\\) and \\((\\color{blue}{100}, 51)\\) and finding the straight-line function that connects the points.\n\n\n\n\n\n\n\n\nFigure 8.4: The input scaling function must transform 10 into 4 and transform 100 into 49 to properly arrange the time scale with the scale for the pattern-book function.\n\n\n\n\n\nYou can calculate for yourself that the function that relates \\(\\color{blue}{\\mathbf{\\text{blue}}}\\) to black is \\[t(\\color{blue}{time}) = \\underbrace{\\frac{1}{2}}_a \\color{blue}{time}  \\underbrace{-1\\LARGE\\strut}_b\\]\nReplacing the pure number \\(t\\) as the input to pattern-book \\(\\sin(t)\\) with the transformed \\(\\frac{1}{2} \\color{blue}{time}\\) we get a new function: \\[g(\\color{blue}{time}) \\equiv \\sin\\left(\\strut {\\small\\frac{1}{2}}\\color{blue}{time} - 1\\right)\\ .\\] Figure 11.6 plots \\(g()\\) along with the actual tide data.\n\n\n\n\n\n\n\n\nFigure 8.5: The sinusoid with input scaling (black) aligns nicely with the tide-level data.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parameters</span>"
    ]
  },
  {
    "objectID": "Modeling/08-parameters.html#sec-output-scaling",
    "href": "Modeling/08-parameters.html#sec-output-scaling",
    "title": "8  Parameters",
    "section": "8.4 Output scaling",
    "text": "8.4 Output scaling\nJust as the natural input needs to be scaled before it reaches the pattern-book function, so the output from the pattern-book function needs to be scaled before it presents a result suited for interpreting in the real world.\n\n\n\n\n\nFigure 8.6: Natural quantities must be scaled to pure numbers before being suited to the pattern-book functions. The output from the pattern-book function is a pure number which is scaled to the natural quantity of interest.\n\n\n\n\n\n\n\n\nThe overall result of input and output scaling is to tailor the pattern-book function so that it is ready to be used in the real world.\nLet’s return to Figure 11.6 which shows that the function \\(g(\\color{blue}{time})\\), which scales the input to the pattern-book sinusoid, has a much better alignment to the tide data. Still, the vertical axes of the two graphs in the figure are not the same.\nThis is the job for output scaling, which takes the output of \\(g(\\color{blue}{time})\\) (bottom graph) and scales it to match the \\(\\color{magenta}{level}\\) axis on the top graph. That is, we seek to align the black vertical scale with the \\(\\color{magenta}{\\mathbf{\\text{magenta}}}\\) vertical scale. To do this, we note that the range of the \\(g(\\color{blue}{time})\\) is -1 to 1, whereas the range of the tide-level is about 0.5 to 1.5. The output scaling will take the straight-line form \\[{\\color{magenta}{\\text{level}}}({\\color{blue}{time}}) = A\\, g({\\color{blue}{time}}) + B\\] or, in graphical terms\n\n\n\n\n\n\n\n\n\nWe can figure out parameters \\(A\\) and \\(B\\) by finding the straight-line function that connects the coordinate pairs \\((-1, \\color{magenta}{0.5})\\) and \\((1, \\color{magenta}{1.5})\\) as in Figure 8.7.\n\n\n\n\n\n\n\n\nFigure 8.7: Finding the straight-line function that converts \\(-1 \\rightarrow \\color{magenta}{0.5}\\) and converts \\(1 \\rightarrow \\color{magenta}{1.5}\\)\n\n\n\n\n\nYou can confirm for yourself that the function that does the job is \\[{\\color{magenta}{\\text{level}}} = 0.5 g({\\color{blue}{time}}) + 1\\ .\\]\nPutting everything together, that is, scaling both the input to pattern-book \\(\\sin()\\) and the output from pattern-book \\(\\sin()\\), we get\n\\[{\\color{magenta}{\\text{level}}}({\\color{blue}{time}}) = \\underbrace{0.5}_A \\sin\\left(\\underbrace{\\small\\frac{1}{2}}_a {\\color{blue}{time}}  \\underbrace{-1}_b\\right) + \\underbrace{1}_B\\]",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parameters</span>"
    ]
  },
  {
    "objectID": "Modeling/08-parameters.html#a-procedure-for-building-models",
    "href": "Modeling/08-parameters.html#a-procedure-for-building-models",
    "title": "8  Parameters",
    "section": "8.5 A procedure for building models",
    "text": "8.5 A procedure for building models\nWe’ve been using pattern-book functions as the intermediaries between input scaling and output scaling, using this format.\n\\[f(x) \\equiv A e^{ax + b} + B\\ .\\] We can use the other pattern-book functions—the gaussian, the sigmoid, the logarithm, the power-law functions—in the same way. That is, the basic framework for modeling is this:\n\\[\\text{model}(x) \\equiv A\\, {g_{pattern\\_book}}(ax + b) + B\\ ,\\] where \\(g_{pattern\\_book}()\\) is one of the pattern-book functions. To construct a basic model, you task has two parts:\n\nPick the specific pattern-book function whose shape resembles that of the relationship you are trying to model. For instance, we picked \\(e^x\\) for modeling COVID cases versus time (at the start of the pandemic). We picked \\(\\sin(x)\\) for modeling tide levels versus time.\nFind numerical values for the parameters \\(A\\), \\(B\\), \\(a\\), and \\(b\\). In ?sec-fitting-by-eye we will show you some ways to make this part of the task easier.\n\nIt is remarkable that models of a very wide range of real-world relationships between pairs of quantities can be constructed by picking one of a handful of functions, then scaling the input and the output. As we move on to other Blocks in MOSAIC Calculus, you will see how to generalize this to potentially complicated relationships among more than two quantities. That is a big part of the reason you’re studying calculus.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parameters</span>"
    ]
  },
  {
    "objectID": "Modeling/08-parameters.html#other-formats-for-scaling",
    "href": "Modeling/08-parameters.html#other-formats-for-scaling",
    "title": "8  Parameters",
    "section": "8.6 Other formats for scaling",
    "text": "8.6 Other formats for scaling\nOften, modelers choose to use input scaling in the form \\(a (x - x_0)\\) rather than \\(a x + b\\). The two are completely equivalent when \\(x_0 = - b/a\\). The choice between the two forms is largely a matter of convention. But almost always the output scaling is written in the format \\(A y + B\\).\n\nFor the COVID case-number data shown in Figure 8.2, we found that a reasonable match to the data can be had by input- and output-scaling the exponential: \\[\\text{cases}(t) \\equiv  \\underbrace{573}_A e^{\\underbrace{0.19}_a\\ t}\\ .\\]\nYou might wonder why the parameters \\(B\\) and \\(b\\) aren’t included in the model. One reason is that cases and the exponential function already have the same range: zero and upwards. So there is no need to shift the output with a parameter B.\nAnother reason has to do with the algebraic properties of the exponential function. Specifically, \\[e^{a x + b}= e^b e^{ax} = {\\cal A} e^{ax}\\] where \\({\\cal A} \\equiv e^b\\).\nIn the case of exponentials, writing the input scaling in the form \\(e^{a(x-x_0)}\\) can provide additional insight.\nA bit of symbolic manipulation of the model can provide some additional insight. As you know, the properties of exponentials and logarithms are such that \\[A e^{at} = e^{\\log(A)} e^{at} = e^{a t + \\log(A)} = e^{a\\left(\\strut t + \\log(A)/a\\right)} = e^{a(t-t_0)}\\ ,\\] where \\[t_0 = - \\log(A)/a = - \\log(593)/0.19 = -33.6\\ .\\] You can interpret \\(t_0\\) as the starting point of the pandemic. When \\(t = t_0\\), the model output is \\(e^{k 0} = 1\\): the first case. According to the parameters we matched to the data for March, the pandemic’s first case would have happened about 33 days before March 1, which is late January. We know from other sources of information, the outbreak began in late January. It is remarkable that even though the curve was constructed without any data from January or even February, the data from March, translated through the curve-fitting process, pointed to the start of the outbreak. This is a good indication that the exponential form for the model is fundamentally correct.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parameters</span>"
    ]
  },
  {
    "objectID": "Modeling/08-parameters.html#parameterization-conventions",
    "href": "Modeling/08-parameters.html#parameterization-conventions",
    "title": "8  Parameters",
    "section": "8.7 Parameterization conventions",
    "text": "8.7 Parameterization conventions\nThere are conventions for the symbols used for input-scaling parameterization of the pattern-book functions. Knowing these conventions makes it easier to read and assimilate mathematical formulas. In several cases, there is more than one conventional option. For instance, the sinusoid has a variety of parameterization forms that get used depending on which feature of the function is easiest to measure. ?tbl-param that are used in practice.\n\n\n\nTable 8.1: Some standard forms of input scaling parameterizations\n\n\n\n\n\n\n\n\n\n\n\nFunction\nWritten form\nParameter 1\nParameter 2\n\n\n\n\nExponential\n\\(e^{kt}\\)\n\\(k\\)\nNot used\n\n\nExponential\n\\(e^{t/\\tau}\\)\n\\(\\tau\\) “time constant”\nNot used\n\n\nExponential\n\\(2^{t/\\tau_2}\\)\n\\(\\tau_2\\) “doubling time”\nNot used\n\n\nExponential\n\\(2^{-\\tau_{1/2}}\\)\n\\(-\\tau_{1/2}\\) “half life”\nNot used\n\n\nPower-law\n\\([x - x_0]^p\\)\n\\(x_0\\) x-intercept\nexponent\n\n\nSinusoid\n\\(\\sin\\left(\\frac{2 \\pi}{P} (t-t_0)\\right)\\)\n\\(P\\) “period”\n\\(t_0\\) “time shift”\n\n\nSinusoid\n\\(\\sin(\\omega t + \\phi)\\)\n\\(\\omega\\) “angular frequency”\n\\(\\phi\\) “phase shift”\n\n\nSinusoid\n\\(\\sin(2 \\pi \\omega t + \\phi)\\)\n\\(\\omega\\) “frequency”\n\\(\\phi\\) “phase shift”\n\n\nGaussian\ndnorm(x, mean, sd)\n“mean” (center)\nsd “standard deviation”\n\n\nSigmoid\npnorm(x, mean, sd)\n“mean” (center)\nsd “standard deviation”\n\n\nStraight-line\n\\(mx + b\\)\n\\(m\\) “slope”\n\\(b\\) “y-intercept”\n\n\nStraight-line\n\\(m (x-x_0)\\)\n\\(m\\) “slope”\n\\(x_0\\) “center”",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Parameters</span>"
    ]
  },
  {
    "objectID": "Modeling/09-assembling-functions.html",
    "href": "Modeling/09-assembling-functions.html",
    "title": "9  Assembling functions",
    "section": "",
    "text": "9.1 Linear combination\nSection 8.4 introduced the notation of input and output scaling. For instance, Figure 11.6 illustrates how the pattern-book \\(\\sin()\\) function can be customized to model a particular situation. Recall that the input scaling involves inserting a straight-line function inside the parentheses, as in\n\\[f_1(t) \\equiv \\sin\\left( a t + b\\right) \\tag{9.1}\\]\nYou may recognize \\(at + b\\) as a straight-line function. Possibly, your recognition would be easier if we wrote \\(at + b\\) using different names for the input and the first parameter: \\(m x + b\\). You have been trained to pronounce \\(m\\) as the “slope” of the line and \\(b\\) as the “y-intercept.”\nUsing both input and output scaling gives a more general kind of function:\n\\[f_2(t) \\equiv A \\sin(a t + b) + B \\tag{9.2}\\]\nMath expression 9.2 is an example of a linear combination of two functions. The two functions are \\(g_1(t) \\equiv 1\\) and \\(g_2(t) \\equiv sin(a t + b)\\).\nIt might be easier to see Math expression 9.2 as a linear combination if the function were written explicitly using the two functions being combined, that is, as\n\\[f_2 \\equiv A g_2(t) + B g_1(t)\\]\nThe combination is made by scaling each of the functions involved then adding the scaled functions together. The two scaling factors, \\(A\\) and \\(B\\), could be called “parameters of \\(f_2()\\),” which indeed they are. It would be good to get used to another word that’s used specifically for the parameters in a linear combination: “coefficients.” The advantage of “coefficients” as a name lies in it marking the parameters as those involved in a linear combination, rather than any of the other ways parameters can be used.\nNote that “coefficients” always refers to parameters that are not inside the parentheses of a function. In contrast, often parameters are inside the parentheses as in Math expression 9.1. The parameters \\(a\\) and \\(b\\) in \\(\\sin(a t + b)\\) are inside the parentheses. Consequently, \\(a\\) and \\(b\\) shouldn’t be called “coefficients.” Sometimes, to emphasize this, parameters in parentheses are called “nonlinear parameters” to distinguish them from coefficients like \\(A\\) and \\(B\\).\nTo illustrate how linear combination is used to create new functions, consider polynomials, for instance, \\[f(x) \\equiv 3 x^2 + 5 x - 2\\ .\\] There are three pattern-book functions in this polynomial. In polynomials the functions being combined are all power-law functions: \\(g_0(x) \\equiv 1\\), \\(g_1(x) \\equiv x\\), and \\(g_2(x) \\equiv x^2\\). With these functions defined, we can write the polynomial \\(f(x)\\) as \\[f(x) \\equiv 3 g_2(x) + 5 g_1(x) - 2 g_0(x)\\] Each of the functions is being scaled by a quantity: 3, 5, and -2 respectively. Then the scaled functions are added up. That is a linear combination; scale and add.\nLinear combination is an extremely important tactic that quantitative workers use throughout their careers. For instance, many physical systems are described by linear combinations. For instance, the motion of a vibrating molecule, a helicopter in flight, or a building shaken by an earthquake are described in terms of simple “modes” which are linearly combined to make up the entire motion. More down to Earth, the timbre of a musical instrument is set by the scalars in a linear combination of pure tones. And throughout work with data in science, commerce, government and other fields a primary data analysis method—called “regression”—is about finding the best linear combination of a set of explanatory variables to create a model function of the response variable.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assembling functions</span>"
    ]
  },
  {
    "objectID": "Modeling/09-assembling-functions.html#linear-combination",
    "href": "Modeling/09-assembling-functions.html#linear-combination",
    "title": "9  Assembling functions",
    "section": "",
    "text": "In high school, polynomials are often presented as puzzles—factor them to find the roots! In calculus, however, polynomials are used as functions for modeling. They are a kind of modeling “clay,” which can be shaped as needed.\n\n\n\n\n\n\n\nIdentical vs proportional vs straight-line\n\n\n\nOne of the pattern book functions is very simple; the output is identical to the input:\n\\[\\text{identity}(x) \\equiv x\\] Multiplying the identity() function by a parameter gives a function that can well be called “proportional().”\n\\[\\text{proportional}(x) \\equiv a\\ \\text{identity}(x) = a\\ x\\] The parameter \\(a\\) is often called the “constant of proportionality.”\nIt’s common to call this closely related function the “linear” function, but a better name is the “straight-line” function. “Straight-line” is the name we shall use in this book.\n\\[\\text{straight_line}(x) \\equiv a\\ x + b\\]",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assembling functions</span>"
    ]
  },
  {
    "objectID": "Modeling/09-assembling-functions.html#sec-function-composition",
    "href": "Modeling/09-assembling-functions.html#sec-function-composition",
    "title": "9  Assembling functions",
    "section": "9.2 Function composition",
    "text": "9.2 Function composition\nFunction composition refers to combining functions by taking the input of one function and feeding it as input to another. “\\(g()\\) composed with \\(h()\\)” means \\(g(h(x))\\).\nTo illustrate, consider again the function defined in Math expression 9.2: \\[f(t) \\equiv A \\sin\\left( a t + b\\right) + B\\]\nYou’ve already seen how ?eq-input-outout-scaled-sin is a linear combination of two functions \\(f_1(t) \\equiv 1\\) and \\(f_2(t) \\equiv sin(a t + b)\\). But \\(f_2()\\) is not a linear combination. Instead, it is a function composition. The two functions being composed are \\(sin(x)\\) and \\(a t + b\\), producing \\(sin(a t + b)\\). Here, \\(\\sin()\\) is the outer function in the composition and \\(at + b\\) is the inner function.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assembling functions</span>"
    ]
  },
  {
    "objectID": "Modeling/09-assembling-functions.html#sec-function-multiplication",
    "href": "Modeling/09-assembling-functions.html#sec-function-multiplication",
    "title": "9  Assembling functions",
    "section": "9.3 Function multiplication",
    "text": "9.3 Function multiplication\nMultiplication is the third in our repertoire of methods for making new functions. To review, here are the first two methods involving combining a function \\(f()\\) with a function $g():\n\nLinear combinations. Example: \\(n_1(t) \\equiv 5 f(t) + 1.2 g(t)\\).\nFunction composition. There are two possibilities which produce two distinct functions:\n\n\\(c_1(t) \\equiv f(g(t))\\), that is, \\(g()\\) is the inner function.\n\\(c_2(t) \\equiv g(f(t))\\), that is, \\(g()\\) is the outer function.\n\n\nNow a new method:\n\nMultiplication of the output of two functions. Example: \\(m(t)\\equiv f(t) g(t)\\). This is perfectly ordinary multiplication. Multiplication is commutative, as illustrated by the equality of \\(5 \\times 2\\) and \\(2 \\times 5\\). Owing to the commutativity of multiplication, it doesn’t matter whether \\(f()\\) is first or \\(g()\\) is first.\n\n\n\nIn function composition, the order of the functions matters: \\(f(g(x))\\) and \\(g(f(x))\\) are in general completely different functions.\nIn function multiplication, the order does not matter because multiplication is commutative, that is, if \\(f()\\) and \\(g()\\) are the functions to be multiplied \\(f(x) \\times g(x) = g(x)\\times f(x)\\).\n\\[\\underbrace{f(x) \\times g(x)}_\\text{multiplication}\\ \\ \\ \\ \\underbrace{{\\Large f(}g(x){\\Large)} \\ \\ \\text{or}\\ \\ \\ {\\Large g(}f(x){\\Large)}}_\\text{composition}\\]\nIn function composition, only one of the functions—the interior function is applied to the overall input, \\(x\\) in the above example. The exterior function is fed its input from the output of the interior function.\nIn multiplication, each of the functions is applied to the input individually. Then their outputs are multiplied to produce the overall output.\n\nTransient vibration\nA guitar string is plucked to produce a note. The sound is, of course, vibrations of the air created by vibrations of the string.\nAfter plucking, the note fades away. An important model of this is a sinusoid (of the correct period to correspond to the frequency of the note) times an exponential.\nFunction multiplication is used so often in modeling that you will see it in many modeling situations. Here’s one example that is important in physics and communication: the wave packet. Overall, the wave packet is a localized oscillation as in Figure 9.2. The packet can be modeled with the product of two pattern-book functions: a gaussian times a sinusoid.\n\n\n\n\n\n\n\n\n\n\nFigure 9.1: The two components of the wave packet in Figure 9.2: an “envelope” and an oscillation. Multiplying these components together produces the wave packet.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.2: A wave packet constructed by multiplying a sinusoid and a gaussian function.\n\n\n\n\n\n\n\n\n\n\n\nMath in the World: Modeling rise and fall\n\n\n\nThe initial rise in popularity of the social media platform Yik Yak was exponential. Then popularity leveled off, promising a steady, if static, business into the future. But, the internet being what it is, popularity collapsed to near zero and the company closed.\nOne way to model this pattern is by multiplying a sigmoid by an exponential. (See Figure 9.3.)\n\n\n\n\n\n\n\n\nFigure 9.3: Subscriptions to the web messaging service Yik Yak grew exponentially in 2013 and 2014, then collapsed. The company closed in 2017.\n\n\n\n\n\n\n\nFunctions constructed as a product of simple functions can look like this in tradition notation: \\[h(t) \\equiv \\sin(t) e^{-t}\\] and like this in computer notation:\n\nh &lt;- makeFun(sin(t)*exp(-t) ~ t)",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assembling functions</span>"
    ]
  },
  {
    "objectID": "Modeling/09-assembling-functions.html#sec-piecewise-intro",
    "href": "Modeling/09-assembling-functions.html#sec-piecewise-intro",
    "title": "9  Assembling functions",
    "section": "9.4 Splitting the domain",
    "text": "9.4 Splitting the domain\nConsider the familiar absolute-value function:\n\n\n\n\n\n\n\n\n\nFigure 9.4: The absolute-value function\n\n\n\n\n\\[abs(x) \\equiv \\left|x\\right|\\] Written this way, the definition of \\(abs()\\) is a tautology: unless you already know what \\(\\left|x\\right|\\) means, you will have no clue what’s going on.\nCan we assemble \\(abs(x)\\) out of pattern-book functions? What’s distinctive about \\(abs(x)\\) is the break at \\(x=0\\). There is no similarly sharp transition in any of the pattern-book functions.\nOne way to construct the sharp transition is to view \\(abs(x)\\) as two functions, one whose domain is the negative half of the number line and the other having a domain that is the non-negative half. That is, we will break the domain of \\(abs()\\) into two pieces. For the right piece of the domain, \\(abs(x)\\) is simply proportional\\((x)\\). For the left piece of the domain, \\(abs(x)\\) is \\(-\\)proportional\\((x)\\).\nA function defined separately on different pieces of its domain is called a piecewise function. In the conventional mathematical notation, there is a large \\(\\LARGE\\left\\{\\right.\\) followed by two or more lines. Each line gives a formula for that part of the function and indicates to which interval the formula applies.\n\\[abs(x) \\equiv \\left\\{\n\\begin{array}{rl}  x & \\text{for}\\ 0 \\leq x \\\\\n- x & \\text{otherwise}\\\\\\end{array}\n\\right.\\]\n\n\n\n\n\n\n\n\n\nFigure 9.5: The Heaviside function\n\n\n\n\nAnother piecewise function widely used in technical work, but not as familiar as \\(abs()\\) is the Heaviside function, which has important uses in physics and engineering.\n\\[\\text{Heaviside}(x) \\equiv \\left\\{\n\\begin{array}{cl} 1 & \\text{for}\\ 0 \\leq x \\\\0 & \\text{otherwise}\\end{array}\n\\right.\\]\nThe Heaviside function is defined on the same two pieces of the number line as \\(abs()\\). To the right of zero, Heaviside is identical to constant(). To the left, it is identical to \\(0\\) times constant\\(()\\).\nThe vertical gap between the two pieces of the Heaviside function is called a discontinuity. Intuitively, you cannot draw a discontinuous function without lifting the pencil from the paper. The Heaviside’s discontinuity occurs at input \\(x=0\\).\n\n9.4.1 Computing notation\nThe usual mathematical notation for piecewise functions, spread out over multiple lines that are connected with a tall brace, is an obvious non-candidate for computer notation. In R, the stitching together of the two pieces can be done with the function ifelse(). The name is remarkably descriptive. The ifelse() function takes three arguments. The first is a question to be asked, the second is the value to return if the answer is “yes,” and the third is the value to return for a “no” answer.\nTo define \\(abs()\\) or Heaviside\\(()\\) the relevant question is, “Is the input on the right or left side of zero on the number line?” In widely-used computing languages such as R, the format for asking a question does not involve a question mark. For example, to ask the question, “Is 3 less than 2?” use the expression:\n\n3 &lt; 2\n\nIn mathematics notation, \\(3 &lt; 2\\) is a declarative statement and is an impossibility. More familiar would be \\(x &lt; 2\\), which is again a declarative statement putting a restriction on the possible values of the quantity \\(x\\).\nIn computing notation, 3 &lt; 2 or x &lt; 2 is not a declaration, it is an imperative  statement that directs the computer to do the calculation to find out if the statement is true or false, or, as written in R, TRUE or FALSE.\n\n\nRemember that the tilde-expressions given as input to makeFun() are declarative, not imperative. makeFun() stores the tilde expression exactly as is, with symbols such as x being names rather than quantities. makeFun() packages up the stored tilde expression in the form of an R function. The assignment command Heaviside &lt;- ... gives the name Heaviside to the function created by makeFun().\nOnly when you apply the function created by makeFun() to an input quantity will the tilde-expression be turned into an imperative statement that asks the question 0 &lt;= x and then chooses the second or third argument to ifelse() as the result.\nHere’s a definition of Heaviside() written with ifelse().\n\nHeaviside &lt;- makeFun(ifelse(0 &lt;= x, 1, 0) ~ x)\n\nTable 9.1 shows computer notation for some common sorts of questions.\n\n\n\n\nTable 9.1: Each of these imperative statements in R asks a question about numbers.\n\n\n\n\n\n\n\n\n\nR notation\nEnglish\n\n\n\n\nx &gt; 2\n“Is \\(x\\) greater than 2?”\n\n\ny &gt;= 3\n“Is \\(y\\) greater than or equal to 3?”\n\n\nx == 4\n“Is \\(x\\) exactly 4?”\n\n\n2 &lt; x & x &lt; 5\n“Is \\(x\\) between 2 and 5?” Literally, “Is \\(x\\) both greater than 2 and less than 5?”\n\n\nx &lt; 2 | x &gt; 6\n“Is \\(x\\) either less than 2 or greater than 6?”\n\n\nabs(x-5) &lt; 2\n“Is \\(x\\) within two units of 5?”\n\n\n\n\n\n\n\n\n\n\n\n\n\nMath in the World: Heating with gas\n\n\n\nFigure 9.6 is a graph of monthly natural gas use in the author’s household versus average temperature during the month. (Natural gas is measured in cubic feet, abbreviated ccf.)\n\n\n\n\n\n\n\n\nFigure 9.6: The amount of natural gas used for heating the author’s home varies with the outside temperature.\n\n\n\n\n\nThe graph looks somewhat like a hockey stick. A sloping straight-line dependence of ccf on temperature for temperatures below \\(60^\\circ\\)F and constant for higher temperatures. The shape originates from the dual uses of natural gas. Gas is used for cooking and domestic hot water, the demand for which is more or less independent of outdoor temperature at about 15 ccf per month. Gas is also used for heating the house, but that is needed only when the temperature is less than about \\(60^\\circ\\)F.\nWe can accomplish the hockey-stick shape with a linear combination of the ramp() function and a constant. The ramp function represents gas used for heating, the constant is the other uses of gas (which are modeled as not depending on temperature. Overall, the model is \\[\\text{gas}(x) \\equiv 4.3\\,  \\text{ramp}(62-x)  + 15\\ .\\] Even simpler is the model for the other uses of natural gas: \\[\\text{other}(x) \\equiv 15\\ .\\]",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assembling functions</span>"
    ]
  },
  {
    "objectID": "Modeling/09-assembling-functions.html#computing-outside-the-domain",
    "href": "Modeling/09-assembling-functions.html#computing-outside-the-domain",
    "title": "9  Assembling functions",
    "section": "9.5 Computing outside the domain",
    "text": "9.5 Computing outside the domain\nEach of our pattern-book functions, with two exceptions, has a domain that is the entire number line \\(-\\infty &lt; x &lt; \\infty\\). No matter how big or small is the value of the input, the function has an output. Such functions are particularly nice to work with since we never have to worry about the input going out of bounds.\nThe two exceptions are:\n\nthe logarithm function, which is defined only for \\(0 &lt; x\\).\nsome of the power-law functions: \\(x^p\\).\n\nWhen \\(p\\) is negative, the output of the function is undefined when \\(x=0\\). You can see why with a simple example: \\(g(x) \\equiv x^{-2}\\). Most students had it drilled into them that “division by zero is illegal,” and \\(g(0) = \\frac{1}{0} \\frac{1}{0}\\), a double law breaker.\nWhen \\(p\\) is not an integer, that is \\(p \\neq 1, 2, 3, \\cdots\\) the domain of the power-law function does not include negative inputs. To see why, consider the function \\(h(x) \\equiv x^{1/3}\\).\n\n\n\nIt can be tedious to make sure that you are on the right side of the law when dealing with functions whose domain is not the whole number line. The designers of the hardware that does computer arithmetic, after several decades of work, found a clever system to make it easier. It is a standard part of such hardware that whenever a function is handed an input that is not part of that function’s domain, one of two special “numbers” is returned. To illustrate:\n\nsqrt(-3)\n## [1] NaN\n(-2)^0.9999\n## [1] NaN\n1/0\n## [1] Inf\n\nNaN stands for “not a number.” Just about any calculation involving NaN will generate NaN as a result, even those involving multiplication by zero or cancellation by subtraction or division.1 For instance:\n\n0 * NaN\n## [1] NaN\nNaN - NaN\n## [1] NaN\nNaN / NaN\n## [1] NaN\n\nDivision by zero produces Inf, whose name is reminiscent of “infinity.” Inf infiltrates any calculation in which it takes part:\n\n3 * Inf\n## [1] Inf\nsqrt(Inf)\n## [1] Inf\n0 * Inf\n## [1] NaN\nInf + Inf\n## [1] Inf\nInf - Inf\n## [1] NaN\n1/Inf\n## [1] 0\n\nTo see the benefits of the NaN / Inf system let’s plot out the logarithm function over the graphics domain \\(-5 \\leq x \\leq 5\\). Of course, part of that graphics domain, \\(-5 \\leq x \\leq 0\\) is not in the domain of the logarithm function and the computer is entitled to give us a slap on the wrists. The NaN provides some room for politeness.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assembling functions</span>"
    ]
  },
  {
    "objectID": "Modeling/09-assembling-functions.html#footnotes",
    "href": "Modeling/09-assembling-functions.html#footnotes",
    "title": "9  Assembling functions",
    "section": "",
    "text": "One that does produce a number is NaN^0.↩︎",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Assembling functions</span>"
    ]
  },
  {
    "objectID": "Modeling/10-functions-with-multiple-inputs.html",
    "href": "Modeling/10-functions-with-multiple-inputs.html",
    "title": "10  Functions with multiple inputs",
    "section": "",
    "text": "10.1 Linear combinations\nHousing prices are determined by several (or many!) factors. Translating the previous sentence into the language of functions, we can say that the price is a function of multiple inputs. Plausible inputs to the function include the amount of living area and the number of bedrooms and bathrooms. The inputs may also include quality of the neighborhood, length of commute, and so on.\nOften, the starting point for building a function with multiple inputs is a data frame whose variables include the function output (say, price) and the various inputs to the function.\nA large fraction of work in the data-oriented quantitative sciences uses just the identity function in the linear combination. Recall that the identify function merely echos as output whatever input is provided, that is, \\(\\text{identity}(x) \\equiv x\\).\nThere is no point in constructing a linear combination of identity functions that take the same input. For example, the linear combination\n\\[4.3\\ \\text{identity}(x) + 1.7\\ \\text{identity}(x) - 2.6\\ \\text{identity}(x)\\]\nis merely a long-winded way of saying \\(3.4\\ \\text{identity}(x)\\).\nWhere a linear combination of identity functions becomes useful is when the inputs to the various functions are different, for example, consider this function of three inputs, \\(x\\), \\(y\\), and \\(z\\).\n\\[4.3\\ \\text{identity}(x) + 1.7\\ \\text{identity}(y) - 2.6\\ \\text{identity}(z)\\]\nBy convention, we rarely write \\(\\text{identity}()\\) preferring instead just to write the name of the input, as with\n\\[4.3\\ x + 1.7\\ y - 2.6\\ z \\tag{10.3}\\]\nEven though there are no parentheses used in Math expression 10.3, it is still a linear combination of functions. Thinking of it this way prompts consideration of what other, non-identity function might have been included, for instance, \\(x^2\\) or \\(x\\times y\\).\nTo illustrate the use of linear combinations of identity functions (with different inputs), consider the following model of house prices:\n\\(\\text{price}(\\text{livingArea}, \\text{bedrooms}, \\text{bathrooms}) \\equiv\\)\n\\[\\ \\ \\ 21000 + 105\\,\\text{livingArea} - 13000\\, \\text{bedrooms} + 26000\\, \\text{bathrooms}\\]\nwhere each of the coefficients is in units of dollars. These coefficients are based on data in the SaratogaHouses data frame which records the sales price of 1728 houses in Saratoga County, New York, USA, in 2006\nThe model function is a simple linear combination, but it effectively quantifies how different aspects of a house contribute to its sales price. The model indicates that an additional square foot of living area is worth about 105 dollars per foot2. An extra bathroom is worth about $25,000. Bedrooms, strangely, are assigned a negative value by the model.\nPossibly you already understand what is meant by “an additional square foot” or “an extra bathroom.” These ideas can be intuitive, but they can be best understood with a grounding in calculus, which we turn to in Block II. For instance, the negative scalar on bedrooms will make sense when you understand “partial derivatives,” the subject of Chapter 25.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Functions with multiple inputs</span>"
    ]
  },
  {
    "objectID": "Modeling/10-functions-with-multiple-inputs.html#function-multiplication-fx-times-gt",
    "href": "Modeling/10-functions-with-multiple-inputs.html#function-multiplication-fx-times-gt",
    "title": "10  Functions with multiple inputs",
    "section": "10.2 Function multiplication: f(x) times g(t)",
    "text": "10.2 Function multiplication: f(x) times g(t)\nWhen a guitar string is at rest it forms a straight line connecting its two fixed ends: one set by finger pressure along the neck of the guitar and the other at the bridge near the center of the guitar body. When the string is plucked, its oscillations follow a sinusoid pattern of displacement. With the right camera and lighting setup, we can see these oscillations in action:\n\n\n\n\n\n\nVideo 10.1: The displacement of each guitar string is a function of position along the string and time. Figure 10.2 shows a single frame from the video, making it easier to see the string’s displacement as a function of position.\n\n\n\n\n\n\n\n\n\nFigure 10.2: The displacement of a vibrating guitar string is a function of both time and space. In the still picture taken from Video 10.1, you see a slice of that function taken at a fixed moment of time.\n\n\n\nFor a string of length \\(L\\), the string displacement is a function of position \\(x\\) along the string and is a linear combination of functions of the form \\[g_k(x) \\equiv \\sin(k \\pi x /L)\\] where \\(k\\) is an integer. A few of these functions are graphed in Figure 10.3 with \\(k=1\\), \\(k=2\\), and \\(k=3\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10.3: The overall shape of a guitar string at a fixed instant is complicated. It is a linear combination of three functions, each of which is called a “mode” of the string.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Functions with multiple inputs</span>"
    ]
  },
  {
    "objectID": "Modeling/11-fitting-features.html",
    "href": "Modeling/11-fitting-features.html",
    "title": "11  Fitting features",
    "section": "",
    "text": "11.1 Gaussian\nThe ability to perceive color comes from “cones”: specialized light-sensitive cells in the retina of the eye. Human color perception involves three sets of cones. The L cones are most sensitive to relatively long wavelengths of light near 570 nanometers. The M cones are sensitive to wavelengths near 540 nm, and the S cones to wavelengths near 430nm.\nThe current generation of Landsat satellites uses nine different wavelength-specific sensors. This makes it possible to distinguish features that would be undifferentiated by the human eye.\nBack toward Earth, birds have five sets of cones that cover a wider range of wavelengths than humans. (Figure 11.4) Does this give them a more powerful sense of the differences between natural features such as foliage or plumage? One way to answer this question is to take photographs of a scene using cameras that capture many narrow bands of wavelengths. Then, knowing the sensitivity spectrum of each set of cones, new “false-color” pictures can be synthesized recording the view from each set.1\nCreating the false-color pictures on the computer requires a mathematical model of the sensitivities of each type of cone. The graph of each sensitivity function resembles a Gaussian function.\nThe Gaussian has two parameters: the “mean” and the “sd” (short for standard deviation). It is straightforward to estimate values of the parameters from a graph, as in Figure 11.5.\nThe parameter “mean” is the location of the peak. The standard deviation is, roughly, half the width of the graph at a point halfway down from the peak.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting features</span>"
    ]
  },
  {
    "objectID": "Modeling/11-fitting-features.html#gaussian",
    "href": "Modeling/11-fitting-features.html#gaussian",
    "title": "11  Fitting features",
    "section": "",
    "text": "Figure 11.3: Two views of the same scene synthesized by combining the output of different types of cones. The top picture uses V, M, and S cones; the bottom only S, M, and L cones. The dark geen leaves revealed in the top picture are not distinguishable in the bottom picture. (Source: Tedore and Nilsson)\n\n\n\n\n\n\n\n\n\n\nFigure 11.4: Sensitivity to wavelength for each of the five types of bird cones. [Source: Tedore and Nilsson]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.5: A Gaussian function annotated to identify the parameters mean (location of peak of graph) and sd (half-width at half-height).",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting features</span>"
    ]
  },
  {
    "objectID": "Modeling/11-fitting-features.html#sinusoid",
    "href": "Modeling/11-fitting-features.html#sinusoid",
    "title": "11  Fitting features",
    "section": "11.2 Sinusoid",
    "text": "11.2 Sinusoid\nWe will use three parameters for fitting a sinusoid to data: \\[A \\sin\\left(\\frac{2\\pi}{P}\\right) + B\\] where\n\n\\(A\\) is the “amplitude”\n\\(B\\) is the “baseline”\n\\(P\\) is the period.\n\n\n\n\n\n\n\n\n\n\nFigure 11.6: A reproduction of the data originally shown in Figure 8.3. The baseline for the sinusoid is midway between the top of the oscillation and the bottom.\n\n\n\n\nThe baseline for the sinusoid is the value mid-way between the top of the oscillations and the bottom. For example, Figure 8.3 (reproduced in the margin) shows the sinusoidal-like pattern of tide levels. Dashed horizontal lines (\\(\\color{brown}{\\text{brown}}\\)) have been drawn roughly going through the top of the oscillation and the bottom of the oscillation. The baseline (\\(\\color{magenta}{\\text{magenta}}\\)) will be halfway between these top and bottom levels.\nThe amplitude is the vertical distance between the baseline and the top of the oscillations. Equivalently, the amplitude is half the vertical distance between the top and the bottom of the oscillations.\nIn a pure, perfect sinusoid, the top of the oscillation—the peaks—is the same for every cycle, and similarly with the bottom of the oscillation—the troughs. The data in Figure 8.3 is only approximately a sinusoid so the top and bottom have been set to be representative. In Figure 11.6, the top of the oscillations is marked at level 1.6, the bottom at level 0.5. The baseline is therefore \\(B \\approx = (1.6 + 0.5)/2 = 1.05\\). The amplitude is \\(A  = (1.6 - 0.5)/2 = 1.1/2 = 0.55\\).\nTo estimate the period from the data, mark the input for a distinct point such as a local maximum, then count off one or more cycles forward and mark the input for the corresponding distinct point for the last cycle. For instance, in Figure 11.6, the tide level reaches a local maximum at an input of about 6 hours, as marked by a black dotted line. Another local maximum occurs at about 106 hours, also marked with a black dotted line. In between those two local maxima you can count \\(n=8\\) cycles. Eight cycles in \\(106-6 = 100\\) hours gives a period of \\(P = 100/8 = 12.5\\) hours.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting features</span>"
    ]
  },
  {
    "objectID": "Modeling/11-fitting-features.html#sec-exponential-water",
    "href": "Modeling/11-fitting-features.html#sec-exponential-water",
    "title": "11  Fitting features",
    "section": "11.3 Exponential",
    "text": "11.3 Exponential\nTo fit an exponential function, such as the ones in ?fig-two-exponentials, we estimate the three parameters: \\(A\\), \\(B\\), and \\(k\\) in \\[A \\exp(kt)+ B\\]\n\n\n\n\n\n\n\n\nExp. growth\nExp. decay\n\n\n\n\n\n\n\n\n\nExponential decay is a left-to-right flip of exponential growth.\n\nThe data in Figure 11.2 illustrates the procedure. The first question to ask is whether the pattern shown by the data resembles an exponential function. After all, the exponential pattern book function grows in output as the input gets bigger, whereas the water temperature is getting smaller—the word decaying is used—as time increases. To model exponential decay, use \\(\\exp(-k t)\\), where the negative sign effectively flips the pattern-book exponential left to right.\nThe exponential function has a horizontal asymptote for negative inputs. The left-to-right flipped exponential \\(\\exp(-k t)\\) also has a horizontal asymptote, but for positive inputs.\nThe parameter \\(B\\), again called the “baseline,” is the location of the horizontal asymptote on the vertical axis. Figure 11.2 suggests the asymptote is located at about 25 deg. C. Consequently, the estimated value is \\(B \\approx 25\\) deg C.\n\n11.3.1 Estimating A\nThe parameter \\(A\\) can be estimated by finding the value of the data curve at \\(t=0\\). In Figure Figure 11.7 that is just under 100 deg C. From that, subtract off the baseline you estimated earlier: (\\(B = 25\\) deg C). The amplitude parameter \\(A\\) is the difference between these two: \\(A = 99 - 25 = 74\\) deg C.\n\n\n11.3.2 Estimating k\nThe exponential has a unique property of “doubling in constant time” as described in ?sec-doubling-time. We can exploit this to find the parameter \\(k\\) for the exponential function.\n\nThe procedure starts with your estimate of the baseline for the exponential function. In Figure 11.7 the baseline has been marked in \\(\\color{magenta}{\\text{magenta}}\\) with a value of 25 deg C.\nPick a convenient place along the horizontal axis. You want a place such that the distance of the data from the baseline to be pretty large. In Figure 11.7 the convenient place was selected at \\(t=25\\).\n\n\n\n\n\n\n\n\n\n\nFigure 11.7: Determining parameter \\(k\\) for the exponential function using the doubling time.\n\n\n\n\n\nMeasure the vertical distance from the baseline at the convenient place. In Figure 11.7 the data curve has a value of about 61 deg C at the convenient place. This is \\(61-25 = 36\\) deg C from the baseline.\nCalculate half of the value from (c). In Figure 11.7 this is \\(36/2=18\\) deg C. But you can just as well do the calculation visually, by marking half the distance from the baseline at the convenient place.\nScan horizontally along the graph to find an input where the vertical distance from the data curve to the baseline is the value from (d). In Figure 11.7 that half-the-vertical-distance input is at about \\(t=65\\). Then calculate the horizontal distance between the two vertical lines. In Figure 11.7 that is \\(65 - 25 = 40\\) minutes. This is the doubling time. Or, you might prefer to call it the “half-life” since the water temperature is decaying over time.\nCalculate the magnitude \\(\\|k\\|\\) as \\(\\ln(2)\\) divided by the doubling time from (e). That doubling time is 40 minutes, so \\(\\|k\\|= \\ln(2) / 40 = 0.0173\\). We already know that the sign of \\(k\\) is negative since the pattern shown by the data is exponential decay toward the baseline. So, \\(k=-0.0173\\).",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting features</span>"
    ]
  },
  {
    "objectID": "Modeling/11-fitting-features.html#graphics-layers",
    "href": "Modeling/11-fitting-features.html#graphics-layers",
    "title": "11  Fitting features",
    "section": "11.4 Graphics layers",
    "text": "11.4 Graphics layers\nWhen fitting a function to data, it is wise to plot out the resulting function on top of the data. This involves making graphics with two layers, as described in Chapter 7. As a reminder, here is an example comparing the cooling-water data to the exponential function we fitted in Section 11.3.\nThe fitted function we found was \\[T_{water}(t) \\equiv 74 \\exp(-0.0173 t) + 25\\] where \\(T\\) stands for “temperature.”\nTo compare \\(T_{water}()\\) to the data, we will first plot out the data with gf_point(), then add a slice plot of the function. We will also show a few bells and whistles of plotting: labels, colors, and such.\n\n\n\n\nT_water &lt;- makeFun(74*exp(-0.0173*t) + 25 ~ t)\ngf_point(temp ~ time, data = CoolingWater, alpha = 0.5 ) %&gt;%\n  slice_plot(T_water(time) ~ time, color = \"blue\") %&gt;%\n  gf_labs(x = \"Time (minutes)\", y = \"Temperature (deg. C)\")\n\n\n\n\n\n\n\n\n\n\nFigure 11.8: A graphic with two layers: one for the cooling-water data and the other with the exponential function fitted to the data.\n\n\n\nThe slice_plot() command inherited the domain interval from the gf_point() command. This happens only when the name of the input used in slice_plot() is the same as that in gf_point(). (it is time in both.) You can add additional data or function layers by extending the pipeline.\nBy the way, the fitted exponential function is far from a perfect match to the data. We will return to this mismatch in Chapter 16 when we explore the modeling cycle.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting features</span>"
    ]
  },
  {
    "objectID": "Modeling/11-fitting-features.html#fitting-other-pattern-book-functions",
    "href": "Modeling/11-fitting-features.html#fitting-other-pattern-book-functions",
    "title": "11  Fitting features",
    "section": "11.5 Fitting other pattern-book functions",
    "text": "11.5 Fitting other pattern-book functions\nThis chapter has looked at fitting the exponential, sinusoid, and Gaussian functions to data. Those are only three of the nine pattern-book functions. What about the others?\n\n\n\n\nTable 11.2: Shapes of the pattern-book functions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconst\nprop\nsquare\nrecip\ngaussian\nsigmoid\nsinusoid\nexp\nln\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn Blocks ?sec-differentiation-part and ?sec-accumulation-part, you will see how the Gaussian and the sigmoid are intimately related to one another. Once you see that relationship, it will be much easier to understand how to fit a sigmoid to data.\nThe remaining five pattern-book functions, the ones we haven’t discussed in terms of fitting, are the logarithm and the four power-law functions included in the pattern-book set. In Chapter 14 we will introduce a technique for estimating from data the exponent of a single power-law function.\nIn high school, you may have done exercises where you estimated the parameters of straight-line functions and other polynomials from graphs of those functions. In professional practice, such estimations are done with an entirely different and completely automated method called regression. We will introduce regression briefly in Chapter 16. However, the subject is so important that all of Block ?sec-vectors-linear-combinations is devoted to it and its background.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting features</span>"
    ]
  },
  {
    "objectID": "Modeling/11-fitting-features.html#sec-polishing-promise",
    "href": "Modeling/11-fitting-features.html#sec-polishing-promise",
    "title": "11  Fitting features",
    "section": "11.6 Polishing parameters",
    "text": "11.6 Polishing parameters\nOften, fitting parameters to match a pattern seen in data can be done automatically (or semi-automatically) by software. When there are multiple inputs to the function, practicality demands that automated techniques be used. And even when it’s easy to estimate parameters by eye, as with the examples in this chapter, they can be improved by use of function-fitting software. We call this improvement in estimated parameters “polishing.”\nAn example of such automated fitting is given in Listing 7.2. We will return to the topic in more depth in Chapter 16.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting features</span>"
    ]
  },
  {
    "objectID": "Modeling/11-fitting-features.html#footnotes",
    "href": "Modeling/11-fitting-features.html#footnotes",
    "title": "11  Fitting features",
    "section": "",
    "text": "Cynthia Tedore & Dan-Eric Nilsson (2019) “Avian UV vision enhances leaf surface contrasts in forest environments”, Nature Communications 10:238 -Figure 11.3 and -Figure 11.4 ↩︎",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fitting features</span>"
    ]
  },
  {
    "objectID": "Modeling/12-low-order-polynomials.html",
    "href": "Modeling/12-low-order-polynomials.html",
    "title": "12  Low-order polynomials",
    "section": "",
    "text": "12.1 Polynomials\nRecall that the monomials are the power-law functions with non-negative, integer exponents: \\(x^0\\), \\(x^1\\), \\(x^2\\), \\(x^3\\), and so on. The “and so on” refers to even higher integer exponents such as \\(x^4\\) or \\(x^{51}\\) or \\(x^{213}\\), to name but a few. The more common name for a linear combination of monomials is polynomial.\nFor instance, a fifth-order polynomial consists of a linear combination of monomials up to order 5. That is, up to \\(x^5\\). This will have six terms because we count the order of the monomials starting with 0. \\[g(t) \\equiv a_0 + a_1 t + a_2 t^2 + a_3 t^3 + a_4 t^4 + a_5 t^5\\ .\\]\nThe challenge in shaping a polynomial is to find the scalar multipliers—usually called coefficients when it comes to polynomials—that give us the shape we want. This might seem to be a daunting task, and it is for a human. But it can easily be handled using volumes of arithmetic, too much arithmetic for a human to take on but ideally suited for computing machines.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Low-order polynomials</span>"
    ]
  },
  {
    "objectID": "Modeling/12-low-order-polynomials.html#polynomials",
    "href": "Modeling/12-low-order-polynomials.html#polynomials",
    "title": "12  Low-order polynomials",
    "section": "",
    "text": "The first two terms in the polynomial \\(g(t)\\) could be written using exponents, like this: \\[ g(t) \\equiv a_0 t^0 + a_1 t^1 + \\cdots\\] In practice, nobody writes out explicitly the \\(t^0\\) function. Instead, recognizing that \\(t^0 = 1\\), we write the first term simply as \\(a_0\\). Similarly, rather than writing \\(t^1\\) in the second term, we write \\(a_1 t\\), without the exponent. This practice makes the formulas for polynomials more concise but at the cost of failing to remind the reader that all the functions in the linear combination are monomials.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Low-order polynomials</span>"
    ]
  },
  {
    "objectID": "Modeling/12-low-order-polynomials.html#low-order-polynomial-models",
    "href": "Modeling/12-low-order-polynomials.html#low-order-polynomial-models",
    "title": "12  Low-order polynomials",
    "section": "12.2 Low-order polynomial models",
    "text": "12.2 Low-order polynomial models\nPolynomials in general can show a wide variety of snake-like patterns. A fifth-order polynomial can have up to four internal curves. A tenth-order polynomial can have 9 internal curves, and so. There is, however, rarely a need for generating functions with all those curves. Instead, a great deal of modeling work can be accomplished with just first-order polynomials (no internal curves) or second-order polynomials (one internal curve).\n\\[\\begin{eqnarray}\n\\textbf{First-order: }\\ \\ \\ \\ \\ & f_1(t) \\equiv b_0 + b_1 t\\\\\n\\textbf{Second-order: }\\ \\ \\ \\ \\ & f_2(t) \\equiv c_0 + c_1 t + c_2 t^2\n\\end{eqnarray}\\]\n\n\nNote that we are using different names for the coefficients in each of the polynomial examples. The only significance of this is a reminder that each of the coefficients can be any number at all and isn’t necessarily related to any of the other coefficients. In addition to the usual \\(a\\), \\(b\\), \\(c\\), we’ve used the Greek alpha, beta, and gamma, that is \\(\\alpha\\), \\(\\beta\\), and \\(\\gamma\\). The subscript on the coefficient name indicates which term it belongs to. For instance, the coefficient on the \\(y^2\\) term of the \\(h_c\\) polynomial is named \\(\\gamma_{yy}\\) while the coefficient on the \\(x y\\) term has the subscript \\(_{xy}\\). Always, the coefficients are constant quantities and not functions of \\(x\\) or any other input.\nIn high-school mathematics, polynomials are often written without subscript, for instance \\(a x^2 + b x + c\\). This can be fine when working with only one polynomial at a time, but in modeling we often need to compare multiple, related polynomials.\nYou may prefer to think about a first-order polynomial as a straight-line function. Similarly, a second-order polynomial is also known as a “quadratic” or even a “parabola.” Nonetheless, it is good to see them as polynomials distinguished by their order. This puts them into a general framework, all of which can be handled by the technology of linear combinations. And polynomials can also involve more than one input. For instance, here are three polynomial forms that involve inputs \\(x\\) and \\(y\\):\n\\[\\begin{eqnarray}\nh_a(x, y) &\\equiv & \\alpha_0 + \\alpha_x\\, x + \\alpha_y\\, y\\\\\nh_b(x, y) &\\equiv & \\beta_0 + \\beta_x\\, x + \\beta_y\\, y + \\beta_{xy}\\, x y\\\\\nh_c(x, y) &\\equiv & \\gamma_0 + \\gamma_x\\, x + \\gamma_y\\, y + \\gamma_{xy}\\, x y + \\gamma_{xx}\\, x^2 + \\gamma_{yy}\\, y^2\n\\end{eqnarray}\\]\nThe reason to work with first- and second-order polynomials is rooted in the experience of modelers. Second-order polynomials provide a useful amount of flexibility while remaining simple and avoiding pitfalls.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Low-order polynomials</span>"
    ]
  },
  {
    "objectID": "Modeling/12-low-order-polynomials.html#eight-simple-shapes",
    "href": "Modeling/12-low-order-polynomials.html#eight-simple-shapes",
    "title": "12  Low-order polynomials",
    "section": "12.3 Eight simple shapes",
    "text": "12.3 Eight simple shapes\nAn easy way to think about how to use low-order polynomials in modeling is to think about the shapes of their graphs. Figure 12.1 shows eight simple shapes for functions with a single input that occur often in modeling.\n\n\n\n\n\n\n\n\nFigure 12.1: The eight simple shapes of functions with one input.\n\n\n\n\n\nRecall that Chapter 6 introduced terms such as concavity, monotonicity, and slope for describing functions. To choose among these shapes, consider your modeling context:\n\nis the relationship positive (slopes up) or negative (slopes down)?\nis the relationship monotonic or not?\nis the relationship concave up, concave down, or neither?\n\nEach of the eight simple shapes corresponds to a particular set of answers to these equations. Consider these modeling contexts as examples:\n\nHow many minutes can you run as a function of speed? Concave down and downward sloping: Shape (F). In everyday terms, you wear out faster if you run at high speed.\nHow much fuel is consumed by an aircraft as a function of distance? For long flights, the function is concave up and positive sloping: Shape (D). In everyday terms: fuel use increases with distance, but the amount of fuel you have to carry also increases with distance. A heavy aircraft uses more fuel per mile.\nHow far can you walk as a function of time? Steep-then-shallow and concave down: Shape (E). Your pace slows as you get tired.\nHow does the stew taste as a function of saltiness? There is a local maximum: Shape (H). The taste improves as the amount of salt increases … up to a point. Too much salt and the stew is unpalatable.\nThe incidence of an out-of-control epidemic versus time is concave up, but shallow-then-steep. As the epidemic is brought under control, the decline is steep-then-shallow and concave up. Over the whole course of an epidemic, there is a maximum incidence. Experience shows that epidemics can have a phase where incidence reaches a local minimum: a decline as people practice social distancing followed by an increase as people become complacent.\nIn micro-economic theory there are production functions that describe how much of a good is produced at any given price, and demand functions that describe how much of the good will be purchased as a function of price. As a rule, production increases with price and demand decreases with price.\nIn the short term, production functions tend to be concave down, since it is hard to squeeze increased production out of existing facilities. Production functions are Shape (E).\n\nFor demand in the short term, functions will be concave up when there is some group of consumers who have no other choice than to buy the product. Downward sloping and concave up: Shape (C). In the long term, consumption functions can be concave down as consumers find alternatives to the high-priced good. For example, high prices of gasoline may, in the long term, prompt a switch to more efficient cars, hybrids, or electric vehicles. This will push demand down steeply.\n\n\nRemarkably, all the eight simple shapes can be generated by appropriate choices for the coefficients in a second-order polynomial: \\(g(x) = a_0 + a_1 x + a_2 x^2\\). So long as \\(a_2 \\neq 0\\), the graph of the second-order polynomial will be a parabola.\n\nThe parabola opens upward if \\(0 &lt; a_2\\). That is the shape of a local minimum.\nThe parabola opens downward if \\(a_2 &lt; 0\\). That is the shape of a local maximum\n\nConsider what happens if \\(a_2 = 0\\). The function becomes simply \\(a_0 + a_1\\, x\\), the straight-line function.\n\nWhen \\(0 &lt; a_1\\) the line slopes upward.\nWhen \\(a_1 &lt; 0\\) the line slopes downward.\n\nTo produce the steep-then-shallow or shallow-then-steep shapes, you also need to restrict the function domain to be on one side or another of the turning point of the parabola as shown in Figure 12.2.\n\n\n\n\n\n\n\n\nFigure 12.2: Four of the eight simple shapes correspond to the sides of the parabola. The labels refer to the graphs in Figure 12.1.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Low-order polynomials</span>"
    ]
  },
  {
    "objectID": "Modeling/12-low-order-polynomials.html#sec-low-order-two",
    "href": "Modeling/12-low-order-polynomials.html#sec-low-order-two",
    "title": "12  Low-order polynomials",
    "section": "12.4 Polynomials with two inputs",
    "text": "12.4 Polynomials with two inputs\nFor functions with two inputs, the low-order polynomial approximation looks like this:\n\\[g(x, y) \\equiv a_0 + a_x x + a_y y + a_{xy} x y + a_{yy} y^2 + a_{xx} x^2\\]\nIt helps to have different names for the various terms. It is not too bad to say something like, “the \\(a_{xy}\\) term.” (Pronunciation: “a sub x y” or “a x y”) But the proper names are: linear terms, quadratic terms, and interaction term. And a shout out to \\(a_0\\), the constant term.\n\\[g(x, y) \\equiv a_0 + \\underbrace{a_x x + a_y y}_\\text{linear terms} \\ \\ \\ +\n\\underbrace{a_{xy} x y}_\\text{interaction term} +\\ \\ \\  \\underbrace{a_{yy} y^2 + a_{xx} x^2}_\\text{quadratic terms}\\]\nThe interaction term arises in models of phenomena such as the spread of epidemics, the population dynamics of predator and prey animals, and the rates of chemical reactions. In each of these situations, one thing is interacting with another: a predator killing a prey animal, an infective individual meeting a person susceptible to the disease, one chemical compound reacting with another.\nUnder certain circumstances, modelers include one or both quadratic terms, as in \\[h_3(x, y) \\equiv c_0 + c_x\\, x + c_y\\, y + c_{xy}\\,x\\, y + \\underbrace{c_{yy}\\, y^2}_\\text{quadratic in y}\\] The skilled modeler can often deduce which terms to include from basic facts about the system being modeled. We will need some additional calculus concepts before we can explain this straightforwardly.\n\n## Loading required namespace: plotly\n\nA second-order polynomial with two inputs can take on any one of three shapes: a bowl, a hilltop, or a saddle.\n\n\n\n\n\n\n\n\nFigure 12.3: The three forms for a second-order polynomial with two inputs.\n\n\n\n\n\nOther shapes for modeling can be extracted from these three basic shapes. For example, the lower-right quadrant of the Saddle has the shape of seats in an amphitheater.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Low-order polynomials</span>"
    ]
  },
  {
    "objectID": "Modeling/12-low-order-polynomials.html#theory-out-of-a-hat",
    "href": "Modeling/12-low-order-polynomials.html#theory-out-of-a-hat",
    "title": "12  Low-order polynomials",
    "section": "12.5 Theory out of a hat",
    "text": "12.5 Theory out of a hat\nThe start of Chapter 11 introduced a little mystery. Newton introduced his Law of Cooling in the 17th century: The rate at which an object cools depends on the difference in temperature between the object and its ambient environment. But in the 17th century, there was no precise way to measure a rate of temperature change. So how did Newton do it?\nEven with primitive thermometers, one can confirm that a mug of hot water will cool and a glass of cold water will warm to room temperature and stay there. So Newton could deduce that the rate of temperature change is zero when the object’s temperature is the same as the environment. Similarly, it is easy to observe with a primitive thermometer that a big difference in temperature between an object and its environment produces a rapid change in temperature, even if you cannot measure the rate precisely. So the rate of cooling is a function of the temperature difference \\(\\Delta T\\) between object and environment.\nWhat kind of function?\nLow-order polynomials to the rescue! The simplest model is that the rate of cooling will be \\(a_0 + a_1 \\Delta T\\), a first-order polynomial. But we know that the rate of cooling is zero when \\(\\Delta T = 0\\), implying that \\(a_0=0\\). All that is left is the first-order term \\(\\Delta T\\), which you can recognize as the proportional() function.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Low-order polynomials</span>"
    ]
  },
  {
    "objectID": "Modeling/13-operations.html",
    "href": "Modeling/13-operations.html",
    "title": "13  Operations on functions",
    "section": "",
    "text": "13.1 Zero finding\nA function is a mechanism for turning any given input into an output. Zero finding is about going the other way: given an output value, find the corresponding input. As an example, consider the exponential function \\(e^x\\). Given a specific input, say \\(x=2.135\\) you can easily compute the corresponding output:\nexp(2.135)\n## [1] 8.457047\nBut suppose the information you have at hand is in the form of an output from the function, say \\(e^{x_0} = 4.93\\). We don’t (yet) know \\(x_0\\) but, whatever it might be, we know that \\(e^{x_0}\\) will produce the value 4.93.\nHow do you find the specific input \\(x_0\\) that will produce that output? The answer typically presented in high school is to apply another function, \\(\\ln()\\), to the output:\nlog(4.93)\n## [1] 1.595339\nTo confirm that the result 1.595339 is correct, apply the exponential function to it and check that the output is the same as the original, given output 4.93.\nexp(1.595339)\n## [1] 4.93\nThis process works because we happen to have a function at hand, the logarithm, that is perfectly set up to “undo” the action of the exponential function. In high school, you learned a handful of function/inverse pairs: exp() and log() as you’ve just seen, sin() and arcsin(), square and square root, etc.\nAnother situation that is usually addressed in high school is inverting low-order polynomial functions. For instance, suppose your modeling function is \\(g(x) \\equiv 1.7 - 0.85 x + 0.063 x^2\\) and you seek the \\(x_0\\) such that \\(g(x_0) = 3\\). High school students are taught to approach such problems in a process using the quadratic formula. to apply the quadratic formula, you need to place the problem into a standard format, not \\[1.7 - 0.85 x + 0.063 x^2 = 3\\] but \\[0.063\\, x^2 - 0.85\\, x - 1.4 = 0\\]\nOne reason that low-order polynomials are popular in modeling is that such operations are straightforward.\nIf none of the high-school approaches are suited to your modeling function, as is often the case, you can still carry out the zero-finding operation.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operations on functions</span>"
    ]
  },
  {
    "objectID": "Modeling/13-operations.html#zero-finding",
    "href": "Modeling/13-operations.html#zero-finding",
    "title": "13  Operations on functions",
    "section": "13.2 Optimization",
    "text": "The name “zero-finding” can be a little misleading. The objective is find \\(x_0\\) such that \\(h(x_0) = b\\). In this sense, “b-finding” would be a more appropriate name. Instead of chasing after honey as “b-finding” suggests, we reformat the problem into finding \\(x_0\\) such that \\(h(x_0) - b = 0\\). In other words, we look for zeros of the function \\(h(x) - b\\).\n\n\n\n13.1.1 Graphical zero-finding\n\n\n\n\n## Scale for y is already present.\n## Adding another scale for y, which will replace the existing scale.\n\n\n\n\n\n\n\n\n\n\nFigure 13.1: Finding an \\(x_0\\) such that \\(h(x_0) = 3\\)\n\n\n\nConsider any function \\(h(x)\\) that you constructed by linear combination and/or function multiplication and/or function composition. To illustrate, we will work with the function \\(h(x)\\) graphed in Figure 13.1.\nSuppose the output for which we want to find a corresponding input is 3, that is, we want to find \\(x_0\\) such that \\(h(x_0)=3\\).\nThe steps of the process are:\n\nGraph the function \\(h(x)\\) over a domain interval of interest.\nDraw a horizontal line located at the value on the right-hand side of the equation \\(h(x_0) = 3\\). (This is the \\(\\color{magenta}{\\text{magenta}}\\) line in Figure 13.1.)\nFind the places, if any, where the horizontal line intersects the graph of the function. In Figure 13.1, there are two such values: \\(x_0 = -3.5\\) or \\(x_0 = 2.75\\).\n\n\nThe graph shows a function \\(g(t)\\). Find a value \\(t_0\\) such that \\(g(t_0) = 5\\).\n\n\n\n\n\n\n\n\n\n\nDraw a horizontal line at output level 5.\nFind the t-value where the horizontal line intersects the function graph. There is only one such intersection and that is at about \\(t=1.2\\).\n\nConsequently, \\(t_0 = 1.2\\), at least to the precision possible when reading a graph.\n\nThe graphical approach to zero finding is limited by your ability to locate positions on the vertical and horizontal axis. If you need more precision than the graph provides, you have two options:\n\nTake a step-by-step, iterative approach. Use the graph to locate a rough value for the result. Then refine that answer by drawing another graph, zooming in on a small region around the result from the first step. You can iterate this process, repeatedly zooming in on the result you got from the previous step.\nUse software implementing a numerical zero-finding algorithm. Such software is available in many different computer languages and a variety of algorithms is available, each with its own merits and demerits.\n\n\n\n13.1.2 Numerical zero finding\nIn this book, for consistency with our notation, we use the R/mosaic Zeros() function. The first argument to Zeros() is a tilde expression and the second argument an interval of the domain over which to search.\nZeros() is set up to find inputs where the mathematical function defined in the tilde expression produces zero as an output. But suppose you are dealing with a problem like \\(f(x) = 10\\)? You can modify the tilde expression so that it implements a slightly different function: \\(f(x) - 10\\). If we can find \\(x_0\\) such that \\(f(x_0) - 10 = 0\\), that will also be the \\(x_0\\) satisfying \\(f(x_0) = 10\\).\n\nThe point of this example is to show how to use Zeros(), so we will define a function \\(f(x)\\) using doodle_fun() from R/mosaic. This constructs a function by taking a linear combination of other functions selected at random. The argument seed=579 determines which functions will be in the linear combination. The function is graphed in Figure 13.2.\n\nf &lt;- doodle_fun( ~ z, seed=579)\n\nWe want to find the zeros of the function \\(f(x) - 10\\) which corresponds to solving \\(f(x) = 10\\). The function Zeros() handles this for us.\n\nZeros(f(z) - 10 ~ z, domain(z=-4:4))\n## # A tibble: 2 × 2\n##         z      .output.\n##     &lt;dbl&gt;         &lt;dbl&gt;\n## 1 -2.92   -0.0000000344\n## 2  0.0795 -0.00000118\n\nThe output produced by Zeros() is a data frame with one row for each of the \\(x_0\\) found. Here, two values were found: \\(x_0 = -2.92\\) and \\(x_0 = 0.0795\\), shown as vertical lines in Figure 13.2. The .output column reports \\(f(x_0)\\). In principal, this should be exactly zero. However, computer arithmetic is not always exactly precise. Even so, numbers as small as those in the .output. column—\\(-3.4 \\times 10^{-8}\\) for example—are miniscule compared to the range of values seen in Figure 13.2.\n\n\n\n\nf &lt;- doodle_fun( ~ z, seed=579)\nslice_plot(f(x) ~ x, bounds(x=-4:4)) |&gt;\n  gf_hline(yintercept = ~ 10, color=\"magenta\") |&gt;\n  gf_vline(xintercept = c(-2.919, 0.0795), color = \"red\", alpha=0.25)\n## Warning: `geom_vline()`: Ignoring `mapping` because `xintercept` was provided.\n\n\n\n\n\n\n\n\n\n\nFigure 13.2\n\n\n\n\n13.2 Optimization\nOptimization problems consist of both a modeling phase and a solution phase (that is, an information extraction phase). We use our knowledge of how the world works for the modeling phase. Then we extract information in the form we want from the solution phase.\nIn this chapter we will deal only with the solution phase. In real work, the modeling phase is essential\n\n13.2.1 Graphical optimization\nLook for local peaks, then read off the input that generates the value at the peak.\n\n\n13.2.2 Numerical optimization\nWhen it comes to functions, maximization is the process of finding an input to the function that produces a larger output than any of the other, nearby inputs.\nTo illustrate, Figure 13.3 shows a function with two peaks.\n\n\n\n\n\n\n\n\n\n\n\n(a) a function with two peaks\n\n\n\n\n\n\n\nFigure 13.3: A function with two peaks\n\n\n\nJust as you can see a mountain top from a distance, so you can see where the function takes on its peak values. Draw a vertical line through each of the peaks. The input value corresponding to each vertical line is called an argmax, short for “the argument2 at which the function reaches a local maximum value.\nMinimization refers to the same technique, but where the vertical lines are drawn at the deepest point in each “valley” of the function graph. An input value located in one of those valleys is called an argmin.\nOptimization is a general term that covers both maximization and minimization.\n\n\n13.2.3 Numerical optimization\nThe R/mosaic argM() function finds a mathematical function’s argmax and argmin over a given domain. It works in exactly the same way as slice_plot(), but rather than drawing a graphic it returns a data frame giving the argmax in one row and the argmin in another. For instance, the function shown in Figure 13.3 is \\(h()\\), generated by doodle_fun():\n\nh &lt;- doodle_fun(~ x, seed=7293)\nargM(h(x) ~ x, domain(x=-5:5))\n## # A tibble: 2 × 3\n##        x .output. concavity\n##    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 -1.68      1.93         1\n## 2  0.173     8.25        -1\n\nThe x column holds the argmax and argmin, the .output. column gives the value of the function output for the input x. The concavity column tells whether the function’s concavity at x is positive or negative. Near a peak, the concavity will be negative; near a valley, the concavity is positive. Consequently, you can see that the first row of the data frame corresponds to a local minimum and the second row is a local maximum.\nargM() is set up to look for a single argmax and a single argmin in the domain interval given as the second argument. In Figure 13.3 there are two local peaks and two local valleys. argM() gives only the largest of the peaks and the deepest of the valleys.\n\n\n\n13.3 Iteration\nMany computations involve starting with a guess followed by a step-by-step process of refining the guess. A case in point is the process for calculating square roots. There isn’t an operational formula for a function that takes a number as an input and produces the square root of that number as the output. When we write \\(\\sqrt{\\strut x}\\) we aren’t saying how to calculate the output, just describing the sort of output we are looking for.\nThe function that is often used to calculate \\(\\sqrt{x}\\) is better():\n\\[\\text{better(guess)} = \\frac{1}{2}\\left( \\text{guess} + \\frac{x}{\\text{guess}}\\right)\\ .\\]\nIt may not be at all clear why this formula is related to finding a square root. Let’s put that matter off until the end of the section and concentrate our attention on how to use it.\nTo start, let’s define the function for the computer:\n\nbetter &lt;- makeFun((guess + x/guess)/2 ~ guess)\n\nNotice that \\(x\\) is cast in the role of a parameter of the function rather than an input to the function.\nSuppose we want to apply the square root function to the input 55, that is, calculate \\(\\sqrt{\\strut x=55}\\). The value we should assign to \\(x\\) is therefore 55.\nTo calculate better(guess) we need not only \\(x=55\\) but a value for the guess. What should be this value and what will we do with the quantity better(guess) when we’ve calculated it.\nWithout explanation, we will use guess = 1, regardless of the value of \\(x\\). Calculating the output …\n\nbetter(1, x=55)\n## [1] 28\n\nNeither our guess 1 nor the output 28 are \\(\\sqrt{\\strut x=55}\\). (Having long-ago memorized the squares of integers, we know \\(\\sqrt{\\strut x=55}\\) will be somewhere between 7 and 8. Neither 1 nor 28 are in that interval.)\nThe people—more than two thousand years ago—who invented the ideas behind the better() function were convinced that better() constructs a better guess for the answer we seek. It is not obvious why 28 should be a better guess than 1 for \\(\\sqrt{\\strut x=55}\\) but, out of respect, let’s accept their claim.\nThis is where iteration comes in. Even if 28 is a better guess than 1, 28 is still not a good guess. But we can use better() to find something better than 28:\n\nbetter(28, x=55)\n## [1] 14.98214\n\nTo iterate an action means to perform that action over and over again. (“Iterate” stems from the Latin word iterum, meaning “again.”) A bird iterates its call, singing it over and over again. In mathematics, “iterate” has a twist. When we repeat the mathematical action, we will draw on the results of the previous angle rather than simply repeating the earlier calculation.\nContinuing our iteration of better() …\n\nbetter(14.98214, x=55)\n## [1] 9.326589\nbetter(9.326589, x=55)\n## [1] 7.611854\nbetter(7.611854, x=55)\n## [1] 7.418713\nbetter(7.418713, x=55)\n## [1] 7.416199\nbetter(7.416199, x=55)\n## [1] 7.416198\n\nIn the last step, the output of better() is practically identical to the input, so no reason to continue. We can confirm that the last output is a good guess for \\(\\sqrt{\\strut x=55}\\):\n\n7.416198^2\n## [1] 54.99999\n\n\n13.3.1 Graphical iteration\nTo iterate graphically, we graph the function to be iterated and mark the initial guess on the horizontal axis. For each iteration step, trace vertically from the current point to the function, then horizontally to the line of identity (blue dots). The result will be the starting point for the next guess.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThree steps of iteration of better() starting with an initial guess of 1. ### Numerical iteration\nUse the R/mosaic Iterate() function. The first argument is a tilde expression defining the function to be iterated. The second is the starting guess. The third is the number of iteration steps. For instance:\n\nIterate(better(guess, x=55) ~ guess, x0=1, n=8)\n##   n     guess\n## 1 0  1.000000\n## 2 1 28.000000\n## 3 2 14.982143\n## 4 3  9.326590\n## 5 4  7.611854\n## 6 5  7.418713\n## 7 6  7.416199\n## 8 7  7.416198\n## 9 8  7.416198\n\nThe output produced by Iterate() is a data frame. The initial guess is in the row with \\(n=0\\). Successive rows give the output, step by step, with each new iteration step.\n\n\n\n\n\n\nWhere does better() come from?\n\n\n\nFor calculating square roots, we used the function \\[\\text{better}(y) = \\frac{1}{2}\\left( y + \\frac{x}{y}\\right)\\ .\\] Let’s suppose you happened on a guess that is exactly right, that is \\(y = \\sqrt{x}\\). There is no way to improve on a guess that is exactly right, so the best better() can do is to return the guess unaltered. Indeed it does: \\[\\text{better}\\left(y=\\!\\!\\sqrt{\\strut x}\\ \\right) = \\frac{1}{2}\\left( \\sqrt{\\strut x} + \\frac{x}{\\sqrt{x}} \\right)\\ = \\frac{1}{2}\\left(\\sqrt{\\strut x} + \\sqrt{\\strut x}\\right) = \\sqrt{\\strut x}.\\]\nOf course, the initial guess \\(y\\) might be wrong. There are two ways to be wrong:\n\nThe guess is too small, that is \\(y &lt; \\sqrt{\\strut x}\\).\nThe guess is too big, that is \\(\\sqrt{\\strut x} &lt; y\\).\n\nThe formula for better() is the average of the guess \\(y\\) and another quantity \\(x/y\\). If \\(y\\) is too small, then \\(x/y\\) must be too big. If \\(y\\) is too big, then \\(x/y\\) must be too small.\nAs guesses, the two quantities \\(y\\) and \\(x/y\\) are equivalent in the sense that \\(\\text{better}(y) = \\text{better}(x/y)\\). The average of \\(y\\) and \\(x/y\\) will be closer to the true result than the worst of \\(y\\) or \\(x/y\\); the average will be a better guess.\n\n\n\n\n\n\nFigure 13.4",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operations on functions</span>"
    ]
  },
  {
    "objectID": "Modeling/13-operations.html#optimization",
    "href": "Modeling/13-operations.html#optimization",
    "title": "13  Operations on functions",
    "section": "",
    "text": "Optimization problems consist of both a modeling phase and a solution phase (that is, an information extraction phase). We use our knowledge of how the world works for the modeling phase. Then we extract information in the form we want from the solution phase.\nIn this chapter we will deal only with the solution phase. In real work, the modeling phase is essential\n\n13.2.1 Graphical optimization\nLook for local peaks, then read off the input that generates the value at the peak.\n\n\n13.2.2 Numerical optimization\nWhen it comes to functions, maximization is the process of finding an input to the function that produces a larger output than any of the other, nearby inputs.\nTo illustrate, Figure 13.3 shows a function with two peaks.\n\n\n\n\n\n\n\n\n\n\n\n(a) a function with two peaks\n\n\n\n\n\n\n\nFigure 13.3: A function with two peaks\n\n\n\nJust as you can see a mountain top from a distance, so you can see where the function takes on its peak values. Draw a vertical line through each of the peaks. The input value corresponding to each vertical line is called an argmax, short for “the argument2 at which the function reaches a local maximum value.\nMinimization refers to the same technique, but where the vertical lines are drawn at the deepest point in each “valley” of the function graph. An input value located in one of those valleys is called an argmin.\nOptimization is a general term that covers both maximization and minimization.\n\n\n13.2.3 Numerical optimization\nThe R/mosaic argM() function finds a mathematical function’s argmax and argmin over a given domain. It works in exactly the same way as slice_plot(), but rather than drawing a graphic it returns a data frame giving the argmax in one row and the argmin in another. For instance, the function shown in Figure 13.3 is \\(h()\\), generated by doodle_fun():\n\nh &lt;- doodle_fun(~ x, seed=7293)\nargM(h(x) ~ x, domain(x=-5:5))\n## # A tibble: 2 × 3\n##        x .output. concavity\n##    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 -1.68      1.93         1\n## 2  0.173     8.25        -1\n\nThe x column holds the argmax and argmin, the .output. column gives the value of the function output for the input x. The concavity column tells whether the function’s concavity at x is positive or negative. Near a peak, the concavity will be negative; near a valley, the concavity is positive. Consequently, you can see that the first row of the data frame corresponds to a local minimum and the second row is a local maximum.\nargM() is set up to look for a single argmax and a single argmin in the domain interval given as the second argument. In Figure 13.3 there are two local peaks and two local valleys. argM() gives only the largest of the peaks and the deepest of the valleys.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operations on functions</span>"
    ]
  },
  {
    "objectID": "Modeling/13-operations.html#iteration",
    "href": "Modeling/13-operations.html#iteration",
    "title": "13  Operations on functions",
    "section": "13.3 Iteration",
    "text": "13.3 Iteration\nMany computations involve starting with a guess followed by a step-by-step process of refining the guess. A case in point is the process for calculating square roots. There isn’t an operational formula for a function that takes a number as an input and produces the square root of that number as the output. When we write \\(\\sqrt{\\strut x}\\) we aren’t saying how to calculate the output, just describing the sort of output we are looking for.\nThe function that is often used to calculate \\(\\sqrt{x}\\) is better():\n\\[\\text{better(guess)} = \\frac{1}{2}\\left( \\text{guess} + \\frac{x}{\\text{guess}}\\right)\\ .\\]\nIt may not be at all clear why this formula is related to finding a square root. Let’s put that matter off until the end of the section and concentrate our attention on how to use it.\nTo start, let’s define the function for the computer:\n\nbetter &lt;- makeFun((guess + x/guess)/2 ~ guess)\n\nNotice that \\(x\\) is cast in the role of a parameter of the function rather than an input to the function.\nSuppose we want to apply the square root function to the input 55, that is, calculate \\(\\sqrt{\\strut x=55}\\). The value we should assign to \\(x\\) is therefore 55.\nTo calculate better(guess) we need not only \\(x=55\\) but a value for the guess. What should be this value and what will we do with the quantity better(guess) when we’ve calculated it.\nWithout explanation, we will use guess = 1, regardless of the value of \\(x\\). Calculating the output …\n\nbetter(1, x=55)\n## [1] 28\n\nNeither our guess 1 nor the output 28 are \\(\\sqrt{\\strut x=55}\\). (Having long-ago memorized the squares of integers, we know \\(\\sqrt{\\strut x=55}\\) will be somewhere between 7 and 8. Neither 1 nor 28 are in that interval.)\nThe people—more than two thousand years ago—who invented the ideas behind the better() function were convinced that better() constructs a better guess for the answer we seek. It is not obvious why 28 should be a better guess than 1 for \\(\\sqrt{\\strut x=55}\\) but, out of respect, let’s accept their claim.\nThis is where iteration comes in. Even if 28 is a better guess than 1, 28 is still not a good guess. But we can use better() to find something better than 28:\n\nbetter(28, x=55)\n## [1] 14.98214\n\nTo iterate an action means to perform that action over and over again. (“Iterate” stems from the Latin word iterum, meaning “again.”) A bird iterates its call, singing it over and over again. In mathematics, “iterate” has a twist. When we repeat the mathematical action, we will draw on the results of the previous angle rather than simply repeating the earlier calculation.\nContinuing our iteration of better() …\n\nbetter(14.98214, x=55)\n## [1] 9.326589\nbetter(9.326589, x=55)\n## [1] 7.611854\nbetter(7.611854, x=55)\n## [1] 7.418713\nbetter(7.418713, x=55)\n## [1] 7.416199\nbetter(7.416199, x=55)\n## [1] 7.416198\n\nIn the last step, the output of better() is practically identical to the input, so no reason to continue. We can confirm that the last output is a good guess for \\(\\sqrt{\\strut x=55}\\):\n\n7.416198^2\n## [1] 54.99999\n\n\n13.3.1 Graphical iteration\nTo iterate graphically, we graph the function to be iterated and mark the initial guess on the horizontal axis. For each iteration step, trace vertically from the current point to the function, then horizontally to the line of identity (blue dots). The result will be the starting point for the next guess.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThree steps of iteration of better() starting with an initial guess of 1. ### Numerical iteration\nUse the R/mosaic Iterate() function. The first argument is a tilde expression defining the function to be iterated. The second is the starting guess. The third is the number of iteration steps. For instance:\n\nIterate(better(guess, x=55) ~ guess, x0=1, n=8)\n##   n     guess\n## 1 0  1.000000\n## 2 1 28.000000\n## 3 2 14.982143\n## 4 3  9.326590\n## 5 4  7.611854\n## 6 5  7.418713\n## 7 6  7.416199\n## 8 7  7.416198\n## 9 8  7.416198\n\nThe output produced by Iterate() is a data frame. The initial guess is in the row with \\(n=0\\). Successive rows give the output, step by step, with each new iteration step.\n\n\n\n\n\n\nWhere does better() come from?\n\n\n\nFor calculating square roots, we used the function \\[\\text{better}(y) = \\frac{1}{2}\\left( y + \\frac{x}{y}\\right)\\ .\\] Let’s suppose you happened on a guess that is exactly right, that is \\(y = \\sqrt{x}\\). There is no way to improve on a guess that is exactly right, so the best better() can do is to return the guess unaltered. Indeed it does: \\[\\text{better}\\left(y=\\!\\!\\sqrt{\\strut x}\\ \\right) = \\frac{1}{2}\\left( \\sqrt{\\strut x} + \\frac{x}{\\sqrt{x}} \\right)\\ = \\frac{1}{2}\\left(\\sqrt{\\strut x} + \\sqrt{\\strut x}\\right) = \\sqrt{\\strut x}.\\]\nOf course, the initial guess \\(y\\) might be wrong. There are two ways to be wrong:\n\nThe guess is too small, that is \\(y &lt; \\sqrt{\\strut x}\\).\nThe guess is too big, that is \\(\\sqrt{\\strut x} &lt; y\\).\n\nThe formula for better() is the average of the guess \\(y\\) and another quantity \\(x/y\\). If \\(y\\) is too small, then \\(x/y\\) must be too big. If \\(y\\) is too big, then \\(x/y\\) must be too small.\nAs guesses, the two quantities \\(y\\) and \\(x/y\\) are equivalent in the sense that \\(\\text{better}(y) = \\text{better}(x/y)\\). The average of \\(y\\) and \\(x/y\\) will be closer to the true result than the worst of \\(y\\) or \\(x/y\\); the average will be a better guess.\n\n\n\n\n\n\nFigure 13.4",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operations on functions</span>"
    ]
  },
  {
    "objectID": "Modeling/13-operations.html#footnotes",
    "href": "Modeling/13-operations.html#footnotes",
    "title": "13  Operations on functions",
    "section": "",
    "text": "We use the phrase “extract information” to reflect the purpose of using the operations discussed in this chapter. You may be more familiar with the phrase “find a solution.”↩︎\nAlso known as an input.↩︎",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Operations on functions</span>"
    ]
  },
  {
    "objectID": "Modeling/14-magnitudes.html",
    "href": "Modeling/14-magnitudes.html",
    "title": "14  Magnitude",
    "section": "",
    "text": "14.1 Order of magnitude\nWe will refer to judging the size of numbers by their count of digits as reading the magnitude of the number. To get started, consider numbers that start with 1 followed by zeros, e.g. 100 or 1000. We will quantify the magnitude as the number of zeros: 100 has a magnitude of 2 and 1000 has a magnitude of 3. In comparing numbers by magnitude, we way things like, “1000 is an order of magnitude greater than 100,” or “1,000,000” is five orders of magnitude larger than 10.\nMany phenomena and quantities are better understood in terms of magnitude than in terms of number. An example: Animals, including humans, go about the world in varying states of illumination, from the bright sunlight of high noon to the dim shadows of a half-moon. To be able to see in such diverse conditions, the eye needs to respond to light intensity across many orders of magnitude.\nThe lux is the unit of illuminance in the Système international. This table1 shows the illumination in a range of familiar outdoor settings:\nFor a creature active both night and day, the eye needs to be sensitive over 7 orders of magnitude of illumination. To accomplish this, eyes use several mechanisms: contraction or dilation of the pupil accounts for about 1 order of magnitude, photopic (color, cones) versus scotopic (black-and-white, rods, nighttime) covers about 3 orders of magnitude, adaptation over minutes (1 order), squinting (1 order).\nMore impressively, human perception of sound spans more than 16 orders of magnitude in terms of the energy impinging on the eardrum. The energy density of perceptible sound ranges from the threshold of hearing at 0.000000000001 Watt per square meter to a conversational level of 0.000001 W/m2 to 0.1 W/m2 in the front rows of a rock concert. But in terms of our subjective perception of loudness, each order of magnitude change is perceived in the same way, whether it be from street traffic to vacuum cleaner or from whisper to normal conversation. (The unit of sound measurement is the decibel (dB), with 10 decibels corresponding to an order of magnitude in the energy density of sound.)\n6, 60, 600, and 6000 miles-per-hour are quantities that differ in size by orders of magnitude. Such differences often point to a substantial change in context. A jog is 6 mph, a car on a highway goes 60 mph, a cruising commercial jet goes 600 mph, and a rocket passes through 6000 mph on its way to orbital velocity. From an infant’s crawl to highway cruising is 3 orders of magnitude in speed.\nOf course, many phenomena are not well represented in terms of orders of magnitudes. For example, the difference between normal body temperature and high fever is 0.01 orders of magnitude in temperature.2 An increase of 1 order of magnitude in blood pressure from the normal level would cause instant death! The difference between a very tall adult and a very short adult is about 1/4 of an order of magnitude.\nOrders of magnitude are used when the relevant comparison is a ratio. “A car is 10 times faster than a person,” refers to the ratio of speeds. In contrast, quantities such as body temperature, blood pressure, and adult height are compared using a difference. Fever is 2\\(^\\circ\\)C higher in temperature than normal. A 30 mmHg increase in blood pressure will likely correspond to developing hypertension. A very tall and a very short adult differ by about 2 feet.\nOne clue that thinking in terms of orders of magnitude is appropriate is when you are working with a set of objects whose range of sizes spans one or many factors of 2. Comparing baseball and basketball players? Probably no need for orders of magnitudes. Comparing infants, children, and adults in terms of height or weight? Orders of magnitude may be useful. Comparing bicycles? Mostly they fit within a range of 2 in terms of size, weight, and speed (but not expense!). Comparing cars, SUVs, and trucks? Differences by a factor of 2 are routine, so thinking in terms of order of magnitude is likely to be appropriate.\nAnother clue is whether “zero” means “nothing.” Daily temperatures in the winter are often near “zero” on the Fahrenheit or Celcius scales, but that in no way means there is a complete absence of heat. Those scales are arbitrary. Another way to think about this clue is whether negative values are meaningful. If so, thinking in terms of orders of magnitude is not likely to be useful.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Magnitude</span>"
    ]
  },
  {
    "objectID": "Modeling/14-magnitudes.html#order-of-magnitude",
    "href": "Modeling/14-magnitudes.html#order-of-magnitude",
    "title": "14  Magnitude",
    "section": "",
    "text": "Illuminance\nCondition\n\n\n\n\n110,000 lux\nBright sunlight\n\n\n20,000 lux\nShade illuminated by entire clear blue sky, midday\n\n\n1,000 lux\nTypical overcast day, midday\n\n\n400 lux\nSunrise or sunset on a clear day (ambient illumination)\n\n\n0.25 lux\nA full Moon, clear night sky\n\n\n0.01 lux\nA quarter Moon, clear night sky\n\n\n\n\n\n\n\n\n\n\nTable 14.1: Energy density of sound in various situations. Sound at 85 dB, for extended periods, can cause permanent hearing loss. Exposure to sound at 120 dB over 30 seconds is dangerous.\n\n\n\n\n\nSituation\nEnergy level (dB)\n\n\n\n\nRustling leaves\n10 dB\n\n\nWhisper\n20 dB\n\n\nMosquito buzz\n40 dB\n\n\nNormal conversation\n60 dB\n\n\nBusy street traffic\n70 dB\n\n\nVacuum cleaner\n80 dB\n\n\nLarge orchestra\n98 dB\n\n\nEarphones (high level)\n100 dB\n\n\nRock concert\n110 dB\n\n\nJackhammer\n130 dB\n\n\nMilitary jet takeoff\n140 dB",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Magnitude</span>"
    ]
  },
  {
    "objectID": "Modeling/14-magnitudes.html#sec-counting-digits",
    "href": "Modeling/14-magnitudes.html#sec-counting-digits",
    "title": "14  Magnitude",
    "section": "14.2 Counting digits",
    "text": "14.2 Counting digits\nImagine having a digit counting function called digits(). It takes a number as input and produces a number as output. We have not yet presented a formula for digits(), but for some inputs, the output can be calculated just by counting. For numbers like 0.01 or 10 or 100000, we will define the number of digits to be the count of zeros before For example:\ndigits(10) \\(\\equiv\\) 1\ndigits(100) \\(\\equiv\\) 2\ndigits(1000) \\(\\equiv\\) 3\n… and so on …\ndigits(1,000,000) \\(\\equiv\\) 6\n… and on.\nFor numbers smaller than 1, like 0.01 or 0.0001, we define the number of digits to be the negative of the number of zeros before the 1.\ndigits(0.1) \\(\\equiv\\) -1\ndigits(0.01) \\(\\equiv\\) -2\ndigits(0.0001) \\(\\equiv\\) -4\nThe digits() function easily can be applied to the product of two numbers. For instance:\n\ndigits(1000 \\(\\times\\) 100) = digits(1000) + digits(100) = 3 + 2 = 5.\n\nSimilarly, applying digits() to a ratio gives the difference of the digits of the numerator and denominator, like this:\n\ndigits(1,000,000 \\(\\div\\) 10) = digits(1,000,000) - digits(10) = 6 - 1 = 4\n\nIn practice, digits() is so useful that it could well have been one of our basic modeling functions. Actually, this is very nearly the case: the logarithm is proportional to the number of digits.\nTo illustrate, consider these three calculations of logarithms:\n\n\n\nActive R chunk 14.1: The output is the order of magnitude of the number given as an argument to log().\n\n\n\n0.4342945 * log(100) \n## [1] 2\n0.4342945 * log(1000) \n## [1] 3\n0.4342945 * log(10000)\n## [1] 4\n\n\n\n\nHere is a formula definition of the digits() function.\n\\[\\text{digits}(x) \\equiv \\ln(x) / \\ln(10) \\tag{14.1}\\]\nIn R/mosaic, the analogous definition is:\n\n\n\nActive R chunk 14.2\n\n\n\ndigits &lt;- makeFun(log(x) / log(10) ~ x)\n\n\n\n\nYou may have guessed that digits() is handy for computing differences in terms of orders of magnitude. Here’s how:\n\nMake sure that the quantities are expressed in the same units.\nCalculate the difference between the digits() of the numerical part of the quantity.\n\n\nWhat is the order-of-magnitude difference in velocity between a snail and a walking human? A snail slides at about 1 mm/sec, a human walks at about 5 km per hour. Putting human speed in the same units as snail speed: \\[\\begin{eqnarray}5 \\frac{km}{hr} = \\left[\\frac{1}{3600} \\frac{hr}{sec}\\right] 5 \\frac{km}{hr} &=& \\\\\n\\left[10^6 \\frac{mm}{km}\\right] \\left[\\frac{1}{3600} \\frac{hr}{sec}\\right] 5 \\frac{km}{hr} &=& 1390 \\frac{mm}{sec}\n\\end{eqnarray}\\] Calculating the difference in digits() between 1 and 1390:\n\nlog10(1390) - log10(1)\n## [1] 3.143015\n\nSo, about 3 orders of magnitude difference in speed. To a snail, we walking humans must seem like rockets on their way to orbit!\n\nThe use of factors of 10 in counting orders of magnitude is arbitrary. A person walking and a person jogging are on the edge of being qualitatively different, although their speeds differ by a factor of only 2. Aircraft that cruise at 600 mph and 1200 mph are qualitatively different in design, although the speeds are only a factor of 2 apart. A professional basketball player (height 2 meters or more) is qualitatively different from a third grader (height about 1 meter).\n\n\n\n\n\n\nMath in the World: The “natural” logarithm\n\n\n\nYou may have noticed in Math expression 14.1 or Active R chunk 14.2 the terms \\(1/\\ln(10)\\) and / log(10) are used as a conversion factor. Similarly, in Active R chunk 14.1 the conversion factor 0.4342945 is used. Actually, 0.4342945 and 1 / log(10) are the same number:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nBut why is a conversion factor needed for the digits() calculation?\nThe point of the conversion factor is to have the “units” of the output of digits() correspond to a factor of 10. Such units are called “decades.” The unit of decade is dimensionless, just as the unit for angles—rads or degrees, as you preferr—is dimensionless.\nNaturally, we use decades because of our human habit of writing numbers in base 10, using the digits 0 to 9. I say “naturally” because base 10 is familiar to us. But different people have different notions of what is “natural.”\nWe could have used \\(1/\\ln(2)\\) as the conversion factor in digits() in which case each multiple of 2 in the input corresponds to a change in the output of 1. Or, I should say, “1 bit” because “bit” is the name given to the unit when \\(1/\\ln(2)\\) (that is, 1.442695) is used as the conversion factor.\nAn aesthetic widely admired in the field of mathematics is that having any such conversion factor at all is “unnatural.” The only mathematically pretty conversion factor is 1, that is, no conversion factor at all. If we had used 1 instead of \\(1/\\ln(10)\\) in digits(), the output would be in different units, not in decades.\nThe name mathematicians have given to the version of digits() where the conversion factor is 1 is the natural logarithm. It’s hard to understand the advantages of the natural logarithm until we get further into Calculus. For the natural logarithm, each increase in the input by a factor of 2.7182818281828… leads to an increase in the output by 1. To illustrate, run the code in Active R chunk 14.3.\n\n\n\nActive R chunk 14.3\n\n\n\nlog(63)\n## [1] 4.143135\nlog(2.718281828 * 63)\n## [1] 5.143135",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Magnitude</span>"
    ]
  },
  {
    "objectID": "Modeling/14-magnitudes.html#sec-magnitude-graphics",
    "href": "Modeling/14-magnitudes.html#sec-magnitude-graphics",
    "title": "14  Magnitude",
    "section": "14.3 Magnitude graphics",
    "text": "14.3 Magnitude graphics\nTo display a variable from data that varies over multiple orders of magnitude, it helps to plot the logarithm rather than the variable itself. Let’s illustrate using the Engine data frame, which contains measurements of many different internal combustion engines of widely varying sizes. For instance, we can graph engine RPM (revolutions per second) versus engine mass, as in Figure 14.1.\n\ngf_point(RPM ~ mass, data = Engines)\n\n\n\n\n\n\n\n\nFigure 14.1: Engine RPM versus mass for 39 different enginges plotted on the standard linear axis.\n\n\n\n\nIn the graph, most of the engines have a mass that is … zero. At least that is what it appears to be. The horizontal scale is dominated by the two huge 100,000-pound monster engines plotted at the right end of the graph.\nPlotting the logarithm of the engine mass spreads things out, as in Figure 14.2.\n\ngf_point(RPM ~ mass, data = Engines) %&gt;%\n  gf_refine(scale_x_log10())\n\n\n\n\n\n\n\n\nFigure 14.2: Engine RPM versus mass on semi-log axes.\n\n\n\n\nNote that the horizontal axis has been labeled with the actual mass (in pounds), with the labels evenly spaced in terms of their logarithm. This presentation, with the horizontal axis constructed this way, is called a semi-log plot.\nWhen both axes are labeled this way, we have a log-log plot, as shown in Figure 14.3.\n\ngf_point(RPM ~ mass, data = Engines) %&gt;%\n  gf_refine(\n    scale_x_log10(),\n    scale_y_log10()\n    )\n\n\n\n\n\n\n\n\nFigure 14.3: Engine RPM versus mass on log-log axes.\n\n\n\n\nSemi-log and log-log axes are widely used in science and economics, whenever data spanning several orders of magnitude need to be displayed. In the case of the engine RPM and mass, the log-log axis shows that there is a graphically simple relationship between the variables. Such axes are very useful for displaying data but can be hard for the newcomer to read quantitatively. For example, calculating the slope of the evident straight-line relationship in Figure 14.3 is extremely difficult for a human reader and requires translating the labels into their logarithms.\n\n\n\n\n\n\nMath in the World: Boyle’s Law\n\n\n\nRobert Boyle (1627-1691) was a founder of modern chemistry and the scientific method in general. As any chemistry student already knows, Boyle sought to understand the properties of gasses. His results are summarized in Boyle’s Law.\nThe data frame Boyle contains two variables from one of Boyle’s experiments as reported in his lab notebook: pressure in a bag of air and volume of the bag. The units of pressure are mmHg and the units of volume are cubic inches.3\nFamously, Boyle’s Law states that, at a constant temperature, the pressure of a constant mass of gas is inversely proportional to the volume occupied by the gas. Figure 14.4 shows a cartoon of the relationship.\n\n\n\n\n\n\n\n\nFigure 14.4: A cartoon illustrating Boyle’s Law. Source: NASA Glenn Research Center\n\n\n\n\n\nFigure 14.5 plots out Boyle’s actual experimental data. I\n\ngf_point(pressure ~ volume, data = Boyle) %&gt;%\n  gf_lm()\n## Warning: Using the `size` aesthetic with geom_line was deprecated in ggplot2 3.4.0.\n## ℹ Please use the `linewidth` aesthetic instead.\n\n\nYou can see a clear relationship between pressure and volume, but it is hardly a linear relationship.\nPlotting Boyle’s data on log-log axes reveals that, in terms of the logarithm of pressure and the logarithm of volume, the relationship is linear.\n\ngf_point(log(pressure) ~ log(volume), data = Boyle) %&gt;%\n  gf_lm()\n\n\n\n\n\n\n\n\n\n\nFigure 14.5: A plot of Boyle’s pressure vs volume data on linear axes. The straight line model is a poor representation of the pattern seen in the data.\n\n\n\n\n\n\n\n\n\n\nFigure 14.6: Plotting the logarithm of pressure against the logarithm of volume reveals a straight-line relationship.\n\n\n\n\nFigure 14.6 shows that Boyle’s log-pressure and log-volume data are a straight-line function. In other words:\n\\[\\ln(\\text{Pressure}) = a + b \\ln(\\text{Volume})\\]\nYou can find the slope \\(b\\) and intercept \\(a\\) from the graph. For now, we want to point out the consequences of the straight-line relationship between logarithms.\nExponentiating both sides gives \\[e^{\\ln(\\text{Pressure})} = \\text{Pressure} = e^{a + b \\ln(\\text{Volume})} = e^a\\  \\left[e^{ \\ln(\\text{Volume})}\\right]^b = e^a\\, \\text{Volume}^b\\] or, more simply (and writing the number \\(e^a\\) as \\(A\\))\n\\[\\text{Pressure} = A\\,  \\text{Volume}^b\\] A power-law relationship!",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Magnitude</span>"
    ]
  },
  {
    "objectID": "Modeling/14-magnitudes.html#sec-reading-log-axes",
    "href": "Modeling/14-magnitudes.html#sec-reading-log-axes",
    "title": "14  Magnitude",
    "section": "14.4 Reading logarithmic scales",
    "text": "14.4 Reading logarithmic scales\nPlotting the logarithm of a quantity gives a visual display of the magnitude of the quantity and labels the axis as that magnitude. A useful graphical technique is to label the axis with the original quantity, letting the position on the axis show the magnitude.\nTo illustrate, Figure 14.7(left) is a log-log graph of horsepower versus displacement for the internal combustion engines reported in the Engines data frame. The points are admirably evenly spaced, but it is hard to translate the scales to the physical quantity. The right panel in Figure 14.7 shows the same data points, but now the scales are labeled using the original quantity.\n\n\n\n\n\n\n\n\nFigure 14.7: Horsepower versus displacement from the Engines data.frame plotted with log-log scales.\n\n\n\n\n\nThe tick marks on the vertical axis in the left pane are labeled for 0, 1, 2, 3, and 4. These numbers do not refer to the horsepower itself, but to the logarithm (base 10) of the horsepower. The right pane has tick labels that are in horsepower at positions marked 1, 10, 100, 1000, and 10000.\nSuch even splits of a 0-100 scale are not appropriate for logarithmic scales. One reason is that 0 cannot be on a logarithmic scale in the first place since \\(\\log(0) = -\\infty\\).\nAnother reason is that 1, 3, and 10 are pretty close to an even split of a logarithmic scale running from 1 to 10. It is something like this:\n1              2            3          5            10     x\n|----------------------------------------------------|\n0               1/3         1/2        7/10          1     log(x)\nIt is nice to have the labels show round numbers. It is also nice for them to be evenly spaced along the axis. The 1-2-3-5-10 convention is a good compromise; almost evenly separated in space yet showing simple round numbers.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Magnitude</span>"
    ]
  },
  {
    "objectID": "Modeling/14-magnitudes.html#mini-project",
    "href": "Modeling/14-magnitudes.html#mini-project",
    "title": "14  Magnitude",
    "section": "14.5 Mini-project",
    "text": "14.5 Mini-project\n\nExercise COVID-19 pandemic\nYou have likely heard the phrase “exponential growth” used to describe the COVID-19 pandemic. Let’s explore this idea using actual data.\nThe COVID-19 Data Hub is a collaborative effort of universities, government agencies, and non-governmental organizaions (NGOs) to provide up-to-date information about the pandemic. We will use the data about the US at the whole-country level. (There is also data at state and county levels. Documentation is available via the link above.)\nPerhaps the simplest display is to show the number of cumulative cases (the confirmed variable) and deaths as a function of time. We will focus on the data up to June 30, 2020.\nCopy the R/mosaic commands below into your R console to produce a graph of confirmed cases in blue and deaths in tan.\n\ngf_line(\n  deaths ~ date, \n  data = Covid_US |&gt; \n    filter(date &lt; as.Date(\"2020-06-30\")), \n  color = \"orange3\") %&gt;%\n  gf_line(confirmed ~ date, color = \"blue\")\n\n\n\n\n\n\n\n\n\n\n**Part A**  As of mid June, 2020 about how many confirmed cases were there? (Note that the labeled tick marks refer to the beginning of the month, so the point labeled `Feb` is February 1.) \nabout 50,000 about 200,000 about 500,000 about 1,000,000 about 2,000,000 about 5,000,000\n\n\nThis code makes the same graphic as above, but taking the logarithm (base 10) of the number of cases (that is, confirmed) and of the number of deaths. Since we are taking the logarithm of only the y-variable, this is called a “semi-log” plot.\n\ngf_point(\n  log10(confirmed) ~ date, \n  data = Covid_US |&gt; \n    filter(date &lt; as.Date(\"2020-06-30\")), \n  color = \"dodgerblue\") %&gt;%\n  gf_point(log10(deaths) ~ date, color = \"orange3\") \n\n\n\n\n\n\n\n\nUp through the beginning of March in the US, it is thought that most US cases were in people traveling into the US from hot spots such as China and Italy and the UK, as opposed to contagion between people within the US. (Such contagion is called “community spread.”) So let’s look at the data representing community spread, from the start of March onward.\nExponential growth appears as a straight-line on a semi-log plot. Obviously, the overall pattern of the curves is not a straight line. The explanation for this is that the exponential growth rate changes over time, perhaps due to public health measures (like business closures, mask mandates, etc.)\nThe first (official) US death from Covid-19 was recorded was recorded on Feb. 29, 2020. Five more deaths occurred two days later, bringing the cumulative number to 6.\n\n\n**Part B**  The tan data points for Feb 29/March 1 show up at zero on the vertical scale for the semi-log plot. The tan data point for March 2 is at around 2 on the vertical scale. Is this consistent with the facts stated above? \n\nNo. The data contradict the facts.\nYes. The vertical scale is in log (base 10) units, so 0 corresponds to 1 death, since \\(\\log_{10} 1 = 0\\).\nNo. The vertical scale does not mean anything.\n\n\n\nOne of the purposes of making a semi-log plot is to enable you to compare very large numbers with very small numbers on the same graph. For instance, in the semi-log plot, you can easily see when the first death occurred, a fact that is invisible in the plot of the raw counts (the first plot in this exercise).\nAnother feature of semi-log plots is that they preserve proportionality. Look at the linear plot of raw counts and note that the curve for the number of deaths is much shallower than the curve for the number of (confirmed) cases. Yet on the semi-log plot, the two curves are practically parallel.\nOn a semi-log plot, the arithmetic difference between the two curves tells you what the proportion is between those curves. The parallel curves mean that the proportion is practically constant. Calculate what the proportion between deaths and cases was in the month of May. Here’s a mathematical hint: \\(\\log_{10} \\frac{a}{b} == \\log_{10} a - \\log_{10} b\\). We are interested in \\(\\frac{a}{b}\\).\n\n\n**Part C**  What is the proportion of deaths to cases during the month of May? \nabout 1% about 2% about 5% about 25% about 75%\n\n\nIn many applications, people use semi-log plots to see whether a pattern is exponential or to compare very small and very large numbers. Often, people find it easier if the vertical scale is written in the original units rather than the log units. To accomplish both, the vertical scale can be ruled with raw units spaced logarithmically, like this:\n\ngf_point(confirmed ~ date, \n         data = Covid_US |&gt; filter(date &lt; as.Date(\"2020-06-30\")), \n         color = \"dodgerblue\") %&gt;%\n  gf_point(deaths ~ date, color = \"orange3\") %&gt;%\n  gf_refine(scale_y_log10())\n\n\n\n\n\n\n\n\nThe labels on the vertical axis show the raw numbers, while the position shows the logarithm of those numbers.\nThe next question has to do with the meaning of the interval between grid lines on the vertical axis. Note that on the horizontal axis, the spacing between adjacent grid lines is half a month.\n\n\n**Part D**  What is the numerical spacing (in terms of raw counts) between adjacent grid lines on the vertical axis? (Note: Two numbers are different by a \"factor of 10\" when one number is 10 times the other.\" Similarly, \"a factor of 100\" means that one number is 100 times the other. \n10 cases 100 cases A factor of 10. A factor of 100.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Magnitude</span>"
    ]
  },
  {
    "objectID": "Modeling/14-magnitudes.html#footnotes",
    "href": "Modeling/14-magnitudes.html#footnotes",
    "title": "14  Magnitude",
    "section": "",
    "text": "Source: https://en.wikipedia.org/wiki/Daylight↩︎\nwe are using the Kelvin scale, which is the only meaningful scale for a ratio of temperatures.↩︎\nBoyle’s notebooks are preserved at the Royal Society in London. The data in the Boyle data frame have been copied from this source.)↩︎",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Magnitude</span>"
    ]
  },
  {
    "objectID": "Modeling/15-dimensions.html",
    "href": "Modeling/15-dimensions.html",
    "title": "15  Dimensions and units",
    "section": "",
    "text": "15.1 Mathematics of quantity\nThe first step in understanding the mathematics of quantity is to make an absolute distinction between two concepts that, in everyday life, are used interchangeably: dimension and unit.\nLength is a dimension. Meters is a unit of length. We also measure length in microns, mm, cm, inches, feet, yards, kilometers, and miles, to say nothing of furlongs, fathoms, astronomical units (AU), and parsecs.\nTime is a dimension. Seconds is a unit of time. We also measure time in micro-seconds, milliseconds, minutes, hours, days, weeks, months, years, decades, centuries, and millenia.\nMass is a dimension. Kilograms is a unit of mass.\nLength, time, and mass are called fundamental dimensions. This is not because length is more important than area or volume. It is because you can construct area and volume by multiplying lengths together. This is evident when you consider units of area like square inches or cubic centimeters, but obscured in the names of units like acre, liter, gallon.\nWe use the notation L, T, and M to refer to the fundamental dimensions. (Electrical current Q is also a fundamental dimension, but we won’t have much use for it in our examples. Also useful are \\(\\Theta\\) (“theta”) for temperature, S for money, and P for a count of organisms such as the population of the US or the size of a sheep herd.)\nBrackets translate between a quantity and the dimension. For instance, [1 yard] = L, [1000 kg] = M, [3 years] = T, [10 \\(\\mu\\) (microns)] = L,",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dimensions and units</span>"
    ]
  },
  {
    "objectID": "Modeling/15-dimensions.html#mathematics-of-quantity",
    "href": "Modeling/15-dimensions.html#mathematics-of-quantity",
    "title": "15  Dimensions and units",
    "section": "",
    "text": "We will write the few basic fundamental dimensions using capital letters: L, T, M, P, S, \\(\\Theta\\). Dimensions are never expressed in terms of units. In contrast, quantities are a number value combined with the unit that value refers to.\nThe square brackets \\([\\) and \\(]\\) signify that we are looking at the dimension of the quantity inside the brackets. For instance, the population of the US state Colorado is about 5.8 million people. Surround that with square brackets and you get [5.8 million people] which is a dimension, namely, P.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dimensions and units</span>"
    ]
  },
  {
    "objectID": "Modeling/15-dimensions.html#compound-dimensions",
    "href": "Modeling/15-dimensions.html#compound-dimensions",
    "title": "15  Dimensions and units",
    "section": "15.2 Compound dimensions",
    "text": "15.2 Compound dimensions\nThere are other dimensions: volume, force, pressure, energy, torque, velocity, acceleration, and such. These are called compound dimensions because we represent them as combinations of the fundamental dimensions, L, T, and M. The notation for these combinations involves multiplication and division. For instance:\n\nVolume is L \\(\\times\\) L \\(\\times\\) L \\(=\\) L\\(^3\\), as in “cubic centimeters”\nVelocity is L/T, as in “miles per hour”\nForce is M L/T\\(^2\\), which is obscure unless you remember Newton’s Second Law that \\(\\text{F} = \\text{m}\\,\\text{a}\\): “force equals mass times acceleration.” In terms of dimension, mass is M, acceleration is L/T\\(^2\\). Multiply the two together and you get the dimension “force.”\n\nMultiplication and division are used to construct a compound dimension from the fundamental dimensions L, T, and M.\nAddition and subtraction are never used to form a compound dimension.\nMuch of the work in understanding dimensions involves overcoming the looseness of everyday speech. Remember the weight scale graduated in pounds and kilograms. The unit kilograms is a way of measuring M, but the unit of pounds is a way of measuring force: M L/T\\(^2\\).\nWeight is not the same as mass. This makes no sense to most people and does not matter in everyday life. It is only when you venture off the surface of the Earth that the difference shows up. The Mars rover Perseverance weighs 1000 kg on Earth. It was weightless for most of its journey to Mars. After landing on Mars, Perseverence weighed just 380 kg. But the rover’s mass didn’t change at all.\nAnother source of confusion carried over from everyday life is that sometimes we measure the same quantity using different dimensions. You can measure a volume by weighing water; a gallon of water weighs 8 pounds; a liter of water has a mass of 1 kg. Serious bakers measure flour by weight; a casual baker uses a measuring cup. We can measure water volume with length because water has a (more-or-less) constant mass density. But 8 pounds of gasoline is considerably more than a gallon. It turns out that the density of flour varies substantially depending on how it is packed, humidity, etc. This is why it matters whether you weigh flour for baking or measure it by volume. You can measure time by the swing of a pendulum. To measure the same time successfully with different pendula they need to have the same length, not the same mass.\nA unit is a conventional amount of a quantity of a given dimension. All lengths are the same dimensionally, but they can be measured with different conventions: inches, yards, meters, … Units for the same dimension can all be converted unambiguously one into the other. A meter is the same quantity of length as 39.3701 inches, a mile is the same length as 1609.34 meters. Liters and gallons are both units of volume (L\\(^3\\)): a gallon is the same as 3.78541 liters.\nYou will hear it said that a kilogram is 2.2 pounds. That is not strictly true. A kilogram has dimension M and a pound has dimension ML/T\\(^2\\). Quantities with different dimensions cannot be “equal” or even legitimately compared to one another. Unless you bring something else into the game that physically changes the situation, for instance, gravity (dimension of acceleration due to gravity (dimension \\(\\text{L}/\\text{T}^2\\)). The weight of a kilogram on the surface of the Earth is 2.2 pounds because gravitational acceleration is (almost) the same everywhere on the surface of the Earth.\nIt is also potentially confusing that sometimes different dimensions are used to get at the same idea. For instance, the same car that gets 35 miles / gallon in the US (dimension \\(\\text{L}/\\text{L}^3 = 1/\\text{L}^2\\)) will use 6.7 liters per 100 kilometers (\\(\\text{L}^3 / L = \\text{L}^2\\)) in Europe. Same car. Same fuel. Different conventions using different dimensions.\nKeeping track of the various compound dimensions can be tricky. For many people, it is easier to keep track of the physical relationships involved and use that knowledge to put together the dimensions appropriately. Often, the relationship can be described using specific calculus operations, so knowing dimensions and units helps you use calculus successfully.\nEasy compound dimensions that you likely already know:\n\n[Area] \\(= \\text{L}^2\\). Some corresponding units to remind you: “square feet”, “square miles”, “square centimeters.”\n[Volume] \\(= \\text{L}^3\\). Units to remind you: “cubic centimeters”, “cubic feet”, “cubic yards.” (What landscapers informally call a “yard,” for instance “10 yards of topsoil” should properly be called “10 cubic-yards of topsoil.”)\n[Velocity] \\(= \\text{L}/\\text{T}\\). Units: “miles per hour,” “inches per second.”\n[Momentum] \\(= \\text{M}\\text{L}/\\text{T}\\). Units: “kilogram meters per second.”\n\nAnticipating that you will return to this section for reference, we’ve also added some dimensions that can be understood through the relevant calculus operations.\n\n[Acceleration] \\(= \\text{L}/\\text{T}^2\\). Units: “meters per second squared,” In calculus, acceleration is the derivative of velocity with respect to time, or, equivalently, the 2nd derivative of position with respect to time.\n[Force] \\(=  \\text{M}\\, \\text{L}/\\text{T}^2\\) In calculus: force is the derivative of momentum with respect to time.\n[Energy] or [Work] \\(=   \\text{M}\\, \\text{L}^2/\\text{T}^2\\) In calculus, energy is the integral of force with respect to length.\n[Power] \\(=  \\text{M}\\, \\text{L}^2/\\text{T}^3\\) In the language of calculus, power is the derivative of energy with respect to time.\n\n\n\n\n\n\n\nMath in the World: Density\n\n\n\nDensity sounds like a specific concept, but there are many different kinds of densities. These have in common that they are a ratio of a physical amount to a geometric extent:\n\na physical amount: which might be mass, charge, people, etc.\na geometric extent: which might be length, area, or volume.\n\nSome examples:\n\n“paper weight” is the mass per area, typically grams-per-square-meter\n“charge density” is the amount of electrical charge, usually per area or volume\n“lineal density of red blood cells” is the number of cells in a capillary divided by the length of the capillary. (Capillaries are narrow. Red blood cells go through one after the other.)\n“population density” is people per area of ground.\n\n\n\n\n\n\n\n\n\nMath in the World: A person as a unit\n\n\n\nThe theory of dimensions and units was developed for the physical sciences. Consequently, the fundamental dimensions are those of physics: length, mass, time, electrical current, and luminous intensity.\nSince proper use of units is important even outside the physical sciences, it is helpful to recognize the dimension of several other kinds of quantity.\n\n“people” / “passengers” / “customers” / “patients” / “cases” / “passenger deaths”: these are different different ways to refer to people. we will consider such quantities to have dimension P, for population.\n“money”: Units are dollars (in many varieties: US, Canadian, Australian, New Zealand), euros, yuan (synonym: renminbi), yen, pounds (many varieties: UK, Egypt, Syria, Lebanon, Sudan, and South Sudan), pesos (many varieties), dinar, franc (Swiss, CFA), rand, riyal, rupee, won, and many others. Conversion rates depend on the situation and national policy, but we will consider money a dimension, denoted by S (from the name of the first coinage, the Mesopotamian Shekel).\n\nExamples:\n\nPassenger-miles is a standard unit of transport.\nPassenger-miles-per-dollar is an appropriate unit of the economic efficiency of transport.\nPassenger-deaths per million passenger-mile is one way to describe the risk of transport.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dimensions and units</span>"
    ]
  },
  {
    "objectID": "Modeling/15-dimensions.html#arithmetic-with-dimensions",
    "href": "Modeling/15-dimensions.html#arithmetic-with-dimensions",
    "title": "15  Dimensions and units",
    "section": "15.3 Arithmetic with dimensions",
    "text": "15.3 Arithmetic with dimensions\nRecall the rules for arithmetic dimensioned quantities. We restate them briefly with the square-bracket notation for “the dimension of.” For instance, “the dimension of \\(b\\)” is written \\([b]\\). We also write \\([1]\\) to stand for the dimension of a pure number, that is, a quantity without dimension.\n\n\n\n\nTable 15.1: Conditions under which functions can be applied to dimensionful quantitities. Note that \\([a] = [b]\\) means that the dimension of \\(a\\) and of \\(b\\) are the same. For instance, even though 1 mm and 500 miles are very different distances, [1 mm]\\(=\\)[500 miles]. Both [1 mm] and [500 miles] are dimension L.\n\n\n\n\n\n\n\n\n\n\n\nOperation\nResult\nOnly if satisfies\nMetaphor\n\n\n\n\nMultiplication\n\\([a \\times b] = [a]  \\times [b]\\)\nanything goes\npromiscuous\n\n\nDivision\n\\([a \\div b] = [a] \\div  [b]\\)\nanything goes\npromiscuous\n\n\nAddition\n\\([a + b] = [a]\\)\n\\([a] = [b]\\)\nmonogomous\n\n\nSubtraction\n\\([a - b] = [a]\\)\n\\([a] = [b]\\)\nmonogomous\n\n\nTrigonometric\n\\([\\sin(a)] = [1]\\)\n\\([a] = [1]\\)\ncelibate\n\n\nExponential\n\\([e^a] = [1]\\)\n\\([a] = [1]\\) (of course, \\([e] = [1]\\))\ncelibate\n\n\nPower-law\n\\([b  ^  a] = \\underbrace{[b]\\times[b]\\times ...\\times [b]}_{a\\ \\ \\text{times}}\\)\n\\([a]  = [1]\\) with \\(a\\) an integer\nexponent celibate\n\n\nSquare root\n\\([\\sqrt{b}] = [c]\\)\n\\([b] = [c\\times c]\\)\nidiosyncratic\n\n\nCube root\n\\([\\sqrt[3]{b}] = [c]\\)\n\\([b] = [c \\times c \\times  c]\\)\nidiosyncratic\n\n\nBump\n\\([\\text{bump}(a)] = [1]\\)\n\\([a] =  [1]\\)\ncelibate\n\n\nSigmoid\n\\([\\text{sigmoid}(a)] =  [1]\\)\n\\([a] = [1]\\)\ncelibate\n\n\n\n\n\n\n\n15.4 Example: Dimensional analysis\nWe want to relate the period (in T) of a pendulum to its length and mass. Acceleration due to gravity also plays a role; that has dimension \\(\\text{L}\\cdot \\text{T}^{-2}\\). For simplicity, we will assume that only the bob at the end of the pendulum cable or rod has mass.\nThe analysis strategy is to combine the four quantities we think play a role into one total quantity that is dimensionless. Since it is dimensionless, it can be constant regardless of the mass, length, period, or gravity of each situation.\n\\[\\text{[Period]}^a \\cdot \\text{[Mass]}^b \\cdot \\text{[Length]}^c \\cdot \\text{[Gravity]}^d = T^a \\cdot M^b \\cdot L^c \\cdot L^d \\cdot T^{-2d} = [1]\\] To be dimensionless:\n\n\\(c = -d\\), cancel out the L\n\\(a = 2d\\), cancel out the T\n\\(b=0\\), there is no other mass term, and we need to cancel out the M\n\nAll of the exponents can be put in terms of \\(d\\). That does not tell us what \\(d\\) should be, but whatever value for \\(d\\) we decide to choose, we get a ratio that is equivalent to:\n\\[ \\frac{[\\text{Gravity}]\\cdot [\\text{Period}]^2}{[\\text{Length}]} = [1]\\]\nThis is a relationship between dimensions of quantities. To render it into a formula involving the quantities themselves we need to take into account the units.\n\\[ \\frac{\\text{Gravity}\\cdot \\text{Period}^2}{\\text{Length}} = B\\]\nWe can experimentally determine the numerical value of the dimensionless constant \\(B\\) by measuring the period and length of a pendulum and (on Earth) recognizing that gravitational acceleration on Earth’s surface is 9.8 meters-per-second-squared. Such experiments and mathematical models using differential equations give \\(B = (2\\pi)^2\\).\n\n\n15.5 Conversion: Flavors of 1\nNumbers are dimensionless but not necessarily unitless. Failure to accept this distinction is one of the prime reasons people have trouble figuring out how to convert from one unit to another.\nThe number one is a favorite of elementary school students because its multiplication and division tables are completely simple. Anything times one, or anything divided by one, is simply that thing. Addition and subtraction are pretty simple, too, a matter of counting up or down.\nWhen it comes to quantities, there is not just one one but many. And often they look nothing like the numeral 1. Some examples of 1 as a quantity:\n\n\\(\\frac{180}{\\pi} \\frac{\\text{degrees}}{\\text{radians}}\\)\n\\(0.621371 \\frac{\\text{mile}}{\\text{kilometer}}\\)\n\\(3.78541 \\frac{\\text{liter}}{\\text{gallon}}\\)\n\\(\\frac{9}{5} \\frac{^\\circ F}{^\\circ C}\\)\n\\(\\frac{1}{12} \\frac{\\text{dozen}}{\\text{item}}\\)\n\nI like to call these and others different flavors of one.\nIn every one of the above examples, the dimension of the numerator matches the dimension of the denominator. The same is true when comparing feet and meters ([feet / meter] is L/L = [1]), or comparing cups and pints ([cups / pint] is \\(\\text{L}^3/\\text{L}^3 = [1]\\)) or comparing miles per hour and feet per second ([miles/hour / ft per sec] = L/T / L/T = [1]). Each of these quantities has units but it has no dimension.\nIt is helpful to think about conversion between units as a matter of multiplying by the appropriate flavor of 1. Such conversion will not change the dimension of the quantity but will render it in new units.\n\nExample: Convert 100 feet-per-second into miles-per-hour. First, write the quantity to be converted as a fraction and alongside it, write the desired units after the conversion. In this case that will be \\[100 \\frac{\\text{feet}}{\\text{second}} \\ \\ \\ \\text{into} \\ \\ \\ \\frac{\\text{miles}}{\\text{hour}}\\]\nFirst, we will change feet into miles. This can be accomplished by multiplying by the flavor of one that has units miles-per-foot. Second, we will change seconds into hours. Again, a flavor of 1 is involved.\nWhat number will give a flavor of one? One mile is 5280 feet, so \\[\\frac{1}{5280} \\frac{\\text{miles}}{\\text{foot}}\\] is a flavor of one.\nNext, we need a flavor of one that will turn \\(\\frac{1}{\\text{second}}\\) into \\(\\frac{\\text{1}}{\\text{hour}}\\). We can make use of a minute being 60 seconds, and an hour being 60 minutes. \\[\\underbrace{\\frac{60\\  \\text{s}}{\\text{minute}}}_\\text{flavor of 1}\\  \\underbrace{\\frac{60\\ \\text{minutes}}{\\text{hour}}}_\\text{flavor of 1} = \\underbrace{3600\\frac{\\text{s}}{ \\text{hour}}}_\\text{flavor of 1}\\]\nMultiplying our carefully selected flavors of one by the initial quantity, we get \\[\n\\underbrace{\\frac{1}{5280} \\frac{\\text{mile}}{\\text{foot}}}_\\text{flavor of 1} \\times \\underbrace{3600 \\frac{\\text{s}}{\\text{hour}}}_\\text{flavor of 1} \\times \\underbrace{100 \\frac{\\text{feet}}{\\text{s}}}_\\text{original quantity} = 100 \\frac{3600}{5280} \\frac{\\text{miles}}{\\text{hour}} = 68.18 \\frac{\\text{miles}}{\\text{hour}}\\]\n\n\n\n15.6 Dimensions and linear combinations\nLow-order polynomials are a useful way of constructing model functions. For instance, suppose we want a model of the yield of corn in a field per inch of rain over the growing season, will call it corn(rain). The output will have units of bushels (of corn). The input will have units of inches (of rain). A second-order polynomial will be appropriate for reasons to be discussed in ?sec-optim-and-shape.\n\\[\\text{corn(rain)} \\equiv a_0 + a_1\\, \\text{rain} + \\frac{1}{2} a_2\\, \\text{rain}^2\\] Of course, the addition in the linear combination will only make sense if all three terms \\(a_0\\), \\(a_1\\,\\text{rain}\\), and \\(\\frac{1}{2}\\, a_2\\, \\text{rain}^2/2\\) have the same dimension. But \\([\\text{rain}] \\neq [\\text{rain}^2]\\). In order for things to work out, the coefficients must themselves have dimension. We know the output of the function will have dimension \\([\\text{volume}] = \\text{L}^3\\). Thus, \\([a_0] = \\text{L}^3\\).\n\\([a_1]\\) must be different, because it has to combine with the \\([\\text{rain}] = \\text{L}\\) and produce \\(\\text{L}^3\\). Thus, \\([a_1] = \\text{L}^2\\).\nFinally, \\([a_2] = \\text{L}\\). Multiplying that by \\([\\text{rain}]^2\\) will give the required \\(\\text{L}^3\\)\n\n\n\n\n\n\n\nMath in the World: Compute in radians\n\n\n\nIn everyday communication as well as in most domains such as construction, geography, navigation, and astronomy we measure angles in degrees. 90 degrees is a right angle. But in mathematics, the unit of angle is radians where a right angle is 1.5708 radians. (1.5708 is the decimal version of \\(\\pi/2\\).) The conversion function, which we will call raddeg(), is \\[\\text{raddeg}(r) \\equiv \\frac{180}{\\pi} r\\] The function that converts degrees to radians, which we will call degrad() is very similar: \\[\\text{degrad}(d) \\equiv \\frac{\\pi}{180} d\\] (Incidentally, \\(\\frac{180}{\\pi} = 57.296\\) while \\(\\frac{\\pi}{180} = 0.017453\\).)\nIn traditional notation, the trigonometric functions such as \\(\\sin()\\) and \\(\\tan()\\) can be written with an argument either in degrees or radians. For instance, \\(\\sin(90^\\circ) = \\sin\\left(\\frac{\\pi}{2}\\right)\\). Similarly, for the inverse functions like \\(\\arccos()\\) the units of the output are not specified. This works because there is always a human to intervene between the written expression and the eventual computation.\nIn R, as in many other computer languages like Python or spreadsheet packages, there is no valid expression like sin(90 deg). In these languages, 90 deg is not a valid expression (although it might be good if it were valid!). In these and many other languages, angles are always given in radians. Such consistency is admirable, but people are not always so consistent. It is a common source of computer bugs that angles in degrees are handed off to functions like \\(\\sin()\\) and that the output of \\(\\arccos()\\) is (wrongly) interpreted as degrees rather than radians.\nFunction composition to the rescue!\nConsider this function given in the Wikipedia article on the position of the sun as seen from Earth.1 \\[\\delta_\\odot(n) \\equiv - 23.44^\\circ \\cdot \\cos \\left [ \\frac{360^\\circ}{365\\, \\text{days}} \\cdot \\left ( n + 10 \\right ) \\right ]\\] Where \\(n\\) is zero at the midnight marking New Years and increases by 1 per day. (The \\(n+10\\) has units of days and translates New Years back 10 days, to the day of the winter solstice.) \\(\\delta_\\odot()\\) gives the declination of the sun: the latitude pieced by an imagined line connecting the centers of the earth and the sun.\nThe Wikipedia formula is well written in that it uses some familiar numbers to help the reader see where the formula comes from. 365 is recognizably the length of the year in days. \\(360^\\circ\\) is the angle traversed when making a full cycle around a circle. \\(23.44^\\circ\\) is less familiar, but the student of geography might recognize it as the latitude of the Tropic of Cancer, the latitude farthest north where the sun is directly overhead at noon (on the day of the summer solstice).\nBut there is a world of trouble for the programmer who implements the formula as\n\ndec_sun &lt;- makeFun(-23.44 * cos((360/365)*(n+10)) ~ n)\n\nFor instance, the equinoxes are around March 21 (n=81) and Sept 21 (n=264). On an equinox, the declination of the sun is zero degrees. But let’s plug \\(n=81\\) and \\(n=264\\) into the formula and see what we get.\n\ndec_sun(81)\n## [1] 5.070321\ndec_sun(264)\n## [1] -23.38324\n\nThe equinoxes aren’t even equal! And they are not close to zero. Does this mean astronomy is wrong?\nThe Wikipedia formula should have been programmed this way, using 2 \\(\\pi\\) radians instead of 360 degrees in the argument to the cosine function:\n\ndec_sun_right &lt;- \n  makeFun(-23.44 * cos(( 2*pi/365)*(n+10)) ~ n)\ndec_sun_right(81)\n## [1] -0.1008749\ndec_sun_right(264)\n## [1] -0.1008749\n\nThe deviation of one-tenth of a degree reflects rounding off the time of the equinox to the nearest day.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dimensions and units</span>"
    ]
  },
  {
    "objectID": "Modeling/15-dimensions.html#sec-pendulum-dimensions",
    "href": "Modeling/15-dimensions.html#sec-pendulum-dimensions",
    "title": "15  Dimensions and units",
    "section": "15.4 Example: Dimensional analysis",
    "text": "15.4 Example: Dimensional analysis\nWe want to relate the period (in T) of a pendulum to its length and mass. Acceleration due to gravity also plays a role; that has dimension \\(\\text{L}\\cdot \\text{T}^{-2}\\). For simplicity, we will assume that only the bob at the end of the pendulum cable or rod has mass.\nThe analysis strategy is to combine the four quantities we think play a role into one total quantity that is dimensionless. Since it is dimensionless, it can be constant regardless of the mass, length, period, or gravity of each situation.\n\\[\\text{[Period]}^a \\cdot \\text{[Mass]}^b \\cdot \\text{[Length]}^c \\cdot \\text{[Gravity]}^d = T^a \\cdot M^b \\cdot L^c \\cdot L^d \\cdot T^{-2d} = [1]\\] To be dimensionless:\n\n\\(c = -d\\), cancel out the L\n\\(a = 2d\\), cancel out the T\n\\(b=0\\), there is no other mass term, and we need to cancel out the M\n\nAll of the exponents can be put in terms of \\(d\\). That does not tell us what \\(d\\) should be, but whatever value for \\(d\\) we decide to choose, we get a ratio that is equivalent to:\n\\[ \\frac{[\\text{Gravity}]\\cdot [\\text{Period}]^2}{[\\text{Length}]} = [1]\\]\nThis is a relationship between dimensions of quantities. To render it into a formula involving the quantities themselves we need to take into account the units.\n\\[ \\frac{\\text{Gravity}\\cdot \\text{Period}^2}{\\text{Length}} = B\\]\nWe can experimentally determine the numerical value of the dimensionless constant \\(B\\) by measuring the period and length of a pendulum and (on Earth) recognizing that gravitational acceleration on Earth’s surface is 9.8 meters-per-second-squared. Such experiments and mathematical models using differential equations give \\(B = (2\\pi)^2\\).",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dimensions and units</span>"
    ]
  },
  {
    "objectID": "Modeling/15-dimensions.html#conversion-flavors-of-1",
    "href": "Modeling/15-dimensions.html#conversion-flavors-of-1",
    "title": "15  Dimensions and units",
    "section": "15.5 Conversion: Flavors of 1",
    "text": "15.5 Conversion: Flavors of 1\nNumbers are dimensionless but not necessarily unitless. Failure to accept this distinction is one of the prime reasons people have trouble figuring out how to convert from one unit to another.\nThe number one is a favorite of elementary school students because its multiplication and division tables are completely simple. Anything times one, or anything divided by one, is simply that thing. Addition and subtraction are pretty simple, too, a matter of counting up or down.\nWhen it comes to quantities, there is not just one one but many. And often they look nothing like the numeral 1. Some examples of 1 as a quantity:\n\n\\(\\frac{180}{\\pi} \\frac{\\text{degrees}}{\\text{radians}}\\)\n\\(0.621371 \\frac{\\text{mile}}{\\text{kilometer}}\\)\n\\(3.78541 \\frac{\\text{liter}}{\\text{gallon}}\\)\n\\(\\frac{9}{5} \\frac{^\\circ F}{^\\circ C}\\)\n\\(\\frac{1}{12} \\frac{\\text{dozen}}{\\text{item}}\\)\n\nI like to call these and others different flavors of one.\nIn every one of the above examples, the dimension of the numerator matches the dimension of the denominator. The same is true when comparing feet and meters ([feet / meter] is L/L = [1]), or comparing cups and pints ([cups / pint] is \\(\\text{L}^3/\\text{L}^3 = [1]\\)) or comparing miles per hour and feet per second ([miles/hour / ft per sec] = L/T / L/T = [1]). Each of these quantities has units but it has no dimension.\nIt is helpful to think about conversion between units as a matter of multiplying by the appropriate flavor of 1. Such conversion will not change the dimension of the quantity but will render it in new units.\n\nExample: Convert 100 feet-per-second into miles-per-hour. First, write the quantity to be converted as a fraction and alongside it, write the desired units after the conversion. In this case that will be \\[100 \\frac{\\text{feet}}{\\text{second}} \\ \\ \\ \\text{into} \\ \\ \\ \\frac{\\text{miles}}{\\text{hour}}\\]\nFirst, we will change feet into miles. This can be accomplished by multiplying by the flavor of one that has units miles-per-foot. Second, we will change seconds into hours. Again, a flavor of 1 is involved.\nWhat number will give a flavor of one? One mile is 5280 feet, so \\[\\frac{1}{5280} \\frac{\\text{miles}}{\\text{foot}}\\] is a flavor of one.\nNext, we need a flavor of one that will turn \\(\\frac{1}{\\text{second}}\\) into \\(\\frac{\\text{1}}{\\text{hour}}\\). We can make use of a minute being 60 seconds, and an hour being 60 minutes. \\[\\underbrace{\\frac{60\\  \\text{s}}{\\text{minute}}}_\\text{flavor of 1}\\  \\underbrace{\\frac{60\\ \\text{minutes}}{\\text{hour}}}_\\text{flavor of 1} = \\underbrace{3600\\frac{\\text{s}}{ \\text{hour}}}_\\text{flavor of 1}\\]\nMultiplying our carefully selected flavors of one by the initial quantity, we get \\[\n\\underbrace{\\frac{1}{5280} \\frac{\\text{mile}}{\\text{foot}}}_\\text{flavor of 1} \\times \\underbrace{3600 \\frac{\\text{s}}{\\text{hour}}}_\\text{flavor of 1} \\times \\underbrace{100 \\frac{\\text{feet}}{\\text{s}}}_\\text{original quantity} = 100 \\frac{3600}{5280} \\frac{\\text{miles}}{\\text{hour}} = 68.18 \\frac{\\text{miles}}{\\text{hour}}\\]",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dimensions and units</span>"
    ]
  },
  {
    "objectID": "Modeling/15-dimensions.html#dimensions-and-linear-combinations",
    "href": "Modeling/15-dimensions.html#dimensions-and-linear-combinations",
    "title": "15  Dimensions and units",
    "section": "15.6 Dimensions and linear combinations",
    "text": "15.6 Dimensions and linear combinations\nLow-order polynomials are a useful way of constructing model functions. For instance, suppose we want a model of the yield of corn in a field per inch of rain over the growing season, will call it corn(rain). The output will have units of bushels (of corn). The input will have units of inches (of rain). A second-order polynomial will be appropriate for reasons to be discussed in ?sec-optim-and-shape.\n\\[\\text{corn(rain)} \\equiv a_0 + a_1\\, \\text{rain} + \\frac{1}{2} a_2\\, \\text{rain}^2\\] Of course, the addition in the linear combination will only make sense if all three terms \\(a_0\\), \\(a_1\\,\\text{rain}\\), and \\(\\frac{1}{2}\\, a_2\\, \\text{rain}^2/2\\) have the same dimension. But \\([\\text{rain}] \\neq [\\text{rain}^2]\\). In order for things to work out, the coefficients must themselves have dimension. We know the output of the function will have dimension \\([\\text{volume}] = \\text{L}^3\\). Thus, \\([a_0] = \\text{L}^3\\).\n\\([a_1]\\) must be different, because it has to combine with the \\([\\text{rain}] = \\text{L}\\) and produce \\(\\text{L}^3\\). Thus, \\([a_1] = \\text{L}^2\\).\nFinally, \\([a_2] = \\text{L}\\). Multiplying that by \\([\\text{rain}]^2\\) will give the required \\(\\text{L}^3\\)\n\n\n\n\n\n\n\nMath in the World: Compute in radians\n\n\n\nIn everyday communication as well as in most domains such as construction, geography, navigation, and astronomy we measure angles in degrees. 90 degrees is a right angle. But in mathematics, the unit of angle is radians where a right angle is 1.5708 radians. (1.5708 is the decimal version of \\(\\pi/2\\).) The conversion function, which we will call raddeg(), is \\[\\text{raddeg}(r) \\equiv \\frac{180}{\\pi} r\\] The function that converts degrees to radians, which we will call degrad() is very similar: \\[\\text{degrad}(d) \\equiv \\frac{\\pi}{180} d\\] (Incidentally, \\(\\frac{180}{\\pi} = 57.296\\) while \\(\\frac{\\pi}{180} = 0.017453\\).)\nIn traditional notation, the trigonometric functions such as \\(\\sin()\\) and \\(\\tan()\\) can be written with an argument either in degrees or radians. For instance, \\(\\sin(90^\\circ) = \\sin\\left(\\frac{\\pi}{2}\\right)\\). Similarly, for the inverse functions like \\(\\arccos()\\) the units of the output are not specified. This works because there is always a human to intervene between the written expression and the eventual computation.\nIn R, as in many other computer languages like Python or spreadsheet packages, there is no valid expression like sin(90 deg). In these languages, 90 deg is not a valid expression (although it might be good if it were valid!). In these and many other languages, angles are always given in radians. Such consistency is admirable, but people are not always so consistent. It is a common source of computer bugs that angles in degrees are handed off to functions like \\(\\sin()\\) and that the output of \\(\\arccos()\\) is (wrongly) interpreted as degrees rather than radians.\nFunction composition to the rescue!\nConsider this function given in the Wikipedia article on the position of the sun as seen from Earth.1 \\[\\delta_\\odot(n) \\equiv - 23.44^\\circ \\cdot \\cos \\left [ \\frac{360^\\circ}{365\\, \\text{days}} \\cdot \\left ( n + 10 \\right ) \\right ]\\] Where \\(n\\) is zero at the midnight marking New Years and increases by 1 per day. (The \\(n+10\\) has units of days and translates New Years back 10 days, to the day of the winter solstice.) \\(\\delta_\\odot()\\) gives the declination of the sun: the latitude pieced by an imagined line connecting the centers of the earth and the sun.\nThe Wikipedia formula is well written in that it uses some familiar numbers to help the reader see where the formula comes from. 365 is recognizably the length of the year in days. \\(360^\\circ\\) is the angle traversed when making a full cycle around a circle. \\(23.44^\\circ\\) is less familiar, but the student of geography might recognize it as the latitude of the Tropic of Cancer, the latitude farthest north where the sun is directly overhead at noon (on the day of the summer solstice).\nBut there is a world of trouble for the programmer who implements the formula as\n\ndec_sun &lt;- makeFun(-23.44 * cos((360/365)*(n+10)) ~ n)\n\nFor instance, the equinoxes are around March 21 (n=81) and Sept 21 (n=264). On an equinox, the declination of the sun is zero degrees. But let’s plug \\(n=81\\) and \\(n=264\\) into the formula and see what we get.\n\ndec_sun(81)\n## [1] 5.070321\ndec_sun(264)\n## [1] -23.38324\n\nThe equinoxes aren’t even equal! And they are not close to zero. Does this mean astronomy is wrong?\nThe Wikipedia formula should have been programmed this way, using 2 \\(\\pi\\) radians instead of 360 degrees in the argument to the cosine function:\n\ndec_sun_right &lt;- \n  makeFun(-23.44 * cos(( 2*pi/365)*(n+10)) ~ n)\ndec_sun_right(81)\n## [1] -0.1008749\ndec_sun_right(264)\n## [1] -0.1008749\n\nThe deviation of one-tenth of a degree reflects rounding off the time of the equinox to the nearest day.",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dimensions and units</span>"
    ]
  },
  {
    "objectID": "Modeling/15-dimensions.html#footnotes",
    "href": "Modeling/15-dimensions.html#footnotes",
    "title": "15  Dimensions and units",
    "section": "",
    "text": "Article accessed on May 30, 2021↩︎",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dimensions and units</span>"
    ]
  },
  {
    "objectID": "Modeling/Modeling-projects.html",
    "href": "Modeling/Modeling-projects.html",
    "title": "Projects",
    "section": "",
    "text": "Block 1 Project 1: Ebola in Guinea, part I\nIn December 2013, an 18-month-old boy from a village in Guinea suffered fatal diarrhea. Over the next months a broader outbreak was discovered, and in mid-March 2014, the Pasteur Institute in France confirmed the illness as Ebola-Virus Disease caused by the Zaire ebolavirus.\nAlthough the outbreak was first recognized in Guinea, it eventually encompassed Liberia and Sierra Leone as well. By July 2014, the outbreak spread to the capitals of all three countries. We will examine the time course of the epidemic using reports that were issued by the World Health Organization along with some concepts and techniques we have been studying in the first block of Math 141Z. Data are provided by the US Centers for Disease Control (CDC).",
    "crumbs": [
      "BLOCK I. Modeling",
      "Projects"
    ]
  },
  {
    "objectID": "Modeling/Modeling-projects.html#block-1-project-1-ebola-in-guinea-part-i",
    "href": "Modeling/Modeling-projects.html#block-1-project-1-ebola-in-guinea-part-i",
    "title": "Projects",
    "section": "",
    "text": "Data\nThe CDC data are provided to you as a dataframe named EbolaAll. The dataframe consists of 182 reports spread out over 18 months during 2014 and 2015. Each report is represented by a single row in the dataframe . Each report gives the number of new cases and disease-related deaths since the last report in each or three countries: Sierra Leon, Liberia, and Guinea. These values have been calculated from the raw, cumulative data. The data have been scrubbed to remove obvious errors.\n\n\nExploring the Data\nWe will begin by looking at some data. Use head(EbolaAll) to see the column headers and first 6 rows of data. As you can see, the EbolaAl dataframe is structured like a table, and each row contains multiple columns of data. The table below lists the column names found in EbolaAll dataframe along with a brief description of what the column records.\n\nDate: Date when the World Health Organization issued the report\nGcases: Number of new cases in Guinea\nGdeaths: Number of new deaths in Guinea\nLcases: Number of new cases in Liberia\nLdeaths: Number of new deaths in Liberia\nSLcases: Number of new cases in Sierra Leone\nSLdeaths: Number of new deaths in Sierra Leone\nTotCases: Total number of cases across all three countries\nTotDeaths: Total number of deaths across all three countries\n\nWe will focus on the Guinea data. As we have done throughout this book, we model this data using the pattern-book functions.\n\nIn a SANDBOX, use gf_point() to plot the number of new cases in Guinea (variables Date and Gcases in dataframe EbolaAll).\n\n**Question 1**: Look at the shape of the graph. Of the functions we have studied, which would be most appropriate as a model for the new cases in Guinea? If none of the functions we have studied best matches this data, select \"None of the above.\" \nAlthough we are tempted to regard data sets as definitive, they are the culmination of an imperfect process of data collection in the field and imperfect processing/correction/amendment by people. For instance, most of the cases (and deaths) in the epidemic were never confirmed by viral testing and are considered “suspected cases.” Cases and deaths reported on one day might possibly be from previous days, and some cases and deaths were, no doubt, not reported.\nIn the Sandbox, run the following command to look at the data:\n\nDT::datatable(EbolaAll)\n\nThe resulting display is interactive; you can page through the many rows of data.\n\nQuestion 2: Look through the data printout and find a data point form one of the World Health Organization reports that seems strange or inconsistent when compared with nearby reports. To receive full credit for this question, you must state the country and date of the report and explain your reasons for identifying this report as inconsistent.\n\n\n\nData Wrangling\nAn essential part of all projects involving data is to prepare the data for analysis: a process often called data wrangling. Although data wrangling is an important skill, it is not the topic of this book. So we will take care of the programming and present you with the results in the form of a data frame EbolaGuinea.\nThe wrangling accomplished two things: averaging the data over 7-report windows and extracting a numerical “day number” from the Date of each report.\nFirst, rather than working with the year:month:day format of Date, it is convenient to use a purely numerical quantity to represent time. So, we will translate the day when reports were issued into number of days from the initial report, presenting the result in the column Days. This makes the data-analysis programming easier, since all our mathematical software knows how to handle numbers, but not necessarily calendar dates.\nSecond, we will smooth the number of cases and the number of deaths. We did this by averaging each day’s number-of-cases and number-of deaths over several report. As you can see in the data, the interval between reports is not constant. Some reports occur just one day after the previous report; sometimes there is a week between reports. The widely spaced reports tend to have much higher cases and deaths than reports that come every day. This is for the simple reason that each report gives the number of new cases and deaths since the previous report; there is more time for the numbers to accumulate when there is a wide spacing between reports. The data reflect not just the process of the Ebola epidemic, but also the way the data were collected and reported, which is not directly of interest to us. To reduce this report-to-report fluctuation, we will average the number of new cases in each report with the three reports before and three reports after: a moving average over seven reports.\nThe result of our wrangling—the dataframe EbolaGuinea—includes three new columns:\n\nDays: When the report was issued in terms of a count of days from the initial report.\nG7Rcases: Number of new cases in Guinea averaged across 7 reports\nG7Rdeaths: Number of new deaths in Guinea averaged across 7 reports\n\n\n\nAnalysis of the outbreak\nIn your Sandbox, use gf_point() to plot the smoothed data in EbolaGuinea to show the time course of the epidemic. The variables you want to plot are G7Rcases versus Days. averaged number of new cases in Guinea (variables Days and G7Rcases in dataframe EbolaGuinea).\n\nQuestion 3 Look at the shape of the graph. Of the functions we have studied, which would be most appropriate as a model for the new cases in Guinea? If none of the functions we have studied best matches this data, select “None of the above.”\n\nIt is well known that the infection rate during an outbreak follows a Gaussian pattern when the population interacts consistently. Ebola initially spreads exponentially as people pass the virus to one another. As people are exposed to the virus, there are fewer and fewer people who are still susceptible. The result is that the rate of exponential growth slows and, when the fraction of the population that is susceptible becomes small, the epidemic slows down and the number of new cases decays exponentially. Overall, the pattern of new cases day-by-day looks like a gaussian: zero cases per day before the outputbreak, exponential growth at first after the virus is introduced to the population, leveling out for a time, and exponential decay back to zero new cases.\nThe graph of G7Rcases versus Days looks only vaguely like a gaussian bump. One possible explanation is that the data summarize more than one outbreak, as the virus spreads from one region to another. We will treat each individual output as a gaussian and try to figure out how many of them make up the whole data. The individual outbreaks will be modelled as a gaussian function with it is own center and width. The center for each gaussian corresponds to the peak of the hypothesized outbreak in one particular region.\nWe will combine the several gaussians in a linear combination. The scalar multiplying each gaussian is related to the size of the population exposed in each outbreak.\nHere is an R/mosaic definition of a linear combination of two gaussian outbreaks.\n\nmy_mod &lt;- makeFun(15800*(A*dnorm(t,m1,sd1) + \n                            (1-A)*dnorm(t,m2,sd2)) ~ t) \n\nThere are five parameters in the function. m1 the time when the first hypothesized outbreak peaked, m2 is the peak time of the second hypothesized outbreaks, and sd1 and sd2 reflect the duration of each outbreak. The parameter A represents the relative population sizes of the two regions. The number 15,800 reflects the total number of cases, which we know retrospectively. (The 15,800 includes a correction for the average day spacing between reports, but that detail need not concern us here.)\nNote that selecting A sets the amplitude for both Gaussians, i.e. A and 1−A. The amplitudes A and 1-A sum up to 1. Arranging things this way effectively make A the size of the one outbreak, leaving 1-A to be the size of the other outbreak.\nTo see how these Gaussians work together, start with the following values: A = 0.5, m1 = 150, sd1 = 50, m2 = 350, sd2 = 50. Do not change the 15800 number which reflects the overall size of the whole outbreak, as opposed to the relative size of the hypothesized individual outbreaks: A and 1-A.\nUse gf_point(), the pipe operator %&gt;%, and slice_plot() to overlay your model on top of the data. Discrepancies between the model and the data can lead you to improve the model by adjusting the parameters. It is to be expected that you will need several cycles of such parameter adjustments until you find a model that seems to agree with the data in a satisfactory way. to you Guinea cases along with your model.\nOnce you have adjusted your model to match the data as closely as you can, write down the parameters you used in your report, as well as the graph comparing the data to your final model.\nQuestion 3: Find the longest time interval during which your model systematically overstates the number of cases. What’s the start and end of that interval? (Your answer can be rough, just so long as it points the reader of your report to the interval you mean.)\n\n\nMore data smoothing\nAccurately modeling the Guinea case data with two gaussian functions is difficult. Despite the smoothing, the numbers in G7Rcases fluctuate irregularly and substantially from report to report.\nAnother way to smooth the data, that is, to reduce the irregular report-to-report fluctuations, is to accumulate the number of new cases to get the total number of cases up through each report. (This accumulation, at the end of the epidemic, will be the total number of people who came down with Ebola illness.)\nSuch a sum of new cases from the beginning of the outbreak through the day of each report is called a cumulative sum or “running total.” Keep in mind that this is not a number, but a new column of data giving the number of cases to date for each of the dates in the data.\nTo illustrate, the small set of numbers: [1 2 3 4 5 6]. The cumulative sum of this set shows the running total: [1 3 6 10 15 21]. Make sure you are comfortable with how the second set of numbers is related to the first.\nThe R function cumsum() calculates the cumulative sum on a column of data. You will need to do a little data wrangling; we cannot do it for you because we don’t know what model you decided to settle on.\nThere are many computer systems for data wrangling. You may have heard of one called Structured Query Language (SQL) which is a required skill listed in many job listings and a multi-billion dollar component of the economy. We will use a system called dplyr specially designed for the R language. In the dplyr system, the function mutate() is used to create a new column in a data frame by carrying out calculations on the existing columns.\nHere is an R/dplyr command to generate the running total of cases. (we are using Gcases instead of the smoothed G7Rcases because the cumulative summing will do the smoothing for us.)\n\nEbolaGuinea &lt;- EbolaGuinea |&gt; mutate(GcasesTotal = cumsum(Gcases)) \n\nIt is worthwhile to parse that command carefully. The part to the right of &lt;- is an instruction for taking the EbolaGuinea data frame and adding a new column. The new column will be called GcasesTotal. The values in the new column will be generated by applying cumsum() to the Gcases column. Altogether, the right-hand side of the statement creates a new data frame that includes the new column. The left-hand side of the statement stores this new data frame under a name. For convenience, we are using EbolaGuinea as that name. Effectively, the command as a whole, including the name assignment EbolaGuinea &lt;- can be seen as updating the data frame with the new column.\nYou can verify that the new column is in place by showing the first several lines of the updated data frame:\n\nhead(EbolaGuinea) \n\nUse gf_point() to plot GCasesTotal versus Days. The graph will show how the number of cases accumulated over time to to the overall total for the epidemic as a whole.\n\nQuestion 5: Consider which of the functions we have studied could be fitted to represent the GCasesTotal versus Days curve in your graph? it is likely that none of the functions we have studied fits the data particularly well, but one of them will be better than the others.\n\nWe haven’t forgotten that you already created a model of the new cases by a linear combination of gaussian functions. What we want to do now is translate your model for the number of new cases each day into a model of the cumulative number of cases to date. In other words, we want to perform the same action that cumsum() does, but applied to your model rather than data.\nTo do this, you will replace the gaussian dnorm() function in your model with the sigmoid pnorm(). No other changes are needed. This works because the gaussian and sigmoid functions are related to one another in just the same way as cumsum() relates the GCases column to the GCasesTotal column.\nRemember that the Gaussian and sigmoid functions are related to one another. The sigmoid is the accumulation of the Gaussian, i.e. It is the cumulative sum of the Gaussian. We will use this relationship to improve your double Gaussian model.\nCreate a new function in your sandbox called my_mod_cumulative(). Plot this new function over the GCasesTotal versus Dates data. As before, you can do this with gf_point(), the pipe operator %&gt;%, and slice_plot().\n\nQuestion 6: Observe the rate of change (slope) of your model. The rate of change represents the number of new infections per day. Find the day on which your rate of change is greatest. Describe how this is related to the argmax of your model from Question 4.\n\nTry adjusting the parameters in my_mod_cumulative() to better match the function to the GCasesTotal data. Some aspects of the outbreak can be better seen from the cumulative number of cases to date, and other aspects may be better seen with the-newcases-each-day data.\nNow we will leverage the relationship between the Gaussian and sigmoid functions. Adjust your double Gaussian model and use the Question6 cumulative sums graph to evaluate your modifications. The procedure is outlined here:\n\n\nFinal thoughts\nThe gaussian as a model for the time course of new cases, and the sigmoid for the time course of accumulated cases to date, are well established. But here we’ve used two gaussians (or, equivalently, two sigmoids). So was there one outbreak or two?\n\nQuestion 7: Think about why the two-gaussian model matches the data better than the one-gaussian model. What might this mean in terms of the structure of the Ebola outbreak in Guinea? Don’t be afraid to speculate and frame your answer in terms that a layman might understand.\n\nThe modeling cycle is all about using your current model to identify ways that you might be able to improve the model. Of course, in practice, you need to present your model for use, so you have to exit the cycle at some point. You’re at that point now, but we ask you to reflect a bit more.\n\nQuestion 8: Given the results of your modeling efforts and your answer to Question 7, might it be better to model the outbreak using 3 or 4 Gaussians in our linear combination? What are the challenges associated with using more Gaussians?\n\n\nQuestion 9: The EbolaAll data frame records the Sierra Leone and Liberia outbreaks as well as the outbreak in Guinea. How might you use that additional data to explore the validity of your modeling process?\n\nAuthors: Prof. Robert Wolverson, USAFA & Daniel Kaplan, Macalester College and USAFA.",
    "crumbs": [
      "BLOCK I. Modeling",
      "Projects"
    ]
  },
  {
    "objectID": "Modeling/Modeling-projects.html#block-1-project-2-orbit-dimensions",
    "href": "Modeling/Modeling-projects.html#block-1-project-2-orbit-dimensions",
    "title": "Projects",
    "section": "Block 1 Project 2: Orbit dimensions",
    "text": "Block 1 Project 2: Orbit dimensions\nThis activity will apply some of the concepts and techniques you’re learning to answer the following question:\n\nHow fast does a satellite move along its orbit?\n\nAs you can imagine, the answer is already known and you could look it up. The point of our reconstructing what is already known is to see the totality of a modeling project, even if it is a very simple one.\nIn textbooks and in-class demonstrations, students are often shown complete, flawless models. In reality, model construction is a matter of trial and error. Whoops! we are supposed to say “modeling cycle.” That phrase does not suggest anything about “error.” But in reality, modelers make mistakes, operate under misconceptions, collect erroneous data, misunderstand the purpose of building a model, and make all sorts of mistakes. To cope with this unhappy situation, good modelers are constantly checking and testing their models for inconsistencies.\nTo start, you should have\n\nA good idea of what the eventual answer will be. Often, that idea comes from somewhat vague and imprecise knowledge. For example, you may have heard that it takes a satellite in low orbit about 90 minutes to complete one circuit of the Earth. You may also know that the length of the equator is roughly 40,000 kilometers. (This is the original definition of the meter.) A velocity is distance traveled over time, so a satellite in low orbit has a velocity of roughly \\(40000 / 90\\) km/minute, which comes out to 7400 meters/second.\nA theory that relates what you want to know to what you already know. For our purposes, that theory comes directly from Isaac Newton in the 1680s: his laws of motion and his theory of universal gravitation.\n\n\nThe theory\nWe won’t assume that you have anything more than a vague understanding of Newton’s laws and theory of gravitation. The diagram shows the situation schematically.\n\n\n\n\n\n\n\n\n\nThe satellite is traveling clockwise along a curved trajectory encircling the Earth. The position of the satellite is shown at several times by the numbered blue dots. Let’s focus on the satellite at time 1.\nThe satellite is an object in motion. Newton’s First Law (“Lex I”) is stated in his 1687 book, Philosophiae Naturis Principia Mathematica (Mathematical principles of natural philosophy) on p.12\n\n\n\n\n\n\n\n\n\nTranslating into English, this is\n\nLaw I: Every body persists in its state of rest or uniform motion in a straight line, unless compelled to change that motion by forces impressed upon it.\n\nThe dashed line connecting the points labeled 1 and 2’ shows the path that the satellite would follow if there were no forces impressed upon it.\nYet there is a force impressed on the satellite: the gravitational attraction between the Earth and the satellite. This force accelerates the satellite perpendicular to its orbit (toward the center of the Earth) causing the satellite to follow a curved path rather than a straight path off into deep space. The acceleration of the satellite traveling at constant speed in orbit depends on both the velocity \\(v\\) of the satellite and the radius \\(r\\) of its orbit.\nTask #1: Let \\(A_1\\) be the acceleration needed to keep the satellite in a circular orbit. Find a plausible relationship between \\(A_1\\), \\(r\\), and \\(v\\). One possibility is that the relationship is a general product of the form \\[A_1 = v^n\\ r^m .\\] Use dimensional analysis to find \\(n\\) and \\(m\\). Recall that acceleration has dimension L/T\\(^{2}\\), velocity has dimension \\(L/T\\) and radius has dimension L.\nOnce you determine \\(n\\) and \\(m\\), write down the relationship \\(A_1\\) as a function of \\(r\\) and \\(v\\).\n\nAs we all know, gravity pulls all objects toward the center of the Earth. The acceleration \\(A_2\\) due to gravity on an object a distance \\(r\\) from the enter of the Earth is proportional to the mass of the Earth and is known to be \\[A_2  = G\\ M_e/r^2\\] where \\(G\\) is a constant of proportionality and \\(M_e\\) is the mass of Earth.\nIn order for the satellite to stay in orbit, the two accelerations \\(A_1\\) (what’s needed to stay in orbit) and \\(A_2\\) (what the Earth’s gravity provides) must be equal.\nTask #2: Set your expression for \\(A_1\\) equal to the expression for \\(A_2\\) and solve for the velocity \\(v\\) of the satellite (our original objective for this exercise). Your answer will involve \\(G\\), \\(M_e\\), and \\(r\\).\nUse the known numerical values for \\(G\\) and \\(M_e\\) given in the next section to check that your answer makes sense.\n\n\nThe data\nThe data here come from scientific observations made over centuries that give us numerical values (with units) of \\(M_e\\) and \\(G\\) in the theory.\n\\(G\\) is a universal constant (according to Newton’s theory of gravitation). The quantity is given by several sources as\n\\[G = 6.674 \\times 10^{-11} m^3 /(s^2 kg).\\]\nSimilarly, the mass of the Earth is given as\n\\[M_e  = 5.972 × 10^{24} kg\\]\nThese reported facts seem plausible, but it is a good practice to check. Toward that end, check\n\nThe dimension and units of \\(A_2(v, r)\\) are consistent.\nThe value of \\(A_2\\) at the Earth’s surface is consistent with the famous value 9.8 m/s\\(^2\\).\n\nTask #3: Finishing up.\nUse the formula you derived for \\(v\\) as a function of \\(r\\), \\(G\\), and \\(M_e\\) to find \\(v\\) for a satellite in low orbit around the Earth. The official extent of the “Low Earth Orbit Region” is up to 2000 km. If you were using the altitude of the International Space Station (400 km), you would set \\(r = r_e + 400km\\), where \\(r_e\\) is the radius of the earth: 6, 378.1 km.\nAs always, you want to do the calculation in a way that helps you to spot possible errors. Here are two good practices:\n\nYou have already confirmed (or should have) that your formula for \\(v\\) as a function of \\(r\\), \\(G\\), and \\(M_e\\) is dimensionally consistent. As you plug in numerical values for \\(r\\), \\(G\\), and \\(M_e\\), make sure to keep track of the units explicitly and that the result you get has proper units for velocity.\nCompare your result to the rough estimate of \\(v\\) for satellites in low orbit that you made at the beginning of this activity. If there is a discrepancy, review both your initial rough estimate as well as your gravity-based derivation of \\(v\\) to figure out where the inconsistency comes from. Then fix it.",
    "crumbs": [
      "BLOCK I. Modeling",
      "Projects"
    ]
  },
  {
    "objectID": "modeling-part.html",
    "href": "modeling-part.html",
    "title": "BLOCK I. Modeling",
    "section": "",
    "text": "A model is a representation of something—for instance, a building—for a specific purpose. A model of a building might take several different forms, depending on the purpose. For example, a blueprint plan shows the layout of rooms, corridors, windows, etc. for the purpose of exploring the design and guiding the construction. A three-dimensional balsa-wood model of a building helps to examine how the building appears from different perspectives. A list of components in the building is a model whose purpose is to facilitate ordering those components and checking whether everything needed is at hand during construction.\nA mathematical model is a model made up of mathematical stuff. Balsa-wood and blueprint paper are not mathematical stuff. This Block introduces some of the different kinds of mathematical stuff that we will use in constructing mathematical models. Of primary importance will be mathematical functions. The block describes various aspects of functions: parameters and the ways to set their values, ways to construct complicated functions out of simpler components, a particularly useful type of function—polynomials—that are a type of modeling “clay.”\nUsing mathematical models often involves performing a mathematical operation on a model. One type of operation emphasized in high-school math is solving which takes information in one form (e.g., a function) and gives information in another form (e.g., the inputs that will cause the function output to take on a specific value). Other kinds of operations are optimization and iteration, which will be introduced in this Block.\nAlthough functions are fundamental to mathematical modeling, there are other mathematical and scientific concepts that make it much easier to think about how a model relates to the real world. Two of these are the mathematics of magnitude and the units and dimension of quantities.\nFinally, the block considers the process of building models and techniques that help a human modeler to get started and then refine the model until it can serve its purpose.",
    "crumbs": [
      "BLOCK I. Modeling"
    ]
  },
  {
    "objectID": "Modeling/16-modeling-scientific-method.html#enr-peak-oil-peak-oil",
    "href": "Modeling/16-modeling-scientific-method.html#enr-peak-oil-peak-oil",
    "title": "16  Modeling and the scientific method",
    "section": "16.8 Enrichment topic 16.1: Peak Oil",
    "text": "16.8 Enrichment topic 16.1: Peak Oil",
    "crumbs": [
      "BLOCK I. Modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Modeling and the scientific method</span>"
    ]
  },
  {
    "objectID": "Differentiation/26-approximation.html",
    "href": "Differentiation/26-approximation.html",
    "title": "26  Local approximations",
    "section": "",
    "text": "26.1 Eight simple, local shapes\nIn many modeling situations with a single input, selecting one of eight simple shapes, those shown in Figure 26.1, can get you far.\nTo choose among these shapes, consider your modeling context:\nConsider these historical examples:\nSome other examples:",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Local approximations</span>"
    ]
  },
  {
    "objectID": "Differentiation/26-approximation.html#eight-simple-local-shapes",
    "href": "Differentiation/26-approximation.html#eight-simple-local-shapes",
    "title": "26  Local approximations",
    "section": "",
    "text": "Figure 26.1: The eight simple shapes of local functions with one input.\n\n\n\n\n\nis the relationship positive (slopes up) or negative (slopes down)\nis the relationship monotonic or not\nis the relationship concave up, concave down, or neither\n\n\n\nNewton’s Law of Cooling. Let the input be the difference in temperature between an object and its environment. Let the output be the rate at which the object’s temperature changes. Newton’s Law of Cooling amounts to a choice of shape (B).\nHooke’s Law, describing the force supplied by a compressed or stretched object such as a spring. Let the input be the how much the object is compressed or stretched: negative for compression, positive for stretching. Let the output be the force supplied by the string, with a positive force meaning away from the spring and negative towards the spring. Hooke’s Law is shape (A).\nInverse square law for gravitational force. The input is the distance between the masses, the output is the force, with a negative force corresponding to attraction. The inverse square law corresponds to shape (C).\nChemistry’s Law of Mass Action for an element or molecule reacting with itself. The input is the concentration of the substance, the output is the rate of production of the compound. Shape (D). (For the Law of Mass Action involving two different substances, we need shapes of functions with two inputs. See Section 26.3.)\n\n\n\nThe incidence of an out-of-control epidemic versus time is concave up, but shallow-then-steep. Shape D. As the epidemic is brought under control, the decline is steep-then-shallow and concave up. Shape C. Notice that in each case we are describing only the local behavior of the function.\nHow many minutes can you run as a function of speed? Concave down and shallow-then-steep; you wear out faster if you run at high speed. How far can you walk as a function of time? Steep-then-shallow and concave down; your pace slows as you get tired. Shape (E).\nHow does the stew taste as a function of saltiness. The taste improves as the amount of salt increases … up to a point. Too much salt and the stew is unpalatable. Shape (G).\nHow much fuel is consumed by an aircraft as a function of distance? For long flights the function is concave up and shallow-then-steep; fuel use increases with distance, but the amount of fuel you have to carry also increases with distance and heavy aircraft use more fuel per mile. Shape (E).\nIn micro-economic theory there are production functions that describe how much of a good is produced at any given price, and demand functions that describe how much of the good will be purchased as a function of price.\n\nAs a rule, production increases with price and demand decreases with price. In the short term, production functions tend to be concave down, since it is hard to squeeze increased production out of existing facilities. Shape (F).\nFor demand in the short term, functions will be concave up when there is some group of consumers who have no other choice than to buy the product. An example is the consumption of gasoline versus price: it is hard in the short term to find another way to get to work. Shape (C). In the long term, consumption functions can be concave down as consumers find alternatives to the high-priced good. For example, high prices for gasoline may, in the long term, prompt a switch to more efficient cars, hybrids, or electric vehicles. This will push demand down steeply. Shape (E).",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Local approximations</span>"
    ]
  },
  {
    "objectID": "Differentiation/26-approximation.html#low-order-polynomials",
    "href": "Differentiation/26-approximation.html#low-order-polynomials",
    "title": "26  Local approximations",
    "section": "26.2 Low-order polynomials",
    "text": "26.2 Low-order polynomials\nThere is a simple, familiar functional form that, by selecting parameters appropriately, can take on each of the eight simple shapes: the second-order polynomial. \\[g(x) \\equiv a + b x + c x^2\\] As you know, the graph of \\(g(x)\\) is a parabola.\n\nThe parabola opens upward if \\(0 &lt; c\\). That is the shape of a local minimum.\nThe parabola opens downward if \\(c &lt; 0\\). That is the shape of a local maximum\n\nConsider what happens if \\(c = 0\\). The function becomes simply \\(a + bx\\), the straight-line function.\n\nWhen \\(0 &lt; b\\) the line slopes upward.\nWhen \\(b &lt; 0\\) the line slopes downward.\n\nWith the appropriate choice of parameters, the form \\(a + bx + cx^2\\) is capable of representing four of the eight simple shapes. What about the remaining four? This is where the idea of local becomes important. Those remaining four shapes are the sides of parabolas, as in Figure 26.2.\n\n\n\n\n\n\n\n\nFigure 26.2: Four of the eight simple shapes correspond to the sides of the parabola. The labels refer to the graphs in Figure 26.1.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Local approximations</span>"
    ]
  },
  {
    "objectID": "Differentiation/26-approximation.html#sec-low-order-two",
    "href": "Differentiation/26-approximation.html#sec-low-order-two",
    "title": "26  Local approximations",
    "section": "26.3 The low-order polynomial with two inputs",
    "text": "26.3 The low-order polynomial with two inputs\nFor functions with two inputs, the low-order polynomial approximation looks like this:\n\\[g(x, y) \\equiv a_0 + a_x x + a_y y + a_{xy} x y + a_{yy} y^2 + a_{xx} x^2\\] In reading this form, note the system being used to name the polynomial’s coefficients. First, we’ve used \\(a\\) as the root name of all the coefficients. Sometimes we might want to compare two or more low-order polynomials, so it is convenient to be able to use \\(a\\) for one, \\(b\\) for another, and so on.\nThe subscripts on the coefficients describes exactly which term in the polynomial involves each coefficient. For instance, the \\(a_{yy}\\) coefficient applies to the \\(y^2\\) term, while \\(a_x\\) applies to the \\(x\\) term.\nEach of \\(a_0, a_x,\\) \\(a_y,\\) \\(a_{xy}, a_{yy}\\), and \\(a_{xx}\\) will, in the final model, be a constant quantity. Don’t be confused by the use of \\(x\\) or \\(y\\) in the name of the coefficients. Each coefficient is a constant and not a function of the inputs. Often, your prior knowledge of the system being modeled will tell you something about one or more of the coefficients, for example, whether it is positive or negative. Finding a precise value is often based on quantitative data about the system.\nIt helps to have different names for the various terms. It is not too bad to say something like, “the \\(a_{xy}\\) term.” (Pronounciation: “a sub x y” or “a x y”) But the proper names are: linear terms, quadratic terms, and interaction term. And a shout out to \\(a_0\\), the constant term.\n\\[g(x, y) \\equiv a_0 + \\underbrace{a_x x + a_y y}_\\text{linear terms} \\ \\ \\ +\n\\underbrace{a_{xy} x y}_\\text{interaction term} +\\ \\ \\  \\underbrace{a_{yy} y^2 + a_{xx} x^2}_\\text{quadratic terms}\\]\n\n## Loading required namespace: plotly\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 26.3: A saddle",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Local approximations</span>"
    ]
  },
  {
    "objectID": "Differentiation/26-approximation.html#sec-partial-thought",
    "href": "Differentiation/26-approximation.html#sec-partial-thought",
    "title": "26  Local approximations",
    "section": "26.4 Thinking partially",
    "text": "26.4 Thinking partially\nThe expression for a general low-order polynomial in two inputs can be daunting to think about all at once: \\[g(x, y) \\equiv a_0 + a_x x + a_y y + a_{xy} x y + a_{xx} x^2 + a_{yy} y^2\\] As with many complicated settings, a good approach can be to split things up into simpler pieces. With a low-order polynomial, one such splitting up involves partial derivatives. There are six potentially non-zero partial derivatives for a low-order polynomial, of which two are the same; so only five quantities to consider.\n\n\\(\\partial_x g(x,y) = a_x + a_{xy}y + 2 a_{xx} x\\)\n\\(\\partial_y g(x,y) = a_y + a_{xy}x + 2 a_{yy} y\\)\n\\(\\partial_{xy} g(x,y) = \\partial_{yx} g(x,y) = a_{xy}\\). These are the so-called mixed partial derivatives. It does not matter whether you differentiate by \\(x\\) first or by \\(y\\) first. The result will always be the same for any smooth function.\n\\(\\partial_{xx} g(x,y) = 2 a_{xx}\\)\n\\(\\partial_{yy} g(x,y) = 2 a_{yy}\\)\n\nThe above list states neutral mathematical facts that apply generally to any low-order polynomial whatsoever.1 Those facts, however, shape a way of asking questions of yourself that can help you shape the model of a given phenomenon based on what you already know about how things work.\nTo illustrate, consider the situation of modeling the effect of study \\(S\\) and of tutoring \\(T\\) (a.k.a. office hours, extended instruction) on performance \\(P(S,T)\\) on an exam. In the spirit of partial derivatives, we will assume that all other factors (student aptitude, workload, etc.) are held constant.\nTo start, pick fiducial values for \\(S\\) and \\(T\\) to define the local domain for the model. Since \\(S=0\\) and \\(T=0\\) are easy to envision, we will use those for the fiducial values.\nNext, ask five questions, in this order, about the system being modeled.\n\nDoes performance increase with study time? Don’t over-think this. Remember that the approximation is around a fiducial point. Here, a reasonable answer is, “yes.” we will take\\(\\partial_S P(S, T) &gt; 0\\) to imply that \\(a_S &gt; 0\\). This is appropriate because close to the fiducial point, the other contributors to \\(\\partial_S P(S, T)\\), namely \\(a_{ST}T + 2 a_{SS} S\\) will be vanishingly small.\nDoes performance increase with time spent being tutored? Again, don’t over-think this. Don’t worry (yet) that your social life is collapsing because of the time spent studying and being tutored, and the consequent emotional depression will cause you to fail the exam. We are building a model here and the heuristic being used is to consider factors in isolation. Since (as we expect you will agree) \\(\\partial_T P(S, T) &gt; 0\\), we have that \\(a_T &gt; 0\\).\n\nNow the questions get a little bit harder and will exercise your calculus-intuition since you will have to think about changes in the rates of change.\n\nThis question has to do with the mixed partial derivative, which we’ve written variously as \\(\\partial_{ST} P(S,T)\\) or \\(\\partial_{TS} P(S,T)\\) and which it might be better to think about as \\(\\partial_S \\left[\\partial_T P(S,T) \\right]\\) or \\(\\partial_T \\left[\\partial S P(S,T)\\right]\\). Although these are mathematically equal, often your intuition will favor one form or the other. Recall that we are working on the premise that \\(\\partial_S P(S,T) &gt; 0\\), or, in other words, study will help you do better on the exam. Now for \\(\\partial_T \\left[\\partial S P(S,T)\\right]\\). This is a the matter of whether some tutoring will make your study more effective. Let’s say yes here, since tutoring can help you overcome a misconception that is a roadblock to effective study. So \\(\\partial_{TS} P(S,T) &gt; 0\\) which implies \\(a_{ST} &gt; 0\\).\n\nThe other way round, \\(\\partial_S \\left[\\partial_T P(S,T) \\right]\\) is a matter of whether increasing study will enhance the positive effect of tutoring. We will say yes here again, because a better knowledge of the material from studying will help you follow what the tutor is saying and doing. From pure mathematics, we already know that the two forms of mixed partials are equivalent, but to the human mind they sometimes (and incorrectly) appear to be different in some subtle, ineffable way.\nIn some modeling contexts, there might be no clear answer to the question of \\(\\partial_{xy}\\, g(x,y)\\). That is also a useful result, since it tells us that the \\(a_{xy}\\) term may not be important to understanding that system.\n\nOn to the question of \\(\\partial_{SS} P(S,T)\\), that is, whether \\(a_{SS}\\) is positive, negative, or negligible. We know that \\(a_{SS} S^2\\) will be small whenever \\(S\\) is small, so this is our opportunity to think about bigger \\(S\\). So does the impact of a unit of additional study increase or decrease the more you study? One point of view is that there is some moment when “it all comes together” and you understand the topic well. But after that epiphany, more study might not accomplish as much as before the epiphany. Another bit of experience is that “cramming” is not an effective study strategy. And then there is your social life … So let’s say, provisionally, that there is an argmax to study, beyond which point you’re not helping yourself. This means that \\(a_{SS} &lt; 0\\).\nFinally, consider \\(\\partial_{TT} P(S, T)\\). Reasonable people might disagree here, which is itself a reason to suspect that \\(a_{TT}\\) is negligible.\n\nAnswering these questions does not provide a numerical value for the coefficients on the low-order polynomial, and says nothing at all about \\(a_0\\), since all the questions are about change.\nAnother step forward in extracting what you know about the system you are modeling is to construct the polynomial informed by questions 1 through 5. Since you don’t know the numerical values for the coefficients, this might seem impossible. But there is a another modeler’s trick that might help.\nLet’s imagine that the domain of both \\(S\\) and \\(T\\) or the interval zero to one. This is not to say that we think one hour of study is the most possible but simply to defer the question of what are appropriate units for \\(S\\) and \\(T\\). Very much in this spirit, for the coefficients we will use \\(+0.5\\) when are previous answers indicated that the coefficient should be greater than zero, \\(-0.5\\) when the answers pointed to a negative coefficient, and zero if we don’t know. Using this technique, here is the model, which mainly serves as a basis for checking whether our previous answers are in line with our broader intuition before we move on quantitatively.\n\n\n\n\nP &lt;- makeFun(0.5*S + 0.5*T + 0.5*S*T - 0.5*S^2 ~ S & T)\ncontour_plot(P(S, T) ~ S & T, bounds(S=0:1, T=0:1))\n\n\n\n\n\n\n\n\n\n\nFigure 26.4: The result of our intuitive investigation of the effects of study and tutoring on exam performance. The units are not yet assigned.\n\n\n\nNotice that for small values of \\(T\\), the horizontal spacing between adjacent contours is large. That is, it takes a lot of study to improve performance a little. At large values of \\(T\\) the horizontal spacing between contours is smaller.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Local approximations</span>"
    ]
  },
  {
    "objectID": "Differentiation/26-approximation.html#finding-coefficients-from-data",
    "href": "Differentiation/26-approximation.html#finding-coefficients-from-data",
    "title": "26  Local approximations",
    "section": "26.5 Finding coefficients from data",
    "text": "26.5 Finding coefficients from data\nLow-order polynomials are often used for constructing functions from data. In this section, I’ll demonstrate briefly how this can be done. The full theory will be introduced in Block 5 of this text.\nThe data I’ll use for the demonstration is a set of physical measurements of height, weight, abdominal circumference, etc. on 252 human subjects. These are contained in the Body_fat data frame, shown below.\n\n\n\n\n\n\nOne of the variables records the body-fat percentage, that is, the fraction of the body’s mass that is fat. This is thought to be an indicator of fitness and health, but it is extremely hard to measure and involves weighing the person when they are fully submerged in water. This difficulty motivates the development of a method to approximation body-fat percentage from other, easier to make measurements such as height, weight, and so on.\nFor the purpose of this demonstration, we will build a local polynomial model of body-fat percentage as a function of height (in inches) and weight (in pounds).\nThe polynomial we choose will omit the quadratic terms. It will contain the constant, linear, and interaction terms only. That is \\[\\text{body.fat}(h, w) \\equiv c_0 + c_h h + c_w w + c_{hw} h w\\] The process of finding the best coefficients in the polynomial is called linear regression. Without going into the details, we will use linear regression to build the body-fat model and then display the model function as a contour plot.\n\nmod &lt;- lm(bodyfat ~ height + weight + height*weight,\n          data = Body_fat)\nbody_fat_fun &lt;- makeFun(mod)\ncontour_plot(body_fat_fun(height, weight) ~ height + weight,\n             bounds(weight=c(100, 250), height = c(60, 80))) %&gt;%\n  gf_labs(title = \"Body fat percentage\")\n\n\n\n\n\n\n\nFigure 26.5: A low order polynomial model of body fat percentage as a function of height (inches) and weight (lbs).\n\n\n\n\n\nBlock 3 looks at linear regression in more detail.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Local approximations</span>"
    ]
  },
  {
    "objectID": "Differentiation/26-approximation.html#footnotes",
    "href": "Differentiation/26-approximation.html#footnotes",
    "title": "26  Local approximations",
    "section": "",
    "text": "Note that any other derivative you construct, for instance \\(\\partial_{xxy} g(x,y)\\) must always be zero.↩︎",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Local approximations</span>"
    ]
  },
  {
    "objectID": "Differentiation/17-continuous-change.html",
    "href": "Differentiation/17-continuous-change.html",
    "title": "17  Continuous change",
    "section": "",
    "text": "17.1 Mathematics in motion\nThe questions that started it all had to do with motion of planets and marbles. In more technical language, “ballistics,” the science of balls. There were words to describe speed: fast and slow. There were words to describe force: strong and weak, heavy and light. And there were words to describe location and distance: far and near, long and short, here and there. But what were the relationships among these things? And how did time fit in, an intangible quantity that had aspects of location (long and short) and speed (quick and slow)?\nGalileo (1564-1642) started the ball rolling.1 As the son of a musician and music theorist, he had a sense of musical time, a steady beat of intervals. When a student of medicine in Pisa, he noted that swinging pendulums kept reliable time, regardless of the amplitude of their swing. After unintentionally attending a geometry lecture, he turned to mathematics and natural philosophy.\nUsing his newly developed apparatus, the telescope, Galileo’s observations put him on a collision course with the accepted classical truth about the nature of the planets. Seeking to understand gravity, he built an apparatus that enabled him accurately to measure the position in time of a ball rolling down a straight ramp. The belled gates he set up to mark the ball’s passage were spaced evenly in musical time: 1, 2, 3, 4, …. To get this even spacing in time, Galileo found he had to position the gates unevenly. Defining as 1 the distance of the first gate from the ball’s release point, the gates were at positions 1, 4, 9, 16, ….\nA re-enactment of Galileo’s rolling-ball experiment. The frets on the ramp are at positions 2 cm, 8 cm, 18 cm, 32 cm, 50 cm, …, that is, 2 cm times 1, 4, 9, 16, 25. ::: ```\nAnyone familiar with the squares of the integers can see the pattern in 1, 4, 9, 16, …. To demonstrate the pattern, Galileo took the difference between the successive positions, what we will call the “first increment.”\n\\[\\underbrace{1 - 0}_1 \\ \\ \\ \\ \\ \\underbrace{4 - 1}_3\\ \\ \\ \\ \\ \\underbrace{9 - 4}_{5}\\ \\ \\ \\ \\ \\underbrace{16-9}_7\\ \\ \\ \\underset{{\\Large\\strut}\\text{first increment}}{\\text{}}\\] Next, Galileo repeated the differencing process on the first increment to produce a “second increment.”\n\\[\\underbrace{3 - 1}_2 \\ \\ \\ \\ \\ \\underbrace{5 - 3}_2\\ \\ \\ \\ \\ \\underbrace{7 - 5}_{2}\\ \\ \\ \\underset{{\\Large\\strut}\\text{second increment}}{\\text{}}\\] ::: {.column-margin} For more about Galileo’s measurements, see Stillman Drake (1986) “Galileo’s physical measurements” American Journal of Physics 54, 302-305 https://doi.org/10.1119/1.14634 :::\nThe rule established by Galileo’s observations for the motion of a ball rolling down the ramp:",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Continuous change</span>"
    ]
  },
  {
    "objectID": "Differentiation/17-continuous-change.html#mathematics-in-motion",
    "href": "Differentiation/17-continuous-change.html#mathematics-in-motion",
    "title": "17  Continuous change",
    "section": "",
    "text": "Table 17.1: Galileo’s observations and their first & second increments.\n\n\n\n\n\n\\(t\\)\n\\(x(t)\\)\nfirst increment\nsecond increment\n\n\n\n\n0\n0\n1\n2\n\n\n1\n1\n3\n2\n\n\n2\n4\n5\n2\n\n\n3\n9\n7\n\n\n\n4\n16\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe second increment of position is constant.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Continuous change</span>"
    ]
  },
  {
    "objectID": "Differentiation/17-continuous-change.html#continuous-time",
    "href": "Differentiation/17-continuous-change.html#continuous-time",
    "title": "17  Continuous change",
    "section": "17.2 Continuous time",
    "text": "17.2 Continuous time\nGalileo’s mathematics of first and second increments was suited to the discrete-time measurements he was able to make. It would be for Newton to develop the continuous-time analog of increments.\nTo start, we can imagine a function \\(x(t)\\) that gives the position of the ball at any instant \\(t\\). With this notation, Galileo’s measured positions were \\(x(0), x(1), x(2), x(3), x(4), \\ldots\\), and the first increments were \\(x(1) - x(0)\\), \\(x(2) - x(1)\\), \\(x(3) - x(2)\\), and so on.\nBut just as position \\(x(t)\\) is a continuous function of time \\(t\\), the first increment can also be written as a continous function: \\[y(t) \\equiv x(t+1) - x(t)\\ .\\] Similarly, there is a second increment function: \\[\\begin{eqnarray}z(t) & \\equiv&  y(t+1) - y(t)\\\\ & = & \\left[x(t+2) - x(t+1)\\right] - \\left[x(t+1) - x(t)\\right] \\\\ &=& x(t+2) - 2 x(t+1) + x(t)\\ .\n\\end{eqnarray}\\]\nThe \\(+1\\) and \\(+2\\) in the first and second increment functions correspond to the time elapsed from one belled gate to the next. More generally, rather than using Galileo’s unit of rhythmic time, we can define the increment functions using a time quantity of our own choice; we will call it \\(h\\).\nRe-written using \\(h\\), the first increment becomes \\[y(t) \\equiv x(t+h) - x(t)\\ .\\] The second increment function is \\[\\begin{eqnarray}z(t) & \\equiv&  y(t+h) - y(t)\\\\ & = & \\left[x(t+2h) - x(t+h)\\right] - \\left[x(t+h) - x(t)\\right] \\\\ &=& x(t+2h) - 2 x(t+h) + x(t)\\ .\n\\end{eqnarray}\\]\nEvidently, the numerical values (dimension L) of the first and second increments depend on \\(h\\), which is a choice made by the experimenter, not a fact of nature. If the experimenter selects a large \\(h\\), the first and second increments will be large.\nIt would be nice to frame the ballistics theory so that \\(h\\) does not appear. Newton’s insight amounts to taking two steps:\n\nReplace the simple difference \\(x(t+h) - x(t)\\) with a rate of change, that is: ::: {.column-margin} Note that we are using the symbol \\({\\cal D}\\_t\\) and naming the rate of change function \\({\\cal D}_t y(t)\\). Read \\({\\cal D}_t\\) as “the rate of change with respect to \\(t\\). ::: \\[\\text{Rate of change of } x(t): \\ \\ \\ \\ {\\cal D}_t y(t) \\equiv \\frac{x(t+h) - x(t)}{h}\\]\n\nLikewise, the second increment will become a “rate of change of a rate of change,” a phrase that is easier to understood when written as a formula:\n\n\nRead \\({\\cal D}_t {\\cal D}_t y(t)\\) as “the rate of change of the rate of change of \\(y(t)\\).”\n\\[\\begin{eqnarray}\n{\\cal D}_t {\\cal D}_t y(t)  &\\equiv&   {\\cal D}_t \\left(\\strut \\frac{{\\cal D}_t y(t+h) - {\\cal D}_t y(h)}{h}\\right) \\\\\n&=& \\frac{y(t+h) - y(t)}{h} \\\\\n&=& \\frac{\\frac{x(t+2h) - x(t+h)}{h}- \\frac{x(t+h) - x(t)}{h}}{h}\\\\\n&=& \\frac{x(t+2h) - 2 x(t+h) + x(t)}{h^2}\\ .\n\\end{eqnarray}\\]\nAdmittedly, this complicated expression for the rate-of-change equivalent of Galileo’s second increment hardly looks like an improvement! And it still depends on \\(h\\).\nThis is where the second step of Newton’s insight comes in.\n\nMake \\(h\\) vanishingly small.\n\nIn the next chapters, we will look at how these two steps—use rate of change rather than change and make \\(h\\) vanishingly small—create mathematical entities that allowed Newton to extend Galileo’s work to become a universal theory of motion.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Continuous change</span>"
    ]
  },
  {
    "objectID": "Differentiation/17-continuous-change.html#change-relationships",
    "href": "Differentiation/17-continuous-change.html#change-relationships",
    "title": "17  Continuous change",
    "section": "17.3 Change relationships",
    "text": "17.3 Change relationships\nAs you know, function is a mathematical idea used to represent a relationship between quantities. For instance, the water volume of a reservoir behind a dam varies with the seasons and over the years. As a function, water volume is a relationship between water volume (one quantity) and time (another quantity). Similarly, the flow in a river feeding the reservoir has its own relationship with time. In spring, the river may be rushing with snow-melt, in late summer the river may be dry, but after a summer downpour the river flow again rises briefly. In other words, river flow is a function of time.\nDifferentiation is a way of describing a relationship between relationships. The water volume in the reservoir has a relationship with time. The river flow has a relationship with time. Those two relationships are themselves related: the river flow feeds the reservoir and thereby influences the water volume.\nIt is not easy to keep straight what’s going on in a “relationship between relationships.” Consequently, we need tools such as differentiation to aid our understanding. For instance, Johannes Kepler (1572-1630) spent years analyzing the data collected by astronomer Tycho Brahe (1546-1601). The data showed a relationship between time and the speed of a planet across the sky. Long-standing wisdom claimed that there is also a specific relationship between a planet’s position and time. From antiquity, it had been claimed that planets moved in circular orbits. Kepler worked hard to find the relationship between the two relationships: speed versus time and position versus time. He was unsuccessful until he dropped the assumption that planetary orbits are circular. Testing the hypothesis that orbits are elliptical, Kepler was able to find a simple relationship between speed vs. time and position vs. time.\nBuilding on Kepler’s earlier work, Newton hypothesized that planets might be influenced by the same gravity that pulls an apple to the ground. It was evident from human experience that gravity has the most trivial relationship with time: gravity is constant! But Newton could not find a link between this notion of gravity as a constant and Kepler’s planetary motion as a function of time. Success came when Newton hypothesized—without any direct evidence from experience—that gravity is a function of distance. Newton’s formulation of the relationship between relationships— gravity-as-a-function-of-distance and orbital-position-as-a-function-of time—became the foundation of modern science. Newton’s theories of gravity, force, and motion created an extremely complicated chain or reasoning that is still hard to grasp. Or, more precisely, it is hard to grasp until you have the language for describing relationships between relationships. Newton invented this language: differentiation. As you learn this language, you will find it easier to express and understand relationships between relationships, that is, the mechanisms that account for the ever-changing quantities around us.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Continuous change</span>"
    ]
  },
  {
    "objectID": "Differentiation/17-continuous-change.html#with-respect-to",
    "href": "Differentiation/17-continuous-change.html#with-respect-to",
    "title": "17  Continuous change",
    "section": "17.4 With respect to …",
    "text": "17.4 With respect to …\nWe’ve introduced a bit of new notation in the previous section, \\({\\cal D}_t\\).  As mentioned previously, \\({\\cal D}\\) stands for “the rate of change of ____.” In use, you put a function in the slot indicated by ____. Which function depends on what you want to describe. For instance, the position of a rolling ball is a function of time: \\(x(t)\\). “The rate of change of \\(x(t)\\)” is written \\({\\cal D}_t x(t)\\). This object \\({\\cal D}_t x(t)\\) is itself a function of time.In Chapter 19, when we carry out the second step of Newton’s program by making \\(h\\) vanishingly small, we will switch from the big \\({\\cal D}\\) to a smaller one, \\(\\partial\\), to remind us that \\(h\\) has vanished from the picture.\nAnother example: consider a water reservoir fed by a spring and drained by the water utility to serve its customers. Suppose \\(w(t)\\) is the volume of water in the reservoir, a quantity that changes over time. Then \\({\\cal D}_t w(t)\\) is the rate of change of water volume in the reservoir. Common sense suggests that the rate of change in water volume will be positive during a wet season and negative in a drought.\nThe subscript on \\({\\cal D}_t\\) is the with-respect-to input. To illustrate, suppose that \\(h(v, w)\\) is the volume of the harvest from a field as a function of the amount of irrigation water \\(w\\) and the amount of fertilizer used during the growing season.  As will be described in Chapter 25, there are two different rate-of-change functions associated with \\(h()\\). One is the rate of change in harvest volume with respect to \\(w\\), the other is the rate change in harvest volume with respect to \\(v\\). In everyday language, \\({\\cal D}_w h(v, w)\\) can be used to predict how much the harvest will change if, next year, the farmer uses less irrigation water. Similarly, \\({\\cal D}_v h(v, w)\\) can inform a farmer’s decision to reduce costs by using less fertilizer.Constructing such a function could be done by collecting data over many years of the harvest, along with the amount of water and fertilizer used each year.\nStrictly speaking, for functions with just one input the subscript on \\({\\cal D}\\) isn’t needed. Even so, we will always include a subscript, if only for the sake of forming good habits to serve us when we do examine rates of change in functions of multiple inputs.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Continuous change</span>"
    ]
  },
  {
    "objectID": "Differentiation/17-continuous-change.html#footnotes",
    "href": "Differentiation/17-continuous-change.html#footnotes",
    "title": "17  Continuous change",
    "section": "",
    "text": "Galileo was not aware of Kepler’s elliptical theory, even though they lived at the same time.↩︎",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Continuous change</span>"
    ]
  },
  {
    "objectID": "Differentiation/18-rate-of-change.html",
    "href": "Differentiation/18-rate-of-change.html",
    "title": "18  Rate of change",
    "section": "",
    "text": "18.1 Outputs versus rates of change\nUsing functions to describe the car-trip situation, we can say that position is a function of time. We will call it \\(p(t)\\). The input to the function is time and the output is position.\nA rate of change for the function can be calculated by choosing two different values for time and evaluating the function at those times. The evaluation produces two different values for the output position. Calling the two times \\(t_0\\) and \\(t_1\\), the corresponding outputs are \\(p(t_0)\\) and \\(p(t_1)\\).\nThe average rate of change of \\(p(t)\\) over the interval \\(t_0 \\leq t \\leq t_1\\) is \\[\\frac{p(t_1) - p(t_0)}{t_1 - t_0}\\ .\\] In ?sec-dimension-and-units we saw that subtraction is legitimate only when the two quantities involved have the same dimension and units. That is the case here. \\(p(t_1)\\) and \\(p(t_2)\\) both have dimension L and miles as the unit. \\(t_1\\) and \\(t_0\\) both have dimension T and hours as the unit.\nThe division of \\(p(t_1) - p(t_0)\\) (dimension L) by \\(t_1 - t_0\\) (dimension T) is also dimensionally legitimate. The simple reason is that division of one quantity by another is always dimensionally legitimate. The division produces a quantity with dimension L/T.\nA quantity with dimension L/T is utterly different than a quantity of dimension L or a quantity of dimension T. In other words, “25 miles per hour” is neither a position nor a time, it is a velocity.\nOne way to see that velocity is a different kind of quantity than position or time is that you measure the quantities in different ways. You might measure position by noting the passage of a mile marker along the side of the road. You can measure time by reference, say, to your level of boredom or by checking a clock or watch. Divide change in position by change in time to get velocity. But you can also sense velocity directly, by the level of noise in the car or the blurring of nearby objects along the road.\nOn a graph, you also measure in different ways changes in the input to a function and the corresponding changes in output. As always, start by picking the endpoints of an interval in the domain of the function. As an example, ?fig-stop-and-go-b marks the endpoints of an interval with \\(\\color{magenta}{magenta}\\) dots.\nDraw a rectangle connecting the function values at the start and end of the interval. The change in input is the horizontal extent of the rectangle. The change in output is the vertical extent of the rectangle. If “vertical” and “horizontal” are enough to point out that the two measures are of different kinds of things, you will be reminded by your having to use two different scales for the two measurements.\nFigure 18.2: Adding a scale for slope to the graph.\nOver the interval marked, the average rate of change of the function is still another kind of perceived quantity, the “slope” of the diagonal of the rectangle. Unfortunately, graphs do not typically include a scale for slope, but we have added a scale to Figure 18.2. From the slope scale, you can easily see that the average rate of change is a little less than 30 miles per hour.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Rate of change</span>"
    ]
  },
  {
    "objectID": "Differentiation/18-rate-of-change.html#slope-at-a-point",
    "href": "Differentiation/18-rate-of-change.html#slope-at-a-point",
    "title": "18  Rate of change",
    "section": "18.2 Slope at a point",
    "text": "18.2 Slope at a point\nWith a slope scale, you can dispense with the laborious process shown in ?fig-stop-and-go-b: marking an interval, drawing a rectangle, measuring the vertical change, etc. The slope scale lets you read off the rate of change at a glance: pick a point in the domain, look at the slope of the function at that point, and compare it to the slope scale.\nPerhaps you can see that formally defining an interval isn’t an absolute necessity for defining a slope. Instead, you can perceive slope directly from a graph, even if it is hard to quantify without a special scale.\n\nUsing the slope scale in Figure 18.3, estimate the function’s slope at input \\(t=0.2\\). How does it compare to the slope at \\(t=0.4\\)?\n\n\n\n\n\n\n\n\n\nFigure 18.3: Adding a scale for slope to the graph.\n\n\n\n\nPlace a ruler on the function graph so that the rule touches the graph at \\(t=0.2\\). Keeping that point of contact, vary the slope of the ruler until it neatly aligns with the curve. Now, without changing the slope of the ruler, slide it over to the slope scale and read the ruler slope off that scale. The slope is a bit more than 20 miles per hour.\nAt \\(t=0.4\\), the function slope is considerably steeper than at \\(t=0.2\\), about 60 miles per hour.\n\nThe function’s slope at a specific input like \\(t=0.2\\) is called the instantaneous slope and corresponds to the instantaneous velocity of the car. , you do not have to measure the car’s velocity by reading the change in position over the interval between two distinct moments in time; you can simply look at the speedometer to get an instantaneous read-out of the velocity. We will translate instantaneous rate of change into the language of functions in Chapter 19.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Rate of change</span>"
    ]
  },
  {
    "objectID": "Differentiation/18-rate-of-change.html#sec-slope-function",
    "href": "Differentiation/18-rate-of-change.html#sec-slope-function",
    "title": "18  Rate of change",
    "section": "18.3 Slope function",
    "text": "18.3 Slope function\nFigure -Figure 18.3 shows that the rate of change of the car-position function changes during the trip. In other words, the rate of change of position is itself a function of time.\nIn general, for a function \\(p(t)\\) the rate of change function will be \\[{\\cal D}_t p(t) \\equiv \\frac{p(t+h) - p(t)}{h}\\] where \\(h\\) is the length of the interval used to compute the rate of change. We will call this the slope function of \\(f(t)\\).\n\n\n\n\n\n\n\n\n\nFigure 18.4: Showing the changing slope of \\(p(t)\\) as a series of segments. The slope scale (in red) indicates the numerical value of each segment.\n\n\n\n\nA fun but unconventional way to display a slope function is to show the literal slope of \\(p(t)\\) as a function of \\(t\\) as in Figure 18.4. It represents the value of \\({\\cal D}_t p(t)\\) as the slope of a little line segment. To read off the numerical value of the slope, refer to the slope scale drawn in red. A picture like Figure 18.4 is a good reminder that the slope function \\({\\cal D}_t p(t)\\) is all about the slope of \\(p(t)\\) and not at all about the actual value of \\(p(t)\\).\nThe conventional way to display a slope function is to show the numerical value of the slope by the position on the vertical axis, as in Figure 18.5. Such a graph is easy to read, but provides nothing but the axis label to remind you that the scale on the vertical axis is the slope of another function.\n\n\n\n\n\n\n\n\n\nFigure 18.5: Graphing the slope function \\({\\cal D}_t p(t)\\). The value of the slope of \\(p(t)\\) can be read from the vertical axis scale.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Rate of change</span>"
    ]
  },
  {
    "objectID": "Differentiation/18-rate-of-change.html#sec-tree-harvest-example",
    "href": "Differentiation/18-rate-of-change.html#sec-tree-harvest-example",
    "title": "18  Rate of change",
    "section": "18.4 Average rate of change",
    "text": "18.4 Average rate of change\nIn Chapter 19, we will start working with the instantaneous rate of change of a function. That concept is so important that you will tend to forget there was any such thing as the “average rate of change” over an interval.\nNevertheless, average rate of change can be a useful concept in many circumstances. To illustrate, Figure 18.6 shows a simplified model of the amount of usable wood harvestable from a typical tree in a managed forest of Ponderosa Pine. (You can see some actual forestry research models here.) Such a model, even if simplified, can provide useful insight for forestry planning.\n\n\n\n\n\n\n\n\nFigure 18.6: A model, somewhat realistic, of the amount of useful wood from a Ponderosa Pine as a function of the number of years from planting to harvest.\n\n\n\n\n\nThe overall pattern in ?fig-over-tree is that the tree continues to grow until year 50, when it seems to have reached an equilibrium: perhaps growth goes to zero, or rot balances growth.\nIf managing a forest for wood production, it seems sensible to try to get as much wood out of the tree as possible. The maximum volume of wood occurs, in ?fig-over-tree, at about year 50. Does that mean that harvesting at year 50 is optimal? If not, when is the best time? Spend a moment thinking about ?fig-over-tree and draw your own conclusions. Right or wrong, this will help you understand the argument we are about to make.\nGood forestry practices are “sustainable.” Forests are managed to be continually productive rather than subject to a one-time extraction of value followed by desolation. For sustainability, it is important to consider the life cycle of the forest. After all, continual productivity implies that the forest will continue to produce value into the indefinite future.\nOne implication of managing for sustainability is that the quantity to optimize is not the volume of wood from a one-time harvest. Rather, it is the rate (per year) at which wood can be sustainably extracted from the forest.\nAround year 25, the tree adds usable wood at the fastest instantaneous rate. This might suggest to some that a good time to harvest is near year 25. But, in fact, it makes no sense to harvest at the time of maximum rate of growth; why kill the tree when it is being most productive?\nA better quantity to look at for deciding when to harvest is the average rate of growth in the volume of wood. Remember that “average rate of change” is the rate of change over an extended interval. For wood harvesting, the relevant interval is the time from planting until harvest.\nHarvesting at year 25 will give a total change of 600 board feet over 25 years, corresponding to an average rate of change of \\(600 \\div 25 = 24\\ \\text{board-feet-per-year}\\). But if you wait until year 35, you will have about 900 board feet, giving an average rate of change of \\(900 \\div 35 = 25.7\\) board-feet-per-year (L3 T-1).\nIt is easy to construct a diagram that indicates whether year 35 is best for the harvest. Recall that our fundamental model of change is the straight-line function. So we will model the model of tree growth as a straight-line function. Like the model in Figure 18.6, our straight-line model will start with zero wood when planted. Furthermore, to be faithful to Figure 18.6, we will insist that the straight-line intersect or touch that curve.\nFigure 18.7 reiterates the Figure 18.6 model of the tree annotated with several straight-line models that all give zero harvest-able wood at planting time. Each green line represents a scenario where harvest occurs at \\(t_1\\), \\(t_2\\), etc. From the perspective of representing the rate of growth per year from planting to harvest, the straight-line green models do not need to replicate the actual growth curve. The complexities of the curve are not relevant to the growth rate. Instead, what’s relevant is the slope of a straight-line model connecting the output at planting time to the output at harvest time. In contrast, the \\(\\color{magenta}{\\text{magenta}}\\) curve is not a suitable model because it does not match the situation at any harvest time; it does not touch the curve anywhere after planting!\n\n\n\n\n\n\n\n\nFigure 18.7: Modeling the tree-growth model with straight lines connecting planting time to various harvest times. The slope of each line is the average rate of growth for that planting time.\n\n\n\n\n\nChoose a harvest time that produces the steepest possible green segment to maximize average lumber volume per year. From Figure 18.7, that steepest line glances the growth curve near year 31 (shown as \\(t_3\\) in the diagram).\nIt is best to find the argmax by creating a function that shows explicitly what one is trying to optimize. (In Chapter 24, we will use the name objective function to identify such function.) Here, the objective function is \\(\\text{ave.growth(year)} \\equiv \\text{volume(year)} / \\text{year}\\). See Figure 18.8.\n\n\n\n\n\n\n\n\nFigure 18.8: Graph of the average-growth function ave_growth(year), constructed by dividing volume(year) by year.\n\n\n\n\n\nThe graph of ave_growth(year) makes clear the maximum average growth from planting to harvest will occur at about year 32.\nThere is no point waiting until after year 50.\nAt year 25, the tree is growing as fast as ever. You will get about 600 board feet of lumber.1 Should you harvest at year 25? No! That the tree is growing so fast means that you will have a lot more wood in years 26, 27, etc. The time to harvest is when the growth is getting smaller so that it is not worth waiting an extra year.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Rate of change</span>"
    ]
  },
  {
    "objectID": "Differentiation/18-rate-of-change.html#sec-dimension-rate-change",
    "href": "Differentiation/18-rate-of-change.html#sec-dimension-rate-change",
    "title": "18  Rate of change",
    "section": "18.5 Dimension of a rate of change",
    "text": "18.5 Dimension of a rate of change\nThe function named \\(\\partial_t f(t)\\) which is the derivative of \\(f(t)\\) takes the same input as \\(f(t)\\); the notation makes that pretty clear. Let’s suppose that \\(t\\) is time and so the dimension of the input is \\([t] = \\text{T}\\).\nThe outputs of the two functions, \\(\\partial_t f(t)\\) and \\(f(t)\\) will not, in general, have the same dimension. Why not? Recall that a derivative is a special case of a slope function, the instantaneous slope function. It is easy to calculate a slope function:\n\\[{\\cal D}_t f(t) \\equiv \\frac{f(t+h) - f(t)}{h}\\] The dimension of the quantity \\(f(t+h) - f(t)\\) must be the same as the dimension of \\(f(t)\\); the subtraction would not be possible otherwise. Likewise, the dimension of \\(h\\) must be the same as the dimension of \\(t\\); the addition \\(t+h\\) wouldn’t make sense otherwise.\n\n\nKeep in mind that the dimension \\([f(t+h) - f(t)]\\) will be the same as \\([f(t)]\\). Why? The result of addition and subtraction will always have the same dimension as the quantities being combined.\nWhereas the dimension of the output \\(f(t)\\) is simply \\(\\left[f(t)\\right]\\), the dimension of the quotient \\(\\frac{f(t+h) - f(t)}{h}\\) will be different. The output of the derivative function \\(\\partial_t f(t)\\) will be \\[\\left[\\partial_t f(t)\\right] = \\left[f(t)\\right] / \\left[t\\right] .\\]\nSuppose \\(x(t)\\) is the position of a car as a function of time \\(t\\). Position has dimension L. Time has dimension T. The function \\(\\partial_t x(t)\\) will have dimension L/T. Familiar units for L/T are miles-per-hour, which you can recognize as velocity.\nAnother example: Imagine a function pressure() with that takes altitude above sea level (in km) and output pressure (in kPa, “kiloPascal”).2 The derivative function, let’s call it \\(\\partial_\\text{altitude} \\text{pressure}()\\), also takes an input in km, but produces an output in kPA per km: a rate.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Rate of change</span>"
    ]
  },
  {
    "objectID": "Differentiation/18-rate-of-change.html#footnotes",
    "href": "Differentiation/18-rate-of-change.html#footnotes",
    "title": "18  Rate of change",
    "section": "",
    "text": "A “board foot” is a volume, dimension L3. It is a square foot (L^2) times an inch (L).↩︎\nAir pressure at sea level is about 100 kiloPascal.↩︎",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Rate of change</span>"
    ]
  },
  {
    "objectID": "Differentiation/19-evanescent-h.html",
    "href": "Differentiation/19-evanescent-h.html",
    "title": "19  Evanescent h",
    "section": "",
    "text": "19.1 Evanescence algebraically\nLet’s look at a slope function using evanescent \\(h\\). To start, we will analyze \\(f(t) \\equiv t^2\\), one of our pattern-book functions. By definition, \\[{\\cal D}_t f(t) \\equiv \\frac{f(t+h) - f(t)}{h}\\ .\\] We can easily evaluate \\(f(t+h)\\) symbolically: \\[f(t+h) \\equiv (t+h)^2 = t^2 + 2 t h + h^2\\] Similarly, we can find the difference \\(f(t+h) - f(t)\\). It is \\[f(t+h) - f(t) = f(t+h) - t^2 = 2 t h + h^2\\ .\\] Notice that there is still some liquid (that is, \\(h\\)) in the difference. Now we let the difference start to dry, taking out the \\(h\\) by dividing the difference by \\(h\\): \\[\\frac{f(t+h) - f(t)}{h} = \\frac{2 t h + h^2}{h} = 2 t + h\\ .\\] The rate of chaange of \\(f(t)\\) has something solid—\\(2 t\\)—along with a little bit of liquid \\(h\\) that we can leave to evaporate to nothing.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Evanescent h</span>"
    ]
  },
  {
    "objectID": "Differentiation/19-evanescent-h.html#differentiation",
    "href": "Differentiation/19-evanescent-h.html#differentiation",
    "title": "19  Evanescent h",
    "section": "19.2 Differentiation",
    "text": "19.2 Differentiation\nBy this point you should be familiar with the definition of the average rate of change of \\(f(t)\\) over an interval from \\(t\\) to \\(t+h\\):\n\\[{\\cal D}_t f(t) \\equiv \\frac{f(t+h) - f(t)}{h}\\] To indicate that we want a rate of change with evanescent \\(h\\), we add a statement to that effect:\n\\[\\partial_t f(t) \\equiv \\lim_{h\\rightarrow 0} {\\cal D}_t f(t) = \\lim_{h\\rightarrow0}\\frac{f(t+h) - f(t)}{h}\\ .\\] A proper mathematical phrasing of \\(\\lim_{h\\rightarrow 0}\\) is, “the limit as \\(h\\) goes to zero.” In terms of the paint metaphor, read \\(\\lim_{h\\rightarrow 0}\\) as “once applied to the wall, let the paint dry.”\nTo save space, write \\(\\lim_{h\\rightarrow 0} {\\cal D}_t f(t)\\)a more compact way: \\(\\partial_t f(t)\\). We use the small symbol \\(\\partial\\) as a reminiscence of the role that small \\(h\\) played in the construction of \\(\\partial_t f(t)\\).\nThe function \\(\\partial_t f(t)\\) is called the derivative of the function \\(f(t)\\). The process of constructing the derivative of a function is called differentiation. The roots of these two words are not the same. “Differentiation” comes from “difference,” a nod to subtraction as in “the difference between 4 and 3 is 1.” In contrast, “derivative” comes from “derive,” whose dictionary definition is “obtain something from a specified source,” as in deriving butter from cream. “Derive” is a general term. But “derivative” and “differentiation” always refers to a specific form of related to rates of change.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Evanescent h</span>"
    ]
  },
  {
    "objectID": "Differentiation/19-evanescent-h.html#sec-d-pattern-book",
    "href": "Differentiation/19-evanescent-h.html#sec-d-pattern-book",
    "title": "19  Evanescent h",
    "section": "19.3 Derivatives of the pattern book functions",
    "text": "19.3 Derivatives of the pattern book functions\nThe pattern-book functions are so widely used that it is helpful to memorize facts about their derivatives. Remember that, as always, the derivative of a function is another function. For every pattern-book function, the derivative is itself built from a pattern-book functions. To emphasize this, the list below states the rules in terms of the names of the functions, rather than formulas.\n\n\\(\\partial_x\\) one(x) \\(=\\) zero(x)\n\\(\\partial_x\\) identity(x) \\(=\\) one(x)\n\\(\\partial_x\\) square(x) \\(=\\) 2 identity(x)\n\\(\\partial_x\\) reciprocal(x) \\(=\\) -1/square(x)\n\\(\\partial_x\\) log(x) \\(=\\) reciprocal(x)\n\\(\\partial_x\\) sin(x) \\(=\\) cos(x)\n\\(\\partial_x\\) exp(x) \\(=\\) exp(x)\n\\(\\partial_x\\) sigmoid(x) \\(=\\) gaussian(x)\n\\(\\partial_x\\) gaussian(x) \\(=\\) - x gaussian(x)\n\n\n\nIn applications, the pattern-book functions are parameterized, e.g. \\(\\sin\\left(\\frac{2\\pi}{P} t\\right)\\). Chapter 23 introduces the derivatives of the parameterized functions.\nNotice that \\(h\\) does not appear at all in the table of derivatives. Instead, to use the derivatives of the pattern-book functions we need only refer to a list of facts, not the process for discovering those facts.\n\n\n\n\n\n\n\n\nFigure 19.3: A diagram showing how differentiation connects the pattern-book functions to one another.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Evanescent h</span>"
    ]
  },
  {
    "objectID": "Differentiation/19-evanescent-h.html#notations-for-differentiation",
    "href": "Differentiation/19-evanescent-h.html#notations-for-differentiation",
    "title": "19  Evanescent h",
    "section": "19.4 Notations for differentiation",
    "text": "19.4 Notations for differentiation\nThere are many notations in wide use for differentiation. In this book, we will denote differentiation in a manner has a close analogy in computer notation.\nWe will write the derivative of \\(f(x)\\) as \\(\\partial_x f(x)\\). If we had a function \\(g(t)\\), with \\(t\\) being the input name, the derivative would be \\(\\partial_t g(t)\\). Since there is nothing special about the name of the input in functions with one input, we could just as well write the one-input function that is the derivative of \\(g()\\) with respect to its input as \\(\\partial_x g(x)\\) or \\(\\partial_z g(z)\\) or even \\(\\partial_{zebra} g(zebra)\\). For functions with just one input, the notation skeptic might argue that there is no need for a subscript on \\(\\partial\\), since it will always match the name of the input to the function being differentiated.\nEarly in the history of calculus, mathematician Joseph-Louis Lagrange (1736-1813) proposed a more compact notation for the derivative of a function with a single input. Rather than \\(\\partial_x f(x)\\), Lagrange wrote \\(f'\\). We pronounce this “f-prime.” This notation is still widely used in calculus textbooks because it is compact. But it is not a viable notation for functions used in modeling since those functions often have more than one input.\nEarlier than Lagrange, Newton used a very compact notation. The historian needs to be careful, because Newton did not use the term “derivative” nor the term “function.” Instead, Newton wrote of “flowing quantities,” that is, quantities that change in time. For Newton, typical names for such flowing quantities were \\(x\\) and \\(y\\). He didn’t use the parentheses that we now associate with functions, just the bare name. Newton used “fluent” to name such flowing quantities. Newton’s fluents were more or less what we call today “functions of time.” What we now call “derivatives,” Newton called “fluxions.” If \\(x\\) is a fluent, then Newton wrote \\(\\dot{x}\\) to stand for the fluxion. This is pronounced “x-dot.” Like Lagrange’s compact prime notation, Newton’s dot notation is still used, particularly in physics.\nThe mathematician Gottfried Wilhelm Leibniz (1646-1716) was a contemporary of Newton. Leibniz developed his own notation for calculus, which was easier to understand than Newton’s. In Leibniz’s notation, the derivative (with respect to \\(x\\)) of \\(f(x)\\) was written \\[\\frac{df}{dx}\\ .\\] The little \\(d\\) stands for “a little bit of” or “a little change in,” so \\(\\frac{df}{dx}\\) makes clear that the derivative is a ratio of two little bits. In the denominator, \\(dx\\) refers to an infinitesimal change in the value of the input \\(x\\). In the numerator, the \\(df\\) names the corresponding change in the output of \\(f()\\) when the input is changed.\nLeibniz’s notation is by far the most widely used in introductory calculus. It has many advantages compared to Newton’s or Lagrange’s notations. For example, it provides an opportunity to name the with-respect-to input. It also provides a nice notation for an operation called “anti-differentiation” which we will meet in Part ?sec-accumulation-part. And many a physics or engineering student has been taught to treat \\(dx\\) as if it were a number when doing algebraic manipulations.\nThe problem with Leibniz’s notation, from the perspective of this book, is that it does not translate well into computer notation. A statement like:\ndf/dx &lt;- x^2 + 3*x\nis a non-starter since the character / is not allowed in a name in most computer languages, including R.\nFor functions with multiple inputs, for instance, \\(h(x,y,z)\\), differentiation can be done with respect to any input. Leibniz’s notation might possibly be used to indicate which is the with-respect-to input; the three derivatives of \\(h()\\) would be written \\(dh/dx\\) and \\(dh/dy\\) and \\(dh/dz\\). However, mathematical notation did not go in this direction. Instead, for functions with multiple inputs, the three derivatives are most usually written \\(\\partial h/\\partial x\\) and \\(\\partial h/partial y\\), and \\(\\partial h/\\partial z\\). In the expression, \\(\\partial h/\\partial y\\), the symbol \\(\\partial\\) is pronounced “partial,” The three different derivatives \\(\\partial h/\\partial x\\), \\(\\partial y /\\partial y\\), and \\(\\partial h/\\partial z\\) are called “partial derivatives” and are the subject of Chapter 25.\nThis book uses \\(\\partial_x h\\), \\(\\partial_y h\\), and \\(\\partial_z h\\) to denote partial derivatives. This adequately identifies the with-respect-to input and has a close analog in computer notation. For instance, if f(x,y,z) has been defined already, the following statements are entirely valid:\ndz_f &lt;- D(f(x,y,z) ~ z)\ndy_f &lt;- D(f(x,y,z) ~ y)\nIt has been more than 300 years since Leibniz’s death. At this point calculus is so we will established that we don’t need the notation \\(df/dx\\) to remind us that a derivative is “a little bit of \\(f\\) divided by a little bit of \\(x\\).”\nThere are several traditional notations for differentiation of a single-input function named \\(f()\\). Here’s a list of some of them, along with the name associated with each:\n\nLeibnitz: \\(\\frac{df}{dx}\\)\nPartial: \\(\\frac{\\partial f}{\\partial x}\\)\nNewton (or “dot”): \\(\\dot{f}\\)\nLagrange (or “prime”): \\(f'\\)\nOne-line (used in this book): \\(\\partial_x f\\)\n\nTo read calculus fluently, you will have to recognize each of these notations. For functions with one input, they all mean the same thing. But when functions have multiple inputs, the choice is between the styles \\(\\partial f / \\partial x\\) and \\(\\partial_x f\\). We use the later because it can easily be incorporated into computer commands.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Evanescent h</span>"
    ]
  },
  {
    "objectID": "Differentiation/19-evanescent-h.html#footnotes",
    "href": "Differentiation/19-evanescent-h.html#footnotes",
    "title": "19  Evanescent h",
    "section": "",
    "text": "https://www.merriam-webster.com/dictionary/evanescent↩︎\nFull title: The Analyst: A Discourse Addressed to an Infidel Mathematician: Wherein It Is Examined Whether the Object, Principles, and Inferences of the Modern Analysis Are More Distinctly Conceived, or More Evidently Deduced, Than Religious Mysteries and Points of Faith↩︎",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Evanescent h</span>"
    ]
  },
  {
    "objectID": "Differentiation/20-computing.html",
    "href": "Differentiation/20-computing.html",
    "title": "20  Constructing derivatives",
    "section": "",
    "text": "20.1 Why differentiate?\nBefore showing the easy computer-based methods for constructing the derivative of a function, it is good to provide some motivation: Why is differentiation so frequently in so many fields of study and application?\nA primary reason lies in the laws of physics. Newton’s Second Law of Motion reads:\nNewton defined used position \\(x(t)\\) as the basis for velocity \\(v(t) = \\partial_t x(t)\\). “Change in motion,” which we call “acceleration,” is in turn the derivative \\(\\partial v(t)\\). Derivatives are also central to the expression of more modern forms of physics such as quantum theory and general relativity.\nMany relationships encountered in the everyday or technical worlds are more understandable if framed in terms of derivatives. For instance,\nOften, we know one member in such function-and-derivative pairs, but to need to calculate the other. Many modeling situations call for putting together different components of change to reveal how some other quantity of interest will change. For example, modeling the financial viability of retirement programs such as the US Social Security involves looking at the changing age structure of the population, the returns on investment, the changing cost of living, and so on. In Block V, we will use derivatives explicitly to construct models of systems, such as an outbreak of disease, with many changing parts.\nDerivatives also play an important role in design. They play an important role in the construction and representation of smooth curves, such as a robot’s track or the body of a car. (See ?sec-splines.) Control systems that work to stabilize a airplane’s flight or regulate the speed and spacing of cars are based on derivatives. The notion of “stability” itself is defined in terms of derivatives. (See ?sec-equilibria.) Algorithms for optimizing design choices also often make use of derivatives. (See ?sec-optimization-and-constraint.)",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Constructing derivatives</span>"
    ]
  },
  {
    "objectID": "Differentiation/20-computing.html#why-differentiate",
    "href": "Differentiation/20-computing.html#why-differentiate",
    "title": "20  Constructing derivatives",
    "section": "",
    "text": "“The change of motion of an object is proportional to the force impressed; and is made in the direction of the straight line in which the force is impressed.”\n\n\n\n\nElectrical power is the rate of change with respect to time of electrical energy.\nBirth rate is one component of the rate of change with respect to time of population. (The others are the death rate and the rates immigration and emigration.)\nInterest, as in bank interest or credit card interest, is the rate of change with respect to time of assets.\nInflation is the rate of change with respect to time of prices.\nDisease incidence is one component of the rate of change with respect to time of disease prevalence. (The other components are death or recovery from disease.)\nForce is the rate of change with respect to position of energy.\nDeficit (as in spending deficits) is the change with respect to time of debt.\n\n\n\n\n\n\n\n\n\nMath in the World: The Wealth of Nations\n\n\n\nEconomics as a field makes considerable use of concepts of calculus—particularly first and second derivatives, the subjects of this Block—although the names used are peculiar to economics, for instance, “elasticity”, “marginal returns” and “diminishing marginal returns.”\nThe origins of modern economics, especially the theory of the free market, are attributed to a book published in 1776, The Wealth of Nations. The author, Adam Smith (1723-1790), lays out dozens of relationships between different quantities — wages, labor, stock, interest, prices, profits, and coinage among others. Yet despite the invention of calculus a century before Wealth of Nations, the book uses no calculus.\nConsider this characteristic statement in Wealth of Nations:\n\nThe market price of every particular commodity is regulated by the proportion between the quantity which is brought to market, and the demand of those who are willing to pay the natural price of the commodity.\n\nWithout calculus and the ideas of functions and their derivatives, Smith was not able to think about prices in a modern way where price is shaped by demand and supply. Instead, for Smith, each item has a “natural price”: a fixed quantity that depends on the amount of labor used to produce the item. Nowadays, we understand that productivity changes as new methods of production and new inventions are introduced. But Smith lived near the end of a centuries-long period of static economies. Transportation, agriculture, manufacture, and population size were all much as they had been for the past 500 years or longer. James Watt’s steam engine was introduced only in 1776 and it would be decades before being adapted to the myriad uses of steam power characteristic of the 19th century. The cotton gin (1793), labor-saving agricultural machines such as the McCormick reaper (1831), the assembly line (1901), and the many other innovations of industry all lay in the future when Smith was writing Wealth of Nations.\n\n\n\nIt took the industrial revolution and nearly a century of intellectual development before economics had to and could embrace the rapid changes in the production process. In this dynamical view, supply and demand are not mere quantities, but functions of which price is the primary input. The tradition in economics is to use the word “curve” instead of “function,” giving us the phrases “supply curve” and “demand curve.” Making the transition from quantity to function, that is, between a single amount and a relationship between amounts, is a core challenge to those learning economics.\n\n\n\n\n\n\nDemand as a function of price, as first published by Antoine-Augustin Cournot in 1836.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Constructing derivatives</span>"
    ]
  },
  {
    "objectID": "Differentiation/20-computing.html#symbolic-differentiation",
    "href": "Differentiation/20-computing.html#symbolic-differentiation",
    "title": "20  Constructing derivatives",
    "section": "20.2 Symbolic differentiation",
    "text": "20.2 Symbolic differentiation\nThe R/mosaic function D() takes a formula for a function and produces the derivative. It uses the same sort of tilde expression used by makeFun() or contour_plot() or the other R/mosaic tools. For instance,\n\nD(t * sin(t) ~ t)\n## function (t) \n## sin(t) + t * cos(t)\n\nIf you prefer, you can use makeFun() to define a function, then hand that function to D() for differentiation.\n\nmyf &lt;- makeFun(sqrt(y * pnorm(1 + x^2, mean=2, sd=3)) ~ x & y)\ndx_myf &lt;- D(myf(x, y) ~ x, y=3)\ndx_myf\n## function (x, y = 3) \n## {\n##     .e1 &lt;- 1 + x^2\n##     x * y * dnorm(.e1, 2, 3)/sqrt(y * pnorm(.e1, mean = 2, sd = 3))\n## }\n\nIn the right side of the tilde expression handed off to D() names the with-respect-to input. This is similar to the tilde expressions used in plotting, which name the inputs that form the graphics domain. But it contrasts with the tilde expressions in makeFun(), where the right-hand side specifies the order in which you want the inputs to appear.\n\nNeedless to say, D() knows the rules for the derivatives of the pattern-book functions introduced in Section 19.3. For instance,\n\nD(sin(t) ~ t)\n## function (t) \n## cos(t)\nD(log(x) ~ x)\n## function (x) \n## 1/x\nD(exp(x) ~ x)\n## function (x) \n## exp(x)\nD(x^2 ~ x)\n## function (x) \n## 2 * x",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Constructing derivatives</span>"
    ]
  },
  {
    "objectID": "Differentiation/20-computing.html#finite-difference-derivatives",
    "href": "Differentiation/20-computing.html#finite-difference-derivatives",
    "title": "20  Constructing derivatives",
    "section": "20.3 Finite-difference derivatives",
    "text": "20.3 Finite-difference derivatives\nWhenever you have a formula amenable to the construction of a symbolic derivative, that is what you should use. Finite-difference derivatives are useful in those situation where you don’t have such a formula. The calculation is simple but has a weakness that points out the advantages of the evanescent-\\(h\\) approach.\nFor a function \\(f(x)\\) and a “small,” non-zero number \\(h\\), the finite-difference approximates the derivative with this formula:\n\\[\\partial_x f(x) \\approx \\frac{f(x+h) - f(x-h)}{2h}\\ .\\] To demonstrate, let’s construct the finite-difference approximation to \\(\\partial_x \\sin(x)\\). Since we already know the symbolic derivative—it is \\(\\partial_x \\sin(x) = \\cos(x)\\)—there is no genuinely practical purpose for this demonstration. Still, it can serve to confirm the symbolic rule.\nWe will call the finite-difference approximation fq_sin() and use makeFun() to construct it:\n\nfq_sin &lt;- makeFun((sin(x+h)- sin(x-h))/(2*h) ~ x, h=0.01)\n\nNotice that fq_sin() has a parameter, h whose default value is being set to 0.01. Whether 0.01 is “small” or not depends on the context. Operationally, we define “small” to be a value that gives practically the same result even if it is made smaller by a factor of 2 or 10.\nAs a demonstration that fq_sin() with \\(h=0.01\\) approximates the genuine \\(\\partial_x \\sin(x)\\), we exploit our knowledge that \\(\\partial_x \\sin(x) = \\cos(x)\\). Figure 20.1 plots out the difference between the the \\(h=0.01\\) approximation and the genuine derivative.\n\nslice_plot(fq_sin(x, h=0.01) - cos(x) ~ x, bounds(x=-10:10)) %&gt;%\n  slice_plot(fq_sin(x, h=0.001) - cos(x) ~ x, color=\"magenta\") %&gt;%\n  gf_labs(y=\"Error from true value.\")\n\n\n\n\n\n\n\nFigure 20.1: Comparing fq_sin() to \\(\\partial_x \\sin(x)\\) for two values of \\(h\\).\n\n\n\n\n\nYou will need to look carefully at the vertical axis scale in Figure 20.1 to see what’s happening. For \\(h=0.01\\), fq_sin() is not exactly the same as cos(), but it is close, always being within $$0.00017. For many purposes, this would be accurate enough. But not for all purposes. We can make the approximation better by using a smaller \\(h\\). For instance, the \\(h=0.001\\) version of fq_sin() is accurate to within $$0.0000017.\nIn practical use, one employs the finite-difference method in those cases where one does not already know the exact derivative function. This would be the case, for example, if the function is a sound wave recorded in the form of an MP3 audio file.\nIn such situations, a practical way to determine what is a small \\(h\\) is to pick one based on your understanding of the situation. For example, much of what we perceive of sound involves mixtures of sinusoids with periods longer than one-two-thousandth of a second, so you might start with \\(h\\) of 0.002 seconds. Use this guess about \\(h\\) to construct a candidate finite-difference approximation. Then, construct another candidate using a smaller h, say, 0.0002 seconds. If the two candidates are a close match to one another, then you have confirmed that your choice of \\(h\\) is adequate.\nIt is tempting to think that the approximation gets better and better as h is made even smaller. But that is not necessarily true for computer calculations. The reason is that quantities on the computer have only a limited precision: about 15 digits. To illustrate, let’s calculate a simple quantity, \\((\\sqrt{3})^2 - 3\\). Mathematically, this quantity is exactly zero. On the computer, however it is not quite zero:\n\nsqrt(3)^2 - 3\n## [1] -4.440892e-16\n\nWe can see this loss of precision at work if we make h very small in the finite-difference approximation to \\(\\partial_x \\sin(x)\\). In Figure 20.2 we are using h = 0.000000000001. The result is unsatisfactory.\n\nslice_plot(  fq_sin(x, h=0.000000000001) - cos(x) ~ x, \n           bounds(x=-10:10)) %&gt;%\n  slice_plot(fq_sin(x, h=0.0000000000001) - cos(x) ~ x,\n             color=\"magenta\") %&gt;%\n  gf_labs(y=\"Error from true value.\")\n\n\n\n\n\n\n\nFigure 20.2: In computer calculations, using too small an h leads to a loss of accuracy in the finite-difference approximation.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Constructing derivatives</span>"
    ]
  },
  {
    "objectID": "Differentiation/20-computing.html#second-and-higher-order-derivatives",
    "href": "Differentiation/20-computing.html#second-and-higher-order-derivatives",
    "title": "20  Constructing derivatives",
    "section": "20.4 Second and higher-order derivatives",
    "text": "20.4 Second and higher-order derivatives\nMany applications call for differentiating a derivative or even differentiating the derivative of a derivative. In English, such phrases are hard to read. They are much simpler using mathematical notation.\n\n\\(f(x)\\) a function\n\\(\\partial_x f(x)\\) the derivative of \\(f(x)\\)\n\\(\\partial_x \\partial_x f(x)\\), the second derivative of \\(f(x)\\), usually written even more concisely as \\(\\partial_{xx}f f(x)\\).\n\nThere are third-order derivatives, fourth-order, and on up, although they are not often used.\nTo compute a second-order derivative \\(\\partial_{xx} f(x)\\), first differentiate \\(f(x)\\) to produce \\(\\partial_x f(x)\\). Then, still using the techniques described earlier in this chapter, differentiate \\(\\partial_x f(x)\\).\nThere is a shortcut for constructing high-order derivatives using D() in a single step. On the right-hand side of the tilde expression, list the with-respect-to name repeatedly. For instance:\n\nThe second derivative \\(\\partial_{xx} \\sin(x)\\):\n\n\nD(sin(x) ~ x & x)\n## function (x) \n## -sin(x)\n\n\nThe third derivative \\(\\partial_{xxx} \\ln(x)\\):\n\n\nD(log(x) ~ x & x & x)\n## function (x) \n## 2/x^3\n\n\nPhysics students learn a formula for the position of an object in free fall dropped from a height \\(x_0\\) and at an initial velocity \\(v_0\\): \\[ x(t) \\equiv -\\frac{1}{2} g t^2 + v_0 t + x_0\\ .\\] The acceleration of the object is the second derivative \\(\\partial_{tt} x(t)\\). Use D() to find the object’s acceleration.\nThe second derivative of \\(x(t)\\) with respect to \\(t\\) is:\n\nD(0.5*g*t^2 + v0*t + x0 ~ t & t)\n## function (t, g, v0, x0) \n## g\n\nThe acceleration does not depend on \\(t\\); it is the constant \\(g\\). No wonder \\(g\\) is called “gravitational acceleration.”",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Constructing derivatives</span>"
    ]
  },
  {
    "objectID": "Differentiation/21-concavity.html",
    "href": "Differentiation/21-concavity.html",
    "title": "21  Concavity and curvature",
    "section": "",
    "text": "21.1 Quantifying concavity and curvature\nIt often happens in building models that the modeler (you!) knows something about the concavity or the curvature of a function. For example, concavity is essential in classical economics; the curve for supply as a function of price is concave down while the curve for demand as a function of price is concave up. For a train, car, or plane, sideways forces depend on the curvature of the track, road, or trajectory. Road designers need to calculate the curvature to know if the road is safe at the indicated speed.\nIt turns out that quantifying these properties of functions or shapes is naturally done by calculating derivatives.\nWe will frame the calculations in terms of a function \\(f(x)\\). Depending on the setting, \\(x\\) might be the price of a product and \\(f(x)\\) the demand for that product. Alternatively, the graph of \\(f(x)\\) might represent the path of a road drawn in \\((x,y)\\) coordinates or the reach of a robot arm as a function of time.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Concavity and curvature</span>"
    ]
  },
  {
    "objectID": "Differentiation/21-concavity.html#quantifying-concavity-and-curvature",
    "href": "Differentiation/21-concavity.html#quantifying-concavity-and-curvature",
    "title": "21  Concavity and curvature",
    "section": "",
    "text": "Math in the World: Designing a highway curve\n\n\n\nImagine designing a highway. Due to the terrain, part of the road is oriented east-west and another part north-south. Those two parts need to be connected together for vehicles to use the road! (In math-speak, we might say that the road has to be continuous, but this is just common sense.)\nExperience with highways shows that the connection will be a smooth curve. If the curve is part of a circle, the design needs to specify the radius of curvature. Too tight a radius and the traffic will not be able to handle the centrifugal force; vehicles will drift or skid off the road. A big radius provides safety, but making the radius bigger than required adds road construction costs.\nReal-world highway on- and off-ramps are usually not precisely sections of a circle, so specifying the shape of the ramp is not as simple as setting the radius of the curve. Instead, the radius changes at the entry and exit of the curve. The American Association of State Highway and Transportation Officials Policy on Geometric Design of Highways and Streets (1994) explains why:\nAny motor vehicle follows a transition path as it enters or leaves a circular horizontal curve. The steering change and the consequent gain or loss of centrifugal force cannot be effected instantly. For most curves the average driver can effect a suitable transition path within the limits of normal lane width. However, with combinations of high speed and sharp curvature the resultant longer transition can result in crowding and sometimes actual occupation of adjoining lanes. In such instances transition curves would be appropriate because they make it easier for a driver to confine the vehicle to his or her own lane. The employment of transition curves between tangents and sharp circular curves and between circular curves of substantially different radii warrants consideration.\n\n\n\n\nRemember that \\(f()\\) is just a pronoun so that we can refer to a particular function without naming it. Likewise with \\(g()\\), \\(h()\\), etc.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Concavity and curvature</span>"
    ]
  },
  {
    "objectID": "Differentiation/21-concavity.html#sec-concavity-deriv",
    "href": "Differentiation/21-concavity.html#sec-concavity-deriv",
    "title": "21  Concavity and curvature",
    "section": "21.2 Concavity",
    "text": "21.2 Concavity\nRecall that to find the slope of a function \\(f(x)\\) at any input \\(x\\), you compute the derivative of that function, which we’ve been writing \\(\\partial_x\\,f(x)\\). Plug in some value for the input \\(x\\) and the output of \\(\\partial_x\\, f(x)\\) will be the slope of \\(f(x)\\) at that input. (Chapter 20 introduced some techniques for computing the derivative of any given function.)\nNow we want to show how differentiation can quantify the concavity of a function. First, remember that when we speak of the “derivative” of a function, we mean the first derivative of the function. That full name naturally suggests that there will be a second derivative, a third derivative, and higher-order derivatives.\nFigure 21.1 shows a simple function that is concave down.\n\n\n\n\n\n\n\n\n\nFigure 21.1: A function that is concave down.\n\n\n\n\nNotice that the concavity is not about the slope. The curve in Figure 21.1 is concave down everywhere in the domain \\(0 \\leq x \\leq 4\\), but the slope is positive for \\(0 \\leq x \\leq 1\\) and negative for larger \\(x\\). Slope and concavity are two different aspects of a function.\nAs introduced in ?sec-fun-describing, the concavity of a function describes not the slope but the change in the slope. Figure 21.2 adds some annotations on top of the graph in Figure 21.1. In the subdomain marked A, the function slope is positive, while in the subdomain B, the function slope is negative. This transition from the slope at A to the slope at B corresponds to the concavity of the function between A and B.\n\n\n\n\n\n\n\n\n\nFigure 21.2: Concavity is about how the slope changes from one place in the domain to another.\n\n\n\n\nSimilarly, the function’s concavity in the interval B to C reflects the transition in the instantaneous slope at B to the different instantaneous slope at C.\nLet’s look at this using symbolic notation. Keep in mind that the function graphed is \\(f(x)\\) while the slope is the function \\(\\partial_x\\,f(x)\\). We’ve seen that the concavity is indicated by the change in slope of \\(f()\\), that is, the change in \\(\\partial_x\\, f(x)\\). We will go back to our standard way of describing the rate of change near an input \\(x\\):\n\\[\\text{concavity.of.f}(x) \\equiv\\ \\text{rate of change in}\\ \\partial_x\\, f(x) = \\partial_x [\\partial_x f(x)] \\\\\n\\\\\n= \\lim_{h\\rightarrow 0}\\frac{\\partial_x f(x+h) - \\partial_x f(x)}{h}\\] We are defining the concavity of a function \\(f()\\) at any input \\(x\\) to be \\(\\partial_x [\\partial_x f(x)]\\). We create the concavity_of_f(x) function by applying differentiation twice to the function \\(f()\\).\nSuch a double differentiation of a function \\(f(x)\\) is called the second derivative of \\(f(x)\\). The second derivative is so important in applications that it has its own compact notation: \\[\\text{second derivative of}\\ f()\\ \\text{is written}\\ \\partial_{xx} f(x)\\] Look carefully to see the difference between the first derivative \\(\\partial_x f(x)\\) and the second derivative \\(\\partial_{xx} f(x)\\): it is all in the double subscript \\(_{xx}\\).\nComputing the second derivative is merely a matter of computing the first derivative \\(\\partial_x f(x)\\) and then computing the (first) derivative of \\(\\partial_x f(x)\\). In R this process looks like:\n\ndx_f  &lt;- D(   f(x) ~ x)   # First deriv. of f()\ndxx_f &lt;- D(dx_f(x) ~ x)   # Second deriv. of f()\n\n\n\nA notation shortcut for the two-step process above: double up on the x on the right-hand side of the tilde: dxx_f &lt;- D(f(x) ~ x & x)",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Concavity and curvature</span>"
    ]
  },
  {
    "objectID": "Differentiation/21-concavity.html#sec-curvature-definition",
    "href": "Differentiation/21-concavity.html#sec-curvature-definition",
    "title": "21  Concavity and curvature",
    "section": "21.3 Curvature",
    "text": "21.3 Curvature\nAs you see from Section 21.2, it is easy to quantify the concavity of a function \\(f(x)\\): just evaluate the second derivative \\(\\partial_{xx} f(x)\\). However, it turns out that people cannot do a good job of estimating the quantitative value of concavity by eye.\nTo illustrate, consider the square function, \\(f(x) \\equiv x^2\\). (See Figure 21.3.)\n\n\n\n\n\n\n\n\n\nFigure 21.3: Does the concavity of the square function vary with \\(x\\)?\n\n\n\n\nThe square function is concave up. Now a test: Looking at the graph of the square function, where is the concavity the largest? Don’t read on until you’ve pointed where you think the concavity is largest.\nWith the answer to the test question in mind, we can calculate the concavity of the square function using derivatives.\n\\[f(x) \\equiv x^2\\ \\text{      so     }\\\n\\partial_x f(x) = 2 x\\ \\text{     and therefore     }\\ \\partial_{xx} f(x) = 2\\]\nThe second derivative of \\(f(x)\\) is positive, as expected for a function that is concave up. Surprisingly, however, the second derivative is constant.\nThe concavity-related property that the human eye reads from the function graph is not the concavity itself but the curvature of the function. The curvature of \\(f(x)\\) at \\(x_0\\) is defined to be the radius of the circle tangent to the function at \\(x_0\\).\nFigure 21.4 illustrates the changing curvature of \\(f(x) \\equiv x^2\\) by inscribing tangent circles at several points on the function graph, marked with dots. That the function’s thin black line goes right down the middle of the broader lines used to draw the circles shows the tangency of the circle to the function graph.\n\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\nFigure 21.4: At any point on the graph of a smooth function, there is a circle tangent to the graph. The radius of this circle is \\(1/{\\cal K}\\).\n\n\n\n\n\nBlack dots along the graph at the points indicate where the function graph is tangent to the inscribed circle. The visual sign of tangency is that the function graph goes right down the circle’s center.\nThe inscribed circle at \\(x=0\\) is tightest, the circle at \\(x=1\\) larger and the radius of the circle at \\(x=-1.5\\) is the largest of all. Whereas the concavity is the same at all points on the graph, the visual impression that the function is most highly curved near \\(x=0\\) is better captured by the radius of the inscribed circle. The radius of the inscribed circle at any point is the reciprocal of a quantity \\({\\cal K}\\) called the curvature.\nThe curvature \\({\\cal K}\\) of a function \\(f(x)\\) depends on both the first and second derivative. The formula for curvature \\(K\\) is somewhat off-putting; you are not expected to memorize it. But you can see where \\(\\partial x f()\\) and \\(\\partial_{xx}f()\\) come into play.\n\\[{\\cal K}_f  \\equiv \\frac{\\left|\\partial_{xx} f(x)\\right|}{\\ \\ \\ \\ \\left|1 + \\left[\\strut\\partial_x f(x)\\right]^2\\right|^{3/2}}\\]\nMathematically, the curvature \\(\\cal K\\) corresponds to the reciprocal of the radius of the tangent circle. When the tangent circle is tight, \\(\\cal K\\) is large. When radius of the tangent circle is large, that is, when the function is very close to approximating a straight line, \\(\\cal K\\) is very small.\n\n\n\n\n\n\nMath in the World: Back to the highway\n\n\n\nReturning to the highway design example earlier in the chapter … The Policy on geometric design of highways and streets called for the curvature of a road to change gently, giving the driver time to adjust the steering and accommodate the centrifugal force of the car going around the curve.\nChanging curvature implies that \\(\\partial_x {\\cal K}\\) is non-zero. Since \\({\\cal K}\\) depends on the first and second derivatives of \\(f(x)\\), the Policy on gradual change means that the third derivative of \\(f(x)\\) is non-zero.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Concavity and curvature</span>"
    ]
  },
  {
    "objectID": "Differentiation/22-cont-and-smooth.html",
    "href": "Differentiation/22-cont-and-smooth.html",
    "title": "22  Continuity and smoothness",
    "section": "",
    "text": "22.1 Continuity\nThe intuition behind continuity is simple: If you can draw the graph of a function without lifting the pencil from the paper, the function is continuous.\nContinuity can be an important attribute of a modeling function. Often, we expect that a small change in input produces a small change in output. For instance, if your income changes by one penny, you would expect your lifestyle not to change by much. If the temperature of an oven changes by 1 degree, you don’t expect the quality of the cake you are baking to change in any noticeable way.\nFigure 22.1: The Heaviside function is piecewise constant with a discontiuity at \\(x=0\\).\nAll of our basic modeling functions are continuous over their entire input domain.1 To illustrate discontinuity we will consider piecewise functions, as introduced in Section 9.4. The Heaviside function, graphed in Figure 22.1 is discontinuous.\nDrawing the graph of the Heaviside function \\(H(x)\\) involves lifting the pencil at \\(x=0\\).\nIn contrast, the piecewise ramp function (Figure 22.2) is continuous; you don’t need to lift the pencil from the paper to draw the ramp function.\nFigure 22.2: The ramp function is a continuous piecewise function.\nImagine that you were constructing a model of plant growth as a function of the amount of water (in cc) provided each day. The plant needs about 20 cc of water to thrive. You use the Heaviside function for the model, say \\(H(W-20)\\), where an output of 1 means the plant thrives and a output 0 means the plant does not. The model implies that with 20.001 cc of water, the plant will thrive. But providing only 19.999 cc of water, the plant will die. In other words, a very small change in the input can lead to a large change in the output.\nCommon sense suggests that a change of 0.002 cc in the amount of water—a small fraction of a drop, 2 cubic millimeters of volume—is not going to lead to a qualitative change in output. So you might prefer to use a sigmoid function as your model rather than a Heaviside function.\nOn the other hand, sometimes a very small change in input does lead to a large change in output. For instance, a sensible model of the hardness of water as a function of temperature would include a discontinuity at \\(32^\\circ\\)F, the temperature at which water turns to ice.\nOne of author Charles Dickens’s famous characters described the relationship between income, expenditure, and happiness this way:\nMacawber referred to the common situation in pre-20th century England of putting debtors in prison, regardless of the size of their debt. Macawber’s statement suggests he would model happiness as a Heaviside function \\(H(\\text{income}- \\text{expenditure})\\).\nWhenever the output of a function is a binary (yes-or-no) value, you can anticipate that a model will involve a discontinuous function.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Continuity and smoothness</span>"
    ]
  },
  {
    "objectID": "Differentiation/22-cont-and-smooth.html#continuity",
    "href": "Differentiation/22-cont-and-smooth.html#continuity",
    "title": "22  Continuity and smoothness",
    "section": "",
    "text": "“Annual income 20 pounds, annual expenditure 19 [pounds] 19 [shillings] and six [pence], result happiness. Annual income 20 pounds, annual expenditure 20 pounds ought and six, result misery.” — the character Wilkins Micawber in David Copperfield",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Continuity and smoothness</span>"
    ]
  },
  {
    "objectID": "Differentiation/22-cont-and-smooth.html#discontinuity",
    "href": "Differentiation/22-cont-and-smooth.html#discontinuity",
    "title": "22  Continuity and smoothness",
    "section": "22.2 Discontinuity",
    "text": "22.2 Discontinuity\nRecall the logical path that led us to the idea of the derivative of a function. We started with the differencing operator, which takes as input a function and a “small” value of \\(h\\): \\[{\\cal D}_x f(x) \\equiv \\frac{f(x+h) - f(x)}{h}\\] Then, through algebraic manipulation and numerical experiments we found that, once \\(h\\) is small enough, the graph of the slope function \\({\\cal D}_x f(x)\\) does not depend on \\(h\\). And so we defined a function \\(\\partial_x f(x)\\) where \\(h\\) does not play a role, writing \\(\\lim_{h\\rightarrow 0}\\) to remember our care to never divide by zero. \\[\\partial_x f(x) \\equiv \\lim_{h\\rightarrow 0} \\frac{f(x+h) - f(x)}{h}\\ .\\] Conveniently, we found that the derivatives of the pattern-book functions can be written in terms of the pattern-book functions without making any reference to \\(h\\). For instance:\n\n\\(\\partial_x \\ln(x) = 1/x\\) No \\(h\\) appears.\n\\(\\partial_x e^x = e^x\\) No \\(h\\) appears\n\\(\\partial_x x^p = p\\, x^{p-1}\\) No \\(h\\) appears.\nand so on.\n\nWith discontinuous functions, we have no such luck. Figure 22.3 shows what happens if we compute \\({\\cal D}_x H(x)\\), the derivative of the Heaviside function, for smaller and smaller \\(h\\).\n\nH &lt;- makeFun(ifelse(x &gt;=0, 1, 0) ~ x)\nDH01   &lt;- makeFun((H(x + 0.1) - H(x))/0.1 ~ x)\nDH001  &lt;- makeFun((H(x + 0.01) - H(x))/0.01 ~ x)\nDH0001 &lt;- makeFun((H(x + 0.001) - H(x))/0.001 ~ x)\nslice_plot(DH01(x) ~ x, bounds(x=-0.02:0.02),\n           npts=500, color=\"red\", size=2) %&gt;%\n  slice_plot(DH001(x) ~ x,\n           color=\"darkgreen\", npts=500, size=3, alpha=0.5) %&gt;%\n  slice_plot(DH0001(x) ~ x,\n           color=\"blue\", npts=500, alpha=0.5, size=2) \n\n\n\n\n\n\n\nFigure 22.3: \\({\\cal D}_x H(x)\\), the slope function of the discontinuous Heaviside, function, depends on the value of \\(h\\) used for the slope function. (Red: \\(h=0.1\\); Green: \\(h=0.01\\); Blue \\(h=0.001\\))\n\n\n\n\n\nDifferencing the Heaviside function produces very different functions depending on the value of \\(h\\). The bump near \\(x=0\\) gets taller and taller as \\(h\\) gets smaller. Mathematicians would describe this situation as \\[\\lim_{h\\rightarrow0}{\\cal D}_x H(x=0) \\equiv \\lim_{h\\rightarrow 0} \\frac{H(0+h) - H(0)}{h}\\ \\ \\ \\text{does not exist}.\\] Of course, for any given value of \\(h\\), e.g. \\(h=0.000001\\), the function \\({\\cal D}_x H(x)\\) has a definite shape. But that shape keeps changing as \\(h \\rightarrow 0\\), so we cannot point to any specific shape as the “limit as \\(h \\rightarrow 0\\).”\nSince there is no convergence in the shape of \\({\\cal D}_x H(0)\\) as \\(h\\) gets smaller, it is fair to say that the Heaviside function does not have a derivative at \\(x=0\\). But away from \\(x=0\\), the Heaviside function has a perfectly sensible derivative: \\(\\partial_x H(x) = 0\\) for \\(x\\neq 0\\). But there is no derivative at \\(x=0\\).",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Continuity and smoothness</span>"
    ]
  },
  {
    "objectID": "Differentiation/22-cont-and-smooth.html#smoothness",
    "href": "Differentiation/22-cont-and-smooth.html#smoothness",
    "title": "22  Continuity and smoothness",
    "section": "22.3 Smoothness",
    "text": "22.3 Smoothness\nSmoothness is a different concept than continuity, although the two are related. Most simply, any discontinuous function is not smooth at any input where a discontinuity occurs. But even the continuous ramp function is not smooth at the start of the ramp. Intuitively, imagine you were sliding your hand along the ramp function. You would feel the crease at \\(x=0\\).\nA function is not smooth if the derivative of that function is discontinuous. For instance, the derivative of the ramp function is the Heaviside function, so the ramp is not smooth at \\(x=0\\).\nAll of our basic modeling functions are smooth everywhere in their domain. In particular, the derivatives of the basic modeling functions are continuous, as are the second derivative, third derivative, and so on down the line. Such functions are called C-infinity, written \\(C^\\infty\\). The superscript \\(\\infty\\) means that every order of derivative is continuous.\n\n\n\nYou cannot tell from the plot that the second derivative is discontinuous. But if you were in a plane flying along that trajectory, you would feel a jerk as you crossed \\(x=0\\).\nMathematicians quantify the “smoothness” of a function by looking at the function’s continuity and the continuity of its derivatives. Smoothness is assessed on a scale \\(C^0, C^1, C^2, \\ldots, C^\\infty\\).\n\n\\(C^0\\): the function \\(f()\\) is continuous. Intuitively, this means that the function’s graph can be drawn without lifting the pencil from the paper.\n\\(C^1\\): the function \\(f()\\) has a derivative over its entire domain and that derivative \\(\\partial_x f(x)\\) is continuous. (See ?fig-c1-function for an example.)\n\\(C^2\\): the function \\(\\partial_x f(x)\\) has a derivative over its entire domain and that derivative is continuous. In other words, \\(\\partial_{xx} f(x)\\) exists and is continuous.\n\\(C^n\\): Like \\(C^2\\), but we are talking about the \\(n\\)th-derivative of \\(f(x)\\) existing and being continuous.\n\\(C^\\infty\\): Usually when we denote a sequence with an infinite number of terms, we write down something like \\(C^0, C^1, C^2, \\ldots\\). It would be entirely valid to do this in talking about the \\(C^n\\) sequence. But many of the mathematical functions we work with are infinitely differentiable, that is \\(C^\\infty\\).\n\nExamples of \\(C^\\infty\\) functions:\n\n\\(\\sin(x)\\): the derivatives are \\(\\partial_x \\sin(x) = \\cos(x)\\), \\(\\partial_{xx} \\sin(x) = -\\sin(x)\\), \\(\\partial_{xxx} \\sin(x) =-\\cos(x)\\), \\(\\partial_{xxxx} \\sin(x) =\\sin(x)\\), … You can keep going infinitely.\n\\(e^x\\): the derivatives are \\(\\partial_x e^x = e^x\\), \\(\\partial_{xx} e^x = e^x\\), and so on.\n\\(x^2\\): the derivatives are \\(\\partial_x x^2 = 2 x\\), \\(\\partial_{xx} x^2 = 2\\), \\(\\partial_{xxx} x^2 = 0\\), … Higher order derivatives are all simply 0. Boring, but still existing.\n\nExample of non-\\(C^2\\) functions: We see these often when we take two or more different \\(C^\\infty\\) functions and split their domain, using one function for one subdomain and the other(s) for other subbounds(s).\n\n\\(|x|\\), the absolute value function. \\(|x|\\) is a pasting together of two \\(C^\\infty\\) functions: \\[|x| \\equiv \\left\\{\\begin{array}{rcl}+x & \\text{for} & 0 \\leq x\\\\-x&\\text{for}& \\text{otherwise}\\end{array} \\right.\\ .\\] The domain is split at \\(x=0\\).\n\n\nFor engineering and design problems, smoothness means something substantially different than described by the mathematical concepts above. In ?sec-splines we will introduce cubic splines which are continuous functions defined by a finite set of coordinate pairs: two variables of a data frame. Each line of the data frame specifies a “knot point.” The spline consists of ordinary cubic polynomials drawn piecewise between consecutive knot points. At a knot point, the cubics on either side have been arranged to have their first and second derivatives match. Thus, the first two derivatives are continuous. The function is at least \\(C^2\\). The second derivative of a cubic is a straight-line function, so the second derivative of a cubic spline is a series of straight-line functions connected at the knot points. The second derivative does not itself have a derivative at the knot points. So, a cubic spline cannot satisfy the requirements for \\(C^3\\); it is \\(C^2\\).",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Continuity and smoothness</span>"
    ]
  },
  {
    "objectID": "Differentiation/22-cont-and-smooth.html#footnotes",
    "href": "Differentiation/22-cont-and-smooth.html#footnotes",
    "title": "22  Continuity and smoothness",
    "section": "",
    "text": "The domain of the function \\(1/x\\) is the whole number line, except 0, where the positive and negative branches fail to meet up.↩︎",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Continuity and smoothness</span>"
    ]
  },
  {
    "objectID": "Differentiation/23-rules.html",
    "href": "Differentiation/23-rules.html",
    "title": "23  Derivatives of assembled functions",
    "section": "",
    "text": "23.1 Using the rules\nWhen you encounter a function that you want to differentiate, you first have to examine the function to decide which rule you want to apply. In the following, we will to use the names \\(f()\\) and \\(g()\\), but in practice the functions will often be basic modeling functions, for instance \\(e^{kx}\\) or \\(\\sin\\left(\\frac{2\\pi}{P}t\\right)\\), etc.\nStep 1: Identify f() and g()\nWe will write the rules in terms of two function names, \\(f()\\) and \\(g()\\), which can stand for any functions whatsoever. It is rare to see the product or the composition written explicitly as \\(f(x)g(x)\\) of \\(f(g(x))\\). Instead, you are given something like \\(e^x \\ln(x)\\). The first step in differentiating the product or composition is to identify what are \\(f()\\) and \\(g()\\) individually.\nIn general, \\(f()\\) and \\(g()\\) might be complicated functions, themselves involving linear combinations, products, and composition. But to get started, we will practice with cases where they are simple, pattern-book functions.\nStep 2: Find f’() and g’()\nFor differentiating either products or compositions, you will need to identify both \\(f()\\) and \\(g()\\) (the first step) and then compute the derivatives \\(\\partial_x f()\\) and \\(\\partial_x g()\\). That is, you will write down four functions.\nStep 3: Apply the relevant rule\nRecall from ?sec-fun-assembling that will will be working with three important forms for creating new functions out of existing functions:",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Derivatives of assembled functions</span>"
    ]
  },
  {
    "objectID": "Differentiation/23-rules.html#sec-using-the-rules",
    "href": "Differentiation/23-rules.html#sec-using-the-rules",
    "title": "23  Derivatives of assembled functions",
    "section": "",
    "text": "Linear combinations, e.g. \\(a f(x) + bg(x)\\)\nProducts of functions, e.g. \\(f(x) g(x)\\)\nCompositions of functions, e.g. \\(f\\left(g(x)\\right)\\)",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Derivatives of assembled functions</span>"
    ]
  },
  {
    "objectID": "Differentiation/23-rules.html#differentiating-linear-combinations",
    "href": "Differentiation/23-rules.html#differentiating-linear-combinations",
    "title": "23  Derivatives of assembled functions",
    "section": "23.2 Differentiating linear combinations",
    "text": "23.2 Differentiating linear combinations\nLinear combination is one of the ways in which we make new functions from existing functions. As you recall, linear combination involves scaling functions and then adding the scaled functions as in \\(a f(x) + b g(x)\\), alinear combination of \\(f(x)\\) and \\(g(x)\\). We can easily use \\(h\\) to show what is the result of differentiating a linear combination of functions. First, let’s figure out what is \\(\\partial_x\\, a f(x)\\), Going back to writing \\(\\partial_x\\) in terms of a slope function: \\[\\partial_x\\, a\\,f(x) = \\frac{a\\, f(x + h) - a\\,f(x)}{h}\\\\\n\\ \\\\\n= a \\frac{f(x+h) - f(x)}{h} = a\\, \\partial_x f(x)\\] In other words, if we know the derivative \\(\\partial_x\\, f(x)\\), we can easily find the derivative of \\(a\\, f()\\). Notice that even though \\(h\\) was used in the derivation, it appears nowhere in the result \\(\\partial_x\\, b\\,f(x) = b\\, \\partial_x\\, f(x)\\). The \\(h\\) is solvent to get the paint on the wall and evaporates once its job is done.\nNow consider the derivative of the sum of two functions, \\(f(x)\\) and \\(g(x)\\): \\[\\begin{eqnarray}\n\\partial_x\\, \\left[f(x) + g(x)\\right] & =\\frac{\\left[f(x + h) + g(x + h)\\right] - \\left[f(x) + g(x)\\right]}{h} \\\\\n\\ \\\\\n&= \\frac{\\left[f(x+h) -f(x)\\right] + \\left[g(x+h) - g(x)\\right]}{h}\\\\\n\\ \\\\\n&= \\frac{\\left[f(x+h) -f(x)\\right]}{h} + \\frac{\\left[g(x+h) - g(x)\\right]}{h}\\\\\n\\ \\\\\n&= \\partial_x\\, f(x) + \\partial_x\\, g(x)\n\\end{eqnarray}\\]\nBecause of how \\(\\partial_x\\) can be “passed through” a linear combination, mathematicians say that differentiation is a linear operator. Consider this new fact about differentiation as a down payment on what will eventually become a complete theory telling us how to differentiate a product of two functions or the composition of two functions. We will lay out the \\(h\\)-theory based algebra of this in the next two sections.\nWe can summarize the h-theory result for linear combinations this way:\n\nThe derivative of a linear combination is the linear combination of the derivatives.\n\nThat is:\n\\[\\partial_x \\left[\\strut \\color{magenta}{a} \\color{brown}{f(x)} + \\color{magenta}{b} \\color{brown}{g(x)}\\right] = \\color{magenta}{a} {\\large\\color{brown}{f'(x)}} + \\color{magenta}{b} {\\large\\color{brown}{g'(x)}}\\] as well as \\[\\partial_x \\left[\\strut \\color{magenta}{a}\\, \\color{brown}{f(x)} + \\color{magenta}{b}\\, \\color{brown}{g(x)}  + \\color{magenta}{c}\\, \\color{brown}{h(x)} + \\cdots\\right] = \\color{magenta}{a}\\, {\\large\\color{brown}{f'(x)}} + \\color{magenta}{b}\\, {\\large\\color{brown}{g'(x)}} + \\color{magenta}{c}\\, {\\large\\color{brown}{h'(x)}} + \\cdots\\]\n\nThe derivative of a polynomial is a polynomial of a lower order.\nConsider the polynomial \\[h(x) = \\color{magenta}{a}\\color{brown}{x^0}  + \\color{magenta}{b} \\color{brown}{x^1} + \\color{magenta}{c} \\color{brown}{x^2}\\] The derivative is \\[\\partial_x h(x) = \\color{brown}{0}\\, \\color{magenta}{a}  + \\color{brown}{1}\\, \\color{magenta}{b}  + \\color{magenta}{c}\\, \\color{brown}{2 x} = \\color{magenta}{b} +  \\color{magenta}{2 c}\\  x\\]",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Derivatives of assembled functions</span>"
    ]
  },
  {
    "objectID": "Differentiation/23-rules.html#product-rule-for-multiplied-functions",
    "href": "Differentiation/23-rules.html#product-rule-for-multiplied-functions",
    "title": "23  Derivatives of assembled functions",
    "section": "23.3 Product rule for multiplied functions",
    "text": "23.3 Product rule for multiplied functions\nThe question at hand is how to compute the derivative \\(\\partial_x f(x) g(x)\\). Of course, you can always use numerical differentiation. But let’s look at the problem from the point of view of symbolic differentiation. And since \\(f(x)\\) and \\(g(x)\\) are just pronoun functions, we will assume you are starting out already knowing the derivatives \\(\\partial_x f(x)\\) and \\(\\partial_x g(x)\\).\nThis situation arises particularly when \\(f(x)\\) and \\(g(x)\\) are pattern-book functions for which you already have memorized \\(\\partial_x f(x)\\) and \\(\\partial_x g(x)\\) or are basic modeling functions whose derivatives you will memorize in Section @ref(basic-derivs).\nThe purpose of this section is to derive the formula for \\(\\partial_x f(x) g(x)\\) in terms of \\(f(x)\\), \\(g(x)\\), \\(\\partial_x f(x)\\) and \\(\\partial_x g(x)\\). This formula is called the product rule. The point of showing a derivation of the product rule is to let you see how the logic of evanescent \\(h\\) plays a role. In practice, everyone simply memorizes the rule, which has a beautiful, symmetric form:\n\\[\\text{Product rule:}\\ \\ \\ \\ \\partial_x \\left[\\strut f(x)g(x)\\right] = \\left[\\strut \\partial_x f(x)\\right]\\, g(x) + f(x)\\, \\left[\\strut\\partial_x g(x)\\right]\\] and is even prettier in Lagrange notation (where \\(\\partial_x f(x)\\) is written \\(f'\\)): \\[ \\left[\\strut f g\\right]' = f' g + g' f\\]\nAs with all derivatives, the product rule is based on the instantaneous rate of change \\[F'(x) \\equiv \\lim_{h\\rightarrow 0} \\frac{F(x+h) - F(x)}{h}\\] introduced in ?sec-instantaneous-rate-of-change.\nWe also need two other statements about \\(h\\) and functions:\n\nThe derivative \\(F'(x)\\) is the slope of of \\(F()\\) at input \\(x\\). Taking a step of size \\(h\\) from \\(x\\) will induce a change of output of \\(h F'(x)\\), so \\[F(x+h) = f(x) + h F'(x)\\ .\\]\nAny result of the form \\(h F(x)\\), where \\(F(x)\\) is finite, gives 0. More precisely, \\(\\lim_{h\\rightarrow 0} h F(x) = 0\\)\n\nAs before, we will put the standard \\(\\lim_{h\\rightarrow 0}\\) disclaimer against dividing by \\(h\\) until there are no such divisions at all, at which point we can safely use the equality \\(h = 0\\).\nSuppose the function \\(F(x) \\equiv f(x) g(x)\\), a product of the two functions \\(f(x)\\) and \\(g(x)\\).\n\\[F'(x) = \\partial_x \\left[\\strut f(x) g(x) \\right] \\equiv \\lim_{h\\rightarrow 0}\\frac{f(x+h) g(x+h) - f(x) g(x)}{h}\\] We will replace \\(g(x_h)\\) with its equivalent \\(g(x) + h g'(x)\\) giving\n\\[= \\lim_{h\\rightarrow 0} \\frac{f(x+h) \\left[\\strut g(x) + h g'(x) \\right] - f(x) g(x)}{h} \\] \\(g(x)\\) appears in both terms in the numerator, once multiplied by \\(f(x+h)\\) and once by \\(f(x)\\). Collecting those terms give:\n\\[=\\lim_{h\\rightarrow 0}\\frac{\\left[\\strut f(x+ h) - f(x)\\right]  g(x) + \\left[\\strut f(x+h) h\\, g'(x)\\right]}{h}\\] This has two bracketed terms added together over a common denominator. Let’s split them into separate terms:\n\\[=\\lim_{h\\rightarrow 0}\\underbrace{\\left[\\strut \\frac{f(x+h) - f(x)}{h}\\right]}_{f'(x)} g(x) + \\lim_{h\\rightarrow 0}\\frac{\\left[\\strut f(x) + h f'(x)\\right]h\\,g'(x)}{h}\\]\nThe first term is \\(g(x)\\) multiplied by the familiar form for the derivative of \\(f(x)\\) \\[= f'(x) g(x) + \\lim_{h\\rightarrow 0}\\frac{f(x) h g'(x)}{h} + \\lim_{h\\rightarrow 0}\\frac{h f'(x) h g'(x)}{h}\\] In each of the last two terms there is an \\(h/h\\) involved. This is safely set to 1, since the \\(\\lim_{h\\rightarrow 0}\\) implies that \\(h\\) will not be exactly zero. There remain no divisions by \\(h\\) so we can drop the \\(\\lim_{h\\rightarrow 0}\\) in favor of \\(h=0\\): \\[= f'(x) g(x) + f(x) g'(x) + \\cancel{h f'(x) g'(x)}\\]\n\\[=f'(x) g(x) + g'(x) f(x)\\]\nThe last step relies on statement (2) above.\nSome people find it easier to read the rule in Lagrange shorthand, where \\(f\\) and \\(g\\) stand for \\(f(x)\\) and \\(g(x)\\) respectivly, and \\(f'\\) (“f-prime”) and \\(g'\\) (“g-prime”) stand for \\(\\partial f()\\) and \\(\\partial g()\\).\n\\[\\large\\text{Lagrange shorthand:}\\ \\   \\partial[\\color{magenta}f \\times \\color{brown}g] = [\\color{magenta}f \\times \\color{brown}g]' = \\color{magenta}{f'}\\color{brown}g + \\color{brown}{g'}\\color{magenta}f\\]\n\nThe expression \\(\\partial_x x^3\\) is the same as \\(\\partial_x \\left[\\strut x\\  x^2\\right]\\). Since we already know \\(\\partial_x x\\) (it is 1) and \\(\\partial_x x^2\\) (it is \\(2x\\)) let’s apply the product rule to find \\(\\partial_x x^3\\): \\[\\large\\partial [\\color{magenta}x \\times \\color{brown}{x^2}] = \\color{magenta}{[\\partial x]} \\times \\color{brown}{x^2} \\ +\\  \\color{brown}{[\\partial x^2]} \\times \\color{magenta}x =\\color{magenta}1\\times \\color{brown}{x^2} + \\color{brown}{2x} \\times \\color{magenta}x = 3 x^2\\]\n\n\nOccasionally, mathematics gives us a situation where being more general produces simplicity.\nIn the case of function products, the generalization is from products of two functions \\(f(x)\\cdot g(x)\\) to products of more than two functions, e.g. \\(u(x) \\cdot v(x) \\cdot w(x)\\).\nThe chain rule here takes a form that makes the overall structure much clearer:\n\\[\\begin{eqnarray}\n\\partial_x \\left[\\strut u(x) \\cdot v(x) \\cdot w(x)\\right] = \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\\\\n\\color{blue}{\\partial_x u(x)} \\cdot v(x) \\cdot w(x)\\ + \\\\\nu(x) \\cdot \\color{blue}{\\partial_x v(x)} \\cdot w(x)\\ + \\\\\nu(x) \\cdot v(x) \\cdot \\color{blue}{\\partial_x w(x)}\\ \\  \\ \\\n\\end{eqnarray}\\]\\end{eqnarray}\nIn the Lagrange shorthand, the pattern is even more evident: \\[\\left[ u\\cdot v\\cdot w\\right]' = \\color{blue}{u'}\\cdot v\\cdot w\\ +\\ u\\cdot \\color{blue}{v'}\\cdot w\\ +\\ u\\cdot v\\cdot \\color{blue}{w}'\\]",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Derivatives of assembled functions</span>"
    ]
  },
  {
    "objectID": "Differentiation/23-rules.html#chain-rule-for-function-composition",
    "href": "Differentiation/23-rules.html#chain-rule-for-function-composition",
    "title": "23  Derivatives of assembled functions",
    "section": "23.4 Chain rule for function composition",
    "text": "23.4 Chain rule for function composition\nA function composition, as described in Section 9.2, involves inserting the output of one function (the “interior function”) as the input of the other function (the “exterior function”). As we so often do, we will be using pronouns a lot. A list might help keep things straight:\n\nThere are two functions involved in a composition. Generically, we call them \\(f(y)\\) and \\(g(x)\\). In the composition \\(f(g(x))\\), the exterior function is \\(f()\\) and the interior function is \\(g()\\).\nEach of the two functions \\(f()\\) and \\(g()\\) has an input. In our examples, we use \\(y\\) to stand for the input to the exterior function and \\(x\\) for the input to the interior function.\nAs with all rules for differentiation, we will need to compute the derivatives of the functions involved, each with respect to its own input. So these will be \\(\\partial_y f(y)\\) and \\(\\partial_x g(x)\\).\n\nA reason to use different pronouns for the inputs to \\(f()\\) and \\(g()\\) is to remind us that the output \\(g(x)\\) is in general not the same kind of quantity as the input \\(x\\). In a function composition, the \\(f()\\) function will take the output \\(g(x)\\) as input. But since \\(g(x)\\) is not necessarily the same kind of thing as \\(x\\), why would we want to use the same name for the input to \\(f()\\) as we use for the input to \\(g()\\).\nWith this distinction between the names of the inputs, we can be even more explicit about the composition, writing \\(f(y=g(x))\\) instead of \\(f(g(x))\\). Had we used the pronound \\(x\\) for the input to \\(f()\\) but our explicit statement, although technically correct, would be confusing: \\(f(x = g(x))\\)!\nWith all these pronouns in mind, here is the chain rule for the derivative \\(\\partial_x f(g(x))\\):\n\\[\\large\\partial_x \\left[\\strut \\color{magenta}{f\\left(\\strut\\right.}\\strut \\color{brown}{g(x)}\\color{magenta}{\\left.\\right)}\\right] = [\\color{magenta}{\\partial_y f}](\\color{brown}{g(x)}) \\times [\\color{brown}{\\partial_xg(x)}]\\] Or, using the Lagrange prime notation, where \\('\\) stands for the derivative of a function with respect to its input, we have \\[\\large\\text{Lagrange shorthand:}\\ \\   [\\color{magenta}f(\\color{brown}g)]' = \\color{magenta}{f'} (\\color{brown}g) \\times \\color{brown}{g}'\\]",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Derivatives of assembled functions</span>"
    ]
  },
  {
    "objectID": "Differentiation/23-rules.html#rates-per-time",
    "href": "Differentiation/23-rules.html#rates-per-time",
    "title": "23  Derivatives of assembled functions",
    "section": "23.5 Rates per time",
    "text": "23.5 Rates per time\nIn news and policy discussions, you will often hear about “inflation rate” or “birth rate” or “interest rate” or “investment rate of return.” In each case, there is a function of time combined with a derivative of that function: with the general form \\[\\frac{\\partial_t f(t)}{f(t)}\\ .\\]\n\nInflation rate: The function is cost_of_living(\\(t\\)). The derivative is the rate of change with respect to time in the cost of living: \\(\\partial_t\\,\\)cost_of_living(\\(t\\)).\nBirth rate: The function is population(\\(t\\)). The derivative is \\(\\partial_t\\,\\)population(\\(t\\)), or at least that component of the overall \\(\\partial_t\\,\\)population(\\(t\\)) that is related to births. (Other components are deaths and the balance of in-migration and out-migration.)\nInterest rate: The function is account_balance(\\(t\\)) and the derivative is \\(\\partial_t\\,\\)account_balance(\\(t\\)).\nInvestment returns: The function is net_worth(\\(t\\)) and the derivative is \\(\\partial_t\\,\\)net_worth(\\(t\\)).\n\nIn all these cases, The “rate” is not merely “per time” as would be the case for \\(\\partial_t f(t)\\). Instead the rate is “per unit of the whole per time.” For birth rate, the “whole” is the population. The birth rate is the number of births in a year divided by the population itself. Birth rates are often stated with the phrase is “per capita per year.”“Per capita” is Latin. It translates to “by head.” Its modern sense is “per unit of population.” Of course, the “unit of population” is a person.\nNotice the two uses of “per” in the phrase: “births per capita per year.” A proportional rate is two rates in one. Births per capita is a proportion of the population. Births per year is an average rate with respect to time. But “births per capita per year” is a rate in the proportion with respect to time.\nThe rate word “per” also appears as part of “percent,” which literally means “per hundred.” A “percentage change” is the amount of change divided by the base amount. Confusingly, perhaps, “percentage change” is often truncated to the shorter “percent.” This is the case with inflation rates, interest rates, and rates of return on investment. The interest rate on a credit-card debt is stated as a proportion of the current debt; all that is packed into the word “percent.” The interest rate itself is the “proportion of the current debt per year”: two rates in one.\nSimilarly for an inflation rate. “Inflation” is stated as the change in prices divided by the current price: a proportional change. “Inflation rate” is the proportional change per unit of time, where the “whole” is current prices and the rate is change in current prices per year divided by current prices.\nThanks to the chain rule, there is a shortcut way of writing proportional rates per time. Exactly equivalent to the ratio \\(\\frac{\\partial_t f(t)}{f(t)}\\) is \\[\\partial_t \\ln(f(t))\\ .\\]\nDerivatives of logarithms appear often in fields such as economics or finance, where it is common to consider the logarithm of the economic quantity to render changes as percent of the whole.\n\n\n\n\n\n\nMath in the World: Linear or logarithmic axes?\n\n\n\nIt always pays to look carefully at the axes in a graph. For instance, consider Figure 23.1 which shows the cumulative number of COVID during a period in 2020, early in the pandemic.\n\n\n\n\n\n\n\n\nFigure 23.1: Growth in the number of Coronavirus cases in Italy and the US early in the pandemic. Source\n\n\n\n\n\nThe two panels in Figure 23.1 show the same data about growing numbers of coronavirus cases, the left graph on linear axes, the right on the now-familiar semi-log axes.\nMost people are excellent at comparing slopes, even if they find it difficult or tedious to quantify a slope with a number and units. For instance, a glance suffices to show that in the left graph, well through mid-March the red curve (Italy) is steeper on any given date than the blue curve (US). Correspondingly, the number of people with coronavirus was growing faster (per day) in Italy.\nThe right graph tells a different story: up until about March 1, the Italian cases were increasing faster than the US cases. Afterwards, the US sees a larger growth rate than Italy until, around March 19, the US growth rate is substantially larger than the Italy growth rate.\nThe previous two paragraphs and their corresponding graphs seem to contradict one another. But they are both accurate, truthful depictions of the same events. What’s different between the two graphs is that the left shows one kind of rate and the right shows another kind of rate. In the left, the slope is new-cases-per-day, the output of the derivative function\nleft graph: \\(\\ \\ \\ \\  \\partial\\_t\\, \\text{daily\\_new\\_cases}(t)\\).\nOn the right, the slope is the proportional increase in cases per day, that is,\nright graph: \\(\\ \\ \\ \\ \\frac{\\partial_t\\, \\text{daily\\_new\\_cases}(t)}{\\text{daily\\_new\\_cases}(t)}\\).\nFrom the chain rule, we know that\n\\[\\partial_t \\left[\\strut\\ln(f(t))\\right] = \\frac{\\partial_t f(t)}{f(t)}\\ .\\]\nSince the right graph is on semi-log axes, the slope we perceive visually is \\(\\partial_t \\left[\\strut\\ln(f(t))\\right]\\). That is an obscure-looking bunch of notation until the chain rule reveals it to be the rate of change at time \\(t\\) divided by the value at time \\(t\\).\n\n\nThe derivation of the chain rule relies on two closely related statements which are expressions of the idea that near any value \\(x\\) a function can be expressed as a linear approximation with the slope equal to the derivative of the function :\n\n\\(g(x + h) = g(x) + h g'(x)\\)\n\\(f(y + \\epsilon) = f(y) + \\epsilon f'(y)\\), which is the same thing as (1) but uses \\(y\\) as the argument name and \\(\\epsilon\\) to stand for the small quantity we usually write with an \\(h\\).\n\nWe will now look at \\(\\partial_x f\\left({\\large\\strut} g(x)\\right)\\) by writing down the fundamental definition of the derivative. This, of course, involves the disclaimer \\(\\lim_{h\\rightarrow 0}\\) until we are sure that there is no division by \\(h\\) involved.\n\\[\\partial_x \\left[{\\large\\strut} f\\left(\\strut g(x)\\right)\\right]  \\equiv \\lim_{h\\rightarrow 0}\\frac{\\color{magenta}{f(g(x+h))} - f(g(x))}{h}\\]\nLet’s examine closely the expression \\(\\color{magenta}{f\\left(\\strut g(x+h)\\right)}\\). Applying rule (1) above turns it into \\[\\lim_{h\\rightarrow 0} f\\left(\\strut g(x) + \\color{blue}{h g'(x)}\\right)\\] Now apply rule (2) but substituting in \\(g(x)\\) for \\(y\\) and \\(\\color{blue}{h g'(x)}\\) for \\(\\epsilon\\), giving\n\\[\\lim_{h\\rightarrow 0} \\color{magenta}{f\\left(\\strut g(x+h)\\right)} = \\lim_{h\\rightarrow 0} \\color{brown}{\\left[{\\large\\strut} f\\left(g(x)\\right) + \\color{blue}{h g'(x)}f'\\left(g(x)\\right)\\right]}\\] We will substitute the \\(\\color{blue}{blue}\\) and \\(\\color{brown}{brown}\\) expression for the \\(\\color{magenta}{magenta}\\) expression in \\[\\partial_x f\\left(\\strut g(x)\\right)  \\equiv \\lim_{h\\rightarrow 0}\\frac{\\color{magenta}{f(g(x+h))} - f(g(x))}{h}\\] giving \\[\\partial_x f\\left(\\strut g(x)\\right)  \\equiv \\lim_{h\\rightarrow 0}\\frac{\\color{brown}{f\\left(g(x)\\right) + \\color{blue}{h g'(x)}f'\\left(g(x)\\right)} - f\\left(g(x)\\right)}{h}\\] In the denominator, \\(f\\left(g(x)\\right)\\) appears twice and cancels itself out. That leaves a single term with an \\(h\\) in the numerator and an \\(h\\) in the denominator. Those \\(h\\)’s cancel out, at the same time obviating the need for \\(\\lim_{h\\rightarrow 0}\\) and leaving us with the chain rule: \\[\\partial_x f\\left(\\strut g(x)\\right)  \\equiv \\lim_{h\\rightarrow 0}\\frac{\\color{brown}{ \\color{blue}{h g'(x)} f'\\left(g(x)\\right)}}{h} = f'\\left(g(x)\\right)\\ g'(x)\\]\n\nUse the chain rule to find the derivative \\(\\partial_x e^{2x}\\).\nRecognize that \\(g(x) \\equiv 2x\\) is the interior function in \\(e^{2x}\\) and \\(f(x) \\equiv \\exp(x)\\) is the exterior function. Thus \\[\\partial_x e^{2x} = f'(g(x)) g'(x) = \\exp(g(x)) 2 = 2 e^{2x}\\ .\\] Happily, this is the same result as we got from using the product rule to find \\(\\partial_x e^{2x}\\).\nRecognizing \\(e^{2x}\\) as \\(e^x \\times e^x\\), we can apply the product rule.\n\n\nThe chain rule can be used in a clever way to find a formula for \\(\\partial_x \\ln(x)\\).\nWe’ve already seen that the logarithm is the inverse function to the exponential, and vice versa. That is: \\[e^{\\ln(y)} = y \\ \\ \\ \\text{and}\\ \\ \\ \\ln(e^y) = x\\] Since \\(\\ln(e^y)\\) is the same function as \\(y\\), the derivative \\(\\partial_y \\ln(e^y) = \\partial_y y = 1\\).\nLet’s differentiate the second form using the chain rule: \\[\\partial_y \\ln(e^y) = \\left[\\partial_y \\ln\\right](e^y)\\, e^x = 1\\] giving \\[\\left[\\partial_y \\ln\\right](e^y) = \\frac{1}{e^y} = \\recip(e^y)\\] Whatever the function \\(\\partial_x \\ln()\\) might be, it takes its input and produces as output the reciprocal of that input. In other words: \\[\\partial_x \\ln(x) = \\frac{1}{x}\\ .\\]\n\n\nKnowing that \\(\\partial_x \\ln(x) = 1/x\\) and the chain rule, we are in a position to demonstrate the power-law rule \\(\\partial_x x^p = p\\, x^{p-1}\\). The key is to use the identity \\(e^{\\ln(x)} = x\\).\n\\[\\partial_x x^p = \\partial_x \\left[e^{\\ln(x)}\\right]^p\\] The rules of exponents allow us to recognize \\[\\left[e^{\\ln(x)}\\right]^p = e^{p \\ln(x)}\\] Thus, \\(x^p\\) can be seen as a composition of the exponential function onto the logarithm function.\nApplying the chain rule to this composition gives \\[\\partial_x e^{p \\ln(x)} = e^{p\\ln(x)}\\partial_x [p \\ln(x)] =\ne^{p\\ln(x)} \\frac{p}{x}\\ .\\] Of course, we already know that \\(e^{p \\ln(x)} = x^p\\), so we have \\[\\partial_x x^p = x^p \\frac{p}{x} = p x^{p-1}\\ .\\]\n\n\n\\(\\large\\partial_x [\\color{brown}\\sin(\\color{magenta}{a x + b})] = [\\partial_x \\color{brown}{\\sin}](\\color{magenta}{a x + b}) \\times \\partial_x [\\color{magenta}{ax + b}] = \\color{brown}{\\cos}(\\color{magenta}{ax + b}) \\times \\color{magenta}a\\).",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Derivatives of assembled functions</span>"
    ]
  },
  {
    "objectID": "Differentiation/23-rules.html#sec-basic-derivs",
    "href": "Differentiation/23-rules.html#sec-basic-derivs",
    "title": "23  Derivatives of assembled functions",
    "section": "23.6 Derivatives of the basic modeling functions",
    "text": "23.6 Derivatives of the basic modeling functions\nThe basic modeling functions are the same as the pattern-book functions, but with bare \\(x\\) replaced by \\(\\line(x)\\). In other words, each of the basic modeling functions is a composition of the corresponding pattern-book function with \\(\\line(x)\\). Consequently, the derivatives of the basic modeling functions can be found using the chain rule.\nSuppose \\(f()\\) is one of our pattern-book functions. Then \\[\\large\\partial_x f(\\color{magenta}{ax + b}) = \\color{brown}{a} f'(\\color{magenta}{ax + b})\\] where \\(\\color{brown}{a}\\) is the derivative with respect to \\(x\\) of \\(\\color{magenta}{ax + b}\\).\nHere are the steps for differentiating a basic modeling function \\(\\color{brown}{f}(\\color{magenta}{a x + b})\\) where \\(f()\\) is one of the pattern-book functions:\n\nStep 1: Identify the particular pattern-book function \\(\\color{brown}{f}()\\) and write down its derivative \\(\\color{brown}{f'}\\). For example, if \\(f()\\) is \\(\\sin()\\), then \\(f'()\\) is \\(\\cos()\\).\nStep 2: Find the derivative of the linear interior function. If the function is \\(\\color{magenta}{ax + b}\\), then the derivative is \\(\\color{magenta}{a}\\). If the interior function is \\(\\frac{2\\pi}{P}(t-t_0)\\), the derivative is \\(\\frac{2 \\pi}{P}\\).\nStep 3: Write down the original function \\(\\large\\color{brown}{f}(\\color{magenta}{a x + b})\\) but replace \\(\\large\\color{brown}{f}\\) with \\(\\large \\color{brown}{f'}\\) and pre-multiply by the derivative of the interior function. For instance, \\[\\partial_x f(\\color{magenta}{ax + b}) = {\\large \\color{magenta}{a}}{\\large f'}(\\color{magenta}{ax + b})\\] Another example: \\[\\partial_t \\color{brown}{\\sin}\\left(\\color{magenta}{\\frac{2 \\pi}{P}(t-t_0)} \\right) = {\\large \\color{magenta}{\\frac{2 \\pi}{P}}}\\color{brown}{\\large\\cos}\\left(\\color{magenta}{\\frac{2 \\pi}{P}(t-t_0) }\\right) \\]\n\nBy convention, there are different ways of writing \\(\\line(x)\\) for the different pattern-book functions, for instance:\n\n\n\n\n\n\n\nPattern-book function \\(\\longrightarrow\\)\nBasic modeling\n\n\n\n\n\\(\\sin(x)\\ \\ \\ \\longrightarrow\\)\n\\(\\sin\\left(\\strut2 \\pi \\left[x-x_0\\right]/P\\right)\\)\n\n\n\\(\\exp(x)\\ \\ \\ \\longrightarrow\\)\n\\(\\exp(k x)\\)\n\n\n\\(x^2 \\ \\ \\ \\longrightarrow\\)\n\\(\\left[mx + b\\right]^2\\)\n\n\n\\(1/x \\ \\ \\ \\longrightarrow\\)\n\\(1/\\left[mx + b\\right]\\)\n\n\n\\(\\ln(x) \\ \\ \\ \\longrightarrow\\)\n\\(\\ln(a x + b)\\)\n\n\n\nThe rule for the derivative of any basic modeling function \\(f(\\line(x))\\) is \\[\\partial_x f(\\line(x)) = \\partial_x \\line(x) \\times \\partial_x f\\left(\\strut\\line(x)\\right)\\]\nTo illustrate:\n\n\\(\\partial_x e^{\\color{magenta}{kx}} = {\\large\\color{magenta}{k}}\\, e^{\\color{magenta}{kx}}\\) where \\(\\line(x) = kx\\).\n\\(\\partial_x \\sin(2\\pi (x-x_0)/P) = \\frac{2\\pi}{P}  \\sin(2\\pi (x-x_0)/P)\\) where \\(\\line(x) = 2\\pi (x-x_0)/P)\\).\n\\(\\partial_x (mx + b)^2 = m\\, 2 (m x + b) = 2 m^2 x + m^2 b\\) where \\(\\line(x) = mx + b\\).\n\\(\\partial_x \\text{reciprocal}(mx + b) = \\partial_x \\frac{1}{mx + b} = - \\frac{m}{(mx + b)^2}\\) where \\(\\line(x) = mx + b\\) and we use the fact that \\(\\partial_x \\text{reciprocal}(x) = - 1/x^2\\)\n\\(\\partial_x \\ln(a x + b) = a/(ax+b)\\)\n\\(\\partial_x \\pnorm(x, \\text{mean}, \\text{sd}) = dnorm(x, \\text{mean}, \\text{sd})\\).\n\\(\\partial_x \\dnorm(x, \\text{mean}, \\text{sd}) =  - \\frac{x-m}{\\text{sd}^2} \\dnorm(x, \\text{mean}, \\text{sd})\\)\n\nYou will be using the derivatives of the basic modeling functions so often, that you should practice and practice until you can write the derivative at a glance.\n\nThere are many possible implementations of the general concept of bump functions and sigmoid functions. This book uses \\(\\dnorm()\\) for the bump and \\(\\pnorm()\\) for the sigmoid.\nThe names \\(\\dnorm\\) and \\(\\pnorm\\) are worth remarking on. As we’ve said before, \\(\\dnorm()\\) is called the gaussian function in many fields of science and engineering. It is also a centrally important function in statistics, where it is called the normal function. (that is how important it is: it is just “normal.”) You may also have heard the normal function described as a “bell-shaped curve.”\nIn statistical nomenclature, \\(\\dnorm()\\) is called the “normal probability density function (PDF)” and \\(\\pnorm()\\) is called the “normal cumulative density function (CDF).” that is way too wordy for our purposes. For brevity, we have adopted the R name for those functions: dnorm() and pnorm().\nOwing to the origin of the names \\(\\dnorm\\) and \\(\\pnorm\\), we are writing the parameters of the functions—mean and sd—using the computer language notation. The pattern-book functions are just \\(\\dnorm(x)\\) and \\(\\pnorm(x)\\), without listing the parameters. But the basic modeling functions, with parameters, are written \\(\\dnorm(x, \\text{mean}, \\text{sd})\\) and \\(\\dnorm(x, \\text{mean}, \\text{sd})\\). This violates the convention that the basic modeling functions are the composition of the pattern-book functions with \\(\\line(x)\\). But \\(\\dnorm()\\) does not work this way because, by convention, the amplitude of the peak of \\(\\dnorm()\\) changes with the input parameter sd. That is not true for any other basic modeling function.\n\n\nComposition or product?\nThere is one family of functions for which function composition accomplishes same thing as multiplying functions: the power-law family.\nConsider, for instance, the function \\(h(x) \\equiv \\left[3x\\right]^4\\). Let’s let \\(g(x) \\equiv 3x\\) and \\(f(y) \\equiv y^4\\). With these definitions, \\(h(x) = f(g(x))\\).\nRecognizing that \\(\\partial_y f(y) = 4 y^3\\) and \\(\\partial_x g(x) = 3\\), the chain rule gives \\[\\partial_x h(x) =\n\\underbrace{4 g(x)^3}_{f'(g(x))} \\times \\underbrace{3}_{g'(x)} = \\underbrace{4 (3 x)^3}_{f'(g(x))} \\times 3 = 4\\cdot 3^4 \\times x^3 = 324\\ x^3\\] Another way to look at the same function is \\(g(x)\\) multiplied by itself 3 times: \\[h(x) = g(x)\\cdot g(x) \\cdot g(x) \\cdot g(x)\\] This is a product of 4 terms. Applying the product rule gives \\[\\begin{eqnarray}\n\\partial_x h(x) &=& \\color{blue}{g'(x)}\\cdot g(x)\\cdot g(x) \\cdot g(x) +\\\\\n&\\ & g(x)\\cdot \\color{blue}{g(x)}'\\cdot g(x) \\cdot g(x) +\\\\\n&\\ & g(x)\\cdot g(x)\\cdot \\color{blue}{g(x)'} \\cdot g(x) +\\\\\n&\\ & g(x)\\cdot g(x)\\cdot g(x) \\cdot \\color{blue}{g'(x)}\n\\end{eqnarray}\\] Since multiplication is commutative, all four terms are the same, each being \\(3^4 x^3\\). The sum of all four is therefore \\(4 \\times 3^4 x^3 = 324 x^3\\).\nThese are two long-winded ways of getting to the result. For most people, differentiating power-law functions algebraically is simplified by using the rules of exponentiation rather than the product or chain rule. Here, \\[h(x) \\equiv \\left[3x\\right]^4 = 3^4 x^4\\]so \\(\\partial_x h(x)\\) is easily handled as a scalar (\\(3^4\\)) times a function \\(x^4\\). Consequently, applying the rule for differentiating power laws, \\[\\partial_x h(x) = 3^4 \\times \\partial_x x^4 = 3^4 \\times 4 x^3 = 324 x^3\\] As another example, take \\(h(x) \\equiv \\sqrt[4]{\\strut x^3}\\). This is, of course, the composition \\(f(g(x))\\) where \\(f(y) \\equiv y^{1/4}\\) and \\(g(x) \\equiv x^3\\). Applying the chain rule to find \\(\\partial_x h(x)\\) will work (of course!), but is more work than applying the rules of exponentiation followed by a simple power-law differentiation. \\[h(x) = \\sqrt[4]{\\strut x^3} = x^{3/4}\\ \\ \\text{so}\\ \\  \\partial_x h(x) = \\frac{3}{4} x^{(3/4 - 1)} = \\frac{3}{4} x^{-1/4}\\]",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Derivatives of assembled functions</span>"
    ]
  },
  {
    "objectID": "Differentiation/23-rules.html#exponentials-and-logarithms-optional",
    "href": "Differentiation/23-rules.html#exponentials-and-logarithms-optional",
    "title": "23  Derivatives of assembled functions",
    "section": "23.7 Exponentials and logarithms (optional)",
    "text": "23.7 Exponentials and logarithms (optional)\nThe natural logarithm function, \\(\\ln(x)\\), is one of our basic modeling functions. As you know, there are other logarithmic functions. The one most often used is the logarithm-base-10, written \\(\\log_{10}(x)\\) or log10(x). Ten is an integer, and a nice number to use in arithmetic. So in practice, it is sensible to use \\(\\log_{10}()\\). (Indeed, \\(\\log_{10}()\\) is the digit() function, introduced in ?sec-magnitudes).\nThe “natural” in the “natural logarithm” means something different.\nThe base of the natural logarithm is the number called Euler’s constant and written \\(e\\). As a celebrity number, \\(e\\) is right up there with \\(\\pi\\) and \\(i\\). Just as \\(\\pi\\) has a decimal expansion that is infinitely long—the familiar \\(\\pi = 3.14159265358979...\\)—Euler’s constant has an infinitely long decimal representation: \\(e = 2.71828182845905...\\)\nIt is not obvious why \\(e = 2.71828182845905...\\) should be called “natural” by mathematicians. The reasons are:\n\n\\(\\ln(x)\\) is the inverse of \\(e^x\\), which is special for being invariant under differentiation: \\(\\partial_x e^x = e^x\\).\nThe derivative \\(\\partial_x \\ln(x)\\) which has a particularly simple form, namely, \\(1/x\\).\n\nLet’s look at the log-base-10 and its computer-savvy cousin log-base-2. The very definition of logarithms means that both 10 and 2 can be written \\[10 = e^{\\ln(10)}\\ \\ \\ \\text{and}\\ \\ \\ 2 = e^{\\ln(2)}\\] This implies that the base-10 and base-2 exponential functions can be written in terms of Euler’s constant \\(e\\):\n\\[10^x = \\left[\\strut e^{\\strut\\ln(10)}\\right]^x = e^{\\ln(10)x} \\ \\ \\ \\text{and}\\ \\ \\ 2^x = \\left[\\strut e^{\\strut\\ln(2)}\\right]^x = e^{\\ln(2) x}\\] Calculating \\(\\partial_x 10^x\\) or \\(\\partial_x 2^x\\) is a matter of applying the chain rule:\n\\[\\partial_x [10^x] = \\partial_x [e^{\\ln(10)x}] = e^{\\ln(10)x} \\times \\ln(10) \\ =\\  10^x \\times 2.3026\\] and \\[\\partial_x [2^x] = \\partial_x [e^{\\ln(2)x}] = e^{\\ln(2)x} \\times \\ln(2) \\ = \\ 2^x \\times 0.6931\\] Like \\(e^x\\), the derivatives of \\(10^x\\) and \\(2^x\\) are proportional to themselves. For \\(e^x\\) the constant of proportionality is 1, a very natural number indeed.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Derivatives of assembled functions</span>"
    ]
  },
  {
    "objectID": "Differentiation/24-optim.html",
    "href": "Differentiation/24-optim.html",
    "title": "24  Optimization",
    "section": "",
    "text": "24.1 Structure of the problem\nIn an optimization problem, there is one or more input quantities whose value you have to choose. The amount of salt; the years to wait from planting to harvesting a tree; the angle of the trail with respect to the slope. We will call this the decision quantity.\nSimilarly, there is one or more output quantity that you value and want to make as good as possible. The taste of the stew; the amount of usable wood harvested; the time it takes to walk up the hill. The output quantities are called the objectives.\nThis chapter deals with optimization problems that involve only a single objective. Problems with multiple objectives are among the most interesting and important in real-world decision making. Single-objective optimization techniques are a component of the more complex decision making, but they are a good place to get started.\nThe model that relates inputs to the objective output is the objective function. Solving an optimization problem—once the modeling phase is complete—amounts to finding a value for the decision quantity (the input to the objective function) that produces the best output from the objective function.\nSometimes the objective is something that you want to minimize, make as small as possible. For instance, in the hiking trail problem, we seek to minimize the time it takes to walk up the trail. Sometimes you want to maximize the objective, as in the wood-harvest problem where the objective is to harvest the most wood per year.\nRecall from Section 6.7 that there are two components to the task of maximization or minimization. The argmax is the input to the objective function which produces the largest output. The maximum is the value of that output.1 Argmin and minimum are the words used in a situation where you seek the smallest value of the objective function.\nOnce you have found the argmax you can plug that value into the objective function to find the output value. That value is the maximum.\nTo illustrate the setup of an optimization problem, imagine yourself in the situation of a contest to see who can shoot a tennis ball the farthest into a field with a slingshot. During the contest, you will adjust the vertical angle of launch, place the ball into the slingshot’s cradle, pull back as far as possible, and let go. To win the contest, you need to optimize how you launch the ball.\nThe objective is to maximize the distance traveled by the ball. The objective function models the distance travelled as a function of the quantities you can control, for instance the vertical angle of launch or the amount by which you pull back the slingshot. For simplicity, we will imagine that the slingshot is pulled back by a standard amount, producing a velocity of the ball at release of \\(v_0\\). You will win or lose based on the angle of launch you choose.\nBefore you head out into the field to experiment, let’s prepare by constructing the objective function. Using some principles of physics and mathematics (which you may not yet understand), we will model how far the ball will travel (horizontally) as a function of the angle of launch \\(\\theta\\) and the initial velocity \\(v_0\\).\nThe mathematics of such problems involves an area called differential equations, an important part of calculus which we will come to later in the course. Since you don’t have the tools yet, we will just state a simple model of how long the ball stays in the air. \\[\\text{duration}(v_0, \\theta) = 2 v_0 \\sin(\\theta)/g\\] \\(g\\) is the acceleration due to gravity, which is about \\(9.8 \\text{m}\\text{s}^{-2}\\), assuming that the contest is being held on Earth.\nThe horizontal distance travelled by the tennis ball will be \\[\\text{hdist}(v_0, \\theta) = \\cos(\\theta) v_0\\, \\text{duration}(v_0, \\theta) = 2 v_0^2 \\cos(\\theta)\\sin(\\theta) / g\\] Our objective function is hdist(), and we seek to find the argmax. The input \\(v_0\\) is (we have assumed) fixed, so the only decision quantity is the angle \\(\\theta\\).\nThe best choice of \\(\\theta\\) will make the quantity \\(\\cos(\\theta)\\sin(\\theta)\\) as large as possible. So in finding the argmax, we don’t need to be concerned with \\(v_0\\) or \\(g\\).\nFinding the argmax can be accomplished simply by plotting the function \\(\\cos(\\theta)\\sin(\\theta)\\). We will implement the function so that the input is in units of degrees.\nFigure 24.1: In the simple model of a tennis ball launched at an angle \\(\\theta\\) from the horizontal, the distance travelled is \\(2 v_0^2 / g\\) times \\(\\cos(\\theta)\\sin(\\theta)\\).\nYou can see that the maximum value is about 0.5 and that this occurs at an argmax \\(\\theta\\) that is a little bit less than 50\\(^\\circ\\).\nZooming in on the \\(\\theta\\) axis let’s you find the argmax with more precision:\nFigure 24.2: Zooming in on the argmax of the objective function. It is important to look at the scale of the vertical axis. Any value of \\(\\theta\\) between about 40 and 50 gives a close approximation to the maximum.\nFrom the graph, especially the zoomed-in version, you can read off the argmax as \\(\\theta = 45^\\circ\\).\nFinding the argmax solves the problem. You may also want to present your solution by reporting the value of hdist() when the argmax is given as input. You can read off the graph that the maximum of \\(\\cos(\\theta)\\sin(\\theta)\\) is 0.5 at \\(\\theta = 45^\\circ\\), so overall the distance will be \\(v_0^2 / g\\)",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "Differentiation/24-optim.html#structure-of-the-problem",
    "href": "Differentiation/24-optim.html#structure-of-the-problem",
    "title": "24  Optimization",
    "section": "",
    "text": "Mathematically, maximization and minimization are the same thing. Every minimization problem can be turned into a maximization problem by putting a negative sign in front of the objective function. To simplify the discussion, in talking about finding the solution to an optimization problem we will imagine that the goal is to maximize. But keep in mind that many circumstances in the real world, “best” can mean minimization.\n\n\n\nPeople often talk about “finding the maximum.” This is misleading. Instead, the idea is to find the input to the objective function—that is, the argmax—that produces the maximum output.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "Differentiation/24-optim.html#interpreting-the-argmax",
    "href": "Differentiation/24-optim.html#interpreting-the-argmax",
    "title": "24  Optimization",
    "section": "24.2 Interpreting the argmax",
    "text": "24.2 Interpreting the argmax\nThe graphical solution given to the slingshot problem is entirely satisfactory. Whether that solution will win the contest depends on whether the model we built for the objective function is correct. We have left out, for instance, air resistence, which is potentially important.\nSolving the optimization problem has prepared us to test the result in the field. Perhaps we will find that the real-world optimum angle is somewhat steeper or shallower than \\(\\theta = 45^\\circ\\).\nBesides the argmax, another important quantity to read from the graph in Figure 24.1 is the precision of the argmax. In strict mathematical terms, the argmax for the tennis-ball problem is exactly 45 degrees at which point \\(\\cos(\\theta)\\sin(\\theta) = 0.5\\). Suppose, however, that the ball were launched at only 40 degrees. Five degrees difference is apparent to the eye, but the result will be essentially the same as for 45 degrees: \\(\\cos(\\theta)\\sin(\\theta) = 0.492\\). The same is true for a launch angle of 50 degrees. For both “sub-optimal” launch angles, the output is within 2 percent of the 45-degree result. It is easy to imagine that a factor outside the scope of the simple model—the wind, for instance—could change the result by as much or more than 2 percent, so a practical report of the argmax should reasonable be “40 to 50 degrees” rather than “exactly 45 degrees.”\nContests are won or lost by margins of less than 1%, so you should not casually deviate from the argmax. On the other hand, \\(45^\\circ\\) is the argmax of the model. Reality may deviate from the model. For instance, suppose that air resistance or wind might might affect distance by 1%. That is. the real-world result might deviate by as much as 1% of the model value. If so, we shouldn’t expect the real-world argmax to be any closer to 45\\(^\\circ\\) than \\(\\pm 5^\\circ\\); anywhere in that domain interval generates an output that is within 1% of the maximum output for the model.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "Differentiation/24-optim.html#derivatives-and-optimization",
    "href": "Differentiation/24-optim.html#derivatives-and-optimization",
    "title": "24  Optimization",
    "section": "24.3 Derivatives and optimization",
    "text": "24.3 Derivatives and optimization\nWe are now going to reframe the search for the argmax and its interpretation in terms of derivatives of the objective function with respect to the decision quantity (\\(\\theta\\) in the slingshot problem). For a function with one input, this will not be an improvement from the look-at-the-graph technique to find the argmax. However, a genuine reason to use derivatives is to set us up in the future to solve problems with more than one input, where it is hard to draw or interpret a graph. Also, describing functions in the language of derivatives can help us think more clearly about aspects of the problem, such as the precision of the argmax.\nWith a graph such as Figure 24.1, it is easy to find the argmax; common sense carries the day. So it won’t be obvious at first why we will take the following approach:\nLet’s denote an argmax of the objective function \\(f(x)\\) by \\(x^\\star\\). Let’s look at the derivative \\(\\partial_x f(x)\\) in the neighborhood of \\(x^\\star\\). Referring to Figure 24.1, where \\(x^\\star = 45^\\circ\\), you may be able to see that \\(\\partial_x f(x^\\star)\\) is zero; the line tangent to the function’s graph at \\(x^\\star\\) is horizontal.\nSeen another way, the slope of \\(f(x)\\) to the left of \\(x^\\star\\) is positive. Moving a tiny bit to the right (that is, increasing \\(x\\) by a very small amount) increases the output \\(f(x)\\). On the other hand, just to the right of \\(x^\\star\\), the slope of \\(f(x)\\) is negative; as you reach the top of a hill and continue on, you will be going downhill. So the derivative function is positive on one side of \\(x^\\star\\) and negative on the other, suggesting that it crosses zero at the argmax.\nCommon sense is correct: Walk uphill to get to the peak, walk downhill to move away from the peak. At the top of a smooth hill, the terrain is level. (Since our modeling functions are smooth, so must be the hills that we visualize the functions with.)\nInputs \\(x^\\star\\) such that \\(\\partial_x f(x^\\star) = 0\\) are called critical points. Why not call them simply argmaxes? Because a the slope will also be zero at an argmin. And it is even possible to have the slope be zero at a point that is neither an argmin or an argmax.\nAt this point, we know that values \\(x^\\star\\) that give \\(\\partial_x f(x^\\star) = 0\\) are “critical points,” but we haven’t said how to figure out whether a given critical point is an argmax, an argmin, or neither. This is where the behavior of \\(\\partial_x f(x)\\) near \\(x=x^\\star\\) is important. If \\(x^\\star\\) is an argmax, then \\(\\partial_x f(x)\\) will be positive to the left of \\(x^\\star\\) and negative to the right of \\(x^\\star\\); walk up the hill to get to \\(x^\\star\\), at the top the hill is flat, and just past the top the hill has a negative slope.\nFor an argmin, changing \\(x\\) from less than \\(x^\\star\\) to greater than \\(x^\\star\\); you will be walking down into the valley, then level at the very bottom \\(x=x^\\star\\), then back up the other side of the valley after you pass \\(x=x^\\star\\). Figure 24.3 shows the situation.\n\n\n\n\n\n\n\n\nFigure 24.3: Top row: An objective function near an argmax (left) and an argmin (right). Bottom row: The derivative of the objective function. A horizontal line (orange) marks zero on the vertical axis.\n\n\n\n\n\nThe bottom row of graphs in Figure 24.3 shows the derivative of the objective function \\(f(x)\\), that is, \\(\\partial_x f(x)\\). You can see that for the argmax of \\(f(x)\\), the derivative \\(\\partial_x f(x)\\) is positive to the left and negative to the right. Similarly, near the argmin of \\(f(x)\\), the derivative \\(\\partial_x f(x)\\) is negative to the left and positive to the right.\nStated another way, the derivative \\(\\partial_x f(x)\\) has a negative slope just to the left of an argmin and a positive slope to the left of an argmax.\nThe second derivative of the objective function \\(f(x)\\) at a critical point \\(x^\\star\\) is what tells us whether the critical point is an argmax, an argmin, or neither.\n\n\n\n\n\n\n\n\n\n\nCritical point \\(x^\\star\\)\n\\(\\partial_x f(x^\\star)\\)\n\\(\\partial_{xx} f(x^\\star)\\)\n\n\n\n\nargmax\n0\nnegative\n\n\nargmin\n0\npositive\n\n\nneither\n0\n0\n\n\n\n\nThroughout Block 2, we have translated features of functions that are evident on a graph into the language of derivatives:\n\nThe slope of a function \\(f(x)\\) at any input \\(x\\) is the value of the derivative function \\(\\partial_x f(x)\\) at that same \\(x\\).\nThe concavity of a function \\(f(x)\\) at any input is the slope of the derivative function, that is, \\(\\partial_{xx} f(x)\\).\nPutting (i) and (ii) together, we get that the concavity of a function \\(f(x)\\) at any input \\(x\\) is the value of the second derivative function, that is, \\(\\partial_{xx} f(x)\\).\nAt an argmax \\(x^\\star\\) of \\(f(x)\\), the value of the derivative function \\(\\partial_x f(x^\\star)\\) is zero and the value of the second derivative function \\(\\partial_{xx} f(x^\\star)\\) is negative. The situation at an argmin is along the same lines, the derivative of the objective function is zero and the second derivative is positive.\n\n\n\nWhat’s the critical point?\nYou’re familiar with the quadratic polynomial: \\[g(x) = a_0 + a_1 x + a_2 x^2\\] The graph of a quadratic polynomial is a parabola, which might be concave up or concave down. A parabola has only one critical point, which might be an argmin or an argmax.\nLet’s find the critical point. We know that the critical point is \\(x^\\star\\) such that \\(\\partial_x g(x^\\star) = 0\\). Since we know how to differentiate a power law, we can see that \\[\\partial_x g(x) = a_1 + 2 a_2 x\\] and, more specifically, at the critical point \\(x^\\star\\) the derivative will be \\[a_1 + 2 a_2 x^\\star = 0\\] The above is an equation, not a definition. It says that whatever \\(x^\\star\\) happens to be, the quantity \\(a_1 + 2 a_2 x^\\star\\) must be zero. Using plain old algebra, we can find the location of the critical point \\[x^\\star = -\\frac{a_1}{2 a_2}\\]\n\n\nApplication area 24.1 —In economics, demand for a good is a function of the good’s price (and other things). This function is called the “demand curve.”\n\n\n\n\n\n\n\nApplication area 24.1 The demand “curve”\n\n\n\nIn economics, a monopoly or similar arrangement can set the price for a good or commodity. Monopolists can set the price at a level that generates the most income for themselves.\n\n\n\n\n\n\n\n\nFigure 24.4: Demand as a function of price, as first published by Antoine-Augustin Cournot in 1836. Source)\n\n\n\n\n\nIn 1836, early economist Antoine-Augustin Cournot published a theory of revenue versus demand based on his conception that demand will be a monotonically decreasing function of price. (That is, higher price means lower demand.) we will write as \\(\\text{Demand}(p)\\) demand as a function of price.\nThe revenue generated at price \\(p\\) is \\(R(p) \\equiv p \\text{Demand}(p)\\): price times demand.\nTo find the revenue-maximizing demand, differentiate \\(R(p)\\) with respect to \\(p\\) and find the argmax \\(p^\\star\\) at with \\(\\partial_p R(p^\\star) = 0).\\) This can be done with the product rule.\n\\[\\partial_p R(p) = p \\ \\partial_p \\text{Demand}(p) + \\text{Demand}(p)\\] At the argmax \\(p^\\star\\) we have: \\[p^\\star \\partial_p \\text{Demand}(p^\\star) + \\text{Demand}(p^\\star) = 0 \\ \\ \\stackrel{\\text{solving for}\\ p^\\star}{\\Longrightarrow} \\ \\ p^\\star = - \\frac{\\text{Demand}(p^\\star)}{\\partial_p \\text{Demand}(p^\\star)}\\]\nIf the monopolist knows the demand function \\(D(p)\\), finding the revenue maximizing price is a simple matter. But in general, the monopolist does not know the demand function in advance. Instead, an informed guess is made to set the initial price \\(p_0\\). Measuring sales \\(D(p_0)\\) gives one point on the demand curve. Then, try another price \\(p_1\\). This gives another point on the demand curve as well as an estimate \\[\\partial_p D(p_0) = \\frac{D(p_1) - D(p_0)}{p_1 - p_0}\\] Now the monopolist is set to model the demand curve as a straight-line function and easily to find \\(p^\\star\\) for the model. For instance, if the demand function is modeled as \\(D_1 (p) = a + b p\\), the optimal price will be \\(p^\\star_1 = - \\frac{a + b p^\\star}{b}\\) which can be solved as \\(p^\\star_1 = - a/2b\\).\n\\(p^\\star_1\\) is just an estimate of the optimum price. Still, the monopolist can try out that price, giving a third data point for the demand function. The new data can lead to a better model of the demand function. With the better estimate, find a new a argmax \\(p^\\star_2\\). This sort of iterative process for finding an argmax of a real-world function is very common in practice.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "Differentiation/24-optim.html#sec-flat-on-top",
    "href": "Differentiation/24-optim.html#sec-flat-on-top",
    "title": "24  Optimization",
    "section": "24.4 Be practical!",
    "text": "24.4 Be practical!\nDecision making is about choosing among alternatives. In some engineering or policy contexts, this can mean finding a value for an input that will produce the “best” outcome. For those who have studied calculus, it is natural to believe that calculus-based techniques for optimization are the route to making the decision.\nWe emphasize that the optimization techniques covered in this chapter are only part of a broader set of techniques for real-world decision-making problems. In particular, most policy contexts involve multiple objectives. For example, in designing a car one goal is to make it cheap to manufacture, another to make it attractive, and still another to make it safe. These different objectives are often at odds with one another. In Block 4 of this text, we will discuss some calculus techniques that help policy-makers in multi-objective settings.\nFor now, sticking with the idealized (and often unrealistic) setting of maximizing a single objective, with one or more inputs. Recall the setting for calculus-type maximization. You have a function with one or more inputs, say, \\(f(x)\\) or \\(g(x,y)\\) or, often, \\(h(x, y, z, \\ldots)\\) where \\(\\ldots\\) might be standing for tens or hundreds or thousands of inputs or more.\nIf you can graph the function (feasible for one- or two-input functions), you can often easily scan the graph by eye to find the peak. The basis of the calculus techniques is the observation that, at the argmax of a smooth function, the derivative of the function is 0.\nFor example, consider a style problem that often appears in calculus textbooks. Suppose you have been tasked to design a container for a large volume V of liquid. The design specifications call for the weight of the container to be as little as possible. (This is a minimization problem, then.) In classical textbook fashion, the specifications might also stipulate that the container is to be a cylinder made out of a particular metal of a particular thickness.\nThe above is a lovely geometry/calculus problem. Whether it is relevant to any genuine, real-world problem is another question.\n\n\n\n\n\n\n\n\n\nUsing the notation in the diagram, the volume and surface area of the cylinder is \\[V(r, h) \\equiv \\pi r^2 h \\ \\ \\ \\text{and}\\ \\ \\ A(r, h) \\equiv 2 \\pi r^2 + 2 \\pi r h\\]\nMinimizing the weight of the cylinder is our objective (according to the problem statement) and the weight is proportional to the surface area. Since the volume \\(V\\) is given (according to the problem statement), we want to re-write the area function to use volume:\n\\[h(r, V) \\equiv V / \\pi r^2 \\ \\ \\ \\implies\\ \\ \\ A(r, V) = 2 \\pi r^2 + 2 \\pi r V/\\pi r^2 = 2 \\pi r^2 + 2 V / r\\] Suppose \\(V\\) is specified as 1000 liters. A good first step is to choose appropriate units for \\(r\\) to make sure the formula for \\(A(r, V)\\) is dimensionally consistent. Suppose we choose \\(r\\) in cm. Then we want \\(V\\) in cubic centimeters (cc). 1000 liters is 1,000,000 cc. Now we can plot a slice of the area function:\n\nA &lt;- makeFun(2*pi*r^2 + 2*V/r ~ r, V=1000000)\nslice_plot(A(r) ~ r, bounds(r=c(10, 100))) %&gt;%\n  gf_labs(x = \"radius (cm)\", y = \"Surface area of container (square cm)\")\n\n\n\n\n\n\n\n\nAs always, the function’s derivative is zero at the optimal \\(r\\). In the graph, the argmin is near \\(r=50\\) cm at which point the minimum is about 50,000 cm\\(^2\\). Since \\(h(r,V) = V/\\pi r^2\\), the required height of cylinder will be near \\(10^6 / \\pi 50^2 = 127\\)cm.\nIn calculus courses, the goal is often to find a formula for the optimal radius as a function of \\(V\\). So we differentiate the objective function—that is, the area function for any \\(V\\) and \\(r\\) with respect to \\(r\\), \\[\\partial_r A(r, V) = 4 \\pi r - 2 V / r^2\\] Setting this to zero (which will be true at the optimal \\(r^\\star\\)) we can solve for \\(r^\\star\\) in terms of \\(V\\): \\[4 \\pi r^\\star - 2 \\frac{V}{\\left[r^\\star\\right]^2} = 0 \\ \\ \\ \\Longrightarrow\\ \\ \\ 4\\pi r^\\star = 2\\frac{V}{\\left[r^\\star\\right]^2} \\Longrightarrow\\ \\ \\ \\left[r^\\star\\right]^3 = \\frac{1}{2\\pi} V \\ \\ \\ \\Longrightarrow\\ \\ \\  r^\\star = \\sqrt[3]{V/2\\pi}\\]\nFor \\(V = 1,000,000\\) cm\\(^3\\), this gives \\(r^\\star = 54.1926\\) cm which in turn implies that the corresponding height \\(h^\\star = V/\\pi (r^\\star)^2 = 108.3852\\) cm.\nWe’ve presented the optimum \\(r^\\star\\) and \\(h^\\star\\) to the nearest micron. (Does that make sense to you? Think about it for a moment before reading on.)\nIn modeling, a good rule of thumb is this: “If you don’t know what a sensible precision is for reporting your result, you don’t yet have a complete grasp of the problem.” Here are two reasonable ways to sort out a suitable precision.\n\nSolve a closely related problem that would have been equivalent for many practical purposes.\nCalculate how much the input can deviate from the argmax while producing a trivial change in the output of the objective function.\n\nApproach (2) is always at hand, since you already know the objective function. Let’s graph the objective function near \\(r = 54.1926\\) …\n\n\n\n\n\n\n\n\n\nLook carefully at the axes scales. Deviating from the mathematical optimum by about 5cm (that is, 50,000 microns) produces a change in the output of the objective function by about 400 units out of 55,000. In other words, about 0.7%.\nIt is true that \\(r^\\star = 54.1926\\) cm gives the “best” outcome. And sometimes such precision is warranted. For example, improving the speed of an elite marathon racer by even 0.1% would give her a 7 second advantage: often the difference between silver and gold!\nWhat’s different is that you know exactly the ultimate objective of a marathon: finish faster. But you may not know the ultimate objective of the system your “optimal” tank will be a part of. For instance, your tank may be part of an external fuel pod on an aircraft. Certainly the aircraft designers want the tank to be as light as possible. But they also want to reduce drag as much as possible. A 54 cm diameter tube has about 17% more drag than a 50 cm tube. It is probably well worth increasing weight by 0.7% to save that much drag.\nIn reporting the results from an optimization problem, you ought to give the decision maker all relevant information. That might be as simple as including the above graph in your report.\nWe mentioned another technique for getting a handle on what precision is meaningful: (1) solve a closely related problem. It can requires some insight and creativity to frame the new problem. For instance, large capacity tanks often are shaped like a lozenge: a cylinder with hemi-spherical ends.\n\n\n\n\n\n\n\n\n\nUsing \\(h\\) for the length of the cylindrical portion of the tank, and \\(r\\) for the radius, the volume and surface area are: \\[V(r, h) = \\pi r^2 h + \\frac{4}{3} \\pi r^3 \\ \\ \\ \\text{and}\\ \\ \\ A(r,h) = 2 \\pi r h + 4 \\pi r^2\\] Again, \\(V\\) is specified as 1000 liters. As detailed in Exercise 24.18, the surface area of this 1000-liter tank is about 48,400 cm\\(^2\\). This is more than 10% less than for the cylindrical tank.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "Differentiation/24-optim.html#footnotes",
    "href": "Differentiation/24-optim.html#footnotes",
    "title": "24  Optimization",
    "section": "",
    "text": "Another word for an “input” is “argument.” Argmax is the contraction of argument producing the maximum output.↩︎",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "Differentiation/25-partial.html",
    "href": "Differentiation/25-partial.html",
    "title": "25  Partial change and the gradient vector",
    "section": "",
    "text": "25.1 Calculus on two inputs\nAlthough we use contour plots for good practical reasons, the graph of a function \\(g(x,y)\\) with two inputs is a surface, as described in Section @ref(surface-plot). The derivative of \\(g(x,y)\\) should encode the information needed to approximate the surface at any input \\((x,y)\\). In particular, we want the derivative of \\(g(x,y)\\) to tell us the orientation of the tangent plane to the surface.\nA tangent plane is infinite in extent. Let’s use the word facet to refer to a little patch of the tangent plane centered at the point of contact. Each facet is flat. (it is part of a plane!) Figure 25.2 shows some facets tangent to a familiar curved surface. No two of the facets are oriented the same way.\nFigure 25.2: A melon as a model of a curved surface such as the graph of a function of two inputs. Each tangent facet has its own orientation. (Disregard the slight curvature of the small pieces of paper. Summer humidity has interfered with my attempt to model a flat facet with a piece of Post-It paper!\nBetter than a picture of a summer melon, pick up a hardcover book and place it on a curved surface such as a basketball. The book cover is a flat surface: a facet. The orientation of the cover will match the orientation of the surface at the point of tangency. Change the orientation of the cover and you will find that the point of tangency will change correspondingly.\nIf melons and basketballs are not your style, you can play the same game on an interactive graph of a function with two inputs. The snapshot below is a link to an applet that shows the graph of a function as a blue surface. You can specify a point on the surface by setting the value of the (x, y) input using the sliders. Display the tangent plane (which will be green) at that point by check-marking the “Tangent plane” input. (Acknowledgments to Alfredo Sánchez Alberca who wrote the applet using the GeoGebra math visualization system.)\nFor the purposes of computation by eye, a contour graph of a surface can be easier to deal with. Figure 25.3 shows the contour graph of a smoothly varying function. Three points have been labeled A, B, and C.\nFigure 25.3: A function of 2 inputs with 3 specific inputs marked A, B, and C\nZooming in on each of the marked points presents a simpler picture for each of them, although one that is different for each point. Each zoomed-in plot contains almost parallel, almost evenly spaced contours. If the surface had been exactly planar over the entire zoomed-in domain, the contours would be exactly parallel and exactly evenly spaced. We can approach such exact parallelness by zooming in more closely around the labeled point.\nFigure 25.4: Zooming in on the neighborhoods of A, B, and C in Figure 25.3 shows a simple, almost planar, local landscape. The bottom row shows the contours of the tangent plane near each of the neighborhoos in the top row.\nJust as the function \\(\\line(x) \\equiv a x + b\\) describes a straight line, the function \\(\\text{plane}(x, y) \\equiv a + b x + c y\\) describes a plane whose orientation is specified by the value of the parameters \\(b\\) and \\(c\\). (Parameter \\(a\\) is about the vertical location of the plane, not it is orientation.)\nIn the bottom row of Figure 25.4, the facets tangent to the original surface at A, B, and C are displayed. Comparing the top and bottom rows of Figure 25.4) you can see that each facet has the same orientation as the surface; the contours face in the same way.\nRemember that the point of constructing such facets is to generalize the idea of a derivative from a function of one input \\(f(x)\\) to functions of two or more inputs such as \\(g(x,y)\\). Just as the derivative \\(\\partial_x f(x_0)\\) reflects the slope of the line tangent to the graph of \\(f(x)\\) at \\(x=x_0\\), our plan for the “derivative” of \\(g(x_0,y_0)\\) is to represent the orientation of the facet tangent to the graph of \\(g(x,y)\\) at \\((x=x_0, y=y_0)\\). The question for us now is what information is needed to specify an orientation.\nOne clue comes from the formula for a function whose graph is a plane oriented in a particular direction:\n\\[\\text{plane}(x,y) \\equiv a + b x + cy\\]\nAn instructive experience is to pick up a rigid, flat object, for instance a smartphone or hardcover book. Hold the object level with pinched fingers at the mid-point of each of the short ends, as shown in Figure 25.5 (left).\nYou can tip the object in one direction by raising or lowering one hand. (middle picture) And you can tip the object in the other coordinate direction by rotating the object around the line joining the points grasped by the left and right hands. (right picture) By combining these two motions, you can orient the surface of the object in a wide range of directions.1\nThe purpose of this lesson is to show that two-numbers are sufficient to dictate the orientation of a plane. In terms of Figure 25.5 these are 1) the amount that one hand is raised relative to the other and 2) the angle of rotation around the hand-to-hand axis.\nSimilarly, in the formula for a plane, the orientation is set by two numbers, \\(b\\) and \\(c\\) in \\(\\text{plane}(x, y) \\equiv a + b x + c y\\).\nHow do we find the right \\(b\\) and \\(c\\) for the tangent facet to a function \\(g(x,y)\\) at a specific input \\((x_0, y_0)\\)? Taking slices of \\(g(x,y)\\) provides the answer. In particular, these two slices: \\[\\text{slice}_1(x) \\equiv g(x, y_0) = a + b\\, x + c\\, y_0 \\\\ \\text{slice}_2(y) \\equiv g(x_0, y) = a + b x_0 + c\\, y\\]\nLook carefully at the formulas for the slices. In \\(\\text{slice}_1(x)\\), the value of \\(y\\) is being held constant at \\(y=y_0\\). Similarly, in \\(\\text{slice}_2(y)\\) the value of \\(x\\) is held constant at \\(x=x_0\\).\nThe parameters \\(b\\) and \\(c\\) can be read out from the derivatives of the respective slices: \\(b\\) is equal to the derivative of the slice\\(_1\\) function with respect to \\(x\\) evaluated at \\(x=x_0\\), while \\(c\\) is the derivative of the slice\\(_2\\) function with respect to \\(y\\) evaluated at \\(y=y_0\\). Or, in the more compact mathematical notation:\n\\[b = \\partial_x \\text{slice}_1(x)\\left.\\strut\\right|_{x=x_0} \\ \\ \\text{and}\\ \\ c=\\partial_y \\text{slice}_2(y)\\left.\\strut\\right|_{y=y_0}\\] These derivatives of slice functions are called partial derivatives. The word “partial” refers to examining just one input at a time. In the above formulas, the \\({\\large |}_{x=x_0}\\) means to evaluate the derivative at \\(x=x_0\\) and \\({\\large |}_{y=y_0}\\) means something similar.\nYou don’t need to create the slices explicitly to calculate the partial derivatives. Simply differentiate \\(g(x, y)\\) with respect to \\(x\\) to get parameter \\(b\\) and differentiate \\(g(x, y)\\) with respect to \\(y\\) to get parameter \\(c\\). To demonstrate, we will make use of the sum rule: \\[\\partial_x g(x, y) = \\underbrace{\\partial_x a}_{=0} + \\underbrace{\\partial_x b x}_{=b} + \\underbrace{\\partial_x cy}_{=0} = b\\] Similarly, \\[\\partial_y g(x, y) = \\underbrace{\\partial_y a}_{=0} + \\underbrace{\\partial_y b x}_{=0} + \\underbrace{\\partial_y cy}_{=c} = c\\]",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Partial change and the gradient vector</span>"
    ]
  },
  {
    "objectID": "Differentiation/25-partial.html#calculus-on-two-inputs",
    "href": "Differentiation/25-partial.html#calculus-on-two-inputs",
    "title": "25  Partial change and the gradient vector",
    "section": "",
    "text": "R/mosaic: Orientation of a plane\n\n\n\nTo explore the roles of the parameters \\(b\\) and \\(c\\) in setting the orientation of the line, open an R/mosaic session. The R/mosaic code below generates a particular instance of \\(\\text{plane}(x,y)\\) and plots it in two ways: a contour plot and a surface plot. Change the numerical values of \\(b\\) and \\(c\\) and observe how the orientation of the planar surface changes in the graphs. You can also see that the value of \\(a\\) is irrelevant to the orientation of the plane, just as the intercept of a straight-line graph is irrelevant to the slope of that line.\n\nplane &lt;- makeFun(a + b*x + c*y ~ x + y, a = 1, b = -2.5, c = 1.6)\ncontour_plot(plane(x, y) ~ x + y, \n             bounds(x=c(-2, 2), y=c(-2, 2))) %&gt;%\n  gf_refine(coord_fixed())\n\n\n\n\n\n\n\n\n\ninteractive_plot(plane(x, y) ~ x + y, \n                 bounds(x=c(-2, 2), y=c(-2, 2)))\n\n\n## Loading required namespace: plotly\n\n\n\n\n\nAs always it can be difficult to extract quantitative information from a surface plot. For the example here, you can see that the high-point on the surface is when \\(x\\) is most negative and \\(y\\) is most positive. Compare that to the contour plot to verify that two modes are displaying the same surface.\n\n\nNote: The gf_refine(coord_fixed()) part of the contour-plot command makes numerical intervals on the horizontal and vertical axes have the same length.)\n\n\n\n\n\n\n\n\n\n\n\n(a) A level surface\n\n\n\n\n\n\n\n\n\n\n\n(b) Rotated along the axis running top to bottom\n\n\n\n\n\n\n\n\n\n\n\n(c) Rotated along the axis running left to right\n\n\n\n\n\n\n\nFigure 25.5: Combining two simple movements can tip a plane to all sorts of different orientations.\n\n\n\n\n\n\n\n\n\n\n\n\nGet in the habit of noticing the subscript on the differentiation symbol \\(\\partial\\). When taking, for instance, \\(\\partial_y f(x,y,z, \\ldots)\\), all inputs other than \\(y\\) are to be held constant. Some examples:\n\\[\\partial_y 3 x^2 = 0\\ \\ \\text{but}\\ \\ \\\n\\partial_x 3 x^2 = 6x\\\\\n\\ \\\\\n\\partial_y 2 x^2 y = 2x^2\\ \\ \\text{but}\\ \\ \\\n\\partial_x 2 x^2 y = 4 x y\n\\]",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Partial change and the gradient vector</span>"
    ]
  },
  {
    "objectID": "Differentiation/25-partial.html#all-other-things-being-equal",
    "href": "Differentiation/25-partial.html#all-other-things-being-equal",
    "title": "25  Partial change and the gradient vector",
    "section": "25.2 All other things being equal …",
    "text": "25.2 All other things being equal …\nRecall that the derivative of a function with one input, say, \\(\\partial_x f(x)\\) tells you, at each possible value of the input \\(x\\), how much the output will change proportional to a small change in the value of the input.\nNow that we are in the domain of multiple inputs, writing \\(h\\) to stand for “a small change” is not entirely adequate. Instead, we will write \\(dx\\) for a small change in the \\(x\\) input and \\(dy\\) for a small change in the \\(y\\) input.\nWith this notation, we write the first-order polynomial approximation to a function of a single input \\(x\\) as \\[f(x+dx) = f(x) + \\partial_x f(x) \\times dx\\] Applying this notation to functions of two inputs, we have: \\[g(x + \\color{magenta}{dx}, y) = g(x,y) + \\color{magenta}{\\partial_x} g(x,y) \\times \\color{magenta}{dx}\\] and \\[g(x, y+\\color{brown}{dy}) = g(x,y) + \\color{brown}{\\partial_y} g(x,y) \\times \\color{brown}{dy}\\]\nEach of these statements is about changing one input while holding the other input(s) constant. Or, as the more familiar expression goes, “The effect of changing one input all other things being equal or all other things held constant.2\nEverything we’ve said about differentiation rules applies not just to functions of one input, \\(f(x)\\), but to functions with two or more inputs, \\(g(x,y)\\), \\(h(x,y,z)\\) and so on.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Partial change and the gradient vector</span>"
    ]
  },
  {
    "objectID": "Differentiation/25-partial.html#gradient-vector",
    "href": "Differentiation/25-partial.html#gradient-vector",
    "title": "25  Partial change and the gradient vector",
    "section": "25.3 Gradient vector",
    "text": "25.3 Gradient vector\nFor functions of two inputs, there are two partial derivatives. For functions of three inputs, there are three partial derivatives. We can, of course, collect the partial derivatives into Cartesian coordinate form. This collection is called the gradient vector.\nJust as our notation for differences (\\(\\cal D\\)) and derivatives (\\(\\partial\\)) involves unusual typography on the letter “D,” the notation for the gradient involves such unusual typography although this time on \\(\\Delta\\), the Greek version of “D.” For the gradient symbol, turn \\(\\Delta\\) on its head: \\(\\nabla\\). That is, \\[\\nabla g(x,y) \\equiv \\left(\\stackrel\\strut\\strut\\partial_x g(x,y), \\ \\ \\partial_y g(x,y)\\right)\\]\nNote that \\(\\nabla g(x,y)\\) is a function of both \\(x\\) and \\(y\\), so in general the gradient vector differs from place to place in the function’s domain.\nThe graphics convention for drawing a gradient vector for a particular input, that is, \\(\\nabla g(x_0, y_0)\\), puts an arrow with its root at \\((x_0, y_0)\\), pointing in direction \\(\\nabla g(x_0, y_0)\\), as in Figure 25.6.\n\n\n\n\n\n\n\n\nFigure 25.6: The gradient vector \\(\\nabla g(x=1,y=2)\\). The vector points in the steepest uphill direction. Consequently, it is perpendicular to the contour passing through its root.\n\n\n\n\n\nA gradient field (see Figure 25.7) is the value of the gradient vector at each point in the function’s domain. Graphically, to prevent over-crowding, the vectors are drawn at discrete points. The lengths of the drawn vectors are set proportional to the numerical length of \\(\\nabla g(x, y)\\), so a short vector means the surface is relatively level, a long vector means the surface is relatively steep.\n\n\n\n\n\n\n\n\nFigure 25.7: A plot of the gradient field \\(\\nabla g(x,y)\\).",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Partial change and the gradient vector</span>"
    ]
  },
  {
    "objectID": "Differentiation/25-partial.html#total-derivative-optional",
    "href": "Differentiation/25-partial.html#total-derivative-optional",
    "title": "25  Partial change and the gradient vector",
    "section": "25.4 Total derivative (optional)",
    "text": "25.4 Total derivative (optional)\nThe name “partial derivative” suggests the existence of some kind of derivative that is not just a part, but the whole thing. The total derivative is such a whole and gratifyingly made up of its parts, that is, the partial derivatives.\nSuppose you are modeling the temperature of some volume of the atmosphere, given as \\(T(t, x, y, z)\\). This merely says that the temperature depends on both time and location, something that is familiar from everyday life.\nThe partial derivatives have an easy interpretation: \\(\\partial_t T()\\) tells how the temperature is changing over time at a given location, perhaps because of the evaporation or condensation of water vapor. \\(\\partial_x T()\\) tells how the temperature changes in the \\(x\\) direction, and so on.\nThe total derivative gives an overall picture of the changes in a parcel of air, which you can thnk of as a tiny balloon-like structure but without the balloon membrane. The temperature inside the “balloon” may change with time (e.g. condensation or evaporation of water), but as the ballon drifts along with the motion of the air (that is, the wind), the evolving location can change the temperature as well. Think of a balloon caught in an updraft: the temperature goes down as the balloon ascends.\nFor an imaginary observer located in the balloon, the temperature is changing with time. Part of this change is the instrinsic change measured by \\(\\partial_t T\\) but we need to add to that the changes induces by the evolving location of the balloon. The partial change in temperature due to a change in altitude is \\(\\partial_z T\\), but it is important to realize that the coordinates of the location are themselves functions of time: \\(x(t), y(t), z(t)\\). Seeing the function \\(T()\\) for the observer in the balloon as a function of \\(t\\), we have \\(T(t, x(t), y(t), z(t))\\). This is a function composition: \\(T()\\) composed with each of \\(x()\\), \\(y()\\), and \\(z()\\). Recall in the chain rule \\(\\partial_v f(g(v)) = \\partial_v f(g(v)) \\partial_v g(v)\\) that the derivative of the composed quantity is the product of two derivatives.\nLikewise, the total derivative of temperature with respect to the observer riding in the balloon will be add together the parts due to changes in time (holding position constant), x-coordinate (holding time and the other space coordinates constant), and the like. Signifying the total differentiation with a capital \\(D\\), we have \\[D\\, T(t) = \\partial_t T() + \\partial_x T() \\cdot\\partial_t x + \\partial_y T()\\cdot \\partial_t y + \\partial_z T() \\cdot\\partial_t z\\] Note that \\(\\partial_t x\\) is the velocity of the balloon in the x-direction, and similarly for the other coordinate directions. Writing these velocities as \\(v_x, v_y, v_z\\), the total derivative for temperature of a parcel of air embedded in a moving atmosphere is\n\\[D\\ T(t) = \\partial_t T + v_x\\, \\partial_x T + v_y\\, \\partial_y T + v_z\\, \\partial_z T\\] Formulations like this, which put the parts of change together into a whole, are often seen in the mathematics of fluid flow as applied in meteorology and oceanology.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Partial change and the gradient vector</span>"
    ]
  },
  {
    "objectID": "Differentiation/25-partial.html#sec-differential-skier",
    "href": "Differentiation/25-partial.html#sec-differential-skier",
    "title": "25  Partial change and the gradient vector",
    "section": "25.5 Differentials",
    "text": "25.5 Differentials\n\nA little bit of this, a little bit of that. — Stevie Wonder, “The Game of Love”\n\nWe have framed calculus in terms of functions: transformations that take one (or more!) quantities as input and return a quantity as output. This was not the original formulation. In this section, we will use the original style to demonstrate how you can sometimes skip the step of constructing a function before differentiating to answer a question of the sort: “If this quantity changes by a little bit, how much will another, related quantity change?”\nAs an example, consider the textbook-style problem of a water skier being pulled along the water by a rope pulled in from the top of a tower of height \\(H\\). The skier is distance \\(x\\) from the tower. As the rope is winched in at a constant rate, does the skier go faster or slower as she approaches the tower.\n\n\n\n\n\n\n\n\n\nIn the function style of approach, we can write the position function \\(x(t)\\) with input the length of the rope \\(L(t)\\). Using the diagram, you can see that \\[x(t) = \\sqrt{\\strut L(t)^2 - H^2}\\ .\\]\nDifferentiate both sides with respect to \\(t\\) to get the velocity of the skier: \\(\\partial_t x(t)\\) through the chain rule: \\[\\underbrace{\\partial_t x(t)}_{\\partial_t f(g(t))} = \\underbrace{\\frac{1}{2\\sqrt{\\strut L(t)^2 - H^2}}}_{\\left[ \\partial_t f \\right](g(t)) } \\times \\underbrace{\\left[2 \\partial_t L(t)\\right]}_{\\partial_t g(t)} = \\frac{\\partial_t L(t)}{\\strut\\sqrt{L(t)^2 - H^2}}\\]\nNow to reformulate the problem without defining a function.\nNewton referred to “flowing quantities” or “fluents” and to what today is universally called derivatives as “fluxions.” Newton did not have a notion of inputs and output.3\nAt about the same time as Newton’s inventions, very similar ideas were being given very different names by mathematicians on the European continent. There, an infinitely small change in a quantity was called a “differential” and the differential of \\(x\\) was denoted \\(dx\\).\nThe first calculus textbook was subtitled, Of the Calculus of Differentials, in other words, how to calculate differentials. (See Figure 25.8.) Section I of this 1696 text is entitled, “Where we give the rules of this calculation,” those rules being recognizably the same as presented in Chapter 23 of this book.\n\n\n\n\n\n\n\n\n\nFigure 25.8: From the start of the first calculus textbook, by le marquis de l’Hôpital, 1696.\n\n\n\n\nDefinition I of Section I states,\n\n“We call quantities variable* that grow or decrease continuously; and to the contrary constant quantities are those that remain the same while the others change. … The infinitely small amount by which a continuous quantity increases or decreases is called the differential.*”\n\nThe differential is not a derivative. The differential is an infinitely small change in a quantity and a derivative is a rate of change. The differential of a quantity \\(x\\) is written \\(dx\\) in the textbook.4\nThe point of Section I of de l’Hôpital’s textbook is to present the rules by which the differentials of complex quantities can be calculated. You will recognize the product rule in de l’Hôpital’s notation:\n\n\n\n\n\n\nThe differential of \\(x\\,y\\) is \\(y\\,dx + x\\,dy\\)\n\n\n\nThe Pythagorean theorem relates the various quantities this way:\n\\[L^2 = x^2 + H^2\\]\nThe differential of each side of the equation refers to “a little bit” of increase in the quantity on that side of the equation: \\[d(L^2) = d(x^2)\\ \\ \\ \\implies\\ \\ \\ 2 L\\, dL = 2 x\\, dx\\] where we’ve used one of the “rules” for calculating differentials. This gives us \\[dx = \\frac{L}{x} dL\\] Think of this as a recipe for calculating \\(dx\\). If you tell me \\(L\\), \\(x\\), and \\(dL\\) then you can calculate the value of \\(dx\\). For instance, suppose the tower is 52 feet tall and that there is \\(L=173\\) feet of tow-rope extending to the skier. The Pythagorean theorem tells us the skier is \\(x=165\\) feet from the base of the tower. The rope is, let us suppose, being pulled in at the top of the tower at \\(dL = 10\\) feet per second. How fast is \\(x\\) changing? \\[dx = \\frac{173\\ \\text{ft}}{165\\ \\text{ft}} \\times 10 \\text{ft s}^{-2} = 10.05\\ \\text{ft s}^{-1}\\]\nWe will return to “a little bit of this” when we explore how to add up little bits to get the whole in ?sec-accum-symbolic.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Partial change and the gradient vector</span>"
    ]
  },
  {
    "objectID": "Differentiation/25-partial.html#footnotes",
    "href": "Differentiation/25-partial.html#footnotes",
    "title": "25  Partial change and the gradient vector",
    "section": "",
    "text": "In describing the orientation of aircraft and ships, three parameters are used: pitch, roll, and yaw. For a geometrical plane (as opposed to an aircraft or ship, which have distinct front and back ends), yaw isn’t applicable.↩︎\nThe Latin phrase for this is ceteris paribus, often used in economics.↩︎\nThe meaning of “output” as “to produce” dates from more than 100 years after Newton’s death.↩︎\nA “warning” is given in the textbook that the symbol \\(d\\) will always be used to mark the differential of a variable quantity and that \\(d\\) will never be used to indicate a parameter.↩︎",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Partial change and the gradient vector</span>"
    ]
  },
  {
    "objectID": "Differentiation/27-taylor.html",
    "href": "Differentiation/27-taylor.html",
    "title": "27  Polynomials",
    "section": "",
    "text": "27.1 Basics of polynomials with one input\nA polynomial is a linear combination of a particular class of functions: power-law functions with non-negative, integer exponents: 1, 2, 3, …. The individual functions are called monomials, a word that echoes the construction of chemical polymers out of monomers; for instance, the material polyester is constructed by chaining together a basic chemical unit called an ester.\nIn one input, say \\(x\\), the monomials are \\(x^1, x^2, x^3\\), and so on. (There is also \\(x^0\\), but that is better thought of as the constant function.) An n-th order polynomial has monomials up to exponent \\(n\\). For example, the form of a third-order polynomial is \\[a_0 + a_1 x^1 + a_2 x^2 + a_3 x^3\\]\nThe domain of polynomials, like the power-law functions they are assembled from, is the real numbers, that is, the entire number line \\(-\\infty &lt; x &lt; \\infty\\). But for the purposes of understanding the shape of high-order polynomials, it is helpful to divide the domain into three parts: a wriggly domain at the center and two tail domains to the right and left of the center.\nFigure 27.1: A \\(n\\)th-order polynomial can have up to \\(n-1\\) critical points that it wriggles among. A 7-th order polynomial is shown here in which there are six local maxima or minima alternatingly.\nFigure 27.1 shows a 7th order polynomial—that is, the highest-order term is \\(x^7\\). In one of the tail domains the function value heads off to \\(\\infty\\), in the other to \\(-\\infty\\). This is a necessary feature of all odd-order polynomials: 1, 3, 5, 7, …\nIn contrast, for even-order polynomials (2, 4, 6, …) the function value in the two tail domains go in the same direction, either both to \\(\\infty\\) (Hands up!) or both to \\(-\\infty\\).\nIn the wriggly domain in Figure 27.1, there are six argmins or argmaxes.\nAn \\(n\\)th-order polynomial can have up to \\(n-1\\) extrema.\nNote that the local polynomial approximations in Chapter 26 are at most 2nd order and so there is at most 1 wriggle: a unique argmax. If the approximation does not include the quadratic terms (\\(x^2\\) or \\(y^2\\)) then there is no argmax for the function.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Polynomials</span>"
    ]
  },
  {
    "objectID": "Differentiation/27-taylor.html#multiple-inputs",
    "href": "Differentiation/27-taylor.html#multiple-inputs",
    "title": "27  Polynomials",
    "section": "27.2 Multiple inputs?",
    "text": "27.2 Multiple inputs?\nHigh-order polynomials are rarely used with multiple inputs. One reason is the proliferation of coefficients. For instance, here is the third-order polynomial in two inputs, \\(x\\), and \\(y\\). \\[\\underbrace{b_0 + b_x x + b_y y}_\\text{first-order terms} + \\underbrace{b_{xy} x y + b_{xx} x^2 + b_{yy} y^2}_\\text{second-order terms} + \\underbrace{b_{xxy} x^2 y + b_{xyy} x y^2 + b_{xxx} x^3 + b_{yyy} y^3}_\\text{third-order terms}\\]\nThis has 10 coefficients. With so many coefficients it is hard to ascribe meaning to any of them individually. And, insofar as some feature of the function does carry meaning in terms of the modeling situation, that meaning is spread out and hard to quantify.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Polynomials</span>"
    ]
  },
  {
    "objectID": "Differentiation/27-taylor.html#sec-high-order-approx",
    "href": "Differentiation/27-taylor.html#sec-high-order-approx",
    "title": "27  Polynomials",
    "section": "27.3 High-order approximations",
    "text": "27.3 High-order approximations\nThe potential attraction of high-order polynomials is that, with their wriggly interior, they can take on a large number of appearances. This chameleon-like behavior has historically made them the tool of choice for understanding the behavior of approximations. That theory has motivated the use of polynomials for modeling patterns in data, but, paradoxically, has shown that high-order polynomials should not be the tool of choice for modeling data.2\nPolynomial functions lend themselves well to calculations, since the output from a polynomial function can be calculated using just the basic arithmetic functions: addition, subtraction, multiplication, and division. To illustrate, consider this polynomial: \\[g(x) \\equiv x - \\frac{1}{6} x^3\\] Since the highest-order term is \\(x^3\\) this is a third-order polynomial. (As you will see, we picked these particular coefficients, 0, 1, 0, -1/6, for a reason.) With such simple coefficients the polynomial is easy to handle by mental arithmetic. For instance, for \\(g(x=1)\\) is \\(5/6\\). Similarly, \\(g(x=1/2) = 23/48\\) and \\(g(x=2) = 2/3\\). A person of today’s generation would use an electronic calculator for more complicated inputs, but the mathematicians of Newton’s time were accomplished human calculators. It would have been well within their capabilities to calculate, using paper and pencil, \\(g(\\pi/4) = 0.7046527\\).3\nOur example polynomial, \\(g(x) \\equiv x - \\frac{1}{6}x^3\\), graphed in color in Figure 27.2, does not look exactly like the sinusoid. If we increased the extent of the graphics domain, the disagreement would be even more striking, since the sinusoid’s output is always in \\(-1 \\leq \\sin(x) \\leq 1\\), while the polynomial’s tails are heading off to \\(\\infty\\) and \\(-\\infty\\). But, for a small interval around \\(x=0\\), exactly aligns with the sinusoid.\n\n\n\n\n\n\n\n\nFigure 27.2: The polynomial \\(g(x) \\equiv x -x^3 / 6\\) is remarkably similar to \\(\\sin(x)\\) near \\(x=0\\).\n\n\n\n\n\nIt is clear from the graph that the approximation is excellent near \\(x=0\\) and gets worse as \\(x\\) gets larger. The approximation is poor for \\(x \\approx \\pm 2\\). We know enough about polynomials to say that the approximation will not get better for larger \\(x\\); the sine function has a range of \\(-1\\) to \\(1\\), while the left and right tails of the polynomial are heading off to \\(\\infty\\) and \\(-\\infty\\) respectively.\nOne way to measure the quality of the approximation is the error \\({\\cal E}(x)\\) which gives, as a function of \\(x\\), the difference between the actual sinusoid and the approximation: \\[{\\cal E}(x) \\equiv |\\strut\\sin(x) - g(x)|\\] The absolute value used in defining the error reflects our interest in how far the approximation is from the actual function and not so much in whether the approximation is below or above the actual function. Figure 27.3 shows \\({\\cal E}(x)\\) as a function of \\(x\\). Since the error is the same on both sides of \\(x=0\\), only the positive \\(x\\) domain is shown.\n\n\n\n\n\n\n\n\nFigure 27.3: The error \\({\\cal E}(x)\\) of \\(x - x^3/6\\) as an approximation to \\(\\sin(x)\\). Top panel: linear scale. Bottom panel: on a log-log scale.\n\n\n\n\n\nFigure 27.3 shows that for \\(x &lt; 0.3\\), the error in the polynomial approximation to \\(\\sin(x)\\) is in the 5th decimal place. For instance, \\(\\sin(0.3) = 0.2955202\\) while \\(g(0.3) = 0.2955000\\).\nThat the graph of \\({\\cal E}(x)\\) is a straight-line on log-log scales diagnoses \\({\\cal E}(x)\\) as a power law. That is: \\({\\cal E}(x) = A x^p\\). As always for power-law functions, we can estimate the exponent \\(p\\) from the slope of the graph. It is easy to see that the slope is positive, so \\(p\\) must also be positive.\nThe inevitable consequence of \\({\\cal E}(x)\\) being a power-law function with positive \\(p\\) is that \\(\\lim_{x\\rightarrow 0} {\\cal E}(x) = 0\\). That is, the polynomial approximation \\(x - \\frac{1}{6}x^3\\) is exact as \\(x \\rightarrow 0\\).\nThroughout this book, we’ve been using straight-line approximations to functions around an input \\(x_0\\). \\[g(x) = f(x_0) + \\partial_x f(x_0) [x-x_0]\\] One way to look at \\(g(x)\\) is as a straight-line function. Another way is as a first-order polynomial. This raises the question of what a second-order polynomial approximation should be. Rather than the polynomial matching just the slope of \\(f(x)\\) at \\(x_0\\), we can arrange things so that the second-order polynomial will also match the curvature of the \\(f()\\). Since the curvature involves only the first and second derivatives of a function, the polynomial constructed to match both the first and the second derivative will necessarily match the slope and curvature of \\(f()\\). This can be accomplished by setting the polynomial coefficients appropriately.\nStart with a general, second-order polynomial centered around \\(x_0\\): \\[g(x) \\equiv a_0 + a_1 [x-x_0] + a_2 [x - x_0]^2\\] The first- and second-derivatives, evaluated at \\(x=x_0\\) are: \\[\\partial_x g(x)\\left.{\\Large\\strut}\\right|_{x=x_0} = a_1 + 2 a_2 [x  - x_0] \\left.{\\Large\\strut}\\right|_{x=x_0} = a_1\\] \\[\\partial_{xx} g(x)\\left.{\\Large\\strut}\\right|_{x=x_0} =  2 a_2\\] Notice the 2 in the above expression. When we want to write the coefficient \\(a_2\\) in terms of the second derivative of \\(g()\\), we will end up with\n\\[a_2 = \\frac{1}{2} \\partial_{xx} g(x)\\left.{\\Large\\strut}\\right|_{x=x_0}\\]\nTo make \\(g(x)\\) approximate \\(f(x)\\) at \\(x=x_0\\), we need merely set \\[a_1 = \\partial_x f(x)\\left.{\\Large\\strut}\\right|_{x=x_0}\\] and \\[a_2 = \\frac{1}{2} \\partial_{xx} f(x) \\left.{\\Large\\strut}\\right|_{x=x_0}\\] This logic can also be applied to higher-order polynomials. For instance, to match the third derivative of \\(f(x)\\) at \\(x_0\\), set \\[a_3 = \\frac{1}{6} \\partial_{xxx} f(x)  \\left.{\\Large\\strut}\\right|_{x=x_0}\\] Remarkably, each coefficient in the approximating polynomial involves only the corresponding order of derivative. \\(a_1\\) involves only \\(\\partial_x f(x)   \\left.{\\Large\\strut}\\right|_{x=x_0}\\); the \\(a_2\\) coefficient involves only \\(\\partial_{xx} f(x)     \\left.{\\Large\\strut}\\right|_{x=x_0}\\); the \\(a_3\\) coefficient involves only \\(\\partial_{xx} f(x)     \\left.{\\Large\\strut}\\right|_{x=x_0}\\), and so on.\nNow we can explain where the polynomial that started this section, \\(x - \\frac{1}{6} x^3\\) came from and why those coefficients make the polynmomial approximate the sinusoid near \\(x=0\\).\n\n\n\n\n\n\n\n\nOrder\n\\(\\sin(x)\\) derivative\n\\(x - \\frac{1}{6}x^3\\) derivative\n\n\n\n\n0\n\\(\\sin(x) \\left.{\\Large\\strut}\\right|_{x=0} = 0\\)\n\\(\\left( 1 - \\frac{1}{6}x^3\\right)\\left.{\\Large\\strut}\\right|_{x=0} = 0\\)\n\n\n1\n\\(\\cos(x) \\left.{\\Large\\strut}\\right|_{x=0} = 1\\)\n\\(\\left(1 - \\frac{3}{6} x^2\\right) \\left.{\\Large\\strut}\\right|_{x=0}= 1\\)\n\n\n2\n\\(-\\sin(x) \\left.{\\Large\\strut}\\right|_{x=0} = 0\\)\n\\(\\left(- \\frac{6}{6} x\\right) \\left.{\\Large\\strut}\\right|_{x=0} = 0\\)\n\n\n3\n\\(-\\cos(x) \\left.{\\Large\\strut}\\right|_{x=0} = -1\\)\n\\(- 1\\left.{\\Large\\strut}\\right|_{x=0} = -1\\)\n\n\n4\n\\(\\sin(x) \\left.{\\Large\\strut}\\right|_{x=0} = 0\\)\n\\(0\\left.{\\Large\\strut}\\right|_{x=0} = 0\\)\n\n\n\nThe first four derivatives of \\(x - \\frac{1}{6} x^3\\) exactly match, at \\(x=0\\), the first four derivatives of \\(\\sin(x)\\).\nThe polynomial constructed by matching successive derivatives of a function \\(f(x)\\) at some input \\(x_0\\) is called a Taylor polynomial.\n\n\n\n\n\n\nTip 27.1: Practice: a Taylor polynomial for \\(e^x\\).\n\n\n\nLet’s construct a 3rd-order Taylor polynomial approximation to \\(f(x) = e^x\\) around \\(x=0\\).\nWe know it will be a 3rd order polynomial: \\[g_{\\exp}(x) \\equiv a_0 + a_1 x + a_2 x^2 + a_3 x^3\\] The exponential function is particularly nice for examples because the function value and all its derivatives are identical: \\(e^x\\). So \\[f(x= 0) = 1\\]\n\\[ \\partial_x f(x=0) = 1\\] \\[\\partial_{xx} f(x=0) = 1\\] \\[\\partial_{xxx} f(x=0) = 1\\] and so on.\nThe function value and derivatives of \\(g_{\\exp}(x)\\) at \\(x=0\\) are: \\[g_{\\exp}(x=0) = a_0\\] \\[\\partial_{x}g_{\\exp}(x=0) = a_1\\] \\[\\partial_{xx}g_{\\exp}(x=0) = 2 a_2\\]\n\\[\\partial_{xxx}g_{\\exp}(x=0) = 2\\cdot3\\cdot a_3 = 6\\, a_3\\] Matching these to the exponential evaluated at \\(x=0\\), we get \\[a_0 = 1\\] \\[a_1 = 1\\] \\[a_2 = \\frac{1}{2}\\] \\[a_3 = \\frac{1}{2 \\cdot 3} = \\frac{1}{6}\\]\nResult: the 3rd-order Taylor polynomial approximation to the exponential at \\(x=0\\) is \\[g_{\\exp}(x) = 1 + x + \\frac{1}{2} x^2 +  \\frac{1}{2\\cdot 3} x^3 +\\frac{1}{2\\cdot 3\\cdot 4} x^4\\]\nFigure 27.4 shows the exponential function \\(e^x\\) and its 3th-order Taylor polynomial approximation near \\(x=0\\):\n\n\n\n\n\n\n\n\nFigure 27.4: The 3th-order Taylor polynomial approximation to \\(e^x\\) arount \\(x=0\\)\n\n\n\n\n\nThe polynomial is exact at \\(x=0\\). The error \\({\\cal E}(x)\\) grows with increasing distance from \\(x=0\\):\n\n\n\n\n\n\n\n\nFigure 27.5: The error from a 3rd-order Taylor polynomial approximation to \\(e^x\\) around \\(x=0\\) is a power-law function with exponent \\(4\\).\n\n\n\n\n\n\n\n\n\n\n\nFigure 27.6: The error from a 3rd-order Taylor polynomial approximation to \\(e^x\\) around \\(x=0\\) is a power-law function with exponent \\(4\\).\n\n\n\n\n\nThe plot of \\(\\log_{10} {\\cal E}(x)\\) versus \\(\\log_{10} | x |\\) in ?fig-taylor-exp-5 shows that the error grows from zero at \\(x=0\\) as a power-law function. Measuring the exponent of the power-law from the slope of the graph on log-log axes give \\({\\cal E}(x) = a |x-x_0|^5\\). This is typical of Taylor polynomials: for a polynomial of degree \\(n\\), the error will grow as a power-law with exponent \\(n+1\\). This means that the higher is \\(n\\), the faster \\(\\lim_{x\\rightarrow x_0}{\\cal E}(x) \\rightarrow 0\\). On the other hand, since \\({\\cal E}_x\\) is a power law function, as \\(x\\) gets further from \\(x_0\\) the error grows as \\(\\left(x-x_0\\right)^{n+1}\\).\n\n\n\n\n\n\n\n\nMath in the World: Polynomial models of other functions\n\n\n\nBrooke Taylor (1685-1731), a near contemporary of Newton, published his work on approximating polynomials in 1715. Wikipedia reports: “[T]he importance of [this] remained unrecognized until 1772, when Joseph-Louis Lagrange realized its usefulness and termed it ‘the main [theoretical] foundation of differential calculus’.”Source\n\n\n\n\n\n\n\n\nFigure 27.7: Brook Taylor\n\n\n\n\n\nDue to the importance of Taylor polynomials in the development of calculus, and their prominence in many calculus textbooks, many students assume their use extends to constructing models from data. They also assume that third- and higher-order monomials are a good basis for modeling data. Both these assumptions are wrong. Least squares is the proper foundation for working with data.\nTaylor’s work preceded by about a century the development of techniques for working with data. One of the pioneers in these new techniques was Carl Friedrich Gauss (1777-1855), after whom the gaussian function is named. Gauss’s techniques are the foundation of an incredibly important statistical method that is ubiquitous today: least squares. Least squares provides an entirely different way to find the coefficients on approximating polynomials (and an infinite variety of other function forms). The R/mosaic fitModel() function for polishing parameter estimates is based on least squares. In Block 5, we will explore least squares and the mathematics underlying the calculations of least-squares estimates of parameters.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Polynomials</span>"
    ]
  },
  {
    "objectID": "Differentiation/27-taylor.html#indeterminate-forms",
    "href": "Differentiation/27-taylor.html#indeterminate-forms",
    "title": "27  Polynomials",
    "section": "27.4 Indeterminate forms",
    "text": "27.4 Indeterminate forms\nLet’s return to an issue that has bedeviled calculus students since Newton’s time. The example we will use is the function \\[\\text{sinc}(x)  \\equiv \\frac{\\sin(x)}{x}\\]\nThe sinc() function (pronounced “sink”) is still important today, in part because of its role in converting discrete-time measurements (as in an mp3 recording of sound) into continuous signals.\nWhat is the value of \\(\\text{sinc}(0)\\)? One answer, favored by arithmetic teachers is that \\(\\text{sinc}(0)\\) is meaningless, because it involves division by zero.\nOn the other hand, \\(\\sin(0) = 0\\) as well, so the sinc function evaluated at zero involves 0/0. This quotient is called an indeterminant form. The logic is this: Suppose we assume that \\(0/0 = b\\) for some number \\(b\\). then \\(0 = 0 \\times b = 0\\). So any value of \\(b\\) would do; the value of \\(0/0\\) is “indeterminant.”\nStill another answer is suggested by plotting out sinc(\\(x\\)) near \\(x=0\\) and reading the value off the graph: sinc(0) = 1.\n\n\n\n\nslice_plot(sin(x) / x ~ x, domain(x=c(-10,10)), npts=500)\n\n\n\n\n\n\n\n\n\n\nFigure 27.8: To judge from this plot, \\(\\sin(0) / 0 = 1\\).\n\n\n\nThe graph of sinc() looks smooth and the shape makes sense. Even if we zoom in very close to \\(x=0\\), the graph continues to look smooth. We call such functions well behaved.\nCompare the well-behaved sinc() to a very closely related function (which does not seem to be so important in applied work): \\(\\frac{\\sin(x)}{x^3}\\).\nBoth \\(\\sin(x)/x\\) and \\(\\sin(x) / x^3\\), evaluated at \\(x=0\\) involve a divide by zero. Both are indeterminate forms 0/0 at \\(x=0\\). But the graph of \\(\\sin(x) / x^3\\) (see Figure 27.9) is not we will behaved. \\(\\sin(x) / x^3\\) does not have any particular value at \\(x=0\\); instead, it has an asymptote.\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nFigure 27.9: Zooming in around the division by zero. Top: The graph of \\(\\sin(x)/x\\) versus \\(x\\). Bottom: The graph of \\(\\sin(x)/x^2\\). The vertical scales on the two graphs are utterly different.\n\n\n\nSince both \\(\\sin(x)/x\\left.{\\Large\\strut}\\right|_{x=0}\\) and \\(\\sin(x)/x^3\\left. {\\Large\\strut}\\right|_{x=0}\\) involve a divide-by-zero, the answer to the utterly different behavior of the two functions is not to be found at zero. Instead, it is to be found near zero. For any non-zero value of \\(x\\), the arithmetic to evaluate the functions is straight-forward. Note that \\(\\sin(x) / x^3\\) starts its mis-behavior away from zero. The slope of \\(\\sin(x) / x^3\\) is very large near \\(x=0\\), while the slope of \\(\\sin(x) / x\\) smoothly approaches zero.\nSince we are interested in behavior near \\(x=0\\), a useful technique is to approximate the numerator and denominator of both functions by polynomial approximations.\n\n\\(\\sin(x) \\approx x - \\frac{1}{6} x^3\\) near \\(x=0\\)\n\\(x\\) is already a polynomial.\n\\(x^3\\) is already a polynomial.\n\nRemember, these approximations are exact as \\(x\\) goes to zero. So sufficiently close to zero,\n\\[\\frac{\\sin(x)}{x} = \\frac{x - \\frac{1}{6} x^3}{x} = 1 + \\frac{1}{6} x^2\\] Even at \\(x=0\\), there is nothing indeterminant about \\(1 + x^2/6\\); it is simply 1.\nCompare this to the polynomial approximation to \\(\\sin(x) / x^3\\): \\[\\frac{\\sin(x)}{x^3} = \\frac{x - \\frac{1}{6} x^3}{x^3} = \\frac{1}{x^2} - \\frac{1}{6}\\]\nEvaluating this at \\(x=0\\) involves division by zero. No wonder it is badly behaved.\nThe procedure for checking whether a function involving division by zero behaves well or poorly is described in the first-ever calculus textbook, published in 1697. The title (in English) is: The analysis into the infinitely small for the understanding of curved lines. In honor of the author, the Marquis de l’Hospital, the procedure is called l’Hopital’s rule.4\nConventionally, the relationship is written \\[\\lim_{x\\rightarrow x_0} \\frac{u(x)}{v(x)} = \\lim_{x\\rightarrow x_0} \\frac{\\partial_x u(x)}{\\partial_x v(x)}\\]\nLet’s try this out with our two example functions around \\(x=0\\):\n\\[\\lim_{x\\rightarrow 0} \\frac{\\sin(x)}{x} = \\frac{\\lim_{x\\rightarrow 0} \\cos(x)}{\\lim_{x \\rightarrow 0} 1} = \\frac{1}{1} = 1\\]\n\\[\\lim_{x\\rightarrow 0} \\frac{\\sin(x)}{x^3} = \\frac{\\lim_{x\\rightarrow 0} \\cos(x)}{\\lim_{x \\rightarrow 0} 3x^2} = \\frac{1}{0} \\ \\ \\text{indeterminate}!\\]",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Polynomials</span>"
    ]
  },
  {
    "objectID": "Differentiation/27-taylor.html#computing-with-indeterminate-forms",
    "href": "Differentiation/27-taylor.html#computing-with-indeterminate-forms",
    "title": "27  Polynomials",
    "section": "27.5 Computing with indeterminate forms",
    "text": "27.5 Computing with indeterminate forms\nIn the early days of electronic computers, division by zero would cause a fault in the computer, often signaled by stopping the calculation and printing an error message to some display. This was inconvenient, since programmers did not always forsee division-by-zero situations and avoid them.\nAs you’ve seen, modern computers have adopted a convention that simplifies programming considerably. Instead of stopping the calculation, the computer just carries on normally, but produces as a result one of two indeterminant forms: Inf and NaN.\nInf is the output for the simple case of dividing zero into a non-zero number, for instance:\n\n17/0\n## [1] Inf\n\nNaN, standing for “not a number,” is the output for more challenging cases: dividing zero into zero, or multiplying Inf by zero, or dividing Inf by Inf.\n\n0/0\n## [1] NaN\n0 * Inf\n## [1] NaN\nInf / Inf\n## [1] NaN\n\nThe brilliance of the idea is that any calculation that involves NaN will return a value of NaN. This might seem to get us nowhere. But most programs are built out of other programs, usually written by other people interested in other applications. You can use those programs (mostly) without worrying about the implications of a divide by zero. If it is important to respond in some particularly way, you can always check the result for being NaN in your own programs. (Much the same is true for Inf, although dividing a non-Inf number by Inf will return 0.)\nPlotting software will often treat NaN values as “don’t plot this.” that is why it is possible to make a sensible plot of \\(\\sin(x)/x\\) even when the plotting domain includes zero.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Polynomials</span>"
    ]
  },
  {
    "objectID": "Differentiation/27-taylor.html#footnotes",
    "href": "Differentiation/27-taylor.html#footnotes",
    "title": "27  Polynomials",
    "section": "",
    "text": "The fundamental theorem of algebra says that an order-n polynomial has n roots (including multiplicities).↩︎\nThe mathematical background needed for those better tools won’t be available to us until Block 5, when we explore linear algebra.↩︎\nUnfortunately for these human calculators, pencils weren’t invented until 1795. Prior to the introduction of this advanced, graphite-based computing technology, mathematicians had to use quill and ink.↩︎\nIn many French words, the sequence “os” has been replaced by a single, accented letter, \\(\\hat{o}\\).↩︎",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Polynomials</span>"
    ]
  },
  {
    "objectID": "Differentiation/Differentiation-projects.html",
    "href": "Differentiation/Differentiation-projects.html",
    "title": "28  Differentiation projects",
    "section": "",
    "text": "28.1 Labor vs capital\nThe Cobb-Douglas production function is a simple mathematical model of how labor \\(L\\) and capital \\(K\\) combine to produce a factory’s output \\(P\\). It is \\[P(L, K) \\equiv A K^\\alpha L^{1-\\alpha}\\ .\\]\nFor simplicity, imagine that capital and labor are both measured in dollars per year—the amount that the labor force is paid in a year and the amount that one could rent a factory for a year.\nWe’ll stick with numbers like \\(K = 10\\) and \\(L = 20\\) to keep things easy to read, but feel free to interpret them as “millions of dollars.”\nCongratuations! Based on your ability to use the Cobb-Douglas model, you’ve been promoted to manager of the factory. One of your jobs is to decide how to balance expenditures on capital and labor to raise productivity.\nOne basic question is what happens when you raise either capital or labor, holding the other one constant. Using ap- propriate partial derivatives evaluated at \\(K = 10\\), \\(L = 20\\), calculate:\nYour economist friend tells you to watch out for “diminishing marginal returns.” This means that, as you increase spending on either labor or capital, the rate of increase in production tends to diminish. You’ll still get increased production as you increase spending, but it won’t increase as fast at high levels of expense as at low levels.\nBut what happens to the value rate of labor when capital spending is increased? You can answer this by comparing the value rate of labor, \\(\\partial_L P\\) , at two different capital spending levels, say \\((K = 10,L = 20)\\) and \\((K = 11,L = 20)\\). Notice that even though you’re looking at the rate with respect to labor, you’re changing the expenditure on capital.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Differentiation projects</span>"
    ]
  },
  {
    "objectID": "Differentiation/Differentiation-projects.html#labor-vs-capital",
    "href": "Differentiation/Differentiation-projects.html#labor-vs-capital",
    "title": "28  Differentiation projects",
    "section": "",
    "text": "If production \\(P(L, K)\\) is also measured in dollars per year (say, the value of the factory output each year), what is the dimension of the constant \\(A\\)?\nAccording to the model, what happens to production if both \\(K\\) and \\(L\\) are increased by a factor constant factor \\(\\beta\\)? (Hint: Substitute in \\(K \\rightarrow \\beta K\\) and \\(L \\rightarrow \\beta L\\) and simplify.)\nConsider a particular factory with \\(A = 2.5\\) and exponent \\(\\alpha = 0.33\\). In a sandbox, implement the function \\(P (K, L)\\). Use your function to compute the production of the factory for \\(K = 10\\) and \\(L=20\\). Confirm that you get \\(P(K=10,L=20)= 39.78\\)\nA factory that rents for $10/yr and where the labor costs $20/yr is silly. Calculate the value \\(P (K, L)\\) when \\(K\\) is $10 million/yr and \\(L\\) is $20 million/yr.\n\n\n\n\n\nThe rate at which an increase in spending on labor will increase productivity.\nThe rate at which an increase in spending on capital will increase productivity.\nBased on the above, if you had to choose between spending on capital or labor, and your goal is to increase productivity as much as possible, which would you spend on, capital or labor?\n\n\n\nCompute the partial derivative of production with respect to labor at a higher level of labor, say \\(L = 21\\), but holding \\(K = 10\\). How does the value of the derivative at \\(L = 21\\) compare to that at \\(L = 20\\)? Is this consistent with the idea of “diminishing marginal returns” for labor?\nDo the same for the partial derivative of production with respect to capital, evaluated at \\(L = 20\\) and \\(K = 11\\). How does the value of the derivative at \\(K = 11\\) compare to that at \\(K = 10\\). Is this consistent with the idea of “diminishing marginal returns” for capital?\nUse an appropriate partial second derivative to find the rate of diminishing partial returns for labor at \\(L = 20\\) and \\(K = 10\\). Show that it is consistent with the difference you got in Part (d).\nUse an appropriate partial second derivative to find the rate of diminishing partial returns for capital at \\(L = 20\\) and \\(K = 10\\). Show that it is consistent with the difference you got in Part (3).\nYou might think of the rate of increase in production with respect to labor as the “value rate” of labor. Similarly, the rate of increase in production with respect to capital is the value rate of capital. Due to diminishing marginal returns, an increase in labor spending, holding capital constant, decreases the value rate of labor. Similarly, an increase in capital spending holding labor spending constant decreases the value rate of capital.\n\n\ni. Compare $\\partial_L P$ at slightly different values of $K$ , holding $L$ constant at 20. Does the value rate of labor increase or decrease with spending on capital?\nii. Similarly, compare $\\partial_K P$ at slightly different values of  $L$, holding $K$ constant at 20. Does the value rate of labor increase or decrease with spending on capital?\niii. Finally, construct and evaluate the mixed partial derivative, $\\partial_L \\partial_K P at $K = 10$, $L = 20$. Compare this to the results you got for the way $\\partial_K P$ changes with increasing $L$ and the way $\\partial_L P$ changes with increasing $K$.\n\n28.1.1 Walking\nIf you’re like many people, you find it harder to walk uphill than down, and find it takes more out of you to walk longer distances than shorter. Let’s build a model of this, using nothing more than your intuition and the method of low-order polynomial approximations.\nLet’s call the map distance walked \\(d\\). (“Map distance” is the horizontal change in position, disregarding vertical changes.) The steepness of the hill will be the “grade” \\(g\\), which is measured as the horizontal distance covered divided by the vertical climb. If you’re going downhill, the grade is negative.\nThe key ingredient in the model: we will measure the “difficulty” or “exertion” to walking as the energy consumed during the walk: \\(E(d, g)\\).\nSome assumptions about walking and energy consumed:\n\nIf you don’t walk, you consume zero energy walking.\nThe energy consumed should be proportional to the length of the walk. This is an assumption, and is probably valid, only for walks of short to medium distances, as opposed to forced marches over tens of miles.\n\nWe will start with the full 2nd-order polynomial in two inputs, and then seek to eliminate terms that aren’t needed.\n\\[E_{big}(d, g) \\equiv a_0 + a_d\\, d + a_g\\, g + a_{dg}\\, d\\, g + a_{dd}\\,d^2 + a_{gg}\\,g^2\\] According to assumption (1), when \\(E(d=0, g) = 0\\). Of course, if you are walking zero distance, it does not matter what the grade is; the energy consumed is still zero.\nConsequently, we know that all terms that don’t include a \\(d\\) should go away. This leaves us with\n\\[E_{medium}(d, g) \\equiv  a_d\\, d + a_{dg}\\, d\\, g + a_{dd}\\,d^2 = d \\left[\\strut a_d + a_{dg}\\, g + a_{dd}\\,d\\right]\\] Assumption (2) says that energy consumed is proportional to \\(d\\). The multiplier on \\(d\\) in \\(E_{medium}()\\) is \\(\\left[\\strut a_d + a_{dg}\\, g + a_{dd}\\,d\\right]\\) which is itself a function of \\(d\\). A proportional relationship implies a multiplier that does not depend on the quantity itself. This means that \\(a_{dd} = 0\\).\nThis leaves us with a very simple model: \\[E(d, g) \\equiv \\left[\\strut a_1 + a_2\\, g\\right]\\, d\\] where we have simplified the labeling on the coefficients since there are only two in the model.\nPerhaps assumption (2) is misplaced and that the energy consumed per unit distance in a walk increases with the length of the walk. If so, we would need to return to the question of \\(a_{dd}\\). This is typical of the modeling cycle. Trying to be economical with model terms highlights the question of which terms are so small they can be ignored.",
    "crumbs": [
      "BLOCK II. Differentiation",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Differentiation projects</span>"
    ]
  },
  {
    "objectID": "differentiation-part.html",
    "href": "differentiation-part.html",
    "title": "BLOCK II. Differentiation",
    "section": "",
    "text": "A mathematical function is a relationship between inputs and an output. An important and useful way to work with functions is to examine change in output as the inputs are changed by a small amount. The process of calculating this change-in-output per change-in-input—a rate of change—is called differentation. Often, the rate of change is itself a function. Such rate-of-change functions are given a special label: a derivative function.\nThis Block introduces the concept of a rate-of-change function, ways of computing them, and how the derivative of a function can be inferred from a graph of the function. We will explore the connection between the value of the rate-of-change function and the location of an input that optimizes the output of the original function. We will consider the idea of rate-of-change for a function that has multiple inputs.\nSometimes, your knowledge of a real-world system takes the form of knowing the behavior of the rate-of-change function. This can be an important guide to constructing mathematical models.",
    "crumbs": [
      "BLOCK II. Differentiation"
    ]
  },
  {
    "objectID": "Linear-combinations/B5-linear-combinations.html",
    "href": "Linear-combinations/B5-linear-combinations.html",
    "title": "30  Linear combinations of vectors",
    "section": "",
    "text": "30.1 Scaling vectors\nTo scale a vector means to change its length without altering its direction. For instance, scaling by a negative number flips the vector tip-for-tail. Figure 30.1 shows two vectors \\(\\vec{v}\\) and \\(\\vec{w}\\) together with several scaled versions.\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\nFigure 30.1: Vectors \\(\\vec{v}\\) and \\(\\vec{w}\\) and some scaled versions of them.\nArithmetically, scaling a vector is accomplished by multiplying each component of the vector by the scalar, e.g.\n\\[\\vec{u} = \\left[\\begin{array}{r}1.5\\\\-1\\end{array}\\right]\\ \\ \\ \\ 2\\vec{u} = \\left[\\begin{array}{r}3\\\\-2\\end{array}\\right]\\ \\ \\ \\\n-\\frac{1}{2}\\vec{u} = \\left[\\begin{array}{r}-0.75\\\\0.5\\end{array}\\right]\\ \\ \\ \\ \\]\nGeometrically, however, a vector corresponds to one step in a journey. For example, a vector scaled by 2.5 is a journey of two-and-a-half steps; scaling by -10 means traveling backward ten steps.\nThe subspace associated with a single vector is the set of all possible journeys that scaling a vector can accomplish. Visually, this corresponds to all the points on an infinitely long line defined by two points: the tip and the tail of the vector.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Linear combinations of vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/B5-linear-combinations.html#adding-vectors",
    "href": "Linear-combinations/B5-linear-combinations.html#adding-vectors",
    "title": "30  Linear combinations of vectors",
    "section": "30.2 Adding vectors",
    "text": "30.2 Adding vectors\nTo add two vectors, place them tip to tail (without changing the direction). The sum is the new vector running from the tail of the first one to the tip of the second. (Figure 30.2)\n\n\n\n\n\n\n\n\nFigure 30.2: Adding two vectors, yellow and green, by placing them tail to tip. The result is the vector going from the tail of yellow to the tip of green. The blue vector shows this result.\n\n\n\n\n\nAdding vectors in this way takes advantage of the rootlessness of a vector. So long as we keep the direction and length the same, we can move a vector to any convenient place. For adding vectors, the convenient arrangement is to place the tail of the second vector at the tip of the first. The result—the blue pencil in Figure 30.2—runs from the first (yellow) pencil’s tail to the second (green) pencil’s tip.\nArithmetically, vector addition is simply a matter of applying addition component-by-component. For instance, consider adding two vectors \\(\\vec{v}\\) and \\(\\vec{w}\\):\n\\[\\underbrace{\\left[\\begin{array}{r}1.5\\\\-1\\\\2\\\\6\\end{array}\\right]}_\\vec{v} + \\underbrace{\\left[\\begin{array}{r}2\\\\4\\\\-2\\\\-3.2\\end{array}\\right]}_\\vec{w} = \\underbrace{\\left[\\begin{array}{r}3.5\\\\3\\\\0\\\\2.8\\end{array}\\right]}_{\\vec{v} + \\vec{w}}\\]\nAdding vectors makes sense only when they inhabit the same embedding space. In other words, the vectors must have the same number of components.\nArithmetic subtraction of one vector from another is a simple componentwise operation. For example:\n\\[\\underbrace{\\left[\\begin{array}{r}1.5\\\\-1\\\\2\\\\6\\end{array}\\right]}_\\vec{v} {\\Large -} \\underbrace{\\left[\\begin{array}{r}2\\\\4\\\\-2\\\\-3.2\\end{array}\\right]}_\\vec{w} = \\underbrace{\\left[\\begin{array}{r}-0.5\\\\-5\\\\4\\\\9.2\\end{array}\\right]}_{\\vec{v} - \\vec{w}}\\ .\\]\nFrom a geometrical point of view, many people like to think of \\(\\vec{v} - \\vec{w}\\) in terms of placing the two vectors tail to tail as in Figure 30.3. Read the result as the vector running from the tip of \\(\\vec{v}\\) to the tip of \\(\\vec{w}\\). In Figure 30.3, the yellow vector is \\(\\vec{v}\\) and the blue vector is \\(\\vec{w}\\). The result of the subtraction is the green vector.\n\n\n\n\n\n\n\n\nFigure 30.3: Subtracting blue from yellow gives green.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Linear combinations of vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/B5-linear-combinations.html#linear-combinations",
    "href": "Linear-combinations/B5-linear-combinations.html#linear-combinations",
    "title": "30  Linear combinations of vectors",
    "section": "30.3 Linear combinations",
    "text": "30.3 Linear combinations\nThinking of a scaled vector as a “step” of a given length in a given direction leads us to conceive of a linear combination of vectors as step-by-step instructions for a journey. A central use for the formalism of vectors is to guide our thinking and our algorithms for figuring out how best to get from one “place” to another. We have used quotation marks around “place” because we are not necessarily referring to a physical destination. We will get to what else we might mean by “place” later in this Block.\nAs a fanciful example of getting to a “place,” consider a treasure hunt. Here are the instructions to get there:\n\n\nOn June 1, go to the flagpole before sunrise.\nAt 6:32, walk 213 paces away from the Sun.\nAt 12:19, walk 126 paces toward the Sun.\n\n\nThe Sun’s position varies over the day. Consequently, the direction of the Sun on June 1 at 6:32 is different than at 12:19. (Figure 30.4)\n\n\n\n\n\n\nFigure 30.4: For June 1: \\(\\color{magenta}{\\text{Sun's direction at 6:32}}\\) and $$. (Location: latitude 38.0091, /longitude -104.8871). Source: suncalc.org\n\n\n\nThe treasure-hunt directions are in the form of a linear combination of vectors. So far, we know the direction of each vector. Imagine that the length is one stride or pace. (Admittedly, not a scientific unit of length.) Scaling \\(\\color{magenta}{\\text{the magenta vector}}\\) by -213 and \\(\\color{blue}{\\text{the blue vector}}\\) by 126, then adding the two scaled vectors gives a vector that takes you from the flagpole to the treasure.\nA stickler for details might point out that the “direction of the sun” has an upward component. Common sense dictates that the walk is in the direction of the Sun as projected onto Earth’s surface. Chapter 31 deals with projections of vectors.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Linear combinations of vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/B5-linear-combinations.html#functions-as-vectors",
    "href": "Linear-combinations/B5-linear-combinations.html#functions-as-vectors",
    "title": "30  Linear combinations of vectors",
    "section": "30.4 Functions as vectors",
    "text": "30.4 Functions as vectors\nSince calculus is about functions, one naturally expects a chapter on vectors in a calculus textbook to make a connection between functions and vectors. Recall from Section 7.4 (entitled “Functions and data”) the idea of representing a function as a table of inputs and the corresponding outputs.\nHere is such a table with some of our pattern-book functions.\n\n\n\n\n\nt\none(t)\nidentity(t)\nexp(t)\nsin(t)\npnorm(t)\n\n\n\n\n0.0\n1\n0.0\n1.000000\n0.0000000\n0.5000000\n\n\n0.1\n1\n0.1\n1.105171\n0.0998334\n0.5398278\n\n\n0.2\n1\n0.2\n1.221403\n0.1986693\n0.5792597\n\n\n0.3\n1\n0.3\n1.349859\n0.2955202\n0.6179114\n\n\n0.4\n1\n0.4\n1.491825\n0.3894183\n0.6554217\n\n\n... 51 rows in total ...\n\n\n\n\n\n\n\n\n\n\n4.6\n1\n4.6\n99.48432\n-0.9936910\n0.9999979\n\n\n4.7\n1\n4.7\n109.94717\n-0.9999233\n0.9999987\n\n\n4.8\n1\n4.8\n121.51042\n-0.9961646\n0.9999992\n\n\n4.9\n1\n4.9\n134.28978\n-0.9824526\n0.9999995\n\n\n5.0\n1\n5.0\n148.41316\n-0.9589243\n0.9999997\n\n\n\n\n\nIn the tabular representation of the pattern-book functions, each function is a column of numbers—a vector.\nFunctions that we construct by linear combination are, in this vector format, just a linear combination of the vectors. For instance, the function \\(g(t) \\equiv 3 - 2 t\\) is \\(3\\cdot \\text{one}(t) - 2 \\cdot \\text{identity}(t)\\)\n\n\n\n\n\nt\none(t)\nidentity(t)\ng(t)\n\n\n\n\n0.0\n1\n0.0\n3.0\n\n\n0.1\n1\n0.1\n2.8\n\n\n0.2\n1\n0.2\n2.6\n\n\n0.3\n1\n0.3\n2.4\n\n\n0.4\n1\n0.4\n2.2\n\n\n... 51 rows in total ...\n\n\n\n\n\n\n\n\n4.6\n1\n4.6\n-6.2\n\n\n4.7\n1\n4.7\n-6.4\n\n\n4.8\n1\n4.8\n-6.6\n\n\n4.9\n1\n4.9\n-6.8\n\n\n5.0\n1\n5.0\n-7.0\n\n\n\n\n\nThe table above is a collection of four vectors: \\(\\vec{\\mathtt t}\\), \\(\\vec{\\mathtt{ one(t)}}\\), \\(\\vec{\\mathtt{identity(t)}}\\), and \\(\\vec{\\mathtt{g(t)}}\\). Each of those vectors has 51 components. In math-speak, we can say that the vectors are “embedded in a 51-dimensional space.”\nIn the table, the function output is tabulated only for select, discrete values of the input. Such discreteness is not a fundamental problem. Interpolation techniques such as that described in Section 7.4 enable evaluation of the function for a continuous input.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Linear combinations of vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/B5-linear-combinations.html#matrices-and-linear-combinations",
    "href": "Linear-combinations/B5-linear-combinations.html#matrices-and-linear-combinations",
    "title": "30  Linear combinations of vectors",
    "section": "30.5 Matrices and linear combinations",
    "text": "30.5 Matrices and linear combinations\nA collection of vectors, such as the one displayed in the previous table, is called a matrix. Each vector in a matrix must have the same number of components.\nAs mathematical notation, we will use bold-faced, capital letters to stand for matrices, for example, \\(\\mathit{M}\\). The symbol \\(\\rightleftharpoons\\) is a reminder that a matrix can contain multiple vectors, just as the symbol \\(\\rightharpoonup\\) in \\(\\vec{v}\\) reminds us that the name “\\(v\\)” refers to a vector. (It is typical in mathematical writing to use single letters to refer to vectors instead of the descriptive, multi-letter names used in data frames.)\nIn the conventions for data, we give a name to each data frame column so that we can refer to it individually. In the conventions used in vector mathematics, single letters refer to the individual vectors.\nAs a case in point, let’s look at a matrix \\(\\mathit{M}\\) containing the two vectors which we’ve previously called \\(\\vec{\\mathtt{one(t)}}\\) and \\(\\vec{\\mathtt{identity(t)}}\\): \\[\\mathit{M} \\equiv \\left[\\begin{array}{rr}1 & 0\\\\\n1 & 0.1\\\\\n1 & 0.2\\\\\n1 & 0.3\\\\\n\\vdots & \\vdots\\\\\n1 & 4.9\\\\\n1 & 5.0\\\\\n\\end{array}\\right]\\ .\\] The linear combination which we might previous have called \\(3\\cdot \\vec{\\mathtt{t}} - 2\\,\\vec{\\mathtt{identity(t)}}\\) can be thought of as\n\\[\\left[\\overbrace{\\begin{array}{r}\n1\\\\\n1 \\\\\n1 \\\\\n1 \\\\\n\\vdots &\\\\\n1 \\\\\n1\n\\end{array}}^{3 \\times}\n\\stackrel{\\begin{array}{r}\n\\\\\n\\\\\n\\\\\n\\\\\n\\\\\n\\\\\n\\\\\n\\\\\n\\end{array}}{\\Large + \\ }\n\\overbrace{\\begin{array}{r}\n0\\\\\n0.1 \\\\\n0.2 \\\\\n0.3 \\\\\n\\vdots\\\\\n4.9 \\\\\n5.0\n\\end{array}}^{-2 \\times}\\right] = \\left[\\begin{array}{r}\n\\\\ \\\\ 3\\\\\n2.8\\\\2.6\\\\2.4\\\\\\vdots\\\\-6.8\\\\-7.0\\\\ \\\\ \\\\\n\\end{array}\\right]\\ ,\\] but this is not conventional notation. Instead, we would write this more concisely as \\[\\stackrel{\\Large\\mathit{M}}{\\left[\\begin{array}{rr}1 & 0\\\\\n1 & 0.1\\\\\n1 & 0.2\\\\\n1 & 0.3\\\\\n\\vdots & \\vdots\\\\\n1 & 4.9\\\\\n1 & 5.0\\\\\n\\end{array}\\right]} \\\n\\stackrel{\\Large\\vec{w}}{\\left[\\begin{array}{r}2\\\\-3\\end{array}\\right]}\\]\nIn symbolic form, the linear combination of the columns of \\(\\mathit{M}\\) using respectively the scalars in \\(\\vec{w}\\) is simply \\(\\mathit{M} \\, \\vec{w}\\). The construction of such linear combinations is called matrix multiplication.\nNaturally, the operation only makes sense if there are as many components to \\(\\vec{w}\\) as columns in \\(\\mathit{M}\\).\n\n“Matrix multiplication” might better have been called “\\(\\mathit{M}\\) linearly combined by \\(\\vec{w}\\).” Nevertheless, “matrix multiplication” is the standard term for such linear combinations.\n\n\nIn R, make vectors with the rbind() command, short for “bind rows,” as in\n\nrbind(2, 5, -3)\n##      [,1]\n## [1,]    2\n## [2,]    5\n## [3,]   -3\n\nNote that the vector components appear as successive arguments to the rbind() function.\nCollect multiple vectors into a matrix with the cbind() command, short for “bind columns.” The arguments to cbind() will typically be vectors created by rbind(). For instance, the matrix \\[\\mathit{A} \\equiv \\left[\\vec{u}\\ \\ \\vec{v}\\right]\\ \\ \\text{where}\\ \\ \\vec{u} \\equiv \\left[\\begin{array}{r}2\\\\5\\\\-3\\end{array}\\right]\\ \\ \\text{and}\\ \\ \\vec{v} \\equiv \\left[\\begin{array}{r}1\\\\-4\\\\0\\end{array}\\right]\\] can be constructed in R with these commands.\n\nu &lt;- rbind(2, 5, -3)\nv &lt;- rbind(1, -4, 0)\nA &lt;- cbind(u, v)\nA\n##      [,1] [,2]\n## [1,]    2    1\n## [2,]    5   -4\n## [3,]   -3    0\n\nTo compute the linear combination \\(3 \\vec{u} + 1 \\vec{v}\\), that is, \\(\\mathit{A} \\cdot \\left[\\begin{array}{r}3\\\\1\\end{array}\\right]\\) you use the matrix multiplication operator %*%. For instance, the following defines a vector \\[\\vec{x} \\equiv \\left[\\begin{array}{r}3\\\\1\\end{array}\\right]\\] to do the job in a way that is easy to read:\n\nx &lt;- rbind(3, 1)\nA %*% x\n##      [,1]\n## [1,]    7\n## [2,]   11\n## [3,]   -9\n\n\n\nIt is a mistake to use * instead of %*% for matrix multiplication. Remember that * is for componentwise multiplication, which is different from matrix multiplication. Componentwise multiplication with vectors and matrices will usually give an error message as with:\n\nA * x\n## Error in A * x: non-conformable arrays\n\nThe phrase “non-conformable arrays” is R-speak for “I do not know how to do componentwise multiplication with two incompatibly shaped objects.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Linear combinations of vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/B5-linear-combinations.html#sub-spaces",
    "href": "Linear-combinations/B5-linear-combinations.html#sub-spaces",
    "title": "30  Linear combinations of vectors",
    "section": "30.6 Sub-spaces",
    "text": "30.6 Sub-spaces\nPreviously, we have said that a vector with \\(n\\) components is “embedded” in an \\(n\\)-dimensional space. Think of an embedding space as a kind of club with restricted membership. For instance, a vector with two elements is properly a member of the 2-dimensional club, but a vector with more or fewer than two elements cannot have a place in the two-dimensional club. Similarly, there are clubs for 3-component vectors, 4-component vectors, and so on.\nThe clubhouse itself is a kind of space, the space in which any and all of the vectors that are eligible for membership can be embedded.\nNow imagine the clubhouse arranged into meeting rooms. Each meeting room is just part of the clubhouse space. Which part? That depends on a set of vectors who sponsor the meeting. For instance, in the ten-dimensional clubhouse, a few members, let’s say \\(\\color{blue}{\\vec{u}}\\) and \\(\\color{magenta}{\\vec{v}}\\) decide to sponsor a meeting. That meeting room, part of the whole clubhouse space, is called a subspace.\nA subspace has its own rules for admission. Vectors belong to the subspace only if they are a linear combination of the sponsoring members. The sponsoring members define the subspace, but the subspace itself consists of an infinity of vectors: all possible vectors that amount to a linear combination of the sponsors.\nAs an example, consider the clubhouse that is open to any and all vectors with three components. The diagram in Figure 30.5 shows the clubhouse with just two members present, \\(\\color{blue}{\\vec{u}}\\) and \\(\\color{magenta}{\\vec{v}}\\).\nAny vector can individually sponsor a subspace. In Figure 30.5 the subspace sponsored by \\(\\color{blue}{\\vec{u}}\\) is the extended line through \\(\\color{blue}{\\vec{u}}\\), that is, all the possible scaled versions of \\(\\color{blue}{\\vec{u}}\\). Similarly, the subspace sponsored by \\(\\color{magenta}{\\vec{v}}\\) is the extended line through \\(\\color{magenta}{\\vec{v}}\\). Each of these subspaces is one-dimensional.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 30.5: Two vectors \\(\\color{blue}{\\vec{u}}\\) and \\(\\color{magenta}{\\vec{v}}\\) embedded in 3-dimensional space. The subspace spanned by an individual vector is shown as a line.\n\n\n\n\n\nMultiple vectors can sponsor a subspace. The subspace sponsored by both \\(\\color{blue}{\\vec{u}}\\) and \\(\\color{magenta}{\\vec{v}}\\) contains all the vectors that can be constructed as linear combinations of \\(\\color{blue}{\\vec{u}}\\) and \\(\\color{magenta}{\\vec{v}}\\); the gray subspace in Figure 30.6.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 30.6: Two vectors \\(\\vec{u}\\) and \\(\\vec{w}\\). The subspace spanned by two vectors is a plane, shown here as a gray surface.\n\n\n\n\n\nOn the other hand, the subspace sponsored by \\(\\color{magenta}{\\vec{v}}\\) and \\(\\color{blue}{\\vec{u}}\\) is not the entire clubhouse. \\(\\color{magenta}{\\vec{v}}\\) and \\(\\color{blue}{\\vec{u}}\\) lie in a common plane, but not all the vectors in the 3-dimensional clubhouse lied in that plane. In fact, if you rotate Figure 30.6 to “look down the barrel” of either \\(\\color{magenta}{\\vec{v}}\\) or \\(\\color{blue}{\\vec{u}}\\), the plane will entirely disappear from view. A subspace is an infinitesimal slice of the embedding space.\n“Sponsored a subspace” is metaphorical. In technical language, we speak of the subspace spanned by a set of vectors in the same embedding space. Usually, we refer to a “set of vectors” as a matrix. For instance, letting \\[\\mathit{M} \\equiv \\left[{\\Large \\strut}\\color{blue}{\\vec{u}}\\ \\ \\color{magenta}{\\vec{v}}\\right]\\ ,\\] the gray plane in Figure 30.6 is the subspace spanned by \\(\\mathit{M}\\) or, more concisely, \\(span(\\mathit{M})\\).\nFor a more concrete, everyday representation of the subspace spanned by two vectors, a worthwhile experiment is to pick up two pencils pointing in different directions. Place the eraser ends together, pinched between thumb and forefinger. Point the whole rigid assembly in any desired direction; the angle between them will remain the same.\nPlace a card on top of the pencils, slipping it between pressed fingers to hold it tightly in place. The card is another kind of geometrical object: a planar surface. The orientation of two vectors together determines the orientation of the surface. This simple fact will be significant later on.\nOne can replace the pencils with line segments drawn on the card underneath each pencil. Now, the angle is readily measurable in two dimensions. The angle between two vectors in three dimensions is the same as the angle drawn on the two-dimension surface that rests on the vectors.\nNotice that one can also lay a card along a single vector. What is different here is that the card can be rolled around the pencil while still staying in contact; there are many different orientations for such a card even while the vector stays fixed. So a single fixed vector does not uniquely determine the orientation of the planar surface in which the vector can reside. It takes two vectors to determine a unique planar surface.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Linear combinations of vectors</span>"
    ]
  },
  {
    "objectID": "Linear-combinations/B5-linear-combinations.html#exercises",
    "href": "Linear-combinations/B5-linear-combinations.html#exercises",
    "title": "30  Linear combinations of vectors",
    "section": "30.7 Exercises",
    "text": "30.7 Exercises\nProblem with NA NA",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Linear combinations of vectors</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MOSAIC Calculus",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#welcome-to-calculus",
    "href": "index.html#welcome-to-calculus",
    "title": "MOSAIC Calculus",
    "section": "Welcome to calculus",
    "text": "Welcome to calculus\nCalculus is the set of concepts and techniques that form the mathematical basis for dealing with motion, growth, decay, and oscillation. The phenomena can be as simple as a ball arcing ballistically through the air or as complex as turbulent airflow over a wing generating lift. Calculus is used in biology and business, chemistry, physics and engineering. It is the foundation for weather prediction and understanding climate change. It is the basis for the algorithms for heart rate and blood oxygen measurement by wristwatches. It is a key part of the language of science. The electron orbitals of chemistry, the stresses of bones and beams, and the business cycle of recession and rebound are all understood primarily through calculus.\nCalculus has been central to science from the very beginnings. It is no coincidence that the scientific method was introduced and the language of calculus was invented by the same small group of people during the historical period known as the Enlightenment in the late 17th century. Learning calculus has always been a badge of honor and an entry ticket to professions. Millions of students’ career ambitions have been enhanced by passing a calculus course or thwarted by lack of access to one.\nIn the 1880s, a hit musical featured “the very model of a modern major general.” One of his claims for modernity: “I’m very good at integral and differential calculus.”\n\n\n\n\nhttps://www.youtube.com/embed/Rs3dPaz9nAo\n\nWhat was modern in 1880 is not modern anymore. Yet, amazingly, calculus today is every bit as central to science and technology as it ever. Indeed, calculus remains central to fields that were not even imagined in 1880, such as logistics, economics, and data science. One reason is that science, engineering, and society have now fully adopted the computer for almost all aspects of work, study, and life. The collection and use of data is growing dramatically. Machine learning has become the way human decision makers interact with such data.\nThink about what it means to become “computerized.” To take an everyday example, consider video. Over the span of a human life, we moved from a system which involved people going to theaters to watch the shadows recorded on cellulose film to the distribution over the airwaves by low-resolution television, to the introduction of high-def broadcast video, to on demand streaming from huge libraries of movies. Just about anyone can record, edit, and distribute their own video. The range of topics (including calculus) on which you can access a video tutorial or demonstration is incredibly vast. All of this recent progress is owed to computers.\nThe “stuff” on which computers operate, transform, and transmit is always mathematical representations stored as bits. The creation of mathematical representations of objects and events in the real world is essential to every task of any sort that any computer performs. Calculus is a key component of inventing and using such representations.\nYou may be scratching your head. If calculus is so important, why is it that many of your friends who took calculus came away wondering what it is for? What’s so important about “slopes” and “areas” and how come your high-school teacher might have had trouble telling you what calculus is for?\nThe disconnect between the enthusiasm expressed in the preceding paragraphs and the lived experience of students is very real. There are two major reasons for that disconnect, both of which we tackle head-on in this book.\nFirst, teachers of mathematics have a deep respect for tradition. Such respect has its merits, but the result is that almost all calculus is taught using methods that were appropriate for the era of paper and pencil—not for the computer era. As you will see, in this book we express the concepts of calculus in a way that carries directly over to the uses of calculus on computers and in genuine work.\nSecond, the uses of calculus are enabled not by the topics of Calc I and Calc II alone, but the courses for which Calc I/II are preliminary: linear algebra and dynamics. Only a small fraction of students who start in Calc I ever reach the parts of calculus that are the most useful. Fortunately, there is a large amount of bloat and rote in the standard textbook topics of Calc I/II. This can be removed to make room for the more important topics, as we try to do in this book.\nThe computer language used in this book is R. This is a mainstream language in high demand by employers in many field. The small amount of R that you need to learn for this book will open doors to much greater possibilities. We have augmented R with a widely used package, {mosaic}, that simplifies access to calculus-related operations. Accordingly, we often refer to the software as “R/mosaic.”\nFor convenience, we placed “Active R Chunks” within the text. Often, as in Active R chunk 1, these will be presented with working R/mosaic commands already included. Just press “Run code” to have the commands evaluated.\n\n\n\nActive R chunk 1\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nSometimes, you will be asked to make modifications to the commands in a chunk so that you can see for yourself out the output changes.\nDepending on her preferences, your instructor may provide a different way of accessing R for homeworks, etc. It’s all the same language, whereever you run it!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#instructors-preface",
    "href": "index.html#instructors-preface",
    "title": "MOSAIC Calculus",
    "section": "Instructor’s preface",
    "text": "Instructor’s preface\nThe “MOSAIC” in the book’s title refers to a movement in undergraduate mathematics to integrate Modeling, Statistics and data science, Computing, and Calculus. Skill in all these areas is needed for successful work in the technical and scientific world. Traditionally, teaching does not honor the strong links among these areas and ignores the advantages of teaching them in a unified way. Indeed, modeling, though often mentioned in calculus textbook blurbs, is hardly taught at all. Introductory statistics courses, even those with a formal pre-requisite of calculus, do not draw on calculus concepts beyond the mention of “area under a curve.” Few and far between are introductory computing courses that reinforce calculus and statistics topics, or calculus courses that develop and build on computing and data skills.\nThe isolation of mathematical calculus from disciplines that ought to be considered allies leads to a devastating gap in the education of students. The half-life of a student in a calculus sequence is, roughly speaking, one course. (The decay in participation starts in algebra and trigonometry.) The result is that only a small fraction of students see relevant mathematics of multiple variables and hardly any encounter the powerful concepts of spaces, vectors, and matrix factorizations traditionally reserved for a junior-year linear algebra course. Contemporary statistical modeling techniques, including machine learning, draw heavily on a small core of linear-algebra topics.\nThe design of the mosaic serves the goal of providing for all students access to a broad, directly useful education in the mathematical sciences. One component of access is keeping the program small enough to fit in with a plausible student schedule. Our working definition of “small enough” is one-quarter of the first two years of university. Fitting within this envelope, students ought to achieve basic competence in computing, statistics, and calculus.\nCalculus provides opportunities to prepare students well for both modern statistics and computing. The most direct ties to statistics are modeling, functions of multiple variables, and concepts of linear spaces. The relevant links of calculus to computing are extensive and go both ways. To give but one example: It’s impractical for students to construct contour plots without a computer. With a computer at hand, students can learn about mathematical ideas such as gradients and iterative optimization. For instance, constrained optimization is hardly a topic of standard introductory calculus and Lagrange multipliers are mysterious even to many professors. The essential concepts become much clearer when they can be presented using graphical tools of contours and gradient fields.\nA book that attempts to build on the connections among historically distinct disciplines must also resolve inconsistencies in nomenclature and notation. We draw the instructor’s attention to some of these and the policies adopted in the book:\n\nVariable. In mathematics “variable” is used with many different reasons, in computing “variable” is used colloquially to mean “the name of an object,” and in statistics a “variable” refers to data: a column of a data frame or, more generally, a specific attribute of the units of observation that form the rows of the data frame. (See 7.1 Data frames for definitions of “data frame” and “unit of observation.”) We reserve “variable” to be used in the statistical sense. Consequently, a “function of several variables” becomes a “function with several inputs.”\nOutput. Evaluating a function, either a mathematically or on a computer, produces an output. Functions take inputs and produce outputs. Typical names for inputs are, following tradition, the last few letters of the alphabet: \\(t, u, v, w, x, y, z\\).\nFunction names. Functions always have a name. \\(f()\\), \\(g()\\), \\(h()\\) are the pronouns for discussing functions in general, but in specific applications functions often have more descriptive names, e.g. population() or elev() or risk(). The empty parentheses are a reminder that the thing being named is a function and not an input or parameter. Since \\(y\\) is used as an input name, we never use it for the name of a function or to identify the output of a function. So, \\(y=mx+b\\) is not an esteemed phrase in this book. Instead, when we want to define a straight-line function we write \\(g(x) \\equiv a x + b\\) or some other parameterization, for instance \\(g(x) \\equiv a (x - x_0)\\).\nSpecial inputs. Often, a problem or application context requires the identification of some special values for inputs to a function, for instance, argmaxes or zero crossings or starting time. These are often constructed by using an output name (often \\(t\\) through \\(z\\)) with a subscript or a non-numerical superscript as in \\(y^\\star\\).\nOutput (part 2). When we mean something like “the output of the function \\(f()\\) at its argmax,” we write \\(f(x_\\star)\\), or something similar. When we mean, “the output of function \\(f()\\) at some as yet unspecified input,” we write, naturally enough, \\(f(x)\\).\nFormulas. An expression like \\(ax + b\\) is a formula. One of the most common ways to define a function is by using a formula. But creating a function from a formula requires some special syntax, as demonstrated earlier with \\(g(x) \\equiv a x + b\\). The names used within the parentheses on the left side of \\(\\equiv\\) are the input names. Other symbols in the formula are called parameters.\nTilde expression. We use the R language in this book. Those familiar with R know that there is a special kind of expression called a “formula,” for instance a*x + b ~ x. One of the main uses for R formulas is to represent a mathematical formula when creating a function. “Formulas representing formulas” can lead to confusion. We address this by violating the technical vocabulary of R and calling an expression like a*x + b ~ x a “tilde expression.” This name properly draws attention to the ~ (“tilde”) character that is an essential component to R-language formulas. A typical use for a tilde expression is to create a computer version of a function. The computer version of \\(g(x) \\equiv a x + b\\) is g &lt;- makeFun(a*x + b ~ x). In this use, the ~ x part of the tilde expression identifies the input name, just as does the \\(x\\) in \\(g(x) \\equiv ...\\). You’ll also use tilde expressions for graphics and operations such as differentiation and anti-differentiation.\n\nThose familiar with R may be tempted to use the native function-building syntax, which looks like\n\ng &lt;- function(x, a, b) {\n  a*x + b\n}\n\nWe strongly encourage you to use makeFun() instead. One reason is to reinforce the use of tilde expressions which are needed to identify the “with-respect-to” input in differentiation and anti-differentiation, as well as the frame of a graph. Another reason has to do with rules of scoping in computer languages, which have no obvious analog in mathematical notation. We prefer to leave scoping to a computer science class, rather than making it a pre-requisite for calculus. makeFun() avoids the scoping difficulties.\n\nDifferential notation. Historically, Leibniz’s lovely ratio notation, for instance, \\[\\frac{dy}{dx}\\ ,\\] helped generations of students learn differential calculus and see the connections to integral calculus. It is, however, wordy, which is why other notations—\\(f'\\) or \\(\\dot{x}\\) or \\(f^{(1)}\\) so often appear. But Leibniz could hardly have anticipated a future in which writing is done mainly with keyboards and linear sequences of characters. There is no mainstream computer language in which df/dx or f' or \\dot{x} or f^{(1)} are valid names. To simplify the use of the computer, we use \\(\\partial_x y\\) notation for differentiation. This can be easily morphed into a legal computer name: dx_y. We use \\(\\partial\\) instead of the Latin \\(d\\), partly to mark differentiation as something special and partly because we will use notation like \\(\\partial_{xt} g\\) when dealing with functions of multiple variables, or \\(\\partial_{xx} f\\) for second derivatives.\n\nThe book is designed to support six to ten credit hours of calculus study. The algebra pre-requisites are kept to a minimum, trigonometry beyond sines and cosines is not needed. In the starting “Preliminaries” part of the book nine “pattern-book” algebraic functions are introduced that form the basis for modeling work. “Preliminaries” also introduces computing notation, particularly that used for graphing functions.\n“Modeling,” Block I, introduces topics that are essential to the rest of the book and is worth spending considerable class time on regardless of the previous experience of students. Block II, “Differentiation,” is self-explanatory to a calculus instructor. Absence of extensive drill on symbolic differentiation of obscure functions or the use of Taylor polynomials or l’Hopital’s rule to provide even more drill is entirely intentional. Much more important that students master differentiation of the nine pattern-book functions and their parameterized version.\nBlock III, “Vectors and linear combinations,” does not depend on previously covering differentiation. The sequence Preliminaries-Modeling-Vectors could make a suitable 3- or 4-credit course for students entering data science. Block III might have been reasonably titled “Linear algebra.” But the universal emphasis on determinants and inverses of square matrices in a conventional linear algebra course is not a suitable introduction for working with data, and we did not want to suggest that all the conventional topics of linear algebra are included.\nBlock IV, “Accumulation,” builds on Block II, where differentiation is treated less as an algebraic process than as a relationship between functions. Our focus is on occasions when anti-differentiation is a useful modeling tool for extracting certain forms of information from a function. Instructors are advised to minimize or wholly avoid the area-under-a-curve metaphor. Like cigarettes, that metaphor is addictive and creates dis-ease with the more important roles for accumulation in contexts like dynamics. The final chapter of Block IV is about symbolic integration of functions constructed from from the pattern-book. Including it is a concession to administrative practices at universities where topics like “integration by parts” are included in hard-to-change course-catalog copy. Those techniques are not used elsewhere in the book.\nBlock V, “Dynamics,” introduces systems, that is, wholes made of multiple connected parts. Although the context used is differential equations, techniques for finding solutions are not central. More important are the phenomena (e.g. oscillation), the opportunities for modeling and showing how simple mathematical models can provide insight to otherwise seemingly complex natural and social systems, extension of the linear-algebra material from Block III to eigenvalues and eigenvectors, and a glimpse at the surprisingly close connection between exponentials and sinusoids.\nThe last block, “Manifestations,” is arranged along different lines than the previous chapters. The point is to show how calculus operations show themselves in a wide variety of contexts. We are not making up opportunities for more drill in symbolic differentiation and anti-differentiation. In the “Probability” chapter, to give an example, what’s central is the relationships between functions. One of these relationships is standard in calculus books, that between what’s called the CDF and the PDF. Less basic and not at all standard, the relationship between prior, likelihood, and posterior functions. It is not required to cover all the chapters in “Manifestations.” They do not much depend one on the other. Choose the ones that are best suited to the directions in which your students are heading. And the topics need not be delayed to the end of the course. For instance, the constrained optimization topic in “Optimization” can be handled with the material up to Block IV.\nMany highly expert calculus instructors have taught with these materials. Some of their experiences may be relevant to instructors who haven’t yet used MOSAIC Calculus. First, the experts are surprised by how many esteemed, traditional calculus topics are given short shrift or even omitted altogether, and even more surprised that their exclusion does not diminish the course. Second, many experts find that the calculus in this book is not calculus as they have been trained to think about it, but that nonetheless “it works.” Third, instructors who try to avoid spending class time on the computational elements of the book find that their students echo the avoidance. As these instructors go through their first year, they discover that there are only a handful of computational patterns (e.g. tilde expressions, domains) and that students would have avoided many headaches by facing them head-on from the start of the course. Some advice: Don’t think that you have to learn R before you can master using R to teach this book. The mosaic software that powers this book is a better place to start than with the many more general introductions and tutorials on R computing available in printed and video form.\nMost universally, even the expert instructors find they are unfamiliar with tranches of the material. Leading examples are dimensions of measurement, splines, mechanics (e.g. torque), the uses of orthogonalization, and, broadly, dynamics. Teaching unfamiliar material is admittedly stressful, but highly beneficial to yourself and your students. Dimensions and units, in particular, are a great guide to thinking; take every opportunity to ask your class what are the dimensions of the inputs to and outputs from a function and whether an operation makes sense in terms of dimensions. Consistently, the experts find that thinking about physical dimension gives them unexpected insight into the tasks and methods of calculus.\n\nDaniel Kaplan, Saint Paul, Minnesota, August 2024",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledements",
    "href": "index.html#acknowledements",
    "title": "MOSAIC Calculus",
    "section": "Acknowledements",
    "text": "Acknowledements\nThis project was initiated by the Mathematical Sciences department at the US Air Force Academy. They recognized that a traditional calculus introduction is ill-suited to the needs of STEM in the 21st century.\nCritical support was given by the ARDI Foundation which awarded the Holland H. Coors Chair in Education Technology to one of the project members, Daniel Kaplan. This made possible a year-long residency at USAFA during which time he was able to work unhindered on this project.\nMacalester College, where Kaplan is DeWitt Wallace Professor of Mathematics, Statistics, and Computer science, was the site where the overall framework and many of the materials for a STEM-oriented calculus were developed. Particularly important in the germination were David Bressoud and Jan Serie, respectively chairs of the Macalester math and biology departments, as well as Prof. Thomas Halverson and Prof. Karen Saxe, who volunteered to team teach with Kaplan the first prototype course. Early grant support from the Howard Hughes Medical Foundation and the Keck Foundation provided the resources to carry the prototype course to a point of development where it became the entryway to calculus for Macalester students.\nProfs. Randall Pruim (Calvin University) and Nicholas Horton (Amherst College) were essential collaborators in developing software to support calculus in R. They and Kaplan formed the core team of Project MOSAIC, which was supported by the US National Science Foundation (NSF DUE-0920350).\nJoel Kilty and Alex McAllister at Centre College admired the Macalester course and devoted much work and ingenuity to write a textbook, Mathematical Modeling and Applied Calculus (Oxford Univ. Press), implementing their own version. Their textbook enabled us to reduce the use of sketchy notes in the first offering of this course at USAFA.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "linear-combinations-part.html",
    "href": "linear-combinations-part.html",
    "title": "BLOCK III. Vectors and linear combinations",
    "section": "",
    "text": "Often, quantities are made out of multiple components. For example, the location of an object in space has \\(x\\), \\(y\\), and \\(z\\) components. An important mathematical strategy for working with multiple components relates to the ideas of a vector and combinations of vectors as well as a set of vectors involved in a combination. (The set of vectors is called a matrix.)\nVectors appear naturally in physics: position, velocity, acceleration. They are also a principal building block of algorithms for machine learning, data science, and statistical modeling.\nThis Block introduces the basics of vectors and operations on vectors. There is a broad mathematical subject called “linear algebra” of which vectors and matrices are a part. Here, we focus on a compact set of ideas that are of particular importance in modeling and statistics.",
    "crumbs": [
      "BLOCK III. Vectors and linear combinations"
    ]
  },
  {
    "objectID": "accumulation-part.html",
    "href": "accumulation-part.html",
    "title": "BLOCK IV. Accumulation",
    "section": "",
    "text": "We have already studied the relationship between functions and their derivatives. In this Block, we will examine techniques that exploit that relationship to re-construct a function from knowledge of its derivative by a process called anti-differentiation.\nJust as derivatives tell about rates of change, anti-derivatives tell about the accumulation of change.",
    "crumbs": [
      "BLOCK IV. Accumulation"
    ]
  },
  {
    "objectID": "manifestations-part.html",
    "href": "manifestations-part.html",
    "title": "BLOCK VI. Manifestations",
    "section": "",
    "text": "The ideas of calculus are used throughout science and technology. Previous blocks have introduced the mathematical ideas themselves, often illustrated with examples of real-world systems. In this Block, we will explore some of the ways calculus concepts are manifested in different fields and for different uses. Of course, each reader may find some fields more interesting than others. Think of this Block as a sampler containing a handful of the myriad ways that calculus concepts are used.",
    "crumbs": [
      "BLOCK VI. Manifestations"
    ]
  }
]