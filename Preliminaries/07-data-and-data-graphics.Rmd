# Data and data graphics {#sec-data-and-data-graphics}

```{r include=FALSE}
source("../starter.R")
```

## Moved from graphics chapter



::: {.callout-note icon=false data-latex=""}
## Math in the world

TRANSLATING DATA TO A FUNCTION. data are generally scattered, while a function has just a single output value for any input. But we often use functions as a simplification to data.

Nerve cells communicate via electrical voltages and currents. For long-distance communications (distances longer than about 1 mm) the signaling takes the form of pulses of voltage occurring repetitively at different rates. The formation of these pulses, called "action potentials," was the subject of an extensive research project in the 1950s involving inserting tiny electrodes into relatively large ("giant") nerve cells that mediate the fleeing reaction of squid. A typical experiment involved regulating artificially the voltage across the cell membrane. Through these experiments, the scientists--- John Eccles, Alan Hodgkin, and Andrew Huxley---were able to describe mathematically the relationship between membrane voltage and the current across the membrane. A calculus model built from the relationships provided a concise description of the biophysics of action potentials. A 1963 Nobel Prize was awarded for this work.

```{r echo=FALSE}
#| label: fig-fhn-model
#| fig.cap: "Data (simulated) from the squid giant axon experiments. A smooth curve is drawn through the data points."
#| column: margin

set.seed(101)
n <- 30
FHN_points <- tibble(
  v = sample(c(-2, -1.5, -1, -.5, 0, .5, 1, 1.5, 2), n, replace=TRUE),
  w = v - v^3/3
) %>% 
  mutate(v=(1.9+v+rnorm(n, 0, 0.01))/36,
         w=w+rnorm(n, 0, 0.1))

gf_point(w ~ I(-v*36*25), data = FHN_points) %>%
  gf_labs(y="Current (mA)", x = "Voltage (mV)") %>%
  slice_plot( -(v/25 + 2) + (v/25 + 2)^3/3 ~ v, 
              domain(v=-100:5)) 
```
In each experimental run, the membrane voltage was fixed at a given level (say, -50 mV) and the current measured. @fig-fhn-model shows what the data might have looked like, each point showing the results of one experimental run.

COME BACK TO THIS IN "MODELING CYCLE" CHAPTER: CAN WE GET A FUNCTION THAT GOES BETTER THROUGH THE DATA POINTS?

The data points themselves might be described metaphorically as "clouds" spotting the voltage vs current "sky." Real-world clouds often show patterns such as animal or country shapes. We might say that a given cloud resembles a rabbit. Similarly, the data clouds show a pattern between current and voltage. We might, for instance, describe the current-voltage relationship as S-shaped. Or, rather than using the letter "S" we could draw a curve through the dots to summarize and simplify the relationship.
:::

The smooth curve in  @fig-fhn-model describes the relationship between current and voltage quantitatively. For example, if you know that the current is 0, you can use the curve to figure out what the voltage will be around -90 mV or  -50 mV or -10 mV. But when the current is 0, the voltage will *not* be, say, -75 or -150.

Graphs such as  @fig-carbon-phases and @fig-fhn-model are good ways of showing relationships. We can even do calculations simply using such graphs. Place your finger on a point of the S-shaped graph and you can read from the axes an allowed pair of voltage and current values. Place your finger on a point on the vertical axis. Moving it to the curve will reveal what current is associated with that voltage.


## Put this in another chapter

AND REFER TO THE GRAPHIC HERE.

On the other hand, functions cannot represent all types of relationships. For instance, the curve in @fig-fhn-model shows a relationship between current and voltage in nerve cells. But there is no mathematical function voltage(current) that does justice to the relationship. The reason is that mathematical functions can have ***one and only one*** output for any given input. There are three reasonable values for membrane voltage that are experimentally consistent with a zero current, not just one.


## Start here

The decade of the 1660s was hugely significant in the emergence of science, although no one realized it at the time. 1665 was a plague year, the last major outbreak of bubonic plague in England. The University of Cambridge closed to wait out the plague. 

Isaac Newton, then a 24-year old Cambridge student, returned home to Woolsthorpe where he lived and worked in isolation for two years. Biographer James Gleich wrote: "The plague year was his transfiguration. Solitary and almost incommunicado, he became the worldâ€™s paramount mathematician." During his years of isolation, Newton developed what we now call "calculus" and, associated with that, his theory of Universal Gravitation. He wrote a tract on his work in 1669, but withheld it from publication until 1711.

Newton (1642-1726) was elected to the prestigious Royal Society at an early age. There, he may well have met another member of the Society, John Graunt(1620-1674). Graunt was well known for collecting data on causes of death in London. @fig-bill-of-mortality shows his data for one week in 1665.

::: {#fig-bill-of-mortality}
![](../Special_topics/www/graunt-bill-1665.png)

Tables from the August 15-22, 1665 Bill of Mortality for London. [source](https://www.lindahall.org/about/news/scientist-of-the-day/john-graunt/) Notice the large number of deaths from plague. A major purpose for the Bill of Mortality was to track the outbreak of the epidemic over time, so that people could decide whether to flee London for the countryside, or whether it was safe to come back.
:::

Neither Newton nor Graunt could have anticipated the situation today. Masses of data are collected and analyzed. Mathematical functions are essential to the analysis of complex data. To give one example: Artificial intelligence products such as large-language models (e.g. Chat-GPT) are based on the construction of mathematical functions (called "neural nets") that take large numbers of inputs and process them into an output. The construction methods are rooted in optimization, one of the topics in Newton's first calculus book.

Since data is so important, and since Calculus underlies data analysis techniques, it makes sense that a course on Calculus should engage data analysis as a core topic. This book emphasizes data-related aspects of Calculus although traditional books, (even in the 21st century) mainly follow an old, data-free curriculum.

This chapter introduces some basic techniques for working with data. This will enable you to apply important data-oriented methods of Calculus that we will encounter in later chapters.

## Data frames {#sec-data-frames}

The organization of data as a *table* (as in @fig-bill-of-mortality) is almost intuitive. However, data science today draws on much more sophisticated structures that are amenable to computer processing. 

Data science is strikingly modern. Relational databases, the prime organization of data used in science, commerce, medicine, and government was invented in the 1960s. All data scientists have to master working with relational databases, but we will use only one component of them, the **data frame**.

Let's consider what Graunt's 1665 data might look like in a modern data frame. Remember that the data in @fig-bill-of-mortality covers only one week (Aug 15-22, 1665) in only one place (London). A more comprehensive data frame might include data from other weeks and other places:

::: {#tbl-bill-data-frame}

condition | deaths | begins | stops | location
----------|--------|------|----------
kingsevil | 10     | 1665-08-15 | 1665-08-22 | London
lethargy  | 1      | 1665-08-15 | 1665-08-22 | London
palsie    | 2      | 1665-08-15 | 1665-08-22 | London
plague    | 3880   | 1665-08-15 | 1665-08-22 | London
*spotted feaver* | *190* | *1665-07-12* | *1665-07-19* | *Paris* 
*consumption* | *219*  | *1665-07-12* | *1665-07-19* | *Paris* 
*cancer* | *5*  | *1665-07-12* | *1665-07-19* | *Paris* 

A data frame organization of some data from @fig-bill-of-mortality. Additional imagined "data" (in *italics*) has been added to illustrate why so many columns are needed, rather than Graunt's two-column layout.
:::

As you can see, the data frame is organized into columns and rows. Each column is called a **variable** and contains entries that are all the same kind of thing. For example, the *location* variable has city names. The *deaths* variable has numbers.

Each row of the table corresponds to a unique kind of thing called a *unit of observation.* It's not essential that you understand exactly what this means. It suffices to say that the unit of observation is a "condition of death during a time interval in a place." Various everyday words are used for a single row: *instance*, *case*, *specimen* (my favorite), *tupple*, or just plane *row*. In a data frame, all the rows must be the same kind of unit of observation.

The modern conception of data makes a clear distinction between data and the construction of summaries of that data for human consumption. Such summaries might be graphical, or in the form of model functions, or even in the form of a set of tables, such as seen in the Bill of Mortality. Learning how to generate such summaries is an essential task in ***statistics*** and data science. The automatic construction of model functions (without much human intervention) is a field called **machine learning**, one kind of "artificial intelligence."

A data scientist would know how to process (or, "*wrangle*") such data, for instance to use the *begins* and *stops* variables to calculate the duration of the interval covered. She would also be able to "join" the data table to others that contain information such as the population of the city or the mean temperature during the interval.

Technology allows us to store very massive data frames along with allied data. For example, a modern "bill of mortality" might have as a unit of observation the death of an individual person, including date, age, sex, occupation, and so on. Graunt's bill of mortality encompasses 5319 deaths. Given that the population of the world in the 1660s was about 550 million, a globally comprehensive data frame on deaths covering only one year would have about 20 million rows. (Even today, there is no such globally comprehensive data, and in many countries births and deaths are not uniformly recorded.)

A modern data wrangler would have no problem with 20 million rows, and would easily be able to pull out the data Graunt needed for his Aug. 15-22, 1665 report, summarizing it by the different causes of death and even breaking it down by age group. Such virtuosity is not needed for our purposes.

The basics that you need for our work with data are:

- Data frames are accessed by a file name.
- Individual columns are accessed by a variable name.
- We will use tilde expressions to identify one variable as the "**response**" variable and other variables as "**explanatory variables**." The response corresponds to the *output* of a function, the explanatory variables are the *inputs*.
- We will use software to construct functions that capture important patterns in the data, but that is a topic for later chapters.

## Accessing data tables


For our work, you can access the data frames we need directly in R by name. For instance, the `Engines` data frame (@tbl-engine-table) records the characteristics of several internal combustion engines of various sizes:

::: {#tbl-engine-table.column-page-right}
Engine | mass | BHP | RPM | bore | stroke
:------|-----:|----:|----:|-----:|---------:
Webra Speed 20 | 0.25 | 0.78 | 22000 | 16.5 | 16
Enya 60-4C | 0.61 | 0.84 | 11800 | 24.0 | 22
Honda 450 | 34.00 | 43.00 | 8500 | 70.0 | 58
Jacobs R-775 | 229.00 | 225.00 | 2000 | 133.0 | 127
Daimler-Benz 609 | 1400.00 | 2450.00 | 2800 | 165.0 | 180
Daimler-Benz 613 | 1960.00 | 3120.00 | 2700 | 162.0 | 180
Nordberg | 5260.00 | 3000.00 | 400 | 356.0 | 407
Cooper-Bessemer V-250 | 13500.00 | 7250.00 | 330 | 457.0 | 508


Various attributes of internal combustion engines, from the very small to the very large. `Engines` has 39 rows; only 8 are seen here.
:::



The fundamental questions to ask first about any data frame are:

i. What constitutes a row? 
ii. What are the variables and what do they stand for?

The answers to these questions, for the data frames we will be using, are available via R documentation. To bring up the documentation for `Engines`, for instance, give the command:

```r
?Engines
```

When working with data, it is common to forget for a moment what are the variables, how they are spelled, and what sort of values each variable takes on. Two useful commands for reminding yourself are (illustrated here with `Engines`):

```{r}
names(Engines) # the names of the variables
head(Engines) # the first several rows
nrow(Engines) # how many rows
```

In RStudio, the command `View(Engines)` is useful for showing a complete table of data in printed format. This may be useful for our work in this book, but is only viable for data frames of moderate size.

## Plotting data

We will use just one graphical format for displaying data: the ***point plot***. In a point plot, also known as a "scatterplot," two variables are displayed, one on each graphical axis. Each case is presented as a dot, whose horizontal and vertical coordinates are the values of the variables for that case. For instance:

::: {#lst-stroke-vs-bore}
```{webr-r}
Engines |> gf_point(stroke ~ bore, data = Engines)
```

Running the code will create a point plot showing the relationship between engine `stroke` and `bore`. Each individual point is one row of the data frame
:::

The data plotted by the code in @lst-stroke-vs-bore show a relationship between the stroke length of a piston and the diameter of the cylinder in which the piston moves. This relationships, however, is not being presented in the form of a function, that is, a single stroke value for each value of the bore diameter. 

For many modeling purposes, it is important to be able to represent a relationship as a function. At one level, this is straightforward: draw a smooth curve through the data and use that curve for the function. 

Later in *MOSAIC Calculus*, we will discuss ways to construct functions that are a good match to data using the pattern-book functions. Here, our concern is graphing such functions on top of a point plot. So, without explanation (until later chapters), we will construct a power-law function, called, stroke(bore), that might be a good match to the data. The we will add a second layer to the point-plot graphic: a slice-plot of the function we've constructed.

::: {#lst-first-two-layers}
```{webr-r}
stroke <- fitModel(stroke ~ A*bore^b, data = Engines)
gf_point(stroke ~ bore, data = Engines) |>
  slice_plot(stroke(bore) ~ bore, color="blue")
```

Code to make a graphic composed of two layers: 1) a point plot; 2) a slice plot of a power-law function named `stroke()` fitted to the data i 

The second layer is made with an ordinary `slice_plot()` command. To place it on top of the point plot we connect the two commands with a bit of punctuation called a "pipe": `|>`.

[The pipe punctuation can never go at the start of a line. Usually, we will use the pipe at the very end of a line; think of the pipe as connecting one line to the next.]{.aside}

`slice_plot()` is a bit clever when it is used after a previous graphics command. Usually, you need to specify the interval of the domain over which you want to display the function, as with 

```{r echo=FALSE, results="hide"}
slice_plot(stroke(bore) ~ bore, bounds(bore=0:1000))
```

You can do that also when `slice_plot()` is the second layer in a graphics command. But `slice_plot()` can also infer the interval of the domain from previous layers.

YOU WERE HERE

## Functions as data {#sec-functions-as-data}

In the previous chapters, we've used ***formulas*** to define functions. The link between functions and formulas is important, but not at all essential to the idea of functions. 

Arguably more important in practice to the representation of functions are ***tables*** and ***algorithms***. The computations behind the calculation of the output of functions such as $\sin()$ or $e^x$ or other foundational functions that we introduced in Chapter @sec-pattern-book-functions relies on computer software that loops and iterates and which is invisible to almost everybody who uses it. Before the advent of modern computing, functions were presented as printed tables. For instance, the logarithm function, invented about 1600, relied almost complete on printed tables, such as the one shown in @fig-log-table.

```{r echo=FALSE}
#| label: fig-log-table
#| fig-cap: "Part of the first table of logarithms, published by Henry Briggs in 1624."
#| fig-cap-location: margin
knitr::include_graphics("www/Briggs-starttable.png")
```

In Chapter @sec-pattern-book-functions we introduced a small set of pattern-book functions. Each of the functions is indeed a pattern that could be written down once and for all in tabular form. Generating such tables originally required the work of human "computers" who undertook extensive and elaborate arithmetical calculations by hand. What's considered the first programmable engine, a mechanical device designed by Charles Babbage (1791-1871) and programmed by Ada Lovelace (1815-1852), was conceived for the specific purpose of generating printed tables of functions. 

It is helpful to think of functions, generally, as a sort of data storage and retrieval device that uses the input value to locate the corresponding output and return that output to the user. Any device capable of this, such as a table or graph with a human interpreter, is a suitable way of implementing a function.

To reinforce this idea, we ask you to imagine a long corridor with a sequence of offices, each identified by a room number. The input to the function is the room number. To ***evaluate*** the function for that input, you knock on the appropriate door and, in response, you will receive a piece of paper with a number to take away with you. That number is the output of the function. 

This will sound at first too simple to be true, but ... In a mathematical function each office gives out the same number every time someone knocks on the door. Obviously, being a worker in such an office is highly tedious and requires no special skill. Every time someone knocks on the worker's door, he or she writes down the *same* number on a piece of paper and hands it to the person knocking. What that person will do with the number is of absolutely no concern to the office worker.

The utility of such functions depends on the artistry and insight of the person who creates them: the ***modeler***. An important point of this course is to teach you some of that artistry. Hopefully you will learn through that artistry to translate your insight to the creation of functions that are useful in your own work. But even if you just use functions created by others, knowing how functions are built will be helpful in using them properly.

In the sort of function just described, all the offices were along a single corridor. Such functions are said to have ***one input***, or, equivalently, to be "functions of one variable." To operate the function, you just need one number: the address of the office from which you will collect the output.

Many functions have more than one input: two, three, four, ... tens, hundreds, thousands, millions, .... In this course, we will work mainly with functions of two inputs, but the skills you develop will be applicable to functions of more than two inputs.

What does a function of two inputs look like in our office analogy? Imagine that the office building has many parallel corridors, each with a numeric ID. To evaluate the function, you need two numeric inputs: the number of the corridor and the number of the door along that corridor. With those two numbers in hand, you locate the appropriate door, knock on it and receive the output number in return. 

Three inputs? Think of a building with many floors, each floor having many parallel corridors, each corridor having many offices in sequence. Now you need three numbers to identify a particular office: floor, corridor, and door.

Four inputs? A street with many three-input functions along it. Five inputs? A city with many parallel four-input streets. And on and on.

Applying inputs to a function to receive an output is only a small part of most calculations. Calculations are usually organized as ***algorithms***, which is just to say that algorithms are descriptions of a calculation. The calculation itself is ... a function!

How does the calculation work? Think of it as a business. People come to your business with one or more inputs. You take the inputs and, following a carefully designed protocol, hand them out to your staff, perhaps duplicating some or doing some simple arithmetic with them to create a new number. Thus equipped with the relevant numbers, each member of staff goes off to evaluate a particular function with those numbers. (That is, the staff member goes to the appropriate street, building, floor, corridor, and door, returning with the number provided at that office.) The staff re-assembles at your roadside stand, you do some sorting out of the numbers they have returned with, again following a strict protocol. Perhaps you combine the new numbers with the ones you were originally given as inputs. In any event, you send your staff out with their new instructions---each person's instructions consist simply of a set of inputs which they head out to evaluate and return to you. At some point, perhaps after many such cycles, perhaps after just one, you are able to combine the numbers that you've assembled into a single result: a number that you return to the person who came to your business in the first place.

A calculation might involve just one function evaluation, or involve a chain of them that sends workers buzzing around the city and visiting other businesses that in turn activate their own staff who add to the urban tumult.

::: {.takenote data-latex=""}

The reader familiar with floors and corridors and office doors may note that the addresses are ***discrete***. That is, office 321 has offices 320 and 322 as neighbors. Calculus is about functions with a ***continuous domain***. But it is easy to create a continuous function out of a discrete table by adding on a small, standard calculation.  

It works like this: for an input of, say, 321.487... the messenger goes to both office 321 and 322 and collects their respective outputs. Let's imagine that they are -14.3 and 12.5 respectively. All that is  needed is a small calculation, which in this case will look like $$-14.3 \times (1 - 0.487...)   + 12.5 \times 0.487...$$ This is called ***linear interpolation*** and lets us construct continuous functions out of discrete data. There are other types of interpolation have have desirable properties, like "smoothness," which we will learn about later.

It is very common in science to work with continuous domains by breaking them up into discrete pieces. As you will see, an important strategy in calculus to to make such discrete pieces very close together, so that they resemble a continuum. 

:::


A table like REFERENCE TO THE TABLE PREVIOUS describes the general relationships between engine attributes. For instance, we might want to understand the relationship (if any) between RPM and engine mass, or relate the diameter (that is, "bore") and depth (that is, "stroke") of the cylinders to the power generated by the engine. Any single entry in the table does not tell us about such general relationships; we need to consider the rows and columns as a whole. 

If you examined the relationship between engine power (`BHP`) and bore, stroke, and RPM, you will find that (as a rule) the larger the bore and stroke, the more powerful the engine. That is a ***qualitative*** description of the relationship. Most educated people are able to understand such a qualitative description. Even if they don't know exactly what "power" means, they have some rough conception of it.

Often, we are interested in having a ***quantitative*** description of a relationship such as the one (bore, stroke) $\rightarrow$ power. Remarkably, many otherwise well-educated people are uncomfortable with the idea of using quantitative descriptions of a relationship: what sort of language the description should be written with; how to perform the calculations to use the description; how to translate between data (such as in the table) and a quantitative description; how to translate the quantitative description to address a particular question or make a decision.

::: {.callout-note icon=false data-latex=""} 
## Math in the World: Pre-computer computing

In today's world, software is the means by which expert knowledge and capability is communicated and applied. Before modern computers were available, the expertise was committed to print in the form of tables. @fig-pierce-pnorm-table shows one function table from *A Short Table of Integrals* published in 1899. [The tabulated function is called the "error function," amusingly written $\\text{Erf}(x)$. It is essentially the same as the gaussian, the relationship being $\\text{Erf}(x) + 1 = 2\\pnorm(x)$ for $0\\leq x < \\infty$.]{.aside} Incredibly, such tables were a standard feature of statistics textbooks up through 2010. 


```{r echo=FALSE}
#| label: fig-pierce-pnorm-table
#| fig-cap: "Before the computer software era, some functions could only be presented using printed tables. This table supports calculations with a gaussian-like function for inputs from 0 to 1."
knitr::include_graphics("www/pierce-pnorm-table.png")
```


In addition to software being more compact and easier to use than printed tables, the interface to numerical integrals can be presented in the same format as any other mathematical function. That has enabled us to include $\pnorm()$ among the pattern book functions. 
:::

## Drill


`r Znotes:::MC_counter$reset(labels="numbers")`

```{r child="Exercises/Drill-preliminaries-07.Rmd"}
```



## Exercises

`r insert_exercises("Preliminaries", "data graphics")`


