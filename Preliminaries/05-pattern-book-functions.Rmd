# Pattern-book functions {#sec-pattern-book-functions}

```{r include=FALSE}
source("../starter.R")
```

One of the major tasks in modeling is to construct functions that represent a situation or phenomenon of interest. As you will see in later chapters, there are many ways to build a function. For example, you may have data about a relationship. Or, sometimes, you can figure out the shape of a function by asking simple questions about the situation or phenomenon.

In this Chapter, we introduce the ***pattern-book functions***---a brief list of basic mathematical functions that apply in a surprisingly large range of situations. Think of the items in the pattern-book list as different actors, each of whom is skilled in portraying an archetypal character: hero, outlaw, lover, fool, comic. A play brings together different characters, costumes them, and relates them to one another through dialog or other means. 


::: {#fig-pattern-book .column-margin}
![](www/theatrical-pattern-book.jpg)

A pattern book of theatrical costumes
:::

Costume designers use their imagination, often enhanced by referencing published collections of patterns and  customizing them to the needs at hand. These references are sometimes called "pattern books." (See @fig-pattern-book.) 


Similarly, we will start with a pattern set of functions that have been collected from generations of experience. To remind us of their role in modeling, we will call these ***pattern-book functions***. These pattern-book functions are useful in describing diverse aspects of the real world and have simple calculus-related properties that make them relatively easy to deal with. There are just a handful of pattern-book functions from which untold numbers of useful modeling functions can be constructed. Mastering calculus is in part a matter of becoming familiar with the mathematical connections among the pattern-book functions, just as understanding a play calls for becoming familiear with the relationships among the characters.

Here is a list of our pattern-book functions written traditionally and in R:

[The list of pattern-book functions is short. You should memorize the names and be able easily to associate each name with the traditional notation.]{.aside}

```{r child="pattern-book-list.Rmd"}
```
     
The input name used in the table, $x$, is entirely arbitrary. You can (and will) use the pattern-book functions with other quantities---$y$, $z$, $t$, and so on, even zebra if you like.

In addition to the names of the pattern-book functions, you should be able to draw their shapes easily. @fig-pattern-book-shapes provides a handy guide.

```{r echo=FALSE, eval=FALSE}
# helper function used to generate the pattern-book png files
# run this chunk interactively
make_shape <- function(f, min, max, file_name = NULL,
                       ylim = as.numeric(c(NA, NA))) {
  Pts <- tibble(
    x = setdiff(seq(min, max, length=300),0), # kill zero
    y = f(x)
  ) # %>% mutate(ifelse(abs(x) > 0.05, y, NaN))
  # plot(Pts$x, Pts$y, type="l", xaxt="n", yaxt="n", 
  #   xlab="", ylab="", bty="n", lwd=12)
  P <- ggplot(Pts, aes(x=x, y=y, group=x>0)) + 
    geom_hline(yintercept=0, color = "blue", alpha = 0.5, linewidth=2) + 
    geom_vline(xintercept=0, color = "blue", alpha = 0.5, linewidth=2) + 
    geom_line(linewidth=3, arrow=NULL) + 
    ylim(ylim) +
    theme_void()
  if (!is.null(file_name)) {
    ggsave(file_name, plot = P, device="png", path = "www")
  } 
  return(P)
}

make_shape(function(x) 1, -5, 5, "pb-const-gg.png",
           ylim = c(-.5, 2)) 
make_shape(function(x) x, -5, 5, "pb-proportional-gg.png")
make_shape(function(x) x^2, -3, 3, "pb-square-gg.png",
           ylim=c(-2, 9))
make_shape(function(x) 1/x, -3, 3, "pb-recip-gg.png",
           ylim=c(-10,10))
make_shape(function(x) dnorm(x), -4, 4, "pb-gauss-gg.png",
           ylim=c(-0.3, .7))
make_shape(function(x) pnorm(x), -4, 4, "pb-sigmoid-gg.png",
           ylim=c(-0.5, 1.5))
make_shape(function(x) sin(x), -7, 7, "pb-sin-gg.png")
make_shape(function(x) exp(x), -3, 3, "pb-exp-gg.png",
           ylim=c(-2,9))
make_shape(function(x) log(x), -3, 6.2, "pb-log-gg.png",
           ylim = c(NA, 3))
```


::: {#fig-pattern-book-shapes .column-page-inset-right}
const | prop | square | recip | gaussian | sigmoid | sinusoid | exp | log
:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:
![](www/pb-const-gg.png) | ![](www/pb-proportional-gg.png) | ![](www/pb-square-gg.png)| ![](www/pb-recip-gg.png)| ![](www/pb-gauss-gg.png) | ![](www/pb-sigmoid-gg.png) | ![](www/pb-sin-gg.png) | ![](www/pb-exp-gg.png) | ![](www/pb-log-gg.png) 

Shapes of the pattern-book functions. Blue axis lines mark the location of zero.
:::


These pattern-book functions are widely applicable. But nobody would confuse the pictures in a pattern book with costumes that are ready for wear. Each pattern must be tailored to fit the actor and customized to fit the theme, setting, and action of the story. We will study how this is done starting in Chapter @sec-parameters.

Each of the pattern-book functions takes a *number* as input and produces a *number* as output.  Naturally, a proper graph of a pattern-book function should display specific numerical values for input and output. This section shows such quantitative graphs.

When used in modeling, however, we need to modify the functions so that their inputs and outputs are *quantities*. Chapters [-@sec-parameters] and [-@sec-assembling] present the main techniques for doing this. This allows us to use the *shape* of a pattern-book function while customizing the quantitative input-output relationship to serve the situation at hand. 
In practice, however, it suffices to memorize just a couple of input/output pairs for each function



```{r eval=FALSE, echo=FALSE, warning=FALSE}
my_arrow <- grid::arrow(angle = 30, 
                        length = grid::unit(0.15, "inches"), 
                        ends = "last", type = "open")
dom <- bounds(x=-1.25:1.25)
Pa <- slice_plot(x ~ x, dom) %>%
  gf_labs(subtitle="Proportional function", x="input") %>%
  gf_hline(yintercept=~0, color="magenta", linetype="dashed") %>%
  gf_segment(1.2 + 1.25 ~  1.2 + 1.25, arrow=my_arrow) %>%
  gf_segment(-1.2+ -1.25 ~  -1.2 + -1.25, arrow=my_arrow)
Pb <- slice_plot(1 ~ x, dom) %>%
  gf_labs(subtitle="'Constant function", x="input") %>%
  gf_lims(y=c(-1.25, 1.25)) %>%
  gf_hline(yintercept=~0, color="magenta", linetype="dashed")
dom <- bounds(x=-3:3)
P1 <- slice_plot(pmax(-100, pmin(x^-1, 100)) ~ x, dom, npts=751, singularities=0) %>%
  gf_labs(subtitle="Reciprocal function") %>%
  gf_lims(y=c(-100, 100)) %>%
  gf_hline(yintercept=~0, color="magenta", linetype="dashed") %>%
  gf_segment(95 + 100 ~  .0105 + .01, arrow=my_arrow) %>%
  gf_segment(-95 + -100 ~  -.0105 + -.01, arrow=my_arrow)
Psquare <- slice_plot(x^2 ~ x, dom) %>% gf_labs(subtitle="Square function", x="input") %>%
  slice_plot(-x^2 ~ x, alpha=0) %>%
  gf_hline(yintercept=~0, color="magenta", linetype="dashed") %>%
  gf_segment(8.7 + 9 ~  2.95 + 3, arrow=my_arrow) %>%
  gf_segment(8.7 + 9 ~  -2.95 + -3, arrow=my_arrow)
```

::: {#fig-constant-pb .column-margin}
![](www/pb-const-gg.png){width="70%"}

The output of the constant function is always 1, regardless of the input.

:::

The ***constant function*** is so simple that you may be inclined to deny that it is a function at all. 
The output of the constant function is always numerical 1, regardless of the input. Its graph is therefore a horizontal line. 

You may wonder why to take the trouble to make a function whose output is always the same. After all, in a formula it shows up simply as the number 1, not looking like a formula at all. But when we start to combine functions in @sec-assembling the constant function will almost always have a role to play.

::: {#fig-proportional-pb .column-margin}
![](www/pb-proportional-gg.png){width="70%"}

The output of the proportional function is simply the input value.
:::

The ***proportional function*** is also simple. 
Whatever is the input will become the output. (Another appropriate name for the proportional function is the "identity function" because the output is identical to the input.) The graph of the proportional function is a straight line going through $(0,0)$ with slope 1.

The remaining pattern-book functions all have curved shapes.

::: {#fig-gauss-pb .column-margin .lightbox}
![](www/pb-gauss-gg.png){width="70%"}

The "bell-shaped" Gaussian function.
:::

The **Gaussian function** is shaped like a mountain or, in many descriptions, like the outline of a bell. As the input becomes larger---in either the positive or negative directions---the output gets closer and closer to zero but never touches zero exactly.

The Gaussian function shows up so often in modeling that it has another widely used name: the ***normal function***. But "normal" has additional meanings in mathematics, so we will not use that name in this book. 

::: {#fig-sigmoid-pb .column-margin}
![](www/pb-sigmoid-gg.png){width="70%"}

The output of the sigmoid function increases smoothly from zero to one as the input increases.
:::.


The ***sigmoid function*** models smooth transitions. For large negative inputs, the output is (very nearly) zero. For large positive inputs, the output is (very nearly) one. The sigmoid is closely related to the Gaussian. As you progress in calculus, the relationship will become evident.

::: {#fig-sinusoid-pb .column-margin}
![](www/pb-sin-gg.png){width="70%"}

The output of the sinusoid function oscillates as the input increases.
:::

A **sinusoid* is a ***periodic function***, meaning that the pattern repeats itself identically from left to right in the graph. 

If you studied trigonometry, you may be used to the sine of an angle in the context of a right triangle. That is the historical origin of the idea. For our purposes, think of the sinusoid just as a function that takes an input and returns an output. Starting in the early 1800s, oscillatory functions found a huge area of application in representing phenomena such as the diffusion of heat through solid materials and, eventually, radio communications. 

In trigonometry, the sine has the cosine as a partner. But the two functions $\sin()$ and $\cos()$ are so closely connected that we will not often need to make the distinction, calling both of them simply "sinusoids."


## Exponential and logarithm functions

It's a common English phrase to say that something is "growing exponentially." For instance, at the outbreak of an epidemic there are few cases and slow growth in the number of cases. But as the number of cases increases, the growth gets faster.

::: {#fig-exponential-pb .column-margin}
![](www/pb-exp-gg.png){width="70%"}

The output of the exponential function grows faster and faster as the input increases.
:::

The ***exponential function*** has important applications throughout science, technology, and the economy. For large negative inputs, the value is very close to zero in much the same way as for the Gaussian or sigmoid functions. But the output increases faster and faster as the input gets bigger. Note that the output of the exponential function is never negative for any input.

There is another way to think about exponential growth that makes it easier to understand. Again, imagine an epidemic in its eary stages. Suppose there are 20 cases. Time goes by and eventually the number of cases reaches 40, double the initial value. Suppose it takes one week for this doubling. That is, the **doubling time** is one week.

Now a second week goes by. If the growth were *proportional* to the input, after the second week the case number would be 60. For exponential growth, by the end of the second week the output will double. So, there will be 80 cases after two weeks. 

At the end of the third week the output of the exponential will double again, to 160 cases. Fourth week: 320 cases. Fifth week: 640 cases. To summarize, @tbl-doubling-epi shows the number of cases as a function of weeks elapsed.

::: {#tbl-doubling-epi .column-margin}

Time (weeks) | Number of cases
-------------------|-------------
0 (initial time) | 20
1   | 40
2   | 80
3   | 160
4   | 320
5   | 640
6   | 1280
... | ...
10  | 20,480
... | ...
15 weeks  | 655,360

The number of cases as it increases over time for an epidemic that grows exponentially.
:::

Note: *You might speculate about where the epidemic will go in the long run. Most things that grow exponentially do so only for a short time. After that, other things come into play that limit or reverse the growth. So, over long periods of time the pattern can be better described as **gaussian** or **sigmoidal**.*

I suspect that most people are comfortable with the idea of exponential growth. But many people, even highly-educated professionals, react with fear to the word "logarithmic." [This fear might originate in the way logarithms are typically taught in high-school is heavily algebraic and taken out of any meaningful contemporary context. Logarithms were invented around 1600 to facilitate by-hand mathematical operations like multiplication and exponentiation. Nowadays, we have computers to perform such calculations.] As we'll see in @sec-magnitude, logarithms are an important tool for understanding quantities that can range from very, very small to very, very large. @tbl-doubling-epi gives an example: the left column is a logarithm of the right column.

::: {#fig-logarithm-pb .column-margin}
![](www/pb-exp-gg.png){width="70%"}

The domain of the logarithm is the positive numbers. 
:::


The ***logarithmic function*** is defined only for positive inputs. As the input increase from just above zero, the output t constantly grows but at a slower and slower rate. It never levels out.

The exponential and logarithmic functions are intimate companions. You can see the relationship by taking the graph of the logarithm, and rotating it 90 degrees, then flipping left for right as in @fig-log-flipped. (Note in @fig-log-flipped that the graph is shown as if it were printed on a transparency which we are looking at from the back.)


```{r echo=FALSE}
#| label: fig-log-flipped
#| column: margin
#| fig-cap: "The exponential function is the same as the logarithm but---and this is a big but---reversing the roles of the input and output. Visually, that reversal of roles amounts to flipping the graph." 
knitr::include_graphics("www/log-flipped.png")
```

::: {.callout-note icon=FALSE data-latex=""}
## Why is it called the "logarithm?"

The name "logarithm" is anything but descriptive. The name was coined by the inventor, John Napier (1550-1617), to emphasize the original purpose of his invention: to simplify the work of multiplication and exponentiation. The name comes from the Greek words *logos*, meaning "reasoning" or "reckoning," and *arithmos*, meaning "number." A catchy marketing term for the new invention, at least for those who speak Greek!

Although invented for the practical work of numerical calculation, the logarithm function has become central to mathematical theory as well as modern disciplines such as thermodynamics and information theory. The logarithm is key to the measurement of information and magnitude. As you know, there are units of information used particularly to describe the information storage capacity of computers: bits, bytes, megabytes, gigabytes, and so on. Very much in the way that there are different units for length (cm, meter, kilometer, inch, mile, ...), there are different units for information and magnitude. For almost everything that is measured, we speak of the "units" of measurement. For logarithms, instead of "units," by tradition another word is used: the ***base*** of the logarithm. The most common units outside of theoretical mathematics are base-2 ("***bit***") and base-10 ("***decade***"). But the unit that is most convenient in mathematical notation is "base e," where $e = 2.71828182845905...$. This is genuinely a good choice for the units of the logarithm, but that is  hardly obvious to anyone encountering it for the first time. To make the choice more palatable, it is marketed as the "base of the natural logarithm." In this book, we will be using this ***natural logarithm*** as our official pattern-book logarithm.
:::



## The power-law family (optional) {#sec-power-law-family}

Four of the pattern-book functions---$1$, $1/x$, $x$, $x^2$--- belong to an infinite family called the ***power-law functions***. Some other examples of power-law functions are $x^3, x^4, \ldots$ as well as $x^{1/2}$ (also written $\sqrt{x}$), $x^{1.36}$, and so on. Some of these also have special (albeit less frequently used) names, but *all* of the power-law functions can be written as $x^p$, where $x$ is the input and $p$ is a number. 

You have been using power-law functions from early in your math and science education.  Some examples:^[The animal lifespan relationship is true when comparing *species*. Individual-to-individual variation within a species does not follow this pattern.]   

::: {#tbl-power-law-examples .column-page-right}

Setting | Function formula | exponent
-------------|------------------|----------
Circumference of a circle | $C(r) = 2 \pi r$ | 1
Area of a circle | $A(r) = \pi r^2$ | 2
Volume of a sphere | $V(r) = \frac{4}{3} \pi r^3$ | 3
Distance traveled by a falling object | $d(t) = \frac{1}{2} g t^2$ | 2
Gas pressure versus volume | $P(V) = \frac{n R T}{V}$ | $-1$
... perhaps less familiar ... | | 
Distance traveled by a diffusing gas | $X(t) = D \sqrt{
\strut t}$ | $1/2$
Animal lifespan (in the wild) versus body mass | $L(M) = a M^{0.25}$ | 0.25
Blood flow versus body mass | $F(M) = b M^{0.75}$ | 0.75

Examples of power-law relationships
:::

One reason why power-law functions are so important in science has to do with the logic of physical quantities such as length, mass, time, area, volume, force, power, and so on. We introduced this topic in @sec-quantity-function-space and will return to it in more detail in @sec-dimensions-and-units.

Within the power-law family, it is helpful to know and be able to distinguish between several groups:

1. The ***monomials***. These are power-law functions such as $m_0(x) \equiv x^0$, $m_1(x) \equiv x^1$, $m_2(x) \equiv x^2$, $\ldots$, $m_p(x) \equiv x^p$, $\ldots$, where $p$ is a whole number (i.e., a non-negative integer). Of course, $m_0()$ is the same as the constant function, since $x^0 = 1$. Likewise, $m_1(x)$ is the same as the identity function since $x^1 = x$. As for the rest, they have just two general shapes: both arms up for even powers of $p$ (like in $x^2$, a parabola); one arm up and the other down for odd powers of $p$ (like in $x^3$, a cubic). Indeed, you can see in @fig-monomial-graphss that $x^4$ has a similar shape to $x^2$ and that $x^5$ is similar in shape to $x^3$. For this reason, high-order monomials are rarely needed in practice.

::: {#fig-monomial-graphs }
```{r echo=FALSE, warning=FALSE}
#| layout-ncol: 2
#| fig-subcap:
#| - $x^0$, that is, 1
#| - $x^1$, that is, x
#| - $x^2$
#| - $x^3$
#| - $x^4$
#| - $x^5$
my_arrow <- grid::arrow(angle = 30, 
                        length = grid::unit(0.15, "inches"), 
                        ends = "last", type = "open")
dom <- domain(x=c(-1.25, 1.25))
P0 <- slice_plot(1 ~ x, dom) %>% gf_labs(subtitle=expression(m[0](x) %==% x^0), x="input") %>%
  gf_lims(y=c(-1, 2)) %>%
  gf_hline(yintercept=~0, color="magenta", linetype="dashed")

P1 <-slice_plot(x ~ x, dom) %>% gf_labs(subtitle=expression(m[1](x) %==% x^1), x="input") %>%
  gf_lims(y=c(-2,2)) %>%
  gf_hline(yintercept=~0, color="magenta", linetype="dashed")
P2 <- slice_plot(x^2 ~ x, dom) %>% gf_labs(subtitle=expression(m[2](x) %==% x^2), x="input") %>%
  slice_plot(-x^2 ~ x, alpha=0) %>%
  gf_hline(yintercept=~0, color="magenta", linetype="dashed")
P3 <- slice_plot(x^3 ~ x, dom) %>% gf_labs(subtitle=expression(m[3](x) %==% x^3), x="input") %>%
  slice_plot(-x^2 ~ x, alpha=0) %>%
  gf_hline(yintercept=~0, color="magenta", linetype="dashed")
P4 <- slice_plot(x^4 ~ x, dom) %>% gf_labs(subtitle=expression(m[4](x) %==% x^4), x="input") %>%
  slice_plot(-x^2 ~ x, alpha=0) %>%
  gf_lims(y=c(-2.5,2.5)) %>%
  gf_hline(yintercept=~0, color="magenta", linetype="dashed")
P5 <- slice_plot(x^5 ~ x, dom) %>% gf_labs(subtitle=expression(m[5](x) %==% x^5), x="input") %>%
  slice_plot(-x^2 ~ x, alpha=0) %>%
  gf_lims(y=c(-2.5,2.5)) %>%
  gf_hline(yintercept=~0, color="magenta", linetype="dashed")
P0; P1; P2; P3; P4; P5
```
The first six monomial functions: $x^0$, $x^1$, $x^2$, $x^3$, $x^4$, and $x^5$. In each plot the dashed $\color{magenta}{\text{magenta}}$ line marks zero output.
:::

2. The ***negative powers***. These are power-law functions where $p<0$, such as $f(x) \equiv x^{-1}$, $g(x) \equiv x^{-2}$, $h(x) \equiv x^{-1.5}$. For negative powers, the size of the output is ***inversely proportional*** to the size of the input. In other words, when the input is large (**not** close to zero) the output is small, and when the input is small (close to zero), the output is *very* large. This behavior happens because a negative exponent like $x^{-2}$ can be rewritten as $\frac{1}{x^2}$; the input is *inverted* and becomes the denominator, hence the term "inversely proportional".

::: {#fig-negative-graphs}
```{r echo=FALSE, warning=FALSE}
#| layout-ncol: 2
#| fig-subcap:
#| - $x^{-1}$, that is, $1/x$
#| - $x^{-2}$, that is, $1/x^2$
#| - $x^{-3}$, that is, $1/x^3$
#| - $x^{-4}$, that is, $1/x^4$
dom <- bounds(x=-1.5:1.5)
P1 <- slice_plot(x^-1 ~ x, dom, npts=281, singularities=0) %>%
  gf_labs(subtitle=expression(x^{-1}), y="") %>%
  gf_lims(y=c(-100, 100)) %>%
  gf_hline(yintercept=~0, color="magenta", linetype="dashed") %>%
  gf_segment(95 + 100 ~  .0105 + .01, arrow=my_arrow) %>%
  gf_segment(-95 + -100 ~  -.0105 + -.01, arrow=my_arrow)
P2 <- slice_plot(x^-2 ~ x, dom, npts=501, singularities=0) %>%
  gf_labs(subtitle=expression(x^{-2}), y="") %>%
  gf_lims(y=c(-100, 100)) %>% 
  gf_hline(yintercept=~0, color="magenta", linetype="dashed")%>%
  gf_segment(95 + 100 ~  -.1026 + -.1, arrow=my_arrow) %>%
  gf_segment(95 + 100 ~  .1026 + .1, arrow=my_arrow)
P3 <-slice_plot(x^-3 ~ x, dom, npts=351, singularities = 0) %>%
  gf_labs(subtitle=expression(x^{-3}), y="") %>%
  gf_lims(y=c(-100, 100)) %>%
  gf_hline(yintercept=~0, color="magenta", linetype="dashed") %>%
  gf_segment(95 + 100 ~  .219 + .2154, arrow=my_arrow) %>%
  gf_segment(-95 + -100 ~  -.219 + -.2154, arrow=my_arrow)
P4 <- slice_plot(x^-4 ~ x, dom, npts=351, singularities = 0) %>%
  gf_labs(subtitle=expression(x^{-4}), y="") %>%
  gf_lims(y=c(-100, 100)) %>%
  gf_hline(yintercept=~0, color="magenta", linetype="dashed") %>%
  gf_segment(95 + 100 ~  0.32 + 0.316, arrow=my_arrow) %>%
  gf_segment(95 + 100 ~  -0.32 + -0.316, arrow=my_arrow)
P1; P2; P3; P4
```

Graphs of power-law functions with negative integer exponents. The arrows point to the output being very large when $x$ is near zero.
:::

3. The **non-integer powers**, e.g. $f(x) = \sqrt{x}$, $g(x) = x^\pi$, and so on. When $p$ is either a fraction or an irrational number (like $\pi$), the real-valued power-law function $x^p$ can only take non-negative numbers as input. In other words, the domain of $x^p$ is $0$ to $\infty$ when $p$ is not an integer. You have likely already encountered this domain restriction when using the power law with $p=\frac{1}{2}$ since $f(x)\equiv x^{1/2}=\sqrt{x}$, and the square root of a negative number is not a *real* number. You may have heard about the *imaginary* numbers that allow you to take the square root of a negative number, but for the moment, you only need to understand that when working with real-valued power-law functions with non-integer exponents, the input must be non-negative. (The story is a bit more complicated since, algebraically, rational exponents like $1/3$ or $1/5$ with an odd-valued denominator can be applied to negative numbers. Computer arithmetic, however, does not recognize these exceptions.)

::: {#fig-non-integer-graphs}
```{r echo=FALSE,  warning=FALSE}
#| layout-ncol: 2
#| fig-subcap:
#| - $x^{1/2}$
#| - $x^\pi$
dom <- bounds(x=c(-1.5, 1.5))
Pa <- slice_plot(x^(1/2) ~ x, dom, npts=501) %>%
  gf_labs(title=expression(x^{1/2})) %>%
  gf_lims(y=c(-2, 2)) %>%
  gf_hline(yintercept=~0, color="magenta", linetype="dashed") %>%
  gf_segment(1.22 + 1.225 ~  1.49 + 1.5, arrow=my_arrow)
Pb <- slice_plot(x^pi ~ x, dom, npts=501) %>%
  gf_labs(title=expression(x^{pi})) %>%
  gf_lims(y=c(-4, 4)) %>%
  gf_hline(yintercept=~0, color="magenta", linetype="dashed") %>%
  gf_segment(3.5 + 3.57 ~  1.49 + 1.5, arrow=my_arrow)
Pa; Pb
  
```

The domain of power-law functions with non-integer power is $0 \leq x < \infty$.
:::

::: {.take-note}
## Note: Computing power-law functions

When a function like $\sqrt[3]{x}$ is written as $x^{1/3}$ make sure to include the exponent in grouping parentheses: `x^(1/3)`. Similarly, later in the book you will encounter power-law functions where the exponent is written as a formula. Particularly common will be power-law functions written $x^{n-1}$ or $x^{n+1}$. In translating this to computer notation, make sure to put the formula within grouping parentheses, for instance `x^(n-1)` or `x^(n+1)`.




## Domains of pattern-book functions

Each of our basic modeling functions, with two exceptions, has a domain that is the entire number line $-\infty < x < \infty$. No matter how big or small is the value of the input, the function has an output. Such functions are particularly nice since we never have to worry about the input going out of bounds.  

The two exceptions are:

1. the logarithm function, which is defined only for $0 < x$.
2. some of the power-law functions: $x^p$. 
    - When $p$ is negative, the output of the function is undefined when $x=0$. You can see why with a simple example: $g(x) \equiv x^{-2}$. Most students had it drilled into them that "division by zero is illegal," and $g(0) = \frac{1}{0} \frac{1}{0}$, a double law breaker. 
    - When $p$ is not an integer, that is $p \neq 1, 2, 3, \cdots$ the domain of the power-law function does not include negative inputs. To see why, consider the function $h(x) \equiv x^{1/3}$. 
    
::: {.takenote data-latex=""}
We will be modeling with both exponential and power-law functions in this chapter. It is essential that you recognize that these are utterly different functions.

An exponential function, for instance, $e^x$ or $2^x$ or $10^x$ has a constant quantity raised to a power set by the input to the function.

A power-law function works the reverse way: the input is raised to a constant quantity, as in $x^2$ or $x^10$.

A mnemonic phrase for exponentials functions is

> *Exponential functions have $x$ in the exponent.*

Of course, the exponential function can have inputs with names other than $x$, for instance, $f(y) \equiv 2^y$, but the name "x" makes for a nice alliteration in the mnemonic.
:::


## Symbolic manipulations (optional)

Several of the pattern book functions appear so often in *MOSAIC Calculus* that it is worth reviewing how to manipulate them symbolically. As an example, consider the function $$g(x) \equiv e^x e^x\ .$$
This is a perfectly good way of defining $g()$, but it is helpful to be able to recognize that the following definitions are exactly equivalent
$$f(x) \equiv e^{x+x}\\
h(x) \equiv e^{2 x}\ .$$
The ***symbolic manipulation*** we touch on in this chapter involves being able to recall and apply such equivalences. we will need only a small set of them, all or most of which are found in a high-school algebra course.


::: {.callout-note icon=false data-latex=""}
## Why do some functions have parentheses and other don't?

How come some function names, like $\sin()$ are written with other parentheses, while others such as $e^x$ have the input name shown?

The $x$ in the name $e^x$ is  a placeholder. A better, if longer, name would be $\exp()$, which would signal that we mean the abstract concept of the exponential function, and not that function applied to an input named $x$. 

The same is true for functions like $x$ or $1/t$ or $z^2$. If absolute consistency of notation were the prime goal, we could have written this book in a style that gives a name to every pattern-book function in the name/parentheses style. Something like this:

```{r}
reciprocal <- makeFun(1/t ~ t)
one <- makeFun(1 ~ z)
square <- makeFun(x^2 ~ x)
```

These would be used in the ordinary way, for instance:

```{r}
reciprocal(7)
one(123.67)
square(19)
```

Writing reciprocal($x$) instead of $1/x$ is long-winded, which is perhaps why you never see it. But when you see $1/x$ you should think of it as a function being applied to $x$ and not as a bit of arithmetic.

By the way ... I used different names for the inputs in these three functions just to remind the reader that, for functions with one input, the name has no significance. You just have to make sure to use the same name on the left- and right-hand sides of the tilde expression. 
:::




### Exponential and logarithm {#sec-exp-log-symb}

Basic symbolic patterns for exponentials are (i) and (ii)

::: {.column-margin}
$\LARGE\mathbf{(i)}\ \ \ \ \ e^x e^y \leftrightarrow e^{x+y}$ 

.
    
$\LARGE\mathbf{(ii)}\ \ \ \ \ \left(e^x\right)^y \leftrightarrow e^{x y}$
:::


Exponentials with a base other than $e$ can be converted to base $e$.

::: {.column-margin}
.

$\LARGE\mathbf{(iii)}\ \ \ \ \ 2^x \leftrightarrow e^{\ln(2) x} = e^{0.69315 x}$

$\LARGE\mathbf{(iv)}\  \ \  \ 10^x \leftrightarrow e^{\ln(10) x} = e^{2.30259 x}$
:::

For mental arithmetic, easier use base 2 or 10. The base $e = 2.718282$ is not conducive to such calculations. In Block 2 we will discuss why it is standard to write an exponential function as $e^x$.

The logarithms, which we will return to in Chapter `r Chaps$magnitudes` are the ***inverse*** function of the exponential: Rule (v).

::: {.column-margin}
$\LARGE\mathbf{(v).a}\ \ \ \ \ \ln(\exp(x)) = x$ 

$\LARGE\mathbf{(v).b}\ \ \ \ \ \exp(\ln(x)) = x$
:::


One place that we will encounter rules (ii) and (v) is in Chapter `r Chaps$parameters` when we look a "doubling times" and "half lives." There we will deal with expressions such as $2^{x/\tau} = e^{\ln(2) x/\tau}$. 

Important symbolic patterns for logarithms are Rules (vi) through (vii).

::: {.column-margin}
$\LARGE\mathbf{(vi)}\ \ \ln(x\ \ \!y) \leftrightarrow \ln(x) + \ln(y)$

$\LARGE\mathbf{(vii)}\ \ \! \ln(x / y) \leftrightarrow \ln(x) - \ln(y)$

$\LARGE\mathbf{(viii)}\ \ \ \ln(x^p) \leftrightarrow p \ln(x)$$
:::

::: {.takenote data-latex=""}
Notice that the symbolic patterns for logarithms involve multiplication, division, and exponentiation, but **not addition**: $\ln(x + y) \neq \ln(x) + \ln(y)$.
:::

### Power-law functions

In power-law functions, the quantity in the exponent is constant: we will call it $m$, $n$, or $p$ in the following examples.

::: {.column-margin}
$\LARGE\mathbf{(ix)}\ \ \ \ x^m x^n \leftrightarrow x^{m+n}$

$\LARGE\mathbf{(x)}\ \ \ \ \ \ \frac{x^m}{x^n} \leftrightarrow x^{m-n}$

$\LARGE\mathbf{(xi)}\ \ \ \ \ \left(x^n\right)^p \leftrightarrow x^{n\,p}$

$\LARGE\mathbf{(xii)}\ \ \ \ \ \ x^{-n} \leftrightarrow \frac{1}{x^n}$

$\LARGE\mathbf{(xiii)}\ \ \ \ \ \ \ x^0 \leftrightarrow 1$
:::

### Sinusoids

$\sin(x)$ is periodic with period $2\pi$. Zero-crossings of $\sin(x)$ are at $x=..., -2\pi, -\pi, 0, \pi, 2\pi, ...$

As we mentioned earlier, we will be calling both the $\sin()$ and $\cos()$ function "sinusoids." They are merely shifted versions of one another:
$$\cos(x) = \sin\left(x + \frac{\pi}{2}\right)\ .$$



### The straight-line function

You are probably used to a function that we call the "straight-line function" $$\line(x) \equiv m x + b\ .$$ The name comes from the shape of a graph of $\line(x)$ versus $x$, which is a straight line. You are likely used to calling the parameter $m$ the "slope" of the line, and the parameter $b$ the "intercept." (In general, by "intercept" we will mean the value of the function output when the input is zero. In high-school, this is often called the "y-intercept.")

There are two simple symbolic manipulations that you will be using often in *MOSAIC Calculus*:

1. Find the input $x=x_0$ for which the output $\line(x=x_0) = 0$. This input has many names in mathematics: the "root," the "$x$-intercept," the "zero crossing," etc. We will call any input value that corresponds to an output of zero to be "a zero of the function."

For the straight-line function, the zero is readily found symbolically: $$x_0 = - b/m\ .$$

2. Re-write the straight-line function in the form $$\line(x) = a \left(x - x_0\right)\ .$$

Here, the slope is designated with $a$. And, of course, $x_0$ is the zero of the function, as you can see by setting $x=x_0$:
$\line(x=x_0) = a (x - x_0){\LARGE\left.\right|}_{x = x_0} = 0\ .$




## Drill

`r Znotes:::MC_counter$reset(labels="numbers")`

```{r child="Exercises/Drill-preliminaries-05.Rmd"}
```


## Exercises

`r insert_exercises("Preliminaries", "pattern book functions")`

## New Concepts

`r insert_exercises("Preliminaries", "pattern book functions (conceptual)")`



